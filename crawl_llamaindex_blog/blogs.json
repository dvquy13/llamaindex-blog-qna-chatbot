[
{"title": "Automate online tasks with MultiOn and LlamaIndex", "content": "Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we'll demonstrate how MultiOn's capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let’s explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\n from  llama_index.agent.openai  import  OpenAIAgent\nopenai.api_key =  \"sk-your-key\" \n\n from  llama_index.tools.multion  import  MultionToolSpec\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\n\ngmail_tool = GmailToolSpec()\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\n    gmail_tool.to_tool_list()[ 1 ],\n    name= \"gmail_search\" ,\n    description= \"\"\"\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\n\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\n        All parameters are required\n        \n        If you need to reply to an email, ask this tool to build the reply directly\n        Examples:\n            query='from:adam subject:dinner', max_results=5, query_str='Where are adams favourite places to eat'\n            query='dentist appointment', max_results=1, query_str='When is the next dentist appointment'\n            query='to:jerry', max_results=1, query_str='summarize and then create a response email to jerrys latest email'\n            query='is:inbox', max_results=5, query_str='Summarize these emails'\n    \"\"\" \n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\n    system_prompt= \"\"\"\n\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\n\t    \n\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\n\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\n\t    \n\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\n    \"\"\" \n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex's Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\n=== Calling Function ===\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\nGot output: Open the email from Julian to view the latest communication.\n========================\n \nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\n\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\n=== Calling Function ===\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\n\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\n========================\n\nBased on the email chain with Julian, here is a summary:\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\n- Instructions for joining the meeting were provided in the email.\n- Attendees included Julian and Nassar, with Julian as the organizer.\n- The email passed SPF and DKIM checks.\n\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\n\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\n=== Calling Function ===\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\nGot output: Email response sent to Julian\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you’re interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.", "author": "MultiOn", "date": "May 23, 2024", "tags": ["automation", "Agents"]},
{"title": "Simplify your RAG application architecture with LlamaIndex + PostgresML", "content": "We’re happy to announce the recent integration of LlamaIndex with PostgresML — a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows. The problem with typical RAG workflows Typical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users. Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges: Increased dev time to master new technologies Complicated maintenance and scalability issues due to multiple points of failure Costly vendors required for multiple services The diagram above illustrates the complexity, showing how each component interacts across different services — exacerbating these problems. Solution The PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows. By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call — resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow. About PostgresML PostgresML [ github  ||  website  ||  docs ] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides. Key highlights: Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning PostgresML has a range of capabilities. In the following sections, we’ll guide you through just one use case – RAG – and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app. How it works in LlamaIndex Let’s look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham’s essays. Step 1: Get Your Database Connection String If you haven’t already,  create your PostgresML account . You’ll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Alternatively, you can pass the pgml_database_url argument when creating the index. Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed Index component: pip install llama_index llama-index-indices-managed-postgresml Then load in the data: mkdir  data\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt Finally create the PostgresML Managed Index: from  llama_index.core.readers  import  SimpleDirectoryReader\n from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name= \"llama-index-example\" \n) Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see:  https://postgresml.org/docs/api/client-sdk/pipelines  Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 . Step 3: Querying Now that we have created our index we can use it for retrieval and querying: retriever = index.as_retriever()\ndocs = retriever.retrieve( \"Was the author puzzled by the IBM 1401?\" )\n for  doc  in  docs:\n     print (doc) PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up. Using the PostgresML Index as a query_engine is just as easy: response = index.as_query_engine().query( \"Was the author puzzled by the IBM 1401?\" )\n print (response) Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming: query_engine = index.as_query_engine(streaming= True )\nresults = query_engine.query( \"Was the author puzzled by the IBM 1401?\" )\n for  text  in  results.response_gen:\n     print (text, end= \"\" , flush= True ) Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable. Key takeaways The PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend. To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro  guide  to setup your account, and the examples above with your own data.", "author": "PostgresML", "date": "May 28, 2024", "tags": ["Managed Indexes"]},
{"title": "LlamaIndex Newsletter 2024-06-04", "content": "Hello, LlamaIndex Family! 🦙 We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need. 🤩  The highlights: Elevating Knowledge Graphs:  The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities.  Docs ,  Tweet . Spreadsheet Insights with LlamaParse:  LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling.  Notebook ,  Tweet . Code Generation with Codestral:  Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages.  Docs ,  Tweet . ✨ Feature Releases and Enhancements: We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support.  Docs ,  Tweet . We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance.  Notebook ,  Tweet . We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages.  Docs ,  Tweet . We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript.  Blogpost ,  Tweet . We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex.  Docs ,  Tweet . 🗺️ Guides: Guide  to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results. Guide  to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results. Guide  to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights. 🖥️ Demos: Omakase RAG Orchestrator , a project developed by  Amir Mehr , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel. gmail-extractor , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision. ✍️ Tutorials: Sherlock Xu’s  tutorial  from BentoML on Serving A LlamaIndex RAG App as REST APIs. 📑 Papers: FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text. 📹 Webinar: Webinar  with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents 📅  Events: Join  Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.", "author": "LlamaIndex", "date": "Jun 4, 2024", "tags": []},
{"title": "Batch inference with MyMagic AI and LlamaIndex", "content": "This is a guest post from MyMagic AI. MyMagic AI  allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for  batch  inference (also known as  offline  or  delayed  inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it's integrated directly into LlamaIndex! Part 1: batch inference How It Works: 1. Setup : Organize Your Data in an AWS S3 or GCS Bucket: Create a folder using your user ID assigned to you upon registration. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. Purpose of the 'Session' Folder: This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. Granting Access to MyMagic AI: To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the  MyMagic AI documentation . 2. Install : Install both MyMagic AI’s API integration and LlamaIndex library: pip install llama-index\npip install llama-index-llms-mymagic 3. API Request:  The llamaIndex library is a wrapper around MyMagic AI’s API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details. import  asyncio\n from  llama_index.llms.mymagic  import  MyMagicAI\n\nllm = MyMagicAI(\n    api_key= \"user_...\" ,  # provided by MyMagic AI upon sign-up \n    storage_provider= \"s3\" ,\n    bucket_name= \"batch-bucket\" ,  # you may name anything \n    session= \"my-session\" ,\n    role_arn= \"arn:aws:iam::<your account id>:role/mymagic-role\" ,\n    system_prompt= \"You are an AI assistant that helps to summarize the documents without essential loss of information\" ,  # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create \n    region= \"eu-west-2\" ,\n) We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests. resp = llm.complete(\n    question= \"Summarise this in one sentence.\" ,\n    model= \"mixtral8x7\" , \n    max_tokens= 20 ,   # default is 10 \n)\n print (resp)\n async   def   main ():\n    aresp =  await  llm.acomplete(\n        question= \"Summarize this in one sentence.\" ,\n        model= \"llama7b\" ,\n        max_tokens= 20 ,\n    )\n     print (aresp)\n\nasyncio.run(main()) This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI’s backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data). Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a “batch way”. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once. Use Cases While there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing. 1. Extraction Imagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones. 2. Classification For businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning. 3. Embedding Embedding text files for further machine learning applications is another powerful use case of MyMagic AI's API. You will be ready for your vector db in a matter of days not weeks. 4. Training (Fine-tuning) Data Generation Imagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI’s API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5. 5. Transcription MyMagic AI’s API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket. Part 2: Integration with LlamaIndex’s RAG Pipeline The output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex's RAG pipeline for effective data storage and retrieval. This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled: pip install llama-index-embeddings-huggingface\ncurl -fsSL https://ollama.com/install.sh | sh\nollama pull llama3 For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json: {\n  \"id_review1\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\n  },\n  \"id_review2\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen's control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson's performance. It concludes by calling the film a great comedy to watch with friends.\"\n  },\n  \"id_review3\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell's flat. Overall, it is a terrific and well-written piece.\"\n  },\n  \"id_review4\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"Petter Mattei's 'Love in the Time of Money' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler's play on the same theme. Mattei's work is appreciated, and viewers look forward to his future projects.\"\n  },\n  \"id_review5\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes the TV show 'Oz', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn't dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\n  },\n  \"token_count\": 3391\n}\n Now let’s embed and store this document and ask questions using LlamaIndex’s query engine. Bring in our dependencies: import  os\n\n from  llama_index.embeddings.huggingface  import  HuggingFaceEmbedding\n from  llama_index.core.indices.vector_store  import  VectorStoreIndex\n from  llama_index.core.settings  import  Settings\n from  llama_index.core.readers  import  SimpleDirectoryReader\n from  llama_index.llms.ollama  import  Ollama Configure the embedding model and Llama3 model embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-base-en-v1.5\" )\nllm = Ollama(model= \"llama3\" , request_timeout= 300.0 ) Update settings for the indexing pipeline: Settings.llm = llm\nSettings.embed_model = embed_model\nSettings.chunk_size =  512   # This parameter defines the size of text chunks for embedding \n\ndocuments = SimpleDirectoryReader( \"reviews_1_5.json\" ).load_data()  #Modify path for your case Now create our index, our query engine and run a query: index = VectorStoreIndex.from_documents(documents, show_progress= True )\n\nquery_engine = index.as_query_engine(similarity_top_k= 3 )\n\nresponse = query_engine.query( \"What is the least favourite movie?\" )\n print (response) Output: Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10. Now we know that the review 1 is the least favorite movie among these reviews. Next Steps This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.  Get started with MyMagic AI’s API  today!", "author": "MyMagic AI", "date": "May 22, 2024", "tags": ["MyMagic AI", "Batch inference"]},
{"title": "LlamaIndex Newsletter 2024-05-28", "content": "Greetings, LlamaIndex Family! 🦙 Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week. 🤩  The highlights: Secure Code Execution with AzureCodeInterpreterTool:  Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution. Build Automated Email Agents:  Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails. LlamaFS for Organized Files:  Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API. RAGApp's No-Code Chatbots:  Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible. ✨ Feature Releases and Enhancements: We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you’re ready to go.  Blogpost ,  Tweet . We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance.  Docs ,  Tweet . We have integrated the Vespa vector store, supporting hybrid search with BM25.  Docs ,  Tweet . We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities.  Docs ,  Tweet . 🗺️ Guides: Guide  to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities. Guide  to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates. 🖥️ Demos: LlamaFS, a project developed by  Alex Reibman  and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency.  Code ,  Tweet . RAGApp, a project developed by  Marcus Schiesser , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API.  Code ,  Tweet . ✍️ Tutorials: Phil Chirchir’s   tutorial  on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse. Pavan Kumar’s   tutorial  on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine’s capabilities. Jayita Bhattacharyya’s  tutorial  on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit. 📹 Webinar: Webinar  with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.", "author": "LlamaIndex", "date": "May 28, 2024", "tags": []},
{"title": "LlamaIndex Newsletter 2024-06-18", "content": "Hey Llama Followers🦙 Welcome to this week’s edition of the LlamaIndex newsletter! We’re bringing you an exciting set of updates and valuable resources from Mixture-of-Agents (MoA) paper as LlamaPack to how AtomicWork’s Atom AI assistant leverages LlamaIndex to boost productivity and manage data effectively. Be sure to check out our in-depth guides, educational tutorials, and webinars for deeper insights into our tools. 🤩  The highlights: Mixture-of-Agents (MoA) LlamaPack:  We have integrated the Mixture-of-Agents (MoA) demonstrating that open-source LLMs can boost task capabilities. MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks.  LlamaPack ,  Tweet . TiDB Integration with LlamaIndex:  PingCap has now integrated their TiDB database with our LlamaIndex’s knowledge graph functionality, making it available as an open-source project.  Docs ,  Tweet . RAG and Agents Cookbook:  We have released a detailed cookbook on building RAG and Agents. This guide features enhanced observability through our LlamaIndex instrumentation module and ArizeAI.  Notebook ,  Tweet . AtomicWork’s Enterprise AI Assistant:  AtomicWork’s enterprise AI assistant, Atom, leverages LlamaIndex to handle diverse data formats, boosting productivity and improving the employee experience. Check out the details in their detailed  blog . Guide  to RAG Over Excel Files: Guide to use LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering. ✨ Feature Releases and Enhancements: We have integrated Mixture-of-Agents (MoA)  paper  from  TogetherAI  as LlamaPack from demonstrating that open-source large language models (LLMs) can enhance task capabilities. The paper shows that MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks.  LlamaPack ,  Tweet . PingCap  has integrated their TiDB database with our LlamaIndex’s knowledge graph functionality, now accessible as an open source project.  Docs ,  Tweet . We have released a detailed cookbook on building RAG and Agents, featuring supercharged observability throughout the call stack, enabled by our LlamaIndex instrumentation module and ArizeAI.  Notebook ,  Tweet . We have released the workshop slides and notebooks from our presentation on “Building an Advanced Research Agent on Databricks” at the Data AI Summit. This workshop focused on enhancing research assistants beyond the standard RAG setups.  Slide deck ,  Notebook1 ,  Notebook2 ,  Tweet . 💡 Real-World Use cases: AtomicWork’s enterprise AI assistant, Atom, utilizes LlamaIndex to handle various data formats, ensuring accurate and secure data retrieval. Atom enhances decision-making and manages unstructured data effectively, boosting productivity and improving the employee experience. Check out the details in their detailed  blog . 🗺️ Guides: Guide  to RAG Over Excel Files using LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering. Guide  to Building a Multimodal RAG Pipeline by  Pavan Belagatti  using Claude-3 and SingleStoreDB. Guide  to building fully local RAG application using MistralAI, Ollama and LlamaIndex. ✍️ Tutorials: Tomaz Bratanic ’s  tutorial  on constructing a knowledge graph, perform entity deduplication, design a custom graph retriever, and implement a question-answering flow. Mervin Praison ’s  tutorial  on creating the core components of an agent defining tools, integrating them into an agent reasoning loop, and wrapping everything with a user interface. using local models and  chainlit . Arkiti ’s  tutorial  on building a dynamic text-to-SQL solution using Llama 3 and GroqInc, highlighting the scalable and fast capabilities of SingleStoreDB Helios for multi-cloud deployments. Kingzzm ’s  tutorial  on Advanced RAG Patterns detailing effective strategies for handling documents with embedded tables, utilizing tools like LlamaParse and Nougat for enhanced QA performance. 📹 Webinar: Webinar  on The Future of Web Agents with MultiOn.  Div Garg  provided a full demo walkthrough and discuss the agentification of the internet.", "author": "LlamaIndex", "date": "Jun 18, 2024", "tags": []},
{"title": "LlamaIndex Newsletter 2024-06-11", "content": "Hello Llama Fans🦙 Step into this week's edition of the LlamaIndex newsletter, where we bring you a slew of exciting updates, in-depth guides, demos, enriching educational tutorials, and webinars designed to enhance your experience and understanding of our platforms and tools. 🤩  The highlights: Enhanced Memory Modules:  New memory modules in LlamaIndex boost agentic RAG capabilities with Vector Memory for message storage and retrieval, and Simple Composable Memory for integrating multiple memory sources.  Notebook1 ,  Notebook2 ,  Tweet . Create-llama and E2B Integration:  Launched integration turns agents into advanced data analysts, enabling Python coding for data analysis and generating detailed files like graph images.  Tweet . LlamaParse and Knowledge Graphs:   Guide  on integrating LlamaParse with Knowledge Graphs to develop RAG pipelines and agents for complex query handling. Prometheus-2 RAG Evaluation:   Guide  on using Prometheus-2, an affordable, transparent LLM based on Mistral models for effective RAG application evaluation with customized criteria. Agentic RAG :   Video tutorial  on Agentic RAG covering memory, planning, and reasoning, enhancing knowledge retrieval and agent capabilities. ✨ Feature Releases and Enhancements: We have introduced new memory modules in LlamaIndex for enhancing agentic RAG capabilities. The Vector Memory module enables storage and retrieval of user messages using vector search, while the Simple Composable Memory module allows for integration of multiple memory sources.  Notebook1 ,  Notebook2 ,  Tweet . We have launched an integration between Create-llama and E2B’s sandbox, transforming agents into powerful data analysts. This new feature allows agents to write Python code for data analysis and return comprehensive files, like graph images, enhancing the scope of what agents can accomplish.  Tweet . We have launched an integration with Nomic-Embed-Vision that transforms Nomic-Embed-Text into a multimodal embedding that excels in handling image, text, and combined tasks, outperforming OpenAI CLIP with open access for all.  Notebook . 🗺️ Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pipeline for sophisticated query retrieval, and create an agent capable of answering complex queries effectively. Guide  to Using Prometheus-2 for RAG Evaluation for assessing RAG applications, built on Mistral base models, it offers an affordable and transparent solution for evaluation, capable of direct assessments, pairwise rankings, and tailored criteria, ensuring alignment with human judgments. Guide  to Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively. 🖥️ Demos: Laurie Voss ’s  LLM-powered file organizer project  that categorizes files into folders based on LLM-decided categories without renaming them, ensuring important filenames remain intact. It organizes your files in multiple passes to balance folder sizes, resulting in descriptive yet practical folder names to help you find files easily. ✍️ Tutorials: Laurie Voss ’s  video tutorial  on transitioning from basic RAG to fully agentic knowledge retrieval, featuring real-world code examples that cover routing, memory, planning, tool use, and advanced agentic reasoning methods like Chain of Thought and Tree of Thought, along with insights into observability, controllability, and customizability. Prince krampah 's  tutorials  on Agentic RAG Systems, offering comprehensive insights into advanced system building with detailed explanations on router query engines, function calling, and multi-step reasoning across complex documents. kingzzm’s   tutorial  on Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively. Rajdeep Borgohain 's  tutorial  to build a customer-support voicebot with advanced features like speech-to-text and text-to-speech, integrated into a RAG pipeline for efficient handling of customer support exchanges using Inferless, LlamaIndex, faster-whisper, Piper, and Pinecone. Pavan Mantha 's  tutorial  on securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant. 📹 Webinar: Join us  for our webinar with  Tomaz Bratanic  from Neo4j on LlamaIndex property graph for insights into high-level and low-level graph construction, retrieval, and knowledge graph agents.", "author": "LlamaIndex", "date": "Jun 11, 2024", "tags": []},
{"title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems", "content": "We're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents. First we’ll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator import  dotenv\ndotenv.load_dotenv()  # our .env file defines OPENAI_API_KEY \n from  llama_agents  import  (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    AgentOrchestrator,\n)\n from  llama_index.core.agent  import  FunctionCallingAgentWorker\n from  llama_index.core.tools  import  FunctionTool\n from  llama_index.llms.openai  import  OpenAI\n import  logging\n\n # turn on logging so we can see the system working \nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\n\n # Set up the message queue and control plane \nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n) Next we create our tools using LlamaIndex’s existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice: # create a tool \n def   get_the_secret_fact () ->  str :\n     \"\"\"Returns the secret fact.\"\"\" \n     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n # Define an agent \nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nagent = worker.as_agent()\n\n # Create an agent service \nagent_service = AgentService(\n    agent=agent,\n    message_queue=message_queue,\n    description= \"General purpose assistant\" ,\n    service_name= \"assistant\" ,\n) Finally we launch the service and the control plane. Note that here we’re using a helper function to run a single query through the system and then exit; next we’ll show how to deploy this to production. # Set up the launcher for local testing \n from  llama_agents  import  LocalLauncher\n\nlauncher = LocalLauncher(\n    [agent_service],\n    control_plane,\n    message_queue,\n)\n\n # Run a single query through the system \nresult = launcher.launch_single( \"What's the secret fact?\" )\n print (result) Deploying Your Multi-Agent System Once you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but we’ve added a second agent service and we’re using a different launcher. Let’s bring in our dependencies and set up our control plane again: import  dotenv\ndotenv.load_dotenv()\n from  llama_agents  import  (\n    AgentService,\n    AgentOrchestrator,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n)\n\n from  llama_index.core.agent  import  FunctionCallingAgentWorker\n from  llama_index.core.tools  import  FunctionTool\n from  llama_index.llms.openai  import  OpenAI\n import  logging\n\n # change logging level to enable or disable more verbose logging \nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\n\n # create our multi-agent framework components \nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n) Then as before we create a tool and an agent, though this time we’ll add a second agent: # create a tool \n def   get_the_secret_fact () ->  str :\n     \"\"\"Returns the secret fact.\"\"\" \n     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n # create our agents \nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\nagent1 = worker1.as_agent()\nagent2 = worker2.as_agent() We turn those agents into services: agent_server_1 = AgentService(\n    agent=agent1,\n    message_queue=message_queue,\n    description= \"Useful for getting the secret fact.\" ,\n    service_name= \"secret_fact_agent\" ,\n    host= \"localhost\" ,\n    port= 8003 \n)\nagent_server_2 = AgentService(\n    agent=agent2,\n    message_queue=message_queue,\n    description= \"Useful for getting random dumb facts.\" ,\n    service_name= \"dumb_fact_agent\" ,\n    host= \"localhost\" ,\n    port= 8004 \n) And finally we launch each service as an independent agent. Here we’re doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently: from  llama_agents  import  ServerLauncher, CallableMessageConsumer\n\n # Additional human consumer \n def   handle_result ( message ) ->  None :\n     print ( f\"Got result:\" , message.data)\n\n\n # the final result is published to a \"human\" consumer \n # so we define one to handle it! \nhuman_consumer = CallableMessageConsumer(\n    handler=handle_result, message_type= \"human\" \n)\n\n # Define Launcher \nlauncher = ServerLauncher(\n    [agent_server_1, agent_server_2],\n    control_plane,\n    message_queue,\n    additional_consumers=[human_consumer]\n)\n\nlauncher.launch_servers() Real-time monitoring One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this: llama-agents monitor --control-plane-url http://127.0.0.1:8000 Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query “What is the secret fact?” You’ll get a job ID which you can then click on to see your results: Building a Query Rewriting RAG System Let's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document. This example demonstrates how to create a more sophisticated system that combines query rewriting with RAG to improve question-answering capabilities. See  this notebook  for a fuller explanation of what’s going on. import  dotenv\ndotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \n from  llama_index.core  import  VectorStoreIndex, Document\n from  llama_index.core.agent  import  FnAgentWorker\n from  llama_index.core  import  PromptTemplate\n from  llama_index.core.query_pipeline  import  QueryPipeline\n from  llama_index.core.query_engine  import  RetrieverQueryEngine\n from  llama_agents  import  (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    PipelineOrchestrator,\n    ServiceComponent,\n)\n from  llama_agents.launchers  import  LocalLauncher\n from  llama_index.llms.openai  import  OpenAI\n import  logging\n\n # change logging level to enable or disable more verbose logging \nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\n\n # Load and index your document \ndocs = [Document(text= \"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\" )]\nindex = VectorStoreIndex.from_documents(docs)\n\n # Define a query rewrite agent \nHYDE_PROMPT_STR = (\n     \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" \n)\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\n\n def   run_hyde_fn ( state ):\n    prompt_tmpl, llm, input_str = (\n        state[ \"prompt_tmpl\" ],\n        state[ \"llm\" ],\n        state[ \"__task__\" ]. input ,\n    )\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\n    output = qp.run(query_str=input_str)\n    state[ \"__output__\" ] =  str (output)\n     return  state,  True \n\nhyde_agent = FnAgentWorker(\n    fn=run_hyde_fn,\n    initial_state={ \"prompt_tmpl\" : HYDE_PROMPT_TMPL,  \"llm\" : OpenAI()}\n).as_agent()\n\n # Define a RAG agent \n def   run_rag_fn ( state ):\n    retriever, llm, input_str = (\n        state[ \"retriever\" ],\n        state[ \"llm\" ],\n        state[ \"__task__\" ]. input ,\n    )\n    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n    response = query_engine.query(input_str)\n    state[ \"__output__\" ] =  str (response)\n     return  state,  True \n\nrag_agent = FnAgentWorker(\n    fn=run_rag_fn,\n    initial_state={ \"retriever\" : index.as_retriever(),  \"llm\" : OpenAI()}\n).as_agent()\n\n # Set up the multi-agent system \nmessage_queue = SimpleMessageQueue()\n\nquery_rewrite_service = AgentService(\n    agent=hyde_agent,\n    message_queue=message_queue,\n    description= \"Query rewriting service\" ,\n    service_name= \"query_rewrite\" ,\n)\n\nrag_service = AgentService(\n    agent=rag_agent,\n    message_queue=message_queue,\n    description= \"RAG service\" ,\n    service_name= \"rag\" ,\n)\n\n # Create the pipeline \npipeline = QueryPipeline(chain=[\n    ServiceComponent.from_service_definition(query_rewrite_service),\n    ServiceComponent.from_service_definition(rag_service),\n])\norchestrator = PipelineOrchestrator(pipeline)\n\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=orchestrator,\n)\n\n # Set up the launcher \nlauncher = LocalLauncher(\n    [query_rewrite_service, rag_service],\n    control_plane,\n    message_queue,\n)\n\n # Run a query \nresult = launcher.launch_single( \"Tell me about rabbits\" )\n print (result) Public roadmap This is an alpha release, meaning that we’d love your feedback on features to better help you build multi-agent systems in production! We’ve created a  public roadmap  showing where we plan to go from here. We’re actively seeking public feedback on what works for you and what doesn’t. Dive in! llama-agents  provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production,  llama-agents  offers the tools you need to bring your AI vision to life. Check out  the repo  to learn more, especially our library of  examples . We're excited to see what the community builds with  llama-agents . Happy coding!", "author": "LlamaIndex", "date": "Jun 26, 2024", "tags": ["Agents"]},
{"title": "LlamaIndex Newsletter 2024-07-02", "content": "Hello, Llama enthusiasts! 🦙 Welcome to this week’s edition of the LlamaIndex newsletter! In this issue, we’re excited to bring you exciting updates about  llama-agents , live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools. Before moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service.  Sign up  now if you’re eager to collaborate and build LLM applications with LlamaCloud. 🤩  The highlights: Launched Llama-Agents Framework:  Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. It’s designed for easy deployment, scalability, and observability.  Blogpost ,  Tweet . create-llama  Integrated with LlamaCloud:  Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance.  Tweet . ✨ Feature Releases and Enhancements: We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability.  Blogpost ,  Tweet . create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems.  Tweet . We have integrated with DSPy for Optimized RAG by utilizing DSPy’s optimization capabilities with LlamaIndex’s data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors.  Cookbook ,  Tweet . 💡 Demos: Automating Code Reviews,  project  by  Composio  with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel.  Codebase . 🗺️ Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services. Guide to AI Agents with LlamaIndex: Andrei’s comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents.  Video ,  Notebook . ✍️ Tutorials: Kingzzm’s   tutorial  on crafting a custom hybrid retriever using LlamaIndex’s flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy. Jeff’s   tutorial  on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs. 1littlecoder’s   tutorial  on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions. Mervin Praison’s   tutorial  on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.", "author": "LlamaIndex", "date": "Jul 2, 2024", "tags": []},
{"title": "LlamaCloud - Built for Enterprise LLM App Builders", "content": "RAG is only as Good as your Data Building production-ready LLM applications is hard. We've been chatting with hundreds of users, ranging from Fortune 500 enterprises to pre-seed startups and here's what they tell us they struggle with: Data Quality Issues : Most companies deal with large sets of complex, heterogeneous documents. Think PDFs with messy formatting, images, tables across multiple pages, different languages - the list goes on. Ensuring high-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications. Scalability Hurdles : Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either. Accuracy Concerns : Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs. Configuration Overload:  Fine-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task. As developers shift from prototypes towards building production applications - complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data. To address these difficulties, we soft-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production-grade context-augmentation to your LLM and RAG applications. LlamaParse can already support 50+ languages and 100+ document formats. The adoption has been incredible - we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here’s an example from Dean Barr, Applied AI Lead at Carlyle: As an AI Applied Data Scientist who was granted one of the first ML patents in the U.S., and who is building cutting-edge AI capabilities at one of the world's largest Private Equity Funds, I can confidently say that LlamaParse from LlamaIndex is currently the best technology I have seen for parsing complex document structures for Enterprise RAG pipelines. Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to maintaining data integrity in advanced RAG and agentic model building. The Rise of Centralized Knowledge Management We have designed LlamaCloud to cater to the need of  production-grade   context-augmentation  for your LLM and RAG applications. Let's take a tour of what LlamaCloud brings to the table: LlamaParse : Our state-of-the-art parser that turns complex documents with tables and charts into LLM-friendly formats. You can learn more about  LlamaParse here . Managed Ingestion : Connect to enterprise data sources and your choice of data sinks with ease. We support  multiple data sources  and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications. Advanced Retrieval : LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents. LlamaCloud Playground : An interactive UI to test and refine your ingestion and retrieval strategies before deployment. Scalability and Security : Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs. This video gives a detailed walk through of LlamaCloud: Our customers tell us that LlamaCloud enables developers to spend less time setting up and iterating on their data pipelines for LLM use cases, allowing them to iterate through the LLM application development lifecycle much more quickly. Here’s what Teemu Lahdenpera, CTO at  Scaleport.ai  had to say: LlamaCloud has really sped up our development timelines. Getting to technical prototypes quickly allows us to show tangible value instantly, improving our sales outcomes. When needed, switching from the LlamaCloud UI to code has been really seamless. The configurable parsing and retrieval features have significantly improved our response accuracy. We've also seen great results with LlamaParse and found it outperforming GPT-4 vision on some OCR tasks! Try it yourself We’ve opened up an official waitlist for LlamaCloud. Here's how you can get involved: Join the LlamaCloud Waitlist :  Sign up here . Get in Touch : Have questions? Want to discuss unlimited commercial use?  Contact us  and let's chat! Note: we support private deployments for a select number of enterprises Stay Updated : Follow us on  Twitter  and join our  Discord community  to stay in the loop. In the meantime, anyone can create an account at  https://cloud.llamaindex.ai/ . While you’re waiting for official LlamaCloud access, anyone can immediately start using our LlamaParse APIs. We’re shipping a  lot  of features in the next few weeks. We look forward to seeing the context-augmented LLM applications that you can build on top of LlamaCloud! 🚀🦙 FAQ Have you got some examples of how to use LlamaCloud? We sure do! One of the strengths of LlamaCloud is how easily the endpoints integrate into your existing code. Our  llamacloud-demo repo  has lots of examples from  getting started  to  running evaluations . Is this competitive with vector databases? No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are integrating LlamaCloud with storage providers based on customer requests.", "author": "LlamaIndex", "date": "Jul 9, 2024", "tags": ["Llamacloud"]},
{"title": "LlamaIndex Newsletter 2024-07-09", "content": "Hello, Llama Lovers! 🦙 Welcome to this week’s edition of the LlamaIndex newsletter! We’re thrilled to share some exciting updates about  llama-agents , along with demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools. Before we dive into our newsletter, we’re excited to announce the return of Community Office Hours. If you have use-cases, in-depth questions, or feedback for the team at LlamaIndex, join us during our community office hours! We’ll set up a 15-30 minute Zoom call to discuss it. Sign up here  to participate. 🤩  The highlights: Multi-Agent Kubernetes Kit Launched:  Deploy multi-agent systems easily with our new Kubernetes Starter Kit featuring ready-to-use tools and configurations.  Notebook ,  Tweet . Enhanced Communication with RabbitMQ:  Boost multi-agent system reliability and scalability in production with our new RabbitMQ integration.  Notebook ,  Tweet . Reflection as a Service Guide:  Improve agent reliability with our guide on building Reflection as a Service, perfect for output validation and correction.  Notebook ,  Tweet . Corrective RAG as a Service Guide:  Create a self-correcting RAG that ensures context relevance and integrates search fallbacks before generation.  Notebook ,  Tweet . Tutorial series on Property Graphs:  6-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j, and Ollama.  Videos ,  Tweet . ✨ Feature Releases and Enhancements: We have launched a Multi-Agent on Kubernetes Starter Kit to build and deploy a multi-agent system using Docker Compose and Kubernetes using llama-agents. This kit includes prebuilt agent loops and tools, as well as Dockerfiles and Kubernetes manifests for easy production deployment.  Notebook ,  Tweet . We have integrated RabbitMQ with llama-agents to enhance multi-agent communication, offering scalability and reliability for handling large request volumes in production.  Notebook ,  Tweet . [ Yi-01.AI ]( http://Yi-01.AIhttps ://x.com/01AI_Yi) is integrated with LlamaIndex for enhanced retrieval and indexing, streamlining the development of smarter, faster RAG applications.  Docs . We have launched a  6-part video series  on Property Graphs in LlamaIndex using MistralAI, Neo4j and Ollama.  Tweet . 💡 Demos: OpenContracts  by  John Scrudato : A fully open-source, AI-powered Document Analytics Tool, integrates genAI capabilities and LlamaIndex for robust query handling and data extraction across documents. This tool is particularly valuable for legal analysis, enabling users to manage, process, and query vast arrays of contracts and legal documents.  Docs . 🗺️ Guides: Guide to build Reflection as a Service to enhance agent reliability with our new standalone service, ideal for validating and correcting outputs across multiple agents.  Notebook ,  Tweet . Guide to build Corrective RAG as a Service, a self-correcting RAG that dynamically validates context relevance, seamlessly integrating web search fallbacks before generation.  Notebook ,  Tweet . ✍️ Tutorials: Pavan Kumar’s   tutorial  to build a RAG pipeline that lives on a Raspberry Pi device with docker, Ollama, Qdrant, and using LlamaIndex as the orchestration layer. Trade Mamba’s  video  tutorial  to build an AI-enabled trading assistant using LlamaIndex’s agent/tool/RAG abstractions for tasks like tracking portfolio values, managing stock orders, and conducting vector searches for semantic information. Giskard’s   toolkit  enables diverse question generation featuring question types like simple, complex, distracting, situational, double, and conversational for RAG evaluation, as demonstrated in the tutorial on using a LlamaIndex pipeline with an IPCC Climate Report. Pavan Kumar’s   tutorial  demonstrates building a Multi-Document Financial Analyst Agent using LlamaIndex RAG and ReAct tools, analyzing categorized SEC documents with SnowflakeDB embeddings and MistralAI via Ollama. Ross A.’s  tutorial  on retrieval evaluations for RAG delves into essential metrics like precision@K and NDCG, and demonstrates how to convert datasets to BEIR format for assessing LlamaIndex retrievers. 🎥 Webinar: Join us for a  webinar  on July 10th, featuring Jerry Liu (LlamaIndex) and Ayush Thakur (Weights & Biases) on  A Principled Approach to RAG Experimentation + Evaluation  to learn how to build, evaluate, and refine RAG pipelines.", "author": "LlamaIndex", "date": "Jul 9, 2024", "tags": []},
{"title": "Building and Evaluating a QA System with LlamaIndex", "content": "Introduction LlamaIndex (GPT Index)  offers an interface to connect your Large Language Models (LLMs) with external data. LlamaIndex provides various data structures to index your data, such as the list index, vector index, keyword index, and tree index. It offers both a high-level API and low-level API — the high-level API allows you to build a Question-Answering (QA) system in just five lines of code, whereas the lower-level API allows you to customize various aspects of retrieval and synthesis. However, taking these systems into production requires careful evaluation of the performance of the overall system — the quality of the outputs given the inputs. Evaluation of retrieval-augmented generation can be challenging because the user would need to come up with a dataset of relevant questions for a given context. To overcome these obstacles, LlamaIndex provides Question Generation and label-free Evaluation modules. In this blog, we will discuss the three-step evaluation process using Question Generation and Evaluation modules: Question Generation from the document Generate answers/source nodes for questions using LlamaIndex QueryEngine abstractions, which manage the interaction between the LLM and data indices. Evaluate if the question (query), answer, and source nodes are matching/inline 1. Question Generation It should be noted that this approach does not require ground-truth labels. The purpose of question generation is to generate an initial dataset of inputs over context that can be used to evaluate the question-answering system. LlamaIndex offers the DataGenerator class, which generates questions from a given document using ListIndex. By default, it uses OpenAI ChatGPT (get-3.5-turbo) for question generation. from  llama_index.evaluation  import  DatasetGenerator\n from  llama_index  import  SimpleDirectoryReader\n\n # Load documents \nreader = SimpleDirectoryReader( \"./data\" )\ndocuments = reader.load_data()\n\n # Generate Question \ndata_generator = DatasetGenerator.from_documents(documents)\nquestion = data_generator.generate_questions_from_nodes() 2. Generate Answers/Source Nodes (Context) Using List Index, we generate answers and source nodes for the generated questions in the response object. \n from  llama_index  import  GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n\n # load documents \ndocuments = SimpleDirectoryReader( './data' ).load_data()\n\n # Create Index \nindex = GPTVectorStoreIndex.from_documents(documents)\n\n # save index to disk \nindex.set_index_id( \"vector_index\" )\nindex.storage_context.persist( 'storage' )\n\n # rebuild storage context \nstorage_context = StorageContext.from_defaults(persist_dir= 'storage' )\n # load index \nindex = load_index_from_storage(storage_context, index_id= \"vector_index\" )\n\n # Query the index \nquery_engine = index.as_query_engine(similarity_top_k= 3 )\nresponse = query_engine.query(&lt;Query&gt;)\n\n # Response object has both response and source nodes. \n 3. Evaluation The evaluation module can be used to answer the following three questions: Are the response generated and source nodes (context) matching? — Response + Source Nodes (Context) Are response generated, source nodes (context), and query matching? — Query + Response + Source Nodes (Context) Which source nodes of the retrieved source nodes are used to generate a response? — Query + Response + Individual Source Nodes (Context) Evaluation can be done with some combination of the query, context, and response, combining these with LLM calls. Response + Source Nodes (Context) This function answers the question: Are the response generated and source nodes (context) matching? The response object for a given query returns both the response and source nodes (context) with which it generated the response. We can now evaluate the response against the retrieved sources — without taking into account the query! This allows you to measure hallucination — if the response does not match the retrieved sources, this means that the model may be “hallucinating” an answer since it is not rooting the answer in the context provided to it in the prompt. The result is a binary response — either “YES/NO”. YES — Response and Source Nodes (Context) are matching. NO — Response and Source Nodes (Context) are not matching. from  llama_index.evaluation  import  ResponseEvaluator\n\n # build service context \nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature= 0 , model_name= \"gpt-4\" ))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n # Build index and get response object \n...\n\n # define evaluator \nevaluator = ResponseEvaluator(service_context=service_context)\n\n # evaluate using the response object \neval_result = evaluator.evaluate(response) Query + Response + Source Nodes (Context) This function answers the question: Are response generated, source nodes (context), and query matching? Often with the “Response + Source Nodes (Context)” approach, the response generated is in line with the source nodes but may not be the answer to the query. Therefore, considering the query along with the response and source nodes is a good approach for a more accurate analysis. The goal is to determine if the response + source context answers the query. The result is a binary response — either “YES/NO”. YES — Query, Response, and Source Nodes (Context) are matching. NO — Query, Response, and Source Nodes (Context) are not matching. from  llama_index.evaluation  import  QueryResponseEvaluator\n\n # build service context \nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature= 0 , model_name= \"gpt-4\" ))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n # Build index and get response object \n...\n\n # define evaluator \nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n # evaluate using the response object \neval_result = evaluator.evaluate(query, response) Query + Response + Individual Source Nodes (Context) This function answers the question: Which source nodes of the retrieved source nodes are used to generate a response? Often in the real world, the source nodes can be nodes from different documents. In these cases, it’s important to understand which source nodes are relevant and show those documents to the users. This mode of evaluation will look at each source node and see if each source node contains an answer to the query. from  llama_index.evaluation  import  QueryResponseEvaluator\n\n # build service context \nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature= 0 , model_name= \"gpt-4\" ))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n # build index and get response object  \n...\n\n # define evaluator \nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n # evaluate using the response object \neval_result = evaluator.evaluate_source_nodes(response) Google Colab notebook for Evaluating QA systems using LlamaIndex — Google Colaboratory Evaluating QA systems using LlamaIndex Conclusion LlamaIndex provides a comprehensive solution for building and evaluating QA systems without the need for ground-truth labels. By using the Question Generation and Evaluation modules, you can ensure that your system is accurate and reliable, making it suitable for production environments.", "author": "Ravi Theja", "date": "May 7, 2023", "tags": ["Large Language Models", "NLP", "Question Answering", "OpenAI", "Gpt 4"]},
{"title": "A New Document Summary Index for LLM-powered QA Systems", "content": "In this blog post, we introduce a brand new LlamaIndex data structure: a Document Summary Index. We describe how it can help offer better retrieval performance compared to traditional semantic search, and also walk through an example. Background One of the core use cases of Large Language Models (LLMs) is question-answering over your own data. To do this, we pair the LLM with a “retrieval” model that can perform information retrieval over a knowledge corpus, and perform response synthesis over the retrieved texts using the LLM. This overall framework is called Retrieval-Augmented Generation. Most users building LLM-powered QA systems today tend to do some form of the following: Take source documents, split each one into text chunks Store text chunks in a vector db During query-time, retrieve text chunks by embedding similarity and/or keyword filters. Perform response synthesis For a variety of reasons, this approach provides limited retrieval performance. Limitations of Existing Approaches There are a few limitations of embedding retrieval using text chunks. Text chunks lack global context.  Oftentimes the question requires context beyond what is indexed in a specific chunk. Careful tuning of top-k / similarity score thresholds.  Make the value too small and you’ll miss context. Make the value too big and cost/latency might increase with more irrelevant context. Embeddings don’t always select the most relevant context for a question.  Embeddings are inherently determined separately between text and the context. Adding keyword filters are one way to enhance the retrieval results. But that comes with its own set of challenges. We would need to adequately determine the proper keywords for each document, either manually or through an NLP keyword extraction/topic tagging model. Also we would need to adequately infer the proper keywords from the query. Document Summary Index A diagram for the Document Summary Index We propose a new index in  LlamaIndex  that will extract/index an  unstructured text summary for each document . This index can help enhance retrieval performance beyond existing retrieval approaches. It helps to index more information than a single text chunk, and carries more semantic meaning than keyword tags. It also allows for a more flexible form of retrieval: we can do both LLM retrieval and embedding-based retrieval. How It Works During build-time, we ingest each document, and use a LLM to extract a summary from each document. We also split the document up into text chunks (nodes). Both the summary and the nodes are stored within our  Document Store  abstraction. We maintain a mapping from the summary to the source document/nodes. During query-time, we retrieve relevant documents to the query based on their summaries, using the following approaches: LLM-based Retrieval:  We present sets of document summaries to the LLM, and ask the LLM to determine which documents are relevant + their relevance score. Embedding-based Retrieval:  We retrieve relevant documents based on summary embedding similarity (with a top-k cutoff). Note that this approach of retrieval for document summaries (even with the embedding-based approach) is different than embedding-based retrieval over text chunks. The retrieval classes for the document summary index retrieve  all nodes  for any selected document, instead of returning relevant chunks at the node-level. Storing summaries for a document also enables  LLM-based retrieval . Instead of feeding the entire document to the LLM in the beginning, we can first have the LLM inspect the concise document summary to see if it’s relevant to the query at all. This leverages the reasoning capabilities of LLM’s which are more advanced than embedding-based lookup, but avoids the cost/latency of feeding the entire document to the LLM Additional Insights Document retrieval with summaries can be thought of as a “middle ground” between semantic search and brute-force summarization across all docs. We look up documents based on summary relevance with the given query, and then return all *nodes* corresponding to the retrieved docs. Why should we do this? This retrieval method gives user more context than top-k over a text-chunk, by retrieving context at a document-level. But, it’s also a more flexible/automatic approach than topic modeling; no more worrying about whether your text has the right keyword tags! Example Let’s walk through an example that showcases the document summary index, over Wikipedia articles about different cities. The rest of this guide showcases the relevant code snippets. You can find the  full walkthrough here  (and here’s the  notebook link ). We can build the  GPTDocumentSummaryIndex  over a set of documents, and pass in a  ResponseSynthesizer  object to synthesize summaries for the documents. from  llama_index  import  (\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n    ResponseSynthesizer\n)\n from  llama_index.indices.document_summary  import  GPTDocumentSummaryIndex\n from  langchain.chat_models  import  ChatOpenAI\n\n # load docs, define service context \n...\n\n # build the index \nresponse_synthesizer = ResponseSynthesizer.from_args(response_mode= \"tree_summarize\" , use_async= True )\ndoc_summary_index = GPTDocumentSummaryIndex.from_documents(\n    city_docs, \n    service_context=service_context,\n    response_synthesizer=response_synthesizer\n) Once the index is built, we can get the summary for any given document: summary = doc_summary_index.get_document_summary(\"Boston\") Next, let’s walk through an example LLM-based retrieval over the index. from  llama_index.indices.document_summary  import  DocumentSummaryIndexRetriever\n\nretriever = DocumentSummaryIndexRetriever(\n    doc_summary_index,\n     # choice_select_prompt=choice_select_prompt, \n     # choice_batch_size=choice_batch_size, \n     # format_node_batch_fn=format_node_batch_fn, \n     # parse_choice_select_answer_fn=parse_choice_select_answer_fn, \n     # service_context=service_context \n)\nretrieved_nodes = retriever.retrieve( \"What are the sports teams in Toronto?\" )\n print (retrieved_nodes[ 0 ].score)\n print (retrieved_nodes[ 0 ].node.get_text())The retriever will retrieve a  set  of relevant nodes  for  a given index. Note that the LLM returns relevance scores in addition to the document text: 8.0\nToronto ( (listen) tə-RON-toh; locally [təˈɹɒɾ̃ə] or [ˈtɹɒɾ̃ə]) is the capital city of the Canadian province of Ontario. With a recorded population of 2,794,356 in 2021, it is the most populous city in Canada... We can also use the index as part of an overall query engine, to not only retrieve the relevant context, but also synthesize a response to a given question. We can do this through both the high-level API as well as lower-level API. High-level API query_engine = doc_summary_index.as_query_engine(\n  response_mode= \"tree_summarize\" , use_async= True \n)\nresponse = query_engine.query( \"What are the sports teams in Toronto?\" )\n print (response) Lower-level API # use retriever as part of a query engine \n from  llama_index.query_engine  import  RetrieverQueryEngine\n\n # configure response synthesizer \nresponse_synthesizer = ResponseSynthesizer.from_args()\n\n # assemble query engine \nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\n # query \nresponse = query_engine.query( \"What are the sports teams in Toronto?\" )\n print (response) Next Steps The approach of autosummarization over any piece of text is really exciting. We’re excited to develop extensions in two areas: Continue exploring autosummarization in different layers. Currently it’s at the doc-level, but what about summarizing a big text chunk into a smaller one? (e.g. a one-liner). Continue exploring LLM-based retrieval, which summarization helps to unlock. Also we’re sharing the example guide/notebook below in case you missed it above: Document Summary Guide Notebook Link", "author": "Jerry Liu", "date": "May 8, 2023", "tags": ["Information Retrieval", "Large Language Models", "Gpt 4", "AI", "Machine Learning"]},
{"title": "Testing Anthropic Claude’s 100k-token window on SEC 10-K Filings", "content": "Anthropic’s  100K Context Window  expansion, just released yesterday, has taken the AI community by storm. A 100k token limit is approximately 75k words (~3x GPT4–32k’s context window, ~25x that of GPT-3.5/ChatGPT); this means that you can now fit 300 pages of text in a  single  inference call . One of the core use cases highlighted in the Anthropic blog is  analyzing an SEC 10-K filing ; the model is capable of ingesting the entire report, and producing answers to different questions. Coincidentally, we  published a tutorial  a few months ago showing how LlamaIndex + Unstructured + GPT3 could help you perform different queries over UBER SEC 10-k filings. LlamaIndex provides a comprehensive toolkit to help manage external data on top of any LLM with limited context windows, and we show that we can execute a diverse range of queries, from questions over a single document to comparing sections across documents. How well does Anthropic’s 100k model do over UBER SEC 10-k filings? Moreover, how well does it do  without  the help of any of LlamaIndex’s more advanced data structures? In this blog we show the performance of Anthropic’s model on different queries, using the simplest data structure available: the list index. High-Level Findings Where Anthropic’s 100k model does well: Holistic understanding of the data (kind of, after some prompt tuning):  Anthropic’s model does demonstrate an impressive capability to synthesize insights across the entire context window to answer the question at hand (assuming we set  response_mode=\"tree_summarize\"  , see below). It can miss details though; see below! Latency:  This one was surprising to us. Anthropic’s model is able to crunch an entire UBER 10-k filing in ~60–90 seconds, which seems long but is much faster than repeated API calls to GPT-3 (which when added up can take minutes). Where Anthropic’s 100k model doesn’t do well: Cost:  This one is obvious. Every query we ran processed hundreds of thousands of tokens. At  $11 per million tokens for Claude-v1 , this equates to $1 per query, which can quickly add up. Reasoning over more complicated prompts:  Anthropic’s model demonstrated a surprising lack of ability to understand our refine prompt for  “create-and-refine” response synthesis , returning incorrect/irrelevant results. We ended up switching to  “tree summarization” instead . See below for results. Overview of Methodology We want to test the capabilities of Anthropic’s 100K model on top of UBER 10-k filings from 2019–2022. We also want to do this while using as little retrieval/synthesis constructs as possible. This means no embeddings, and no fancy retrieval mechanisms. Ideally, we can directly insert an entire 10-k filing (or even all four 10-k filings) into the prompt. However, we found that a single UBER 10-k filing actually consists of ~ 160k tokens, which is greater than the 100k context window.  This means that we still have to chunk up each filing! We end up using our  list index data structure  — we split each text up into massive ~100k token chunks, and use our  response synthesis strategies  to synthesize an answer across multiple chunks. We run some queries over each filing as well as over multiple filings, similar to our original blog post. We report the results below. Tutorial Setup Our data ingestion is the same as the LlamaIndex + Unstructured blog post. We use Unstructured’s HTML parser to parse the HTML DOM into nicely formatted text. We then create a Document object for each SEC filing. You can access Unstructured data loaders on  LlamaHub . from  llama_index  import  download_loader\n from  pathlib  import  Path\n\nUnstructuredReader = download_loader( \"UnstructuredReader\" , refresh_cache= True )\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nyears = [ 2022 ,  2021 ,  2020 ,  2019 ]\n for  year  in  years:\n    year_doc = loader.load_data(file=Path( f'./data/UBER/UBER_ {year} .html' ), split_documents= False )[ 0 ]\n     # insert year metadata into each year \n    year_doc.extra_info = { \"year\" : year}\n    doc_set[year] = year_doc\n    all_docs.append(year_doc) Next, we want to setup the Anthropic LLM. We’re using claude-v1 by default. We also want to manually define the new 100k-token input size within our  PromptHelper  object; this will help us figure out how to “compact” context into the input prompt space during response synthesis. We set the  max_input_size  to 100k and the output length to 2048. We also set the context chunk size to a high value (95k, leaving some buffer room for rest of the prompt). Context will only be chunked if the number of tokens exceeds this limit. from  llama_index  import  PromptHelper, LLMPredictor, ServiceContext\n from  langchain.llms  import  Anthropic\n\n # define prompt helper \n # set maximum input size \nmax_input_size =  100000 \n # set number of output tokens \nnum_output =  2048 \n # set maximum chunk overlap \nmax_chunk_overlap =  20 \nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\nllm_predictor = LLMPredictor(llm=Anthropic(model= \"claude-v1.3-100k\" , temperature= 0 , max_tokens_to_sample=num_output))\nservice_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor, prompt_helper=prompt_helper,\n    chunk_size_limit= 95000 \n) Analyzing a Single Document Let’s first analyze queries over a single document. We build a list index over the 2019 UBER 10-K: list_index = GPTListIndex.from_documents([doc_set[ 2019 ]], service_context=service_context)\n print ( len (list_index.index_struct.nodes)) As mentioned, the 10-K exceeds the 100k token limit, and so there are two nodes within the list index. We then ask a query: “What were some of the biggest risk factors in 2019?” Recall that there are two approaches within LlamaIndex for response synthesis across multiple nodes (where the total context exceeds the context window): a “create-and-refine” strategy, and a “tree summarize” strategy. Create-and-Refine:  sequentially go through each retrieved  Node . Use a Question-Answer Prompt for the first Node, and use a Refine Prompt for subsequent Nodes. Make a separate LLM call per Node. Tree Summarize:  Given a set of  Node  objects and the query, recursively construct a tree using the Question Answer Prompt and return the root node as the response. Good for summarization purposes. Claude-v1 100K does not do well with the “create-and-refine” strategy (which is the default). # NOTE: the default create/refine approach does not give good answers\nquery = \"What were some of the biggest risk factors in 2019?\"\nquery_engine = list_index.as_query_engine(service_context=service_context)\nresponse = query_engine.query(query) Here is the retrieved answer: Original answer:\n\nSome of the biggest risk factors  for  Uber  in   2019  included:\n\n• Regulatory challenges  and  uncertainty. Uber faced regulatory challenges  and  uncertainty  in  many markets, including restrictions on its products  and  services, caps on pricing,  and  licensing requirements. For example, California 's AB5 law and other similar laws increased the risk of Drivers being classified as employees. Uber also faced regulatory scrutiny and bans in London, Barcelona, and other markets.\n\n• Competition. The markets in which Uber operates are highly competitive, and Uber faced significant competition from well-established and low-cost alternatives in 2019. Competitors also aggressively competed for Drivers and consumers by offering significant incentives and discounts. \n\n• Safety and security. There were risks related to the safety and security of Uber' s platform, including risks  from  vehicle  or  scooter accidents, assaults,  and  other incidents. Uber released a safety report  in   2019  detailing reports of sexual assaults on its platform which brought additional scrutiny.\n\n• Financial performance  and  profitability. Uber continued to experience significant losses  in   2019   as  it invested  in  growth. There was no guarantee that Uber would achieve  or  sustain profitability  in  the future.  \n\n• Reliance on third parties. Uber relied on third parties  for  key elements of its business like cloud computing services, payment processing,  and  background checks. If these third parties failed to provide services  or  increased costs, it could adversely impact Ube r's business.\n\n• Macroeconomic conditions. Uber' s business was sensitive to economic conditions  and  changes that impacted discretionary consumer spending. A decline  in  the economy could reduce demand  for  Ube r's products and services.  \n\n• Reliance on Drivers and Restaurants. Uber' s success depended on attracting  and  retaining enough Drivers  and  Restaurants to its platform. If Uber was unable to attract  or  retain enough Drivers  and  Restaurants, it could negatively impact its operations.\n\n• Intellectual  property . If Uber was unable to protect its intellectual  property ,  or   if  third parties claimed Uber was infringing on their intellectual  property , it could harm Ube r's business. Uber relied on a combination of copyright, trademark, patent, and trade secret laws to establish its intellectual property rights.\n\nRefined answer:\n\nSome of the biggest risk factors for Uber in 2019 included:\n\n• Regulatory challenges and uncertainty. Uber faced significant regulatory challenges in 2019, including AB5 in California which increased the risk of Drivers being classified as employees and regulatory scrutiny and temporary bans in London and other markets. These regulatory issues created uncertainty and posed risks to Uber' s business model  and  financial results.\n\n• Competition. The ridesharing  and  meal delivery markets are highly competitive,  and  competitors offered significant incentives  and  discounts to take market share  from  Uber  in   2019.  This competition could negatively impact Ube r's growth and profitability.\n\n• Safety and security. There were risks related to the safety and security of Uber' s platform, including risks  from  vehicle  or  scooter accidents, assaults,  and  other incidents. Ube r's safety report detailing sexual assault reports brought additional scrutiny and risks. Any failure to ensure safety could significantly damage Uber' s reputation  and  business.\n\n• Financial performance  and  profitability. Uber continued to experience substantial losses  in   2019  due to investments  in  growth,  and  there was no guarantee of future profitability. Ube r's path to profitability was uncertain, and failure to become profitable could threaten its business model and access to capital.  \n\n• Reliance on third parties. Uber relied on third parties for services like cloud computing, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could negatively impact Uber' s offerings, growth,  and  profitability.  \n\n• Macroeconomic conditions. Ube r's business was sensitive to the economy and consumer discretionary spending. An economic downturn could reduce demand for Uber' s offerings  and  threaten its growth  and  financial performance.\n\n• Reliance on Drivers  and  Restaurants. Ube r's success depended on attracting and retaining enough Drivers and Restaurants to support growth and meet consumer demand. Failure to attract or retain Drivers and Restaurants could significantly impact Uber' s business.  \n\n• Intellectual  property . Failure to protect intellectual  property   or  infringement of others ' intellectual property could damage Uber' s competitive position  and  business. Uber relied on laws  and  policies to protect intellectual  property , but monitoring infringement  and  unauthorized use was difficult. As shown, the answer contains the prompt within it; the point of the refine prompt was to produce just the refined answer, not produce both the original and refined answers. For some reason, using the Tree Summarize strategy works much better. # NOTE: tree_summarize gives better answers\nquery = \"What were some of the biggest risk factors in 2019?\"\nquery_engine = list_index.as_query_engine(service_context=service_context, response_mode=\"tree_summarize\")\nresponse = query_engine.query(query) Returned response: • Regulatory challenges and uncertainty: Uber faced significant regulatory challenges and uncertainty in  2019 , including AB5 in California which codified a  new   test   for  determining whether workers should be classified as employees or independent contractors. Uber also faced regulatory scrutiny and bans in other markets like London, UK. These regulatory issues created uncertainty and risk around Uber 's business model and operations.\n\n• Safety and security: Uber received negative publicity around safety incidents on its platform which could damage its brand and reputation. Uber released a safety report in 2019 on sexual assaults and other incidents which led to additional scrutiny. Safety and security risks remain an ongoing issue for Uber' s business.\n\n• Competition: The markets in which Uber competes are intensely competitive, and Uber faces competition from  new   and  existing companies in the various segments it operates in like ridesharing, food delivery, and logistics. Increased competition could make it difficult  for  Uber to attract and retain platform users.\n\n• Financial losses: Uber continued to experience significant losses from operations in  2019  as it invested in growth. Uber may not achieve or sustain profitability in the future which could negatively impact its business and stock price.\n\n• Reliance on third-parties: Uber relies on third-party data centers, cloud computing services, and software which exposes it to risks  if  these third-parties experience any disruptions or interference. Any disruption of third-party services could negatively impact Uber 's business.\n\n• Macroeconomic conditions: Uber' s business is sensitive to macroeconomic conditions, and any downturn could reduce discretionary consumer spending which would harm Uber 's business. Issues like a recession, increased unemployment, or reduced consumer confidence could all potentially impact Uber' s business.\n\n• Reliance on Drivers and Restaurants: Uber 's business depends on attracting and retaining Drivers and Restaurants to its platform. If Uber is unable to attract or maintain a critical mass of Drivers and Restaurants, its platform will become less appealing to platform users and its financial results would be adversely impacted. The answer is much better. It goes into details about the US-China trade war, slowing economic growth, Brexit, and more (keep in mind 2019 is pre-COVID). Token Usage and Latency The document contains around ~170K tokens. For some reason, this number is not reflected on the Anthropic usage logs (the “Prompt Tokens” section seems capped at 10240). But the Prompt Length (in characters) is logged, as well as the model latency. Given the pricing, ~170K tokens would be equivalent to $1.5–2 USD. A query through one Uber SEC-10K takes around  150   seconds , including all LLM calls. This is actually a bit faster than repeated calls to ChatGPT/davinci. Each ChatGPT/davinci call (with the 4K token window maximized), empirically can take 6–10 seconds to complete →  125–250 seconds ( or more). Analyzing Multiple Documents A popular example in our  previous blog post  was showcasing that you could compare/contrast different documents with LlamaIndex graph structures. We test whether we can do that here as well, by feeding in multiple SEC reports into Claude-v1 100k. Caveat:  Considering that one UBER SEC-10K filing doesn’t even fit in the context window, we’ll of course also need to implement response synthesis strategies in order to handle ingesting multiple 10K filings. We build a list index over all 4 10K filings: 2019, 2020, 2021, and 2022. list_index = GPTListIndex.from_documents(all_docs, service_context=service_context)\n print ( len (list_index.index_struct.nodes)) We then ask our question using our Tree Summarize response mode. query = \"How are the risk factors changing across years? Compare/contrast the risk factors across the SEC filings.\"\nquery_engine = list_index.as_query_engine(response_mode=\"tree_summarize\")\nresponse = query_engine.query(query) The full answer is given below: The risk factors disclosed in Uber ' s SEC filings have evolved over time based on Uber ' s business  and  industry changes. Some of the key differences in risk factors across the filings are:\n\n 2017   10 -K:\n- Focused heavily on risks related to negative publicity, competition, dependence on independent contractors,  and  regulatory challenges as Uber was still facing backlash from various PR crises  and  regulatory pushback. \n- Also highlighted risks from intellectual property litigation given various IP disputes at the time.\n\n 2018   10 -K:\n- Added more risks related to autonomous vehicles as Uber ramped up its self-driving car efforts. Specifically called out risks from accidents, technical challenges,  and  competition in the AV space.\n- Removed some risks related to negative publicity  and  PR crises as those issues had subsided. But added risks related to corporate culture  and  workplace environment given the Fowler scandal.\n\n 2019   10 -K: \n- Further expanded AV risks to include risks from partnerships  and  third party relationships as Uber relied more on partnerships with other companies like Toyota  and  Volvo. \n- Added risks related to environmental regulations  and  climate change as those issues received more mainstream attention. \n- Removed risks related to initial scaling challenges as Uber had matured as a company. But added risks related to growth into  new  offerings  and  markets as Uber expanded into freight shipping, scooters, etc.\n\nIn summary, Uber ' s risk factors have evolved to reflect the key issues  and  challenges facing the company at any point in time. The risks highlight both external factors like regulations as well as internal factors related to Uber ' s business, technology,  and  operations. The changes over time show how an innovative company ' s risks can shift quite quickly.\n\nSome of the key trends in how the risk factors are changing include:\n\n 1.  Risks related to negative publicity  and  PR crises decreased over time as those issues subsided,  while  risks related to other issues like AV technology, workplace culture,  and   new  initiatives increased. This shows how Uber ' s risks evolved as the company matured.\n\n 2.  Risks tend to increase in areas where Uber is actively investing  or  expanding into. For example, risks related to AVs, partnerships,  and   new  mobility offerings increased as Uber ramped up efforts in those areas. This highlights how risk profiles change with a company ' s strategic priorities.\n\n 3.  External risks like regulations  and  climate change were added as those issues gained more mainstream attention  and  importance. This shows how companies have to adapt their risk factors to account  for  changes in the overall business environment. \n\n 4.  Certain foundational risks around competition, growth,  and  reliance on independent contractors persisted over time. But the specifics  and  details provided  for  those risks evolved based on Uber ' s current challenges  and  priorities. So  while  the themes remained, the risks were actively updated.\n\nIn summary, Uber ' s risk factors changed over time to provide more details on the company ' s priorities, challenges, partnerships,  and  external environment at any given point. But certain core risks inherent to Uber ' s business model have also remained consistently highlighted, demonstrating how those foundational risks are  long -term in nature. The changes in risks over time provide a glimpse into how an innovative company ' s risk profile is constantly evolving. This response only contains risk refactor analysis over the 2019 10-K (which in turn contains risk refactors for 2017 and 2018). It does not contain the years from 2020 onwards. Part of this is potentially due to our tree summarize response synthesis strategy. Nevertheless, it shows that trying to naively “stuff” documents into big 100K token chunks with simple response synthesis strategies still does not produce the optimal answers. Token Usage and Latency As expected, feeding all four documents into Anthropic necessitates many more chained LLM calls, which consumes way more tokens and takes a lot longer (on the order of 9–10 minutes). Conclusion In general, the new 100K context window is incredibly exciting and offers developers a new mode of feeding in data into the LLM for different tasks/queries. It offers coherent analysis with a marginal token cost that is much cheaper than that of GPT-4. That said, trying to maximize this context window with each inference call does come with tradeoffs in terms of latency and cost. We look forward to doing more experiments/comparisons/thought pieces on top of Claude! Let us know your feedback. Resources You can check out our  full Colab notebook here .", "author": "Jerry Liu", "date": "May 12, 2023", "tags": ["Large Language Models", "Artificial Intelligence", "Machine Learning", "Financial"]},
{"title": "Using LLM’s for Retrieval and Reranking", "content": "Summary This blog post outlines some of the core abstractions we have created in  LlamaIndex  around LLM-powered retrieval and reranking, which helps to create enhancements to document retrieval beyond naive top-k embedding-based lookup. LLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost. We show how using embedding-based retrieval as a first-stage pass, and second-stage retrieval as a reranking step can help provide a happy medium. We provide results over the Great Gatsby and the Lyft SEC 10-k. Two-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based reranking Introduction and Background There has been a wave of “Build a chatbot over your data” applications in the past few months, made possible with frameworks like  LlamaIndex  and  LangChain . A lot of these applications use a standard stack for retrieval augmented generation (RAG): Use a vector store to store unstructured documents (knowledge corpus) Given a query, use a  retrieval model  to retrieve relevant documents from the corpus, and a  synthesis model  to generate a response. The  retrieval model  fetches   the top-k documents by embedding similarity to the query. In this stack, the retrieval model is not a novel idea; the concept of top-k embedding-based semantic search has been around for at least a decade, and doesn’t involve the LLM at all. There are a lot of benefits to embedding-based retrieval: It’s very fast to compute dot products. Doesn’t require any model calls during query-time. Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There’s a class of queries where embedding-based retrieval returns very relevant results. Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant context to the query, which in turn degrades the quality of the overall RAG system, regardless of the quality of the LLM. This is also not a new problem: one approach to resolve this in existing IR and recommendation systems is to create a  two stage process . The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision. Then the second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to “rerank” the existing retrieved candidates. Covering the downsides of embedding-based retrieval is worth an entire series of blog posts. This blog post is an initial exploration of an alternative retrieval method and how it can (potentially) augment embedding-based retrieval methods. LLM Retrieval and Reranking Over the past week, we’ve developed a variety of initial abstractions around the concept of “LLM-based” retrieval and reranking. At a high-level, this approach uses the LLM to decide which document(s) / text chunk(s) are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric. Simple diagram of how LLM-based retrieval works An example prompt would look like the following: \n\n A list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.\n  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n  as the relevance score. The relevance score is a number from 1–10 based on how relevant you think the document is to the question.\n  Do not include any documents that are not relevant to the question.\n  Example format:\n  Document 1:\n  <summary of document 1>\n  Document 2:\n  <summary of document 2>\n  …\n  Document 10:\n  <summary of document 10>\n  Question: <question>\n  Answer:\n  Doc: 9, Relevance: 7\n  Doc: 3, Relevance: 4\n  Doc: 7, Relevance: 3\n  Let's try this now:\n  {context_str}\n  Question: {query_str}\n  Answer: \n\n\n\n\n\n\n The prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document: You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk. You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new  document summary index , but you can also choose to do it yourself. Given a collection of documents, we can then create document “batches” and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches. You can use our abstractions in two forms: as a standalone retriever module ( ListIndexLLMRetriever ) or a reranker module ( LLMRerank ). The remainder of this blog primarily focuses on the reranker module given the speed/cost. LLM Retriever (ListIndexLLMRetriever) This module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index. from  llama_index  import  GPTListIndex\n from  llama_index.indices. list .retrievers  import  ListIndexLLMRetriever\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\n # high - level API \nquery_str =  \"What did the author do during his time in college?\" \nretriever = index.as_retriever(retriever_mode= \"llm\" )\nnodes = retriever.retrieve(query_str)\n # lower-level API \nretriever = ListIndexLLMRetriever()\nresponse_synthesizer = ResponseSynthesizer.from_args()\nquery_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\nresponse = query_engine.query(query_str) Use Case:  This could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes. LLM Reranker (LLMRerank) This module is defined as part of our  NodePostprocessor  abstraction, which is defined for second-stage processing after an initial retrieval pass. The postprocessor can be used on its own or as part of a  RetrieverQueryEngine  call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index. from  llama_index.indices.query.schema  import  QueryBundle\nquery_bundle = QueryBundle(query_str)\n # configure retriever \nretriever = VectorIndexRetriever(\nindex=index,\nsimilarity_top_k=vector_top_k,\n)\nretrieved_nodes = retriever.retrieve(query_bundle)\n # configure reranker \nreranker = LLMRerank(choice_batch_size= 5 , top_n=reranker_top_n, service_context=service_context)\nretrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle) Limitations/Caveats There are certain limitations and caveats to LLM-based retrieval, especially with this initial version. LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete. Using third-party LLM API’s costs money. The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents. Using the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful. Initial Experimental Results Let’s take a look at how well LLM reranking works! We show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don’t showcase as many results given that it tends to run a lot slower than either of the first two approaches). We analyze results over two very different sources of data: the Great Gatsby and the 2021 Lyft SEC 10-k. We only analyze results over the “retrieval” portion and not synthesis to better isolate the performance of different retrieval methods. The results are presented in a qualitative fashion. A next step would definitely be more comprehensive evaluation over an entire dataset! The Great Gatsby In our first example, we load in the Great Gatsby as a  Document  object, and build a vector index over it (with chunk size set to 512). # LLM Predictor (gpt-3.5-turbo) + service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n# load documents\ndocuments = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context) We then define a  get_retrieved_nodes  function — this function can either do just embedding-based retrieval over the index, or embedding-based retrieval + reranking. def   get_retrieved_nodes ( \n    query_str, vector_top_k= 10 , reranker_top_n= 3 , with_reranker= False \n ):\n  query_bundle = QueryBundle(query_str)\n   # configure retriever \n  retriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=vector_top_k,\n  )\n  retrieved_nodes = retriever.retrieve(query_bundle)\n   if  with_reranker:\n     # configure reranker \n    reranker = LLMRerank(choice_batch_size= 5 , top_n=reranker_top_n, service_context=service_context)\n    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n   return  retrieved_nodes We then ask some questions. With embedding-based retrieval we set k=3. With two-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based reranking. Question: ”Who was driving the car that hit Myrtle?” (For those of you who are not familiar with the Great Gatsby, the narrator finds out later on from Gatsby that Daisy was actually the one driving the car, but Gatsby takes the blame for her). The top retrieved contexts are shown in the images below. We see that in embedding-based retrieval, the top two texts contain semantics of the car crash but give no details as to who was actually responsible. Only the third text contains the proper answer. Retrieved context using top-k embedding lookup (baseline) In contrast, the two-stage approach returns just one relevant context, and it contains the correct answer. Retrieved context using two-stage pipeline (embedding lookup then rerank) 2021 Lyft SEC 10-K We want to ask some questions over the 2021 Lyft SEC 10-K, specifically about the COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a ctrl-f for “COVID-19” returns 127 matches. We use a similar setup as the Gatsby example above. The main differences are that we set the chunk size to 128 instead of 512, we set k=5 for the embedding retrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage approach. We then ask the following questions and analyze the results. Question: “What initiatives are the company focusing on independently of COVID-19?” Results for the baseline are shown in the image above. We see that results corresponding to indices 0, 1, 3, 4, are about measures directly in response to Covid-19, even though the question was specifically about company initiatives that were independent of the COVID-19 pandemic. Retrieved context using top-k embedding lookup (baseline) We get more relevant results in approach 2, by widening the top-k to 40 and then using an LLM to filter for the top-5 contexts. The independent company initiatives include “expansion of Light Vehicles” (1), “incremental investments in brand/marketing” (2), international expansion (3), and accounting for misc. risks such as natural disasters and operational risks in terms of financial performance (4). Retrieved context using two-stage pipeline (embedding lookup then rerank) Conclusion That’s it for now! We’ve added some initial functionality to help support LLM-augmented retrieval pipelines, but of course there’s a ton of future steps that we couldn’t quite get to. Some questions we’d love to explore: How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.) What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance. Exploring different prompts and text summarization methods to help determine document relevance Exploring if there’s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?) Resources You can play around with the notebooks yourself! Great Gatsby Notebook 2021 Lyft 10-K Notebook", "author": "Jerry Liu", "date": "May 17, 2023", "tags": ["Artificial Intelligence", "Machine Learning", "Large Language Models", "Llamaindex", "NLP"]},
{"title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions", "content": "One of the many amazing feats that LLMs are capable of is generating executable code. This can be used to solve a variety of complex problems that require calculations and fixed logic that traditional computing excels at but LLMs can struggle to perform directly. When building agents to perform complex tasks, equipping your agent with code execution as an available tool can be a powerful strategy. However, this strategy comes with a major drawback: executable code can be flawed or even dangerous to execute, and detecting whether code will be problematic prior to executing it is arguably an expression of the  Halting Problem , making it impossible to guarantee success at detection. The solution is  sandboxing , to isolate potentially problematic code from the host environment. Now, thanks to dynamic sessions in Azure Container Apps, the ability to execute sandboxed code generated by an LLM is simple directly from LlamaIndex. It’s implemented as a tool that can be used by any LlamaIndex agent. In this blog post we’ll show you exactly how to use the new Azure Code Interpreter tool and walk you through a couple of examples of how to make the most of it. You can see the full code in  this notebook  and read more in the  tool documentation  on LlamaHub and on  learn.microsoft.com . Set up Azure Container Apps dynamic sessions First, install our python packages including the tool: pip install llama-index\npip install llama-index-llms-azure\npip install llama-index-tools-azure-code-interpreter In  the notebook  we’re using GPT 3.5 Turbo hosted on Azure as the LLM, but you can use any LLM capable of tool use: from  llama_index.llms.azure_openai  import  AzureOpenAI\nllm = AzureOpenAI(\n    model= \"gpt-35-turbo\" ,\n    deployment_name= \"gpt-35-deploy\" ,\n    api_key=api_key,\n    azure_endpoint=azure_endpoint,\n    api_version=api_version,\n) Once you’ve got your LLM set up, you’ll need to  create a session pool  to host your executions. Doing this will give you a pool management endpoint URL that you can provide to LlamaIndex like this: # Import the AzureCodeInterpreterToolSpec from llama_index \n from  llama_index.tools.azure_code_interpreter  import  (\n    AzureCodeInterpreterToolSpec,\n)\n\n # Create the AzureCodeInterpreterToolSpec with the pool_managment_endpoint set to your session management endpoint \n # It is optional to set the local_save_path, but it is recommended to set it to a path where the tool can automatically save any intermediate data generated from Python code's output. \nazure_code_interpreter_spec = AzureCodeInterpreterToolSpec(\n    pool_managment_endpoint= \"your-pool-management-endpoint\" ,\n    local_save_path= \"local-file-path-to-save-intermediate-data\" ,\n) This sets up a tool ready to be used with LlamaIndex. You’re now ready to set up your agent: # Import the ReActAgent \n from  llama_index.core.agent  import  ReActAgent\n\n # Create the ReActAgent and inject the tools defined in the AzureDynamicSessionsToolSpec \nagent = ReActAgent.from_tools(\n    azure_code_interpreter_spec.to_tool_list(), llm=llm, verbose= True \n) In this example we’re providing only a single tool, but you could provide any other tools you like to your ReAct agent. Now you’ve got an agent, you’re ready to ask it to perform tasks! Dynamic sessions code interpreter in action In our first example, we’re going to ask the agent the time in Seattle. This is usually a tricky task for LLMs, which don’t know what time it is anywhere! # Test the agent with simple answers that could leverage Python codes \n print (agent.chat( \"Tell me the current time in Seattle.\" )) The agent generates python code to determine the time and convert it to the correct time zone. It passes this code to Azure Container Apps dynamic sessions, which execute the code and return the answer: Thought: To provide the current time in Seattle, I need to calculate it based on the current UTC time and adjust for Seattle's time zone, which is Pacific Daylight Time (PDT) during daylight saving time and Pacific Standard Time (PST) outside of daylight saving time. PDT is UTC-7, and PST is UTC-8. I can use the code interpreter tool to get the current UTC time and adjust it accordingly.\nAction: code_interpreter\nAction Input: {'python_code': \"from datetime import datetime, timedelta; import pytz; utc_now = datetime.now(pytz.utc); seattle_time = utc_now.astimezone(pytz.timezone('America/Los_Angeles')); seattle_time.strftime('%Y-%m-%d %H:%M:%S %Z%z')\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '2024-05-04 13:54:09 PDT-0700', 'executionTimeInMilliseconds': 120}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The current time in Seattle is 2024-05-04 13:54:09 PDT.\nThe current time in Seattle is 2024-05-04 13:54:09 PDT.\n You can also use the tool to safely inspect and manipulate data, as in this example where we ask it to open a CSV file and answer questions about it: # Upload a sample temperature file of a day in Redmond Washington and ask a question about it \nres = azure_code_interpreter_spec.upload_file(\n    local_file_path= \"./TemperatureData.csv\" \n)\n if   len (res) !=  0 :\n     print (\n        agent.chat( \"Find the highest temperature in the file that I uploaded.\" )\n    ) It doesn’t just read data from the CSV, it performs math on it to determine the highest temperature: Thought: I need to use the list_files tool to get the metadata for the uploaded file, and then use python to read the file and find the highest temperature.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: I have the metadata for the file. I need to use python to read the file and find the highest temperature.\nAction: code_interpreter\nAction Input: {'python_code': \"import csv\\n\\nwith open('/mnt/data/TemperatureData.csv', 'r') as f:\\n    reader = csv.reader(f)\\n    next(reader)\\n    highest_temp = float('-inf')\\n    for row in reader:\\n        temp = float(row[1])\\n        if temp > highest_temp:\\n            highest_temp = temp\\nprint(highest_temp)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '12.4\\n', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 26}\nThought: I have the highest temperature. I can answer the question.\nAnswer: The highest temperature in the file is 12.4 degrees.\nThe highest temperature in the file is 12.4 degrees. The tool can also safely modify your uploaded files, as in this example where we get it to sort the data in the CSV: # Ask the LLM to perform modifications on the sample temperature file \n print (\n    agent.chat(\n         \"Rearrange the temperature data in a descending order and save it back to the original csv file.\" \n    )\n) In this example, you’ll see that the LLM initially produces code with a syntax error, but the agent is smart enough to figure out what went wrong and correct itself: Thought: To proceed, I need to first list the files in the session pool to identify the uploaded temperature data file.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureCurve.png', size_in_bytes=35512, file_full_path='/mnt/data/TemperatureCurve.png'), RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: The temperature data file is identified as 'TemperatureData.csv'. I need to read its contents, rearrange the temperature data in descending order, and then save the changes back to the original CSV file.\nAction: code_interpreter\nAction Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\\n'filename': 'TemperatureData.csv'\"}\nObservation: {'$id': '1', 'status': 'Failure', 'stdout': '', 'stderr': \"illegal target for annotation (3407605186.py, line 6)  Cell In[2363], line 6\\n    'filename': 'TemperatureData.csv'\\n    ^\\nSyntaxError: illegal target for annotation\\n\\n\", 'result': None, 'executionTimeInMilliseconds': 9}\nThought: There was a syntax error in the Python code due to an incorrect line. I need to correct the code and execute it again to sort the temperature data and save it back to the CSV file.\nAction: code_interpreter\nAction Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 91}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.\nThe temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'. Modifying files would not be useful if you couldn’t retrieve them after modification, which is done like this: # Download the modified file \nazure_code_interpreter_spec.download_file_to_local(\n    remote_file_path= \"TemperatureData.csv\" ,\n    local_file_path= \"/.../SortedTemperatureData.csv\" ,\n) Endless possibilities The scope of tasks that you can achieve with sandboxed code execution is as broad as programming itself, and having safe execution guaranteed allows you to confidently hand agents tasks that previously you might have been hesitant about. We think this is an amazing addition to our LLM agent capabilities and we’re excited to see what you build with it.", "author": "LlamaIndex", "date": "May 21, 2024", "tags": ["Microsoft", "Azure"]},
{"title": "LlamaIndex Newsletter 2024-05-21", "content": "Hello LlamaIndex Community! 🦙 Welcome to another exciting weekly update from LlamaIndex! Last week was a standout in the AI world with significant updates on GPT-4o and Gemini models. We're thrilled to bring you a host of exceptional integration updates, guides, tutorials, webinars, and events. 🤩  The highlights: LlamaIndex on Vertex AI (Google Cloud):  Introducing the new RAG API powered by advanced LlamaIndex modules on Vertex AI (Google Cloud).  Docs ,  Tweet . Enhanced Document Parsing with GPT-4o:  Integrated GPT-4o in LlamaParse for superior document parsing.  Notebook ,  Tweet . Cookbook on Structured Image Extraction with GPT-4o:  Check out our new cookbook on using GPT-4o for Structured Image Extraction.  Notebook ,  Tweet . ✨ Feature Releases and Enhancements: We have launched LlamaIndex on Vertex AI (Google Cloud) to introduce the new RAG API, enhanced by LlamaIndex's advanced modules. This integration simplifies setup and enhances user access for the developers with the flexibility to connect a variety of data sources and file types. It fully supports the latest LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini 1.0 models.  Docs ,  Tweet . We have introduced GPT-4o with LlamaParse, offering enhanced document parsing into markdown for complex files, ensuring higher data quality for your RAG pipeline. Note the increased cost of $0.60 USD per page. Note the increased cost of $0.60 USD per page compared to the standard $0.003 per page.  Notebook ,  Tweet . We have released a cookbook on using GPT-4o for Structured Image Extraction, showing how to convert images into structured JSONs with a 0% failure rate and higher quality than GPT-4V.  Notebook ,  Tweet . LlamaParse integration with Quivr to enhance document parsing capabilities. Now, you can easily process complex documents like PDFs, PPTX, and Markdown files, ensuring clean data storage and accurate retrieval in your personalized AI assistants.  Docs ,  Tweet . 🗺️ Guides: Guide  to Enhanced QA with LlamaParse on complex tables like train schedules. This approach uses spatial text layout and GPT-4o to preserve essential information, ensuring accurate and error-free data interpretation. Guide  to Speeding Up Vector Search with Minimal Accuracy Loss using Jina Embeddings to achieve 32x faster vector search performance at just a 4% cost in accuracy. It involves encoding your data as binary digits, significantly reducing storage and compute requirements. ✍️ Tutorials: Kate Silverstein  tutorial  on building local research assistant**,** set up a local, private research assistant on your laptop effortlessly with Mozilla's llamafile. Tutorial covers everything from downloading and activating the model, to connecting via LlamaIndex and managing your data. Plaban Nayak  tutorial  on Multi-document Agentic RAG using Llama-Index and Mistral. Diptiman Raichaudhuri  tutorial  on fully local Text-to-SQL using DuckDB as the database, Ollama + Mixtral-8x7B as the model, and LlamaIndex for text-to-SQL orchestration. Mandar Karhade  tutorial  on showing an end-to-end experimentation pipeline for iterating on chunk sizes, generating a synthetic dataset, and measuring how it affects evaluation metrics. 📹 Webinar: Join us for a  webinar  on \"Open-Source Longterm Memory for Autonomous Agents\" this Thursday at 9am PT, where we'll explore the memary architecture with Julian Saks, Kevin Li, Seyeong Han and rest of memary team, diving deep into the challenges and future of long-term memory for autonomous systems. 📅 Events: We are having our first-ever  meetup  at our new office in San Francisco!  Join us  to connect with our team and friends from Activeloop and Tryolabs, as we discuss the latest developments in generative AI.", "author": "LlamaIndex", "date": "May 21, 2024", "tags": []},
{"title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs", "content": "We're thrilled to announce a new feature in LlamaIndex that expands our knowledge graph capabilities to be more flexible, extendible, and robust. Introducing the Property Graph Index! Why Property Graphs? Traditional knowledge graph representations like knowledge triples (subject, predicate, object) are limited in expressiveness. They lack the ability to: Assign labels and properties to nodes and relationships Represent text nodes as vector embeddings Perform both vector and symbolic retrieval Our existing  KnowledgeGraphIndex  was burdened with these limitations, as well as general limitations on the architecture of the index itself. The Property Graph Index solves these issues. By using a labeled property graph representation, it enables far richer modeling, storage and querying of your knowledge graph. With Property Graphs, you can: Categorize nodes and relationships into types with associated metadata Treat your graph as a superset of a vector database for hybrid search Express complex queries using the Cypher graph query language This makes Property Graphs a powerful and flexible choice for building knowledge graphs with LLMs. Constructing Your Graph The Property Graph Index offers several ways to extract a knowledge graph from your data, and you can combine as many as you want: 1. Schema-Guided Extraction : Define allowed entity types, relationship types, and their connections in a schema. The LLM will only extract graph data that conforms to this schema. from  llama_index.indices.property_graph  import  SchemaLLMPathExtractor\n\nentities =  Literal [ \"PERSON\" ,  \"PLACE\" ,  \"THING\" ]\nrelations =  Literal [ \"PART_OF\" ,  \"HAS\" ,  \"IS_A\" ]\nschema = {\n     \"PERSON\" : [ \"PART_OF\" ,  \"HAS\" ,  \"IS_A\" ],\n     \"PLACE\" : [ \"PART_OF\" ,  \"HAS\" ], \n     \"THING\" : [ \"IS_A\" ],\n}\n\nkg_extractor = SchemaLLMPathExtractor(\n  llm=llm, \n  possible_entities=entities, \n  possible_relations=relations, \n  kg_validation_schema=schema,\n  strict= True ,   # if false, allows values outside of spec \n) 2. Implicit Extraction : Use LlamaIndex constructs to specify relationships between nodes in your data. The graph will be built based on the  node.relationships  attribute. For example, when running a document through a node parser, the  PREVIOUS ,  NEXT  and  SOURCE  relationships will be captured. from  llama_index.core.indices.property_graph  import  ImplicitPathExtractor\n\nkg_extractor = ImplicitPathExtractor() 3. Free-Form Extraction : Let the LLM infer the entities, relationship types and schema directly from your data in a free-form manner. (This is similar to how the  KnowledgeGraphIndex  works today.) from  llama_index.core.indices.property_graph  import  SimpleLLMPathExtractor\n\nkg_extractor = SimpleLLMPathExtractor(llm=llm) Mix and match these extraction approaches for fine-grained control over your graph structure. from  llama_index.core  import  PropertyGraphIndex\n\nindex = PropertyGraphIndex.from_documents(docs, kg_extractors=[...]) Embeddings By default, all graph nodes are embedded. While some graph databases support embeddings natively, you can also specify and use any vector store from LlamaIndex on top of your graph database. index = PropertyGraphIndex(..., vector_store=vector_store) Querying Your Graph The Property Graph Index supports a wide variety of querying techniques that can be combined and run concurrently. 1. Keyword/Synonym-Based Retrieval : Expand your query into relevant keywords and synonyms and find matching nodes. from  llama_index.core.indices.property_graph  import  LLMSynonymRetriever\n\nsub_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm) 2. Vector Similarity : Retrieve nodes based on the similarity of their vector representations to your query. from  llama_index.core.indices.property_graph  import  VectorContextRetriever\n\nsub_retriever = VectorContextRetriever(\n  index.property_graph_store, \n  vector_store=index.vector_store,\n  embed_model=embed_model,\n) 3. Cypher Queries : Use the expressive Cypher graph query language to specify complex graph patterns and traverse multiple relationships. from  llama_index.core.indices.property_graph  import  CypherTemplateRetriever\n from  llama_index.core.bridge.pydantic  import  BaseModel, Field\n\n class   Params ( BaseModel ):\n “””Parameters  for  a cypher query.”””\n names:  list [ str ] = Field(description=”A  list  of possible entity names  or  keywords related to the query.”)\n \ncypher_query =  \"\"\"\n   MATCH (c:Chunk)-[:MENTIONS]->(o) \n   WHERE o.name IN $names\n   RETURN c.text, o.name, o.label;\n\"\"\" \n   \nsub_retriever = CypherTemplateRetriever(\n index.property_graph_store, \n Params, \n cypher_query,\n llm=llm,\n) Instead of providing a template, you can also let the LLM write the entire cypher query based on context from the query and database: from  llama_index.core.indices.property_graph  import  TextToCypherRetriever\n\nsub_retriever = TextToCypherRetriever(index.property_graph_store, llm=llm) 4. Custom Graph Traversal : Define your own graph traversal logic by subclassing key retriever components. These retrievers can be combined and composed for hybrid search that leverages both the graph structure and vector representations of nodes. from  llama_index.indices.property_graph  import  VectorContextRetriever, LLMSynonymRetriever\n\nvector_retriever = VectorContextRetriever(index.property_graph_store, embed_model=embed_model)  \nsynonym_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)\n\nretriever = index.as_retriever(sub_retrievers=[vector_retriever, synonym_retriever]) Using the Property Graph Store Under the hood, the Property Graph Index uses a  PropertyGraphStore  abstraction to store and retrieve graph data. You can also use this store directly for lower-level control. The store supports: Inserting and updating nodes, relationships and properties Querying nodes by ID or properties Retrieving relationship paths from a starting node Executing Cypher queries (if the backing store supports it) from  llama_index.graph_stores.neo4j  import  Neo4jPGStore\n\ngraph_store = Neo4jPGStore(\n    username= \"neo4j\" ,\n    password= \"password\" ,\n    url= \"bolt://localhost:7687\" ,\n)\n\n # insert nodes \nnodes = [\n    EntityNode(name= \"llama\" , label= \"ANIMAL\" , properties={ \"key\" :  \"value\" }),\n    EntityNode(name= \"index\" , label= \"THING\" , properties={ \"key\" :  \"value\" }), \n]\ngraph_store.upsert_nodes(nodes)\n\n # insert relationships   \nrelations = [\n    Relation(\n        label= \"HAS\" ,\n        source_id=nodes[ 0 ]. id , \n        target_id=nodes[ 1 ]. id ,\n    )\n]\ngraph_store.upsert_relations(relations)\n\n # query nodes \nllama_node = graph_store.get(properties={ \"name\" :  \"llama\" })[ 0 ]\n\n # get relationship paths   \npaths = graph_store.get_rel_map([llama_node], depth= 1 )\n\n # run Cypher query \nresults = graph_store.structured_query( \"MATCH (n) RETURN n LIMIT 10\" )   Several backing stores are supported, including in-memory, disk-based, and Neo4j. Learn More Property Graph Index Overview Basic Usage Notebook Advanced Usage with Neo4j Using the Property Graph Store Directly A huge thanks to our partners at  Neo4j  for their collaboration on this launch, especially  Tomaz Bratanic  for the detailed integration guide and design guidance. We can't wait to see what you build with the new Property Graph Index! As always, feel free to join our  Discord  to share your projects, ask questions, and get support from the community. Happy building! The LlamaIndex Team", "author": "LlamaIndex", "date": "May 29, 2024", "tags": ["Knowledge Graphs"]},
{"title": "Dumber LLM Agents Need More Constraints and Better Tools", "content": "Summary In this article, we compare how well LLM-powered agents with different degrees of complexity perform over practical data tasks (financial analysis). We compare the performance of agents with more  complex, unrestrained  interaction behavior (ReAct) with agents that contain  simpler, more constrained  interactions (routing). We specifically analyze how much complexity can be added to the agent layer vs. the tool layer. We find that the choice of the language model matters a lot. ReAct agents that are powered by “dumber” models (in a tongue-in-cheek fashion we are referring to any non GPT-4 model as “dumb”) struggle to return relevant results over data. We find that constraining agent interaction behavior, and giving them access to more tools that can more explicitly perform complex actions, can help improve query performance over these less sophisticated LLMs. In contrast, more sophisticated models (GPT-4) can more reliably utilize the ReAct loop to execute a variety of complex data queries. This blog post is quite detailed; we provide a  lot  of experiments and results below. Best of all, you can run this all yourself with our  example notebook ! Overview of Agents Building LLM-powered agents have gotten increasingly popular in the past few months. Frameworks like  LangChain  have made it much easier to create these agents according to a set of common abstractions. At a high-level, an “agent” is essentially an automated decision engine, that can be used to interact with an external environment. The core agent loop looks something like the following: The agent has access to a set of “tools”, which are generic functions that it can perform. It has an awareness of each tool through some attached metadata, and it can call each tool (either as a function call or structured API). User feeds in a natural language input to the agent. Given the input, the agent  interacts with the set of tools  in some fashion and returns the response. There’s a variety of ways to perform  agent-tool interaction. The most popular is probably  ReAct : the agent reasons over the next action, constructs an action command, executes the action. It repeats these steps in an iterative loop until the task is complete. There are other interaction modes too. Recently there was a paper on  Plan-and-solve Prompting , which generates a plan beforehand (to decompose a complex task into simpler ones). Before ReAct there have also been related techniques on  Self-Ask  and  Chain of Thought Prompting . “Complex” vs. “Simple” Agent Interaction Techniques We classify techniques like ReAct are more  complex and unconstrained:  this is because they perform iterative reasoning and also break the input into smaller steps. Complicated agent interaction loops allow for more  freedom of behavior,  and   create an increased burden on the LLM being used. The pro of complex interaction frameworks is that they can be more general and handle a broader class of queries over simple tools. The con is that if the LLM is not up to par, then these frameworks are prone to making mistakes; unconstrained behavior can lead to unexpected results. On the other end of the spectrum, you can imagine a  simple and constrained  agent interaction mechanism, where the agent does one-step selection of the underlying tool to use, and returns the response from the tool. The agent essentially just acts as a router from the query to Tool. There are no steps to break down the question into smaller ones, and no iterative chain-of-thought loops. The pro here is that the model will likely make fewer errors. The con here is that the interaction technique allows for less freedom and imposes more constraints on behavior. Investigating Agent Interaction Techniques for Data Querying We at LlamaIndex are interested in how agents can help augment data tasks. More specifically, we are interested in how agents can help perform complex user queries over a diverse range of data sources. This includes not only asking questions over a single document, but being able to synthesize insights across multiple documents and return that to the user. LlamaIndex query engines can be used as Tools within an agent construct to query your data (we provide  seamless integrations with LangChain ). These Tools can vary in complexity. For instance, a  simple  Tool could be our  vector store query engine , which does top-k embedding retrieval from a vector store. A more  advanced  tool could be a query engine over our graph data structure, which can be setup to  explicitly provide compare/contrast capabilities  over any subset of documents. The tool itself can contain “agent-like” decision-making capabilities under the hood. LlamaIndex provides a variety of modules around  routing ,  query decomposition , and  multi-part query planning . In this blog post, we are interested in comparing the following approaches to designing agents and tools to see which approach can provide good answers to different user queries in a robust fashion: more  complex and unconstrained  agent interaction (ReAct) over a set of  simple  Tools more  simple and constrained  agent interaction (simple routing) that uses more  complex  Tools Complex Agents with Simple Tools, Simple Agents with Complex Tools Essentially what we are interested in is how much complexity can be pushed to the agent interaction layer vs. being left in the Tool layer. We explore the following concrete example: let’s say the user query is to compare/contrast two different documents (a relatively complex query). If the set of Tools are all just vector indices over different documents, could the agent interaction loop figure out how to execute that query reliably against the vector indices? On the other hand, if we push the complexity down to the Tool layer, then we could  explicitly  have a Tool that can perform “compare/contrast” over your Documents. Then the burden on the agent is to simply call this Tool instead of interacting with a set of other tools in a more complex fashion. High-Level Findings The high-level finding is that  less sophisticated agents need more constraints.  More specifically, we found that using a GPT-3 powered agent in a ReAct loop did not provide good results over complex queries; it was not able to figure out the proper interaction pattern over the provided set of Tools in order to surface the results. Instead, by adding more constraints to the agent behavior and providing more sophistication in the Tool itself, we were able to get a GPT-3 agent to produce better results. Smarter agents require fewer constraints.  We did find that GPT-4 agents with ReAct were able to provide better query results than GPT-3 agents when presented with a set of simple Tools over the data. This implies that more powerful agents may not need as many tools to “explicitly” perform tasks when much of that logic can be handled in the agent interaction loop. Setup Our data consists of three Uber 10-Q filings (quarterly financial reports) in 2022: March, June, and September. We wish to execute different queries over this data; the bulk of these queries are around comparing different bits of information between these documents. march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\njune_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\nsept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data() We use LlamaIndex to define a vector index over each document, which just stores the document chunks + embeddings in a vector store. We can then query each vector index using a simple QueryEngine  . We create a tool for each of these QueryEngine  objects. # define indices\nmarch_index = GPTVectorStoreIndex.from_documents(march_2022)\njune_index = GPTVectorStoreIndex.from_documents(june_2022)\nsept_index = GPTVectorStoreIndex.from_documents(sept_2022)\n\n# define query engine\nmarch_engine = march_index.as_query_engine(similarity_top_k=3)\njune_engine = june_index.as_query_engine(similarity_top_k=3)\nsept_engine = sept_index.as_query_engine(similarity_top_k=3) We also define a  ComposableGraph  over these three documents. The composable graph roughly follows the  guide described here . This graph is explicitly setup to perform compare/contrast queries over these three documents. graph = ComposableGraph.from_indices(\n    GPTListIndex,\n    children_indices=[march_index, june_index, sept_index],\n    index_summaries=[\n        \"Provides information about Uber quarterly financials ending March 2022\",\n        \"Provides information about Uber quarterly financials ending June 2022\",\n        \"Provides information about Uber quarterly financials ending September 2022\"\n    ]\n) The graph can be queried with a  ComposableGraphQueryEngine  : # define decompose_transform \ndecompose_transform = DecomposeQueryTransform(verbose= True )\n\n # define custom query engines \ncustom_query_engines = {}\n for  index  in  [march_index, june_index, sept_index]:\n    query_engine = index.as_query_engine(service_context=service_context)\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={ 'index_summary' : index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\n\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    service_context=service_context,\n    streaming= True ,\n)\n\n # define graph \ng_engine = graph.as_query_engine(\n    custom_query_engines=custom_query_engines\n) We try the following agent setups: GPT-3 ReAct agent:  A zero-shot GPT-3 ReAct agent with three Tools: each Tool corresponds to the vector index over a 10-Q filing. GPT-4 ReAct agent:  Same as above but using GPT-4 instead. Simple Router agent:  A simple router “agent” with four Tools: the three Tools listed above + the  ComposableGraphQueryEngine  explicitly setup to perform compare/contrast queries. The code snippets for initializing these agents are below. For the simple router agent, we use the native  RouterQueryEngine  within LlamaIndex, though you should also be able to achieve similar results in LangChain through either the zero-shot agent (with tweaked settings) or the router chain. GPT-3/GPT-4 ReAct Agent Setup # initializing zero-shot ReAct agent \n\nuber_config_sept = IndexToolConfig(\n    query_engine=sept_engine, \n    name= f\"Uber 10Q September 2022\" ,\n    description= f\"Provides information about Uber quarterly financials ending September 2022\" ,\n    tool_kwargs={ \"return_direct\" :  False }\n)\nuber_config_june = IndexToolConfig(\n    query_engine=june_engine, \n    name= f\"Uber 10Q June 2022\" ,\n    description= f\"Provides information about Uber quarterly financials ending June 2022\" ,\n    tool_kwargs={ \"return_direct\" :  False }\n)\nuber_config_march = IndexToolConfig(\n    query_engine=march_engine, \n    name= f\"Uber 10Q March 2022\" ,\n    description= f\"Provides information about Uber quarterly financials ending March 2022\" ,\n    tool_kwargs={ \"return_direct\" :  False }\n)\n\ntoolkit = LlamaToolkit(\n    index_configs=[uber_config_sept, uber_config_june, uber_config_march],\n)\n\n # this is a light wrapper around `initialize_agent` in langchain (which defaults to zero-shot) \nagent_chain = create_llama_agent(\n    toolkit,\n    llm,  # can be GPT-3 or GPT-4  \n    verbose= True \n) Simple Router Agent Setup \nquery_tool_sept = QueryEngineTool.from_defaults(\n    query_engine=sept_engine,\n    description=f\"Provides information about Uber quarterly financials ending September 2022\",\n)\nquery_tool_june = QueryEngineTool.from_defaults(\n    query_engine=june_engine,\n    description=f\"Provides information about Uber quarterly financials ending June 2022\",\n)\nquery_tool_march = QueryEngineTool.from_defaults(\n    query_engine=march_engine,\n    description=f\"Provides information about Uber quarterly financials ending March 2022\",\n)\nquery_tool_graph = QueryEngineTool.from_defaults(\n    query_engine=g_engine,\n    description=f\"Provides comparisons between Uber financials across quarters in 2022. Can be used to answer \"\n                 \"any questions that require analysis across multiple quarters.\",\n)\n\n# our \"router\" query engine is effectively a simple agent that can only perform routing\nquery_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(),\n    query_engine_tools=[\n        query_tool_sept,\n        query_tool_june,\n        query_tool_march,\n        query_tool_graph\n    ]\n) Now that we’ve described the setup, let’s take a look at the results below! Findings and Experiments At a high-level, we find using GPT-3 in ReAct agents produces suboptimal results over these queries. They tend to exhibit the following characteristics: Unpredictability in the set of chosen tools:  The set of tools chosen can differ even if the questions are semantically similar, leading to variability in the responses. Lack of coverage in the set of chosen tools:  Oftentimes we expect that a given question is able to make use of all three 10-Q statements, but only a subset of them are picked. Erroneous chain-of-thought processing:  Sometimes the agent uses tools throughout the CoT process that are irrelevant to the question. In contrast, we find that GPT-4 ReAct agents provide answers that are more relevant, predictable, and exhibit fewer errors in intermediate results. Finally, we find that using a simpler routing-only GPT-3 agent with access to an explicit “compare/contrast” tool allows the agent to perform better. As a reminder, full results are in the notebook:  https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing GPT-3 ReAct Agent Results Query 1 agent_chain.run(input=\"Analyze Uber revenue growth over the last few quarters\") Response: We see that only the September 10-Q filing is chosen to answer the question. The September 10-Q does contain some information about revenue growth compared to the same period in 2021, but that doesn’t explicitly answer the question, which is about revenue growth the past few quarters. Query 2 agent_chain.run(input=\"Analyze changes in risk factors for Uber\") Response: The September and June 10-Q filings are chosen, but not March. Moreover, the answer is vague and doesn’t provide much detail regarding concrete risk factors for Uber (and also mentions that the risk factors “have changed over the past three quarters” even though it’s only using two Tools). Query 3 In this query, we more explicitly showcase how slight changes in prompts can induce different chain-of-thought paths through different Tools, and as a result produce different answers. # Prompt variation 1 \nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over time\") Response: # Prompt variation 2\nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over quarters\") The main difference between these two queries is “over time” versus “over quarters.” As we can see, not only are the selected Tools different between the two variations, but the inputs are different as well — in the first it’s “financials”, and in the second it’s “Revenue growth and risk factors.” Since the Tool input in the first variant is unrelated to the question, the answer is similarly vague: “Uber’s revenue growth and risk factors can be analyzed by comparing the financials…” Query 4: Here instead of asking a compare/contrast question let’s just ask a question about a given statement. agent_chain.run(input=\"How much cash did Uber have in sept 2022?\") We see that the agent makes two errors 1) it is not able to supply an action input to each Tool, and 2) ends up looking through the June and March filings which are irrelevant to the question. GPT-4 ReAct Agent Results GPT-4 ReAct agents perform a lot better than GPT-3 agents. They comprehensively go through the set of available Tools, and provide much more detailed observation extraction and response synthesis. We won’t go through all of these examples, but they can be found in the example notebook! Query 1: agent_chain_gpt4.run(input=\"Analyze Uber revenue growth over the last few quarters\") Response: Unlike the GPT-3 agent, here the GPT-4 agent at least goes through every filing and synthesizes the result. Query 2 agent_chain_gpt4.run(input=\"Analyze changes in risk factors for Uber\") Response: Here the GPT-4 agent still only looks at September and June (and skips March), but the response is way more detailed, and references concrete facts within the report. GPT-3 Router Agent Results Reminder: the router agent doesn’t do any CoT and has additional access to our ComposableGraph query engine, which can explicitly perform compare/contrast queries. Let’s take a look at how this agent does. Query 1 response = query_engine.query(\"Analyze Uber revenue growth over the last few quarters\") Response (intermediate steps): Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n> Current query: Analyze Uber revenue growth over the last few quarters\n> New query:  What was Uber's revenue growth from the last quarter ending March 2022 compared to the previous quarter?\n> Current query: Analyze Uber revenue growth over the last few quarters\n> New query:  What was Uber's revenue growth from the last quarter ending June 2022 compared to the previous quarter?\n> Current query: Analyze Uber revenue growth over the last few quarters\n> New query:  What was Uber's revenue growth from the last few quarters ending September 2022?\n Final Response Based on the limited context information provided, we can only analyze Uber's revenue growth for two specific quarters:\n\n1. For the quarter ending March 2022, Uber's revenue growth increased compared to the first quarter of 2021. However, the exact percentage of growth is not provided.\n\n2. For the quarter ending September 2022, Uber's revenue growth was 72%.\n\nUnfortunately, we do not have enough data to analyze the revenue growth for the quarters ending June 2022 or to compare it to the previous quarter. To provide a more comprehensive analysis, additional data on Uber's revenue growth for other quarters would be needed. To be fair, this response does not cover the month of June. But the fault of that lies with the implementation of the Tool itself rather than the agent interaction step. The agent’s only job is to pick the right Tool to solve the task at hand, and it does that job correctly; it decides to choose the Tool corresponding to our  ComposableGraphQueryEngine  abstraction (which can perform compare/contrast queries). Query 2 response = query_engine.query(\"Analyze changes in risk factors for Uber\") Response (intermediate steps): INFO :llama_index .query_engine. router_query_engine: Selecting  query engine  3 :  Provides  comparisons between  Uber  financials across quarters  in   2022 .  Can  be used to answer any questions that  require  analysis across multiple quarters..\n Selecting  query engine  3 :  Provides  comparisons between  Uber  financials across quarters  in   2022 .  Can  be used to answer any questions that  require  analysis across multiple quarters..\n&gt;  Current   query:   Analyze  changes  in  risk factors  for   Uber \n&gt;  New   query:    What  are the risk factors  for   Uber   in  the quarter ending  March   2022 ?\n&gt;  Current   query:   Analyze  changes  in  risk factors  for   Uber \n&gt;  New   query:    What  are the risk factors  for   Uber   in  the quarter ending  June   2022 ?\n&gt;  Current   query:   Analyze  changes  in  risk factors  for   Uber \n&gt;  New   query:    What  are the risk factors  for   Uber   in  the quarter ending  September   2022 ? Final Response From the context information provided, we can analyze the changes in risk factors  for  Uber across the three quarters as follows:\n\n 1.  COVID -19  pandemic: The ongoing pandemic remains a constant risk factor across all quarters, with its impact on driver  and  courier supply constraints, consumer demand,  and  government restrictions. However, the specific concerns related to the pandemic may evolve, such as the emergence of  new  virus variants  or  changes in vaccination rates.\n\n 2.  Driver  and  courier supply constraints: This risk factor is present in the quarter ending March  2022  but is  not  explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially  if  the pandemic continues to impact driver availability.\n\n 3.  Changes in consumer demand: This risk factor is mentioned in the quarter ending March  2022 , with an imbalance between Mobility  and  Delivery services. It is  not  explicitly mentioned in the subsequent quarters, but fluctuations in revenue due to changes in Gross Bookings  and  Mobility Trip volumes in the quarter ending June  2022  could be related to changes in consumer demand.\n\n 4.  Government restrictions  and  regulations: This risk factor is present in the quarter ending March  2022 , with the impact of restrictions on Mobility offerings. In the quarter ending June  2022 , the focus shifts to the potential impact of regulatory changes  and  legal claims, such as driver classification in the UK. In the quarter ending September  2022 , the risk factor of driver classification as employees, workers,  or  quasi-employees becomes more prominent.\n\n 5.  Uncertainty around future developments: This risk factor is mentioned in the quarter ending March  2022  but is  not  explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially with the ongoing impact of COVID -19   and  potential permanent changes to end-users '  behavior.\n\n 6.  Accounting pronouncements: This risk factor is introduced in the quarter ending June  2022 , with the adoption  and  impact of recently issued accounting pronouncements potentially affecting Uber ' s consolidated financial statements.\n\n 7.  Integration  and  performance of acquired businesses: This risk factor is introduced in the quarter ending June  2022 , with the acquisition of Transplace contributing to the increase in Freight revenue.\n\n 8.  Competition: This risk factor is introduced in the quarter ending September  2022 , highlighting the highly competitive nature of the mobility, delivery,  and  logistics industries,  and  the need to lower fares, offer incentives,  and  promotions to remain competitive.\n\n 9.  Significant losses  and  increasing operating expenses: This risk factor is introduced in the quarter ending September  2022 , emphasizing the financial challenges Uber faces in achieving profitability. The job of the agent is once again simple: it chooses the graph query engine that is optimized to perform “compare/contrast” queries. The result is much more detailed than the result obtained from the GPT-3 ReAct agent (and is comparable to the GPT-4 ReAct agent). It contains a detailed analysis of all risk factors in bullet points, and offers a comparison across time for all bullet points. Query 3 response = query_engine.query(\"Analyze Uber revenue growth and risk factors over the last few quarters\") Response (intermediate steps): > Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What were Uber's total revenues for the quarter ending September 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What were Uber's total revenues for the quarter ending September 2022? Final Response Uber 's revenue growth has been impressive over the past few quarters, with the quarter ending March 2022 showing a 136% year-over-year growth and the quarter ending June 2022 showing a 105% year-over-year growth. The revenue for the quarter ending June 2022 was $8,343 million. \n\nThe risk factors for Uber' s business have been largely related to the ongoing impacts of the COVID- 19  pandemic, including reduced  global  demand  for  Mobility rides, supply constraints,  and  potential permanent changes to end-user behavior. These risk factors have been present in both the quarter ending March  2022   and  the quarter ending June  2022 , though the specific risks have varied slightly. Concluding Thoughts ReAct-based agents offer a powerful, general reasoning loop, and have the potential to solve complex tasks over your data. But they tend to only work reliably with more powerful language models such as GPT-4. Less sophisticated models (e.g. GPT-3) will make more unpredictable and erroneous decisions, leading to subpar query performance over your data sources. Agents implemented with “dumber” models need more interaction constraints in order to make more reliable, less erroneous decisions. We find that if we explicitly constrain the agent interface and push the complexity down to the Tool layer, we can still create agents that offer good performance over your data. Of course, this is just an initial analysis and there’s a few caveats/limitations: You may be able to “prompt hack” the default ReAct loop to get more consistent results, and we did not try that. We only tested this over a set of three financial documents. There’s a lot more work that needs to be done if we want to test this out on thousands of docs. We only compared GPT-3 and GPT-4, there’s so many more models to compare/benchmark, e.g ChatGPT, any open-source model, Anthropic Claude, etc. We did not test out other agent interaction patterns besides ReAct: “plan and solve” agents (though we do have similar formulations in LlamaIndex), AutoGPT-like task management, and more. Whether you’ve run into similar findings or you disagree with our analysis, let us know! We’d love to facilitate this discussion on our  Discord . Notebook Walkthrough You can find the full notebook walkthrough here:  https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing", "author": "Jerry Liu", "date": "May 23, 2023", "tags": ["Artificial Intelligence", "Large Language Models", "NLP", "Machine Learning"]},
{"title": "Vellum <> LlamaIndex Integration", "content": "Co-Authors: Akash Sharma, founder and CEO, Vellum Jerry Liu, co-founder and CEO, LlamaIndex About Us The central mission of  LlamaIndex  is to provide an interface between Large Language Models (LLM’s), and your private, external data. Over the past few months, it has become one of the most popular open-source frameworks for LLM data augmentation (context-augmented generation), for a variety of use cases: question-answering, summarization, structured queries, and more. Vellum  is a developer platform to build high quality LLM applications. The platform provides best-in-class tooling for prompt engineering, unit testing, regression testing, monitoring & versioning of in-production traffic and model fine tuning. Vellum’s platform helps companies save countless engineering hours to build internal tooling and instead use that time to build end user facing applications. Why we partnered on this integration Until recently, LlamaIndex users did not have a way to do prompt engineering and unit testing pre-production and versioning/monitoring the prompts post production. Prompt engineering and unit testing is key to ensure that your LLM feature is producing reliable results in production. Here’s an example of simple prompt that produces vastly different results between GPT-3, GPT-3.5 and GPT-4: Unit testing your prompts Creating a unit test bank is a proactive approach to ensure prompt reliability — it’s best practice to run 50–100 test cases before putting prompts in production. The test bank should comprise scenarios & edge cases anticipated in production, think of this as QAing your feature before it goes to production. The prompts should “pass” these test cases based on your evaluation criteria. Use Vellum Test Suites to upload test cases in bulk via CSV upload. Regression testing in production Despite how well you test before sending a prompt in production, edge cases can appear when in production. This is expected, so no stress! Through the Vellum integration, LlamaIndex users can change prompts and get prompt versioning without making any code changes. While doing that, however, it’s best practice to run historical inputs that were sent to the prompt in production to the new prompt and confirm it doesn’t break any existing behavior. LLMs are sometimes unpredictable, even changing the word “good” to “great” in a prompt can result in differing outputs! Best practices to leverage the integration How to access the integration This  demo notebook goes into detail on how you can use Vellum to manage prompts within LlamaIndex. Prerequisites Sign up for a free Vellum account at  app.vellum.ai/signup Go to  app.vellum.ai/api-keys  and generate a Vellum API key. Note it down somewhere. Auto-Register Prompts & Make Predictions Through Vellum If you import a prompt in LlamaIndex, the VellumPredictor class will used to auto-register a prompt with Vellum to make predictions. By registering a prompt with Vellum, Vellum will create: A “Sandbox” — an environment where you can iterate on the prompt, it’s model, provider, params, etc.; and A “Deployment” — a thin API proxy between you and LLM providers and offering prompt versioning, request monitoring, and more You can use VellumPromptRegistry to retrieve information about the registered prompt and get links to open its corresponding Sandbox and Deployment in Vellum’s UI. More details about Vellum’s Sandbox and Deployment features can be found  here Prompt engineering tips in context augmented use cases Think of the Large Language Model as a smart college graduate that needs instructions if the task at hand is not clear. If you’re not getting good results with the default prompt templates, follow these instructions: Add use case specific details to the prompt to guide what the model focuses on. Create 5–10 input scenarios to test performance Iterate a few times: (i) Tweak the prompt by adding more specific instructions or examples for the scenarios with bad results, (ii) Evaluate against the target response for each scenario In parallel, test out different foundation models and model providers using Vellum’s Sandbox. Maybe Claude or PaLM does better than GPT-4 for your use case. If you would like additional reasoning or explanation, use a more prescriptive approach: Add detailed step by step instructions to the end of the prompt and ask the LLM to walk though those steps when creating it’s answer: e.g. (1) … (2) … (3) … … (6) Output a JSON with the following typescript schema This is convenient because it’s simple to parse out the JSON blob from the LLM output However this causes more tokens to be generated so is slower and costs more, but it’s not nearly as expensive and slow as chaining multiple calls Measuring prompt quality, before production One of the common reasons why evaluating LLM model quality is hard is that there’s no defined framework. The evaluation metric depends on your use case. This  blog  goes in more detail, but in summary, the evaluation approach depends on type of use case: Classification:  accuracy, recall, precision, F score and confusion matrices for a deeper evaluation Data extraction:  Validate that the output is syntactically valid and the expected keys are present in the generated response SQL/Code generation:  Validate that the output is syntactically valid and running it will return the expected values Creative output:  Semantic similarity between model generated response and target response using cross-encoders Vellum’s Sandbox and Test Suites offer  Exact Match, Regex Match, Semantic Similarity & Webhook  as evaluation criteria. You get a clear indication of which test cases “pass”, given your evaluation criteria Testing in Vellum Sandbox Testing in Vellum Test Suites Measuring prompt quality, once in production User feedback is the ultimate source of truth for model quality — if there’s a way for your users to either implicitly or explicitly tell you whether they the response is “good” or “bad,” that’s what you should track and improve! Explicit user feedback is collected when your users respond with something like a 👍 or 👎 in your UI when interacting with the LLM output. Asking explicitly may not result in enough volume of feedback to measure overall quality. If your feedback collection rates are low, we suggest using implicit feedback if possible. Implicit feedback is based on how users react to the output generated by the LLM. For example, if you generate a first draft of en email for a user and they send it without making edits, that’s likely a good response! If they hit regenerate, or re-write the whole thing, that’s probably not a good response. Implicit feedback collection may not be possible for all use cases, but it can be a powerful gauge of quality. Use Vellum’s Actuals endpoint to track the quality of each completion and track results in the Completions and Monitoring tabs of your Deployment.", "author": "Jerry Liu", "date": "Jun 5, 2023", "tags": ["NLP", "AI", "Mlops", "Observability", "Prompt Engineering"]},
{"title": "Customizing property graph index in LlamaIndex", "content": "Learn how to implement entity deduplication and custom retrieval methods to increase GraphRAG accuracy This is a guest post by Neo4J The  property graph index  is an excellent addition to LlamaIndex and an upgrade from the previous knowledge graph integration. First, the data representation is slightly different. In the previous integration, the graph was represented with triples, but now we have a proper property graph integration where nodes have labels and optionally node properties. Example of a property graph model. Each node is assigned a label indicating its type, such as Person, Organization, Project, or Department. Nodes and relationships may also store node properties for other relevant details, such as the date of birth or project start and end date, as shown in this example. Second, the property graph index is designed to be modular, so you can use one or multiple (custom) knowledge graph constructors as well as retrievers, making it an incredible tool to build your first knowledge graph or customize the implementation for your specific needs. Property graph workflow The image illustrates the property graph integration within the LlamaIndex , beginning with documents being passed to graph constructors. These constructors are modular components responsible for extracting structured information, which is then stored in a knowledge graph. The graph can be built using various or custom modules, highlighting the system’s flexibility to adapt to different data sources or extraction needs. Graph retrievers then access the knowledge graph to retrieve data. This stage is also modular, allowing for the use of multiple retrievers or custom solutions designed to query specific types of data or relationships within the graph. Finally, the retrieved data is used by a LLM to generate an answer, representing the output or the insight derived from the process. This flow emphasizes a highly adaptable and scalable system where each component can be independently modified or replaced to enhance the overall functionality or to tailor it to specific requirements. In this blog post you will learn how to: Construct a knowledge graph using a schema-guided extraction Perform entity deduplication using a combination of text embedding and word similarity techniques Design a custom graph retriever Finally, you will implement a question answering flow using the custom retriever The code is available on  GitHub . Environment setup In this blog post, we will use Neo4j as the underlying graph store. The easiest way is to get started is to a free instance on  Neo4j Aura , which offers cloud instances of the Neo4j database. Alternatively, you can also set up a local instance of the Neo4j database by downloading the  Neo4j Desktop  application and creating a local database instance. from  llama_index.graph_stores.neo4j  import  Neo4jPGStore\n\nusername= \"neo4j\" \npassword= \"stump-inlet-student\" \nurl= \"bolt://52.201.215.224:7687\" \n\ngraph_store = Neo4jPGStore(\n    username=username,\n    password=password,\n    url=url,\n) Additionally, you will require a working OpenAI API key. import  os\n\nos.environ[ \"OPENAI_API_KEY\" ] =  \"sk-\" Dataset In this blog post, we will use a  sample news article dataset fetched from Diffbot , which I’ve made available on  GitHub for easier access . Sample records from the dataset. Since the property graph index operates with documents, we have to wrap the text from the news as LlamaIndex documents. import  pandas  as  pd\n from  llama_index.core  import  Document\n\nnews = pd.read_csv(\n   \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\" )\ndocuments = [Document(text= f\" {row[ 'title' ]} :  {row[ 'text' ]} \" )  for  i, row  in  news.iterrows()] Graph construction As mentioned, LlamaIndex provides multiple  out-of-the-box graph constructors . In this example, we will use the  SchemaLLMPathExtractor , which allows us to define the schema of the graph structure we want to extract from documents. Schema-guided graph structure extraction. We begin by defining the types of nodes and relationships we want the LLM to extract. entities =  Literal [ \"PERSON\" ,  \"LOCATION\" ,  \"ORGANIZATION\" ,  \"PRODUCT\" ,  \"EVENT\" ]\nrelations =  Literal [\n     \"SUPPLIER_OF\" ,\n     \"COMPETITOR\" ,\n     \"PARTNERSHIP\" ,\n     \"ACQUISITION\" ,\n     \"WORKS_AT\" ,\n     \"SUBSIDIARY\" ,\n     \"BOARD_MEMBER\" ,\n     \"CEO\" ,\n     \"PROVIDES\" ,\n     \"HAS_EVENT\" ,\n     \"IN_LOCATION\" ,\n] As you can see, we are focusing our graph extraction around people and organizations. Next, we will specify the relationships associated with each node label. # define which entities can have which relations \nvalidation_schema = {\n     \"Person\" : [ \"WORKS_AT\" ,  \"BOARD_MEMBER\" ,  \"CEO\" ,  \"HAS_EVENT\" ],\n     \"Organization\" : [\n         \"SUPPLIER_OF\" ,\n         \"COMPETITOR\" ,\n         \"PARTNERSHIP\" ,\n         \"ACQUISITION\" ,\n         \"WORKS_AT\" ,\n         \"SUBSIDIARY\" ,\n         \"BOARD_MEMBER\" ,\n         \"CEO\" ,\n         \"PROVIDES\" ,\n         \"HAS_EVENT\" ,\n         \"IN_LOCATION\" ,\n    ],\n     \"Product\" : [ \"PROVIDES\" ],\n     \"Event\" : [ \"HAS_EVENT\" ,  \"IN_LOCATION\" ],\n     \"Location\" : [ \"HAPPENED_AT\" ,  \"IN_LOCATION\" ],\n} For example, a person can have the following relationships: WORKS_AT BOARD_MEMBER CEO HAS_EVENT The schema is quite specific except for the EVENT node label, which is slightly more ambiguous and allows the LLM to capture various types of information. Now that we have defined the graph schema, we can input it into the  SchemaLLMPathExtractor  and use it to construct a graph. from  llama_index.core  import  PropertyGraphIndex\n\nkg_extractor = SchemaLLMPathExtractor(\n    llm=llm,\n    possible_entities=entities,\n    possible_relations=relations,\n    kg_validation_schema=validation_schema,\n     # if false, allows for values outside of the schema \n     # useful for using the schema as a suggestion \n    strict= True ,\n)\n\nNUMBER_OF_ARTICLES =  250 \n\nindex = PropertyGraphIndex.from_documents(\n    documents[:NUMBER_OF_ARTICLES],\n    kg_extractors=[kg_extractor],\n    llm=llm,\n    embed_model=embed_model,\n    property_graph_store=graph_store,\n    show_progress= True ,\n) This code extracts graph information from 250 news articles, but you can adjust the number how you see fit. There are 2500 articles in total. Note that extracting 250 articles takes about 7 minutes with GPT-4o. However, you can accelerate the process by employing parallelization through the   num_workers   parameter. We can visualize a small subgraph to inspect what was stored. Text chunks are blue, while entity nodes are all the rest. The constructed graph contains both text chunks (blue), which contain text and embeddings. If an entity was mentioned in the text chunk, there is a  MENTIONS  relationships between the text chunk and entity. Additionally, entities can have relationships to other entities. Entity deduplication Entity deduplication or disambiguation is an important but often overlooked step in graph construction. Essentially, it is a cleaning step where you try to match multiple nodes that represent a single entity and merge them together into a single node for better graph structural integrity. For example, in our constructed graph I could find some examples that could be merged. Potential entity duplicates. We will use a combination of text embedding similarity and word distance to find potential duplicates. We start by defining the vector index on our entities in the graph. graph_store.structured_query( \"\"\"\nCREATE VECTOR INDEX entity IF NOT EXISTS\nFOR (m:`__Entity__`)\nON m.embedding\nOPTIONS {indexConfig: {\n `vector.dimensions`: 1536,\n `vector.similarity_function`: 'cosine'\n}}\n\"\"\" ) The next Cypher query finds duplicates and is quite involved and I took me, Michael Hunger, and Eric Monk a couple of hours to perfect it. similarity_threshold =  0.9 \nword_edit_distance =  5 \ndata = graph_store.structured_query( \"\"\"\nMATCH (e:__Entity__)\nCALL {\n  WITH e\n  CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n  YIELD node, score\n  WITH node, score\n  WHERE score > toFLoat($cutoff)\n      AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n           OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n      AND labels(e) = labels(node)\n  WITH node, score\n  ORDER BY node.name\n  RETURN collect(node) AS nodes\n}\nWITH distinct nodes\nWHERE size(nodes) > 1\nWITH collect([n in nodes | n.name]) AS results\nUNWIND range(0, size(results)-1, 1) as index\nWITH results, index, results[index] as result\nWITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n        CASE WHEN index <> index2 AND\n            size(apoc.coll.intersection(acc, results[index2])) > 0\n            THEN apoc.coll.union(acc, results[index2])\n            ELSE acc\n        END\n)) as combinedResult\nWITH distinct(combinedResult) as combinedResult\n// extra filtering\nWITH collect(combinedResult) as allCombinedResults\nUNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\nWITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\nWHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n    WHERE x <> combinedResultIndex\n    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n)\nRETURN combinedResult  \n\"\"\" , param_map={ 'cutoff' : similarity_threshold,  'distance' : word_edit_distance})\n for  row  in  data:\n     print (row) Without getting into too many details, we use a combination of text embeddings and word distance to find potential duplicates in our graph. You can tune  similarity_threshold  and  word_distance  to find the best combination that detects as many duplicates without too much false positives. Unfortunately, entity disambiguation is a hard problem and there are no perfect solutions. With this approach, we get quite good results, but there are some false positives in there as well: [ '1963 AFL Draft' ,  '1963 NFL Draft' ]\n[ 'June 14, 2023' ,  'June 15 2023' ]\n[ 'BTC Halving' ,  'BTC Halving 2016' ,  'BTC Halving 2020' ,  'BTC Halving 2024' ,  'Bitcoin Halving' ,  'Bitcoin Halving 2024' ] It is up to you to tweak the dials, and maybe add some manual exceptions before merging duplicate nodes. Implementing a custom retriever Great, we have constructed a knowledge graph based on the news dataset. Now, let’s examine our retriever options. At the moment, there are  four existing retrievers available : LLMSynonymRetriever : takes the query, and tries to generate keywords and synonyms to retrieve nodes (and therefore the paths connected to those nodes). VectorContextRetriever : retrieves nodes based on their vector similarity, and then fetches the paths connected to those nodes TextToCypherRetriever : uses a graph store schema, your query, and a prompt template in order to generate and execute a cypher query CypherTemplateRetriever : Rather than letting the LLM have free-range of generating any cypher statement, we can instead provide a cypher template and have the LLM fill in the parameters. Additionally, implementing a custom retriever is straightforward, so that is exactly what we will do here. Our custom retriever will first identify entities in the input query and then execute the VectorContextRetriever for each identified entity separately. First, we will define the entity extraction model and prompt. from  pydantic  import  BaseModel\n from  typing  import   Optional ,  List \n\n\n class   Entities ( BaseModel ):\n     \"\"\"List of named entities in the text such as names of people, organizations, concepts, and locations\"\"\" \n    names:  Optional [ List [ str ]]\n\n\nprompt_template_entities =  \"\"\"\nExtract all named entities such as names of people, organizations, concepts, and locations\nfrom the following text:\n{text}\n\"\"\" Now we can progress to the custom retriever implementation. from  typing  import   Any ,  Optional \n\n from  llama_index.core.embeddings  import  BaseEmbedding\n from  llama_index.core.retrievers  import  CustomPGRetriever, VectorContextRetriever\n from  llama_index.core.vector_stores.types  import  VectorStore\n from  llama_index.program.openai  import  OpenAIPydanticProgram\n\n\n class   MyCustomRetriever ( CustomPGRetriever ):\n     \"\"\"Custom retriever with entity detection.\"\"\" \n     def   init ( \n        self,\n         ## vector context retriever params \n        embed_model:  Optional [BaseEmbedding] =  None ,\n        vector_store:  Optional [VectorStore] =  None ,\n        similarity_top_k:  int  =  4 ,\n        path_depth:  int  =  1 ,\n        include_text:  bool  =  True ,\n        **kwargs:  Any ,\n     ) ->  None :\n         \"\"\"Uses any kwargs passed in from class constructor.\"\"\" \n        self.entity_extraction = OpenAIPydanticProgram.from_defaults(\n            output_cls=Entities, prompt_template_str=prompt_template_entities\n        )\n        self.vector_retriever = VectorContextRetriever(\n            self.graph_store,\n            include_text=self.include_text,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            path_depth=path_depth,\n        )\n\n     def   custom_retrieve ( self, query_str:  str ) ->  str :\n         \"\"\"Define custom retriever with entity detection.\n\n        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n        \"\"\" \n        entities = self.entity_extraction(text=query_str).names\n        result_nodes = []\n         if  entities:\n             print ( f\"Detected entities:  {entities} \" )\n             for  entity  in  entities:\n                result_nodes.extend(self.vector_retriever.retrieve(entity))\n         else :\n            result_nodes.extend(self.vector_retriever.retrieve(query_str))\n        final_text =  \"\\n\\n\" .join(\n            [n.get_content(metadata_mode= \"llm\" )  for  n  in  result_nodes]\n        )\n         return  final_text The  MyCustomRetriever  class has only two methods. You can use the  init  method to instantiate any functions or classes you will be using in the retriever. In this example, we instantiate the entity detection OpenAI program along with the vector context retriever. The  custom_retrieve  method is called during retrieval. In our custom retriever implementation, we first identify any relevant entities in the text. If any entities are found, we iterate and execute the vector context retriever for each entity. On the other hand, if no entities are identified we pass the entire input to the vector context retriever. As you can observe, you can easily customize the retriever for your use-case by incorporating existing retrievers or starting from scratch as you can easily execute Cypher statements by using the  structured_query  method of the graph store. Question-answering flow Let’s wrap it up by using the custom retriever to answer an example question. We need to pass the retriever to the  RetrieverQueryEngine  . from  llama_index.core.query_engine  import  RetrieverQueryEngine\n\ncustom_sub_retriever = MyCustomRetriever(\n    index.property_graph_store,\n    include_text= True ,\n    vector_store=index.vector_store,\n    embed_model=embed_model\n)\n\nquery_engine = RetrieverQueryEngine.from_args(\n    index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n) Let’s test it out! response = query_engine.query(\n     \"What do you know about Maliek Collins or Darragh O’Brien?\" \n)\n print ( str (response))\n # Detected entities: ['Maliek Collins', \"Darragh O'Brien\"] \n # Maliek Collins is a defensive tackle who has played for the Dallas Cowboys, Las Vegas Raiders, and Houston Texans. Recently, he signed a two-year contract extension with the Houston Texans worth $23 million, including a $20 million guarantee. This new deal represents a raise from his previous contract, where he earned $17 million with $8.5 million guaranteed. Collins is expected to be a key piece in the Texans' defensive line and fit well into their 4-3 alignment. \n # Darragh O’Brien is the Minister for Housing and has been involved in the State’s industrial relations process and the Government. He was recently involved in a debate in the Dáil regarding the pay and working conditions of retained firefighters, which led to a heated exchange and almost resulted in the suspension of the session. O’Brien expressed confidence that the dispute could be resolved and encouraged unions to re-engage with the industrial relations process. Summary In this blog post, we’ve explored the intricacies of customizing the property graph index within LlamaIndex, focusing on implementing entity deduplication and designing custom retrieval methods to enhance GraphRAG accuracy. The property graph index allows for a modular and flexible approach, utilizing various graph constructors and retrievers to tailor the implementation to your specific needs. Whether you’re building your first knowledge graph or optimizing for a unique dataset, these customizable components offer a powerful toolkit. We invite you to test out the property graph index integration to see how they can elevate your knowledge graph projects. As always, the code is available on  GitHub .", "author": "Tomaz Bratanic", "date": "Jun 11, 2024", "tags": ["Neo4j", "Knowledge Graphs"]},
{"title": "LlamaIndex and Weaviate", "content": "Co-authors: Jerry Liu (co-founder/CEO of LlamaIndex) Erika Cardenas (Developer Advocate, Weaviate) While large language models (LLMs) like GPT-4 have impressive capabilities in generation and reasoning, they have limitations in terms of their ability to access and retrieve specific facts, figures, or contextually relevant information. A popular solution to this problem is setting up a retrieval-augmented generation (RAG) system: combine the language model with an external storage provider, and create an overall software system that can orchestrate the interactions with and between these components in order to create a “chat with your data” experience. The combination of Weaviate and LlamaIndex provide the critical components needed to easily setup a powerful and reliable RAG stack, so that you can easily deliver powerful LLM-enabled experiences over your data, such as search engines, chatbots, and more. First, we can use Weaviate as the vector database that acts as the external storage provider. Next, we can use a powerful data framework such as LlamaIndex to help with data management and orchestration around Weaviate when building the LLM app. In this blog post, we walk through an overview of LlamaIndex and some of the core data management and query modules. We then go through an initial demo notebook. We’re kicking off a new series to guide you on how to use LlamaIndex and Weaviate for your LLM applications. An Introduction to LlamaIndex LlamaIndex is a data framework for building LLM applications. It provides a comprehensive toolkit for ingestion, management, and querying of your external data so that you can use it with your LLM app. Data Ingestion On data ingestion, LlamaIndex offers connectors to 100+ data sources, ranging from different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack, Discord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data connectors are primarily hosted on [LlamaHub]( https://llamahub.ai/ ). This makes it easy for users to integrate data from their existing files and applications. Data Indexing Once the data is loaded, LlamaIndex offers the ability to index this data with a wide variety of data structures and storage integration options (including Weaviate). LlamaIndex supports indexing unstructured, semi-structured, and structured data. A standard way to index unstructured data is to split the source documents into text “chunks”, embed each chunk, and store each chunk/embedding in a vector database. Data Querying Once your data is ingested/stored, LlamaIndex provides the tools to define an advanced retrieval / query “engine” over your data. Our retriever constructs allow you to retrieve data from your knowledge base given an input prompt. A query engine construct allows you to define an interface that can take in an input prompt, and output a knowledge-augmented response — it can use retrieval and synthesis (LLM) modules under the hood. Some examples of query engine “tasks” are given below, in rough order from easy to advanced: Semantic Search: Retrieve the top-k most similar items from the knowledge corpus by embedding similarity to the query, and synthesize a response over these contexts. Structured Analytics: Convert natural language to a SQL query that can be executed Query Decomposition over Documents: Break down a query into sub-questions, each over a subset of underlying documents. Each sub-question can be executed against its own query engine. Demo Notebook Walkthrough Let’s walk through a simple example of how LlamaIndex can be used with Weaviate to build a simple Question-Answering (QA) system over the Weaviate blogs! The full code can be found in the  Weaviate recipes repo . The first step is to setup your Weaviate client. In this example, we connect to a local Weaviate instance through port  http://localhost:8080 : import  weaviate\n# connect to your weaviate instance\nclient = weaviate.Client( \"http://localhost:8080\" ) The next step is to ingest the Weaviate documentation and parse the documents into chunks. You can choose to use one of our many web page readers to scrape any website yourself — but luckily, the downloaded files are already readily available in the recipes repo. from  llama_index.node_parser  import  SimpleNodeParser\n # load the blogs in using the reader \nblogs = SimpleDirectoryReader( './data' ).load_data()\n # chunk up the blog posts into nodes \nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(blogs) Here, we use the SimpleDirectoryReader to load in all documents from a given directory. We then use our  SimpleNodeParser  to chunk up the source documents into Node objects (text chunks). The next step is to 1) define a  WeaviateVectorStore , and 2) build a vector index over this vector store using LlamaIndex. # construct vector store\nvector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n# setting up the storage for the embeddings\nstorage_context = StorageContext.from_defaults(vector_store = vector_store)\n# set up the index\nindex = VectorStoreIndex(nodes, storage_context = storage_context) Our WeaviateVectorStore abstraction creates a central interface between our data abstractions and the Weaviate service. Note that the  VectorStoreIndex  is initialized from both the nodes and the storage context object containing the Weaviate vector store. During the initialization phase, the nodes are loaded into the vector store. Finally, we can define a query engine on top of our index. This query engine will perform semantic search and response synthesis, and output an answer. ​​query_engine = index.as_query_engine()\nresponse = query_engine.query( \"What is the intersection between LLMs and search?\" )\n print (response) You should get an answer like the following: The intersection between LLMs and search is the ability to use LLMs to improve search capabilities, such as retrieval-augmented generation, query understanding, index construction, LLMs in re-ranking, and search result compression. LLMs can also be used to manage document updates, rank search results, and compress search results. LLMs can be used to prompt the language model to extract or formulate a question based on the prompt and then send that question to the search engine, or to prompt the model with a description of the search engine tool and how to use it with a special `[SEARCH]` token. LLMs can also be used to prompt the language model to rank search results according to their relevance with the query, and to classify the most likely answer span given a question and text passage as input. Next Up in this Series This blog post shared an initial overview of the LlamaIndex and Weaviate integration. We covered an introduction to the toolkits offered in LlamaIndex and a notebook on how to build a simple QA engine over Weaviate’s blog posts. Now that we have a baseline understanding, we will build on this by sharing more advanced guides soon. Stay tuned! What’s next ​ Check out  Getting Started with Weaviate , and begin building amazing apps with Weaviate. You can reach out to us on  Slack  or  Twitter , or  join the community forum . Weaviate is open source, and you can follow the project on  GitHub . Don’t forget to give us a ⭐️ while you are there!", "author": "Jerry Liu", "date": "Jun 22, 2023", "tags": ["NLP", "AI", "Vector Database", "Question Answering", "Machine Learning"]},
{"title": "LlamaIndex Newsletter 2024-06-25", "content": "Hello to All Llama Lovers!🦙 Welcome to this week’s issue of the LlamaIndex newsletter! This edition is packed with thrilling updates, comprehensive guides, and detailed tutorials to help you gain a deeper understanding of our tools. 🤩  The highlights: CrewAI Multi-Agent Integration:  Integrated with CrewAI to enhance task-solving with specialized agent crews and LlamaIndex integrations.  Notebook ,  Tweet . MistralAI Fine-Tuning API Integration:  Enhance model training and performance monitoring with our new integration of MistralAI’s Fine-Tuning API.  Notebook ,  Tweet . ✨ Feature Releases and Enhancements: We have launched a Multi-Agent integration with CrewAI to build a  crew  of specialized agents that collaboratively solve tasks. Enhance these agents with external knowledge and third-party tools through easy integrations with LlamaIndex, including advanced RAG query engines and tools from LlamaHub.  Notebook ,  Tweet . We have integrated MistralAI’s Fine-Tuning API to create and synthesize training and evaluation datasets, assess model after fine-tuning, and monitor performance metrics with RAGAS and Weights & Biases.  Notebook ,  Tweet . 💡 Demos: RAGapp  by  **Marcus Schiesser ** simplifies Agentic RAG in enterprise settings with functionalities akin to using GPTs by OpenAI. The latest version includes a code interpreter and a tool to call any OpenAPI, all built using LlamaIndex. 🗺️ Guides: Guide  to Multi-Document Agentic RAG Using LightningAI: Jay Shah’s template that enables you to set up a multi-document agent for search and summarization across research notebooks. This out-of-the-box solution, integrated with Streamlit, allows for full visualization and is part of LightningAI’s suite of tools for developing and sharing ML and genAI native apps. Guide  to Making a Serverless RAG Chatbot: Azure’s quick start repository for creating a serverless RAG chatbot using LlamaIndex and AzureOpenAI. Guide  to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic agent, using local models, adding RAG, enhancing retrieval with LlamaParse, and developing custom tools. ✍️ Tutorials: JinoRohit’s  tutorial on using a LlamaIndex pipeline with MLflow for systematic tracking and tuning of RAG parameters, enhancing answer accuracy through precise evaluation metrics and datasets. Hanane Dupouy’s  tutorial  demonstrates how to apply CRAG (Corrective RAG) for financial analysis using LlamaIndex’s CRAG LlamaPack. This technique assesses retrieval quality and supplements the knowledge base with web searches to ensure contextual accuracy and relevance. Soham’s   tutorial  to create an agent that automates GitHub commits using Composio and LlamaIndex Tools. Aruna Withanage’s   tutorial  on creating custom text-to-SQL pipelines using LlamaIndex’s DAG capabilities.", "author": "LlamaIndex", "date": "Jun 25, 2024", "tags": []},
{"title": "Llama Index & Prem AI Join Forces", "content": "Co-authors:  Simone Giacomelli (co-founder at Prem), Jerry Liu (co-founder/CEO at LlamaIndex) We’re pleased to share the successful integration of Prem App and Llama Index, a union that brings a new level of privacy to AI development. Prem’s self-hosting AI models and Llama’s versatile data framework enhances the ability to build AI applications in a customizable and flexible manner. Integration Details By combining Prem’s self-hosting AI models with Llama Index’s data framework, developers can now connect custom data sources to large language models easily. This simplifies the process of data ingestion, indexing, and querying, streamlining the overall AI development cycle. Getting Started To leverage this integration, simply download the Prem App and connect your data sources through the Llama Index platform. This allows you to self-host your AI models with Prem App and utilize Llama Index’s capabilities to manage your data efficiently. This integration, therefore, significantly boosts AI application development, giving developers greater control and flexibility over their projects. Getting Started Install Prem You can run Prem in two different ways: MacOS: go to  https://premai.io  and download Prem App. Server: run the installer script:  wget -q <https://get.prem.ninja/install.sh> -O install.sh; sudo bash ./install.sh Run the services in the GUI When the UI is up and running, you can see all the services available. With just one click you can download the service you are interested in. In the background, the docker image associated with the service will be downloaded based on your hardware requirements. While waiting for the download to be completed, read more about the service, in the detail view. Just click on the card and you will be redirected to the service page. Each service page is packaged with some general info as well as complete documentation giving more details into the model exposed. When the download has been completed, just click on Open and the service will start. You can interact with the service from the playground or from APIs. You can check the port on which the service is running from the service detail view. Start Building Your App In this quick tutorial will show you how to build a simple Talk to your Data use case using Prem landing page content. In order to achieve that we will need to run three services: Redis: we will use Redis as a vector store to store the embeddings. Vicuna 7B Q4: we will use Vicuna in order to generate a proper response for the user based on the most similar document we get using Redis similarity search All MiniLM L6 V2: we will use sentence transformers in order to generate the embeddings out of our documents. If all the services necessary are running, you will see a similar interface as the one beyond. You can now start integrating the services using Llama Index library. In the following code snippets, we will show you how you can build a simple talk to your data use case using Prem and Llama Index. Import all necessary dependencies and assign a random string to  OPENAI_API_KEY  environment variable. import  os\n\n from  llama_index.vector_stores  import  RedisVectorStore\n from  llama_index.storage.storage_context  import  StorageContext\n from  llama_index  import  ListIndex, LLMPredictor, Document\n\n from  langchain.chat_models  import  ChatOpenAI\n from  langchain.embeddings.openai  import  OpenAIEmbeddings\n\n from  llama_index  import  LangchainEmbedding, ServiceContext\n\nos.environ[ \"OPENAI_API_KEY\" ] =  \"random-string\" 2. Load the Data / Create some Documents. In this example, I am using Prem landing page content creating manually some documents. doc1 = Document(text=\"Prem is an easy to use open source AI platform. With Prem you can quickly build privacy preserving AI applications.\")\ndoc2 = Document(text=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(text=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\") 3. Instantiate the LLMs connecting to the running services. # Instantiate a llm predictor using Langchain pointing to vicuna-7b-q4 service\nllm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_base=\"http://localhost:8111/api/v1\", max_tokens=128))\n\n# Instantiate the embeddings object using Langchain pointing to all-MiniLM-L6-v2 service\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/api/v1\")\nembed_model = LangchainEmbedding(embeddings)\n\n# define a service context using the embeddings and llm defined above.\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor) 4. Configure the Vector Store # instantiate the vectorstore connecting to Redis service\nvector_store = RedisVectorStore(\n    index_name=\"prem_landing\",\n    index_prefix=\"llama\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store) 5. Index the documents index = ListIndex.from_documents([doc1, doc2, doc3], storage_context=storage_context) 6. Perform an example query query_engine = index.as_query_engine(\n    retriever_mode= \"embedding\" , \n    verbose= True , \n    service_context=service_context\n)\nresponse = query_engine.query( \"What are Prem benefits?\" )\n print (response) The benefits of using Prem include: Effortless Integration, Ready for the Real World, Rapid Iterations, Instant Results, Privacy Above All, Comprehensive Documentation, Preserve Your Anonymity, and an intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party. And Done 🎉 You are now using Prem with Llama Index. More Information Check out our documentation at:  https://github.com/premai-io/prem-app Check out a simple talk to your data notebook with Llama Index:  https://github.com/premAI-io/prem-daemon/blob/main/resources/notebooks/llama_index.ipynb Checkout our YouTube tutorials Getting Started with Prem:  https://www.youtube.com/watch?v=XixH46Ysl5A Deploy Prem in your Paperspace instance:  https://www.youtube.com/watch?v=aW8t6wouwx0 Join Us Our partnership is based on a shared understanding that the future of AI is open, composable, and privacy-centric. Join us on this journey !", "author": "Jerry Liu", "date": "Jun 23, 2023", "tags": ["Large Language Models", "NLP", "AI", "Machine Learning"]},
{"title": "LlamaIndex Update — 06/26/2023", "content": "Greetings, LlamaIndex community! We’re excited to introduce our new blog series, the LlamaIndex Update. Recognizing the fast pace of our open-source project, this series will serve as your continual guide, tracking the latest advancements in features, webinars, hackathons, and community events. Our goal is simple: to keep you updated, engaged, and inspired. Whether you’re a long-time contributor or a new joiner, these updates will help you stay in sync with our progress. So, let’s explore the recent happenings in our premier edition of the LlamaIndex Update. Features And Integrations: LLMs with Knowledge Graphs, supported by NebulaGraph. This new stack enables unique retrieval-augmented generation techniques. Our Knowledge Graph index introduces a GraphStore abstraction, complementing our existing data store types.\n Docs ,  Tweet Better LLM app UX supports in-line citations of its sources, enhancing interpretability and traceability. Our new  CitationQueryEngine  enables these citations and ensures they correspond with retrieved documents. This feature marks a leap towards improving transparency in LlamaIndex applications.\n Docs ,  Tweet LlamaIndex integrates with Microsoft Guidance to ensure structured outputs from LLMs. It allows direct prompting of JSON keys and facilitates the conversion of Pydantic objects into the Guidance format, enhancing structured interactions. It can be used independently or with the SubQuestionQueryEngine.\n Docs ,  Tweet The GuidelineEvaluator module allows users to set text guidelines, thereby aiding in the evaluation of LLM-generated text responses. This paves the way toward automated error correction capabilities.\n Notebook ,  Tweet We now include a simple  OpenAIAgent , offering an agent interface capable of sequential tool use and async callbacks. This integration was made possible with the help of the OpenAI function API and the LangChain abstractions.\n Tweet OpenAIPydanticProgram  in LlamaIndex enhances structured output extraction. This standalone module allows any LLM input to be converted into a Pydantic object, providing a streamlined approach to data structuring.\n Docs ,  Tweet We now incorporate the FLARE technique for a knowledge-augmented long-form generation. FLARE uses iterative retrieval to construct extended content, deciding to perform retrieval with each sentence. Unlike conventional vector index methods, our FLARE implementation builds a template iteratively, filling gaps with retrieval for more pertinent responses. Please note, this is a beta feature and works best with GPT-4.\n Docs ,  Tweet We now employ the Maximal Marginal Relevance (MMR) algorithm to enhance diversity and minimize redundancy in retrieved results. This technique measures the similarity between a candidate document and the query while minimizing similarity with previous documents, depending on a user-specified threshold. Please note that careful calibration is necessary to ensure that increased diversity doesn’t introduce irrelevant context. The threshold value is key to balancing diversity and relevance.\n Docs ,  Tweet We now support recursive Pydantic objects for complex schema extraction. This enhancement, inspired by parsing directory trees, employs a mix of recursive (Node) and non-recursive (DirectoryTree) Pydantic models, facilitating more sophisticated agent-tool interactions.\n Tweet We have developed agents that can perform advanced query planning over data using the Function API and Pydantic. These agents input a full Pydantic graph in the function signature of a query plan tool, which is then executed. This system can work with any tool and has the potential to construct complex query plans. However, it has limitations like difficulty in producing deep nesting and the possibility of outputting invalid responses.\n Docs ,  Tweet `OpenAIAgent` is capable of advanced data retrieval and analysis, such as auto-vector database retrieval and joint text-to-SQL and semantic search. We have also built a query plan tool interface that allows the agent to generate structured/nested query plans, which can then be executed against any set of tools, enabling advanced reasoning and analysis.\nDocs:  OpenAI Agent + Query Engine ,  Retrieval Augmented OpenAI Agent ,  OpenAI Agent Query Planning .\n Tweet The new multi-router feature allows for QA over complex data collections, where answers may be spread across multiple sources. It uses a “MultiSelector” object to select relevant choices given a query. The router can pick up to a maximum number of choices. It can use either a raw LLM completion API or the OpenAI Function API. If the Function API is used, schema validity can be enforced. A simple usage example involves a RouterQueryEngine, where the PydanticMultiSelector selects the relevant vector and keyword index to synthesize an answer.\n Docs ,  Tweet We have made a significant upgrade to our token tracking feature. Users can now easily track prompt, completion, and embedding tokens through the platform’s callback handler. The upgrade aims to make token counting more efficient and user-friendly.\n Docs ,  Tweet We released a guide that demonstrates how to build a custom retriever that combines vector similarity search with knowledge graphs in LLM RAG systems. It involves constructing a vector index and a knowledge graph index and combining the results from both during query time. This method can improve results by providing additional context for entities. However, it may lead to a slight increase in latency.\n Docs ,  Tweet In an LLM workflow, managing large amounts of data, including PDFs, agent Tools, SQL table schemas, etc., requires efficient indexing. To handle this, we introduce our Object Index, a wrapper over our existing index data structures. This allows any object to be converted into an indexable text format, providing a unified interface that enhances the functionality of our indices over various data types.\n Tweet The OpenBB Finance Terminal is a great platform for investment research and is completely open-source. It now includes a feature called AskOBB, powered by Llama Index, which allows users to easily access any financial data through natural language.\n Tweet The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest release. This new feature allows developers to evaluate and track their experiments more efficiently. It automatically evaluates various components of the application stack, including app inputs and outputs, LLM calls, retrieved-context chunks from an index, and latency. This is part of an ongoing collaboration between the LlamaIndex and TruLens teams to improve the development, evaluation, and iteration of LLM apps.\n Notebook ,  Blogpost Prem App has successfully integrated with Llama Index, enhancing privacy in AI development. This union allows developers to connect custom data sources to large language models easily, simplifying data ingestion, indexing, and querying. To use this integration, download the Prem App and connect your data sources through the Llama Index platform. This allows for efficient data management and boosts AI application development, providing developers with more control and flexibility.\n Notebook ,  Blogpost We now enable the extraction of tabular data frames from unstructured text. This feature, powered by the OpenAI Function API and Pydantic models, simplifies text-to-SQL or text-to-DF conversions within structured data workflows. Note that effective use may require significant prompt optimization.\n Docs ,  Tweet Tutorials: James Brigg’s tutorial  on using LlamaIndex with Pinecone. Jerry Liu's tutorial  on using LlamaIndex with Weaviate. Sophia Yang tutorial  on LlamaIndex overview, Use cases, and integration with LangChain. Anil Chandra Naidu is building a  course  on LlamaIndex. The course presently covers topics such as introduction, fundamentals, and data connectors. OpenAI cookbook by Simon  on how to perform financial analysis with LlamaIndex. Webinars And Podcasts: Webinar  on Demonstrate-Search-Predict (DSP) with Omar Khattab. Webinar  on Practical challenges of building a Legal Chatbot over your PDFs with Sam Yu MaML podcast  with Jerry Liu. Hackathons: The LlamaIndex team has presented at the UC Berkeley Hackathon and the Stellaris VP Hackathon in India. The community has warmly welcomed LlamaIndex, and teams at these hackathons have developed intriguing use cases — Customer support during emergency cases, Understanding Legal documents. Events: Jerry Liu spoke on Building and troubleshooting an AI Search & Retrieval System at Arize — LlamaIndex event. Ravi Theja presented about LlamaIndex and its applications at Together in India. That’s all for this edition of the LlamaIndex Update. We hope you found this information useful and are as excited as we are about the progress we’re making. We’re grateful for the continued support and contributions from our community. Remember, your feedback and suggestions are invaluable to us, so don’t hesitate to reach out. Stay tuned for our next update, where we’ll share more exciting developments from the LlamaIndex project. Until then, happy indexing!", "author": "Ravi Theja", "date": "Jun 26, 2023", "tags": ["Large Language Models", "Llamaindex", "NLP", "AI", "OpenAI"]},
{"title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray", "content": "Co-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at Anyscale) ( note:  this is cross-posted from the original blog post on Anyscale’s website.  Check it out here !) In this blog, we showcase how you can use LlamaIndex and Ray to build a query engine to answer questions and generate insights about Ray itself, given its documentation and blog posts. We’ll give a quick introduction of LlamaIndex + Ray, and then walk through a step-by-step tutorial on building and deploying this query engine. We make use of both Ray Datasets to parallelize building indices as well as Ray Serve to build deployments. Introduction Large Language Models (LLMs) offer the promise of allowing users to extract complex insights from their unstructured text data. Retrieval-augmented generation pipelines have emerged as a common pattern for developing LLM applications allowing users to effectively perform semantic search over a collection of documents. Example of retrieval augmented generation. Relevant context is pulled from a set of documents and included in the LLM input prompt. However, when productionizing these applications over many different data sources, there are a few challenges: Tooling for indexing data from many different data sources Handling complex queries over different data sources Scaling indexing to thousands or millions of documents Deploying a scalable LLM application into production Here, we showcase how  LlamaIndex  and  Ray  are the perfect setup for this task. LlamaIndex is a data framework for building LLM applications, and solves Challenges #1 and #2. It also provides a comprehensive toolkit allowing users to connect their private data with a language model. It offers a variety of tools to help users first ingest and index their data — convert different formats of unstructured and structured data into a format that the language model can use, and query their private data. Ray is a powerful framework for scalable AI that solves Challenges #3 and #4. We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud. More specifically, we showcase a very relevant use case — highlighting Ray features that are present in both the documentation as well as the Ray blog posts! Data Ingestion and Embedding Pipeline We use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below. Code for this part of the blog is  available here . Sequential pipeline with “ingest”, “parse” and “embed” stages. Files are processed sequentially resulting in poor hardware utilization and long computation time. Parallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized. Load Data We start by ingesting these two sources of data. We first fetch both data sources and download the HTML files. We then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API’s, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by  Unstructured . from  typing  import   Dict ,  List \n from  pathlib  import  Path\n\n from  llama_index  import  download_loader\n from  llama_index  import  Document\n\n # Step 1: Logic for loading and parsing the files into llama_index documents. \nUnstructuredReader = download_loader( \"UnstructuredReader\" )\nloader = UnstructuredReader()\n\n def   load_and_parse_files ( file_row:  Dict [ str , Path] ) -&gt;  Dict [ str , Document]:\n    documents = []\n    file = file_row[ \"path\" ]\n     if  file.is_dir():\n         return  []\n     # Skip all non-html files like png, jpg, etc. \n     if  file.suffix.lower() ==  \".html\" :\n        loaded_doc = loader.load_data(file=file, split_documents= False )\n        loaded_doc[ 0 ].extra_info = { \"path\" :  str (file)}\n        documents.extend(loaded_doc)\n     return  [{ \"doc\" : doc}  for  doc  in  documents] Unstructured offers a robust suite of parsing tools on top of various files. It is able to help sanitize HTML documents by stripping out information like tags and formatting the text accordingly. Scaling Data Ingest Since we have many HTML documents to process, loading/processing each one serially is inefficient and slow. This is an opportunity to use Ray and distribute execution of the `load_and_parse_files` method across multiple CPUs or GPUs. import ray\n\n # Get the paths for the locally downloaded documentation. \nall_docs_gen =  Path ( \"./docs.ray.io/\" ).rglob( \"*\" )\nall_docs = [{ \"path\" : doc.resolve()}  for  doc  in  all_docs_gen]\n\n # Create the Ray Dataset pipeline \nds = ray.data.from_items(all_docs)\n\n # Use `flat_map` since there is a 1:N relationship. \n # Each filepath returns multiple documents. \nloaded_docs = ds.flat_map(load_and_parse_files) Parse Files Now that we’ve loaded the documents, the next step is to parse them into Node objects — a “Node” object represents a more granular chunk of text, derived from the source documents. Node objects can be used in the input prompt as context; by setting a small enough chunk size, we can make sure that inserting Node objects do not overflow the context limits. We define a function called `convert_documents_into_nodes` which converts documents into nodes using a simple text splitting strategy. # Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks. \n from  llama_index.node_parser  import  SimpleNodeParser\n from  llama_index.data_structs  import  Node\n\n def   convert_documents_into_nodes ( documents:  Dict [ str , Document] ) -&gt;  Dict [ str , Node]:\n    parser = SimpleNodeParser()\n    document = documents[ \"doc\" ]\n    nodes = parser.get_nodes_from_documents([document]) \n     return  [{ \"node\" : node}  for  node  in  nodes] Run Parsing in Parallel Since we have many documents, processing each document into nodes serially is inefficient and slow. We use Ray `flat_map` method to process documents into nodes in parallel: \n# Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\nnodes = loaded_docs.flat_map(convert_documents_into_nodes) Generate Embeddings We then generate embeddings for each Node using a Hugging Face Sentence Transformers model. We can do this with the help of LangChain’s embedding abstraction. Similar to document loading/parsing, embedding generation can similarly be parallelized with Ray. We wrap these embedding operations into a helper class, called `EmbedNodes`, to take advantage of Ray abstractions. # Step 3: Embed each node using a local embedding model. \n from  langchain.embeddings.huggingface  import  HuggingFaceEmbeddings\n\n class   EmbedNodes :\n     def   __init__ ( self ):\n        self.embedding_model = HuggingFaceEmbeddings(\n             # Use all-mpnet-base-v2 Sentence_transformer. \n             # This is the default embedding model for LlamaIndex/Langchain. \n            model_name= \"sentence-transformers/all-mpnet-base-v2\" , \n            model_kwargs={ \"device\" :  \"cuda\" },\n             # Use GPU for embedding and specify a large enough batch size to maximize GPU utilization. \n             # Remove the \"device\": \"cuda\" to use CPU instead. \n            encode_kwargs={ \"device\" :  \"cuda\" ,  \"batch_size\" :  100 }\n            )\n\n     def   __call__ ( self, node_batch:  Dict [ str ,  List [Node]] ) -&gt;  Dict [ str ,  List [Node]]:\n        nodes = node_batch[ \"node\" ]\n        text = [node.text  for  node  in  nodes]\n        embeddings = self.embedding_model.embed_documents(text)\n         assert   len (nodes) ==  len (embeddings)\n\n         for  node, embedding  in   zip (nodes, embeddings):\n            node.embedding = embedding\n         return  { \"embedded_nodes\" : nodes} Afterwards, generating an embedding for each node is as simple as calling the following operation in Ray: # Use `map_batches` to specify a batch size to maximize GPU utilization. \n # We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once.  \n\n # This state can be reused for multiple batches. \nembedded_nodes = nodes.map_batches(\n    EmbedNodes, \n    batch_size= 100 , \n     # Use 1 GPU per actor. \n    num_gpus= 1 ,\n     # There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors. \n    compute=ActorPoolStrategy(size= 4 ))\n\n # Step 5: Trigger execution and collect all the embedded nodes. \nray_docs_nodes = []\n for  row  in  embedded_nodes.iter_rows():\n    node = row[ \"embedded_nodes\" ]\n     assert  node.embedding  is   not   None \n    ray_docs_nodes.append(node) Data Indexing The next step is to store these nodes within an “index” in LlamaIndex. An index is a core abstraction in LlamaIndex to “structure” your data in a certain way — this structure can then be used for downstream LLM retrieval + querying. An index can interface with a storage or vector store abstraction. The most commonly used index abstraction within LlamaIndex is our vector index, where each node is stored along with an embedding. In this example, we use a simple in-memory vector store, but you can also choose to specify any one of LlamaIndex’s 10+ vector store integrations as the storage provider (e.g. Pinecone, Weaviate, Chroma). We build two vector indices: one over the documentation nodes, and another over the blog post nodes and persist them to disk. Code is  available here . from llama_index import GPTVectorStoreIndex\n\n# Store Ray Documentation embeddings\nray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\nray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n\n# Store Anyscale blog post embeddings\nray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\nray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\") That’s it in terms of building a data pipeline using LlamaIndex + Ray Data ! Your data is now ready to be used within your LLM application. Check out our next section for how to use advanced LlamaIndex query capabilities on top of your data. Data Querying LlamaIndex provides both simple and advanced query capabilities on top of your data + indices. The central abstraction within LlamaIndex is called a “query engine.” A query engine takes in a natural language query input and returns a natural language “output”. Each index has a “default” corresponding query engine. For instance, the default query engine for a vector index first performs top-k retrieval over the vector store to fetch the most relevant documents. These query engines can be easily derived from each index: ray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n\nray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context) LlamaIndex also provides more advanced query engines for multi-document use cases — for instance, we may want to ask how a given feature in Ray is highlighted in both the documentation and blog. `SubQuestionQueryEngine` can take in other query engines as input. Given an existing question, it can decide to break down the question into simpler questions over any subset of query engines; it will execute the simpler questions and combine results at the top-level. This abstraction is quite powerful; it can perform semantic search over one document, or combine results across multiple documents. For instance, given the following question “What is Ray?”, we can break this into sub-questions “What is Ray according to the documentation”, and “What is Ray according to the blog posts” over the document query engine and blog query engine respectively. # Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False) Have a look at  deploy_app.py  to review the full implementation. Deploying with Ray Serve We’ve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components. To do this, you just need to do the following steps: Define an outer class that can “wrap” a query engine, and expose a “query” endpoint Add a `@ray.serve.deployment` decorator on this class Deploy the Ray Serve application It will look something like the following: from  ray  import  serve\n\n @serve.deployment \n class   QADeployment :\n     def   __init__ ( self ):\n self.query_engine = ...\n\n     def   query ( self, query:  str ):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str =  \"\" \n             for  i  in   range ( len (source_nodes)):\n                node = source_nodes[i]\n                source_str +=  f\"Sub-question  {i+ 1 } :\\n\" \n                source_str += node.node.text\n                source_str +=  \"\\n\\n\" \n             return   f\"Response:  { str (response)}  \\n\\n\\n  {source_str} \\n\" \n\n     async   def   __call__ ( self, request: Request ):\n        query = request.query_params[ \"query\" ]\n         return   str (self.query(query))\n\n # Deploy the Ray Serve application. \ndeployment = QADeployment.bind() Have a look at the  deploy_app.py  for full implementation. Example Queries Once we’ve deployed the application, we can query it with questions about Ray. We can query just one of the data sources: Q: \"What is Ray Serve?\"\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard for monitoring applications. But, we can also provide complex queries that require synthesis across both the documentation and the blog posts. These complex queries are easily handled by the subquestion-query engine that we defined. Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n\nResponse: \nThe Ray docs and the Ray blogs both present Ray Serve as a web interface\nthat provides metrics, charts, and other features to help Ray users\nunderstand and debug Ray applications. However, the Ray docs provide more\ndetailed information, such as a Quick Start guide, user guide, production\nguide, performance tuning guide, development workflow guide, API reference,\nexperimental Java API, and experimental gRPC support. Additionally, the Ray\ndocs provide a guide for migrating from 1.x to 2.x. On the other hand, the\nRay blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\nhelp users get started and understand the features of Ray Serve.\nAdditionally, the Ray blogs provide examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\n\n---\n\nSub-question 1\n\nSub question: How does the Ray docs present Ray Serve\n\nResponse: \nThe Ray docs present Ray Serve as a web interface that provides metrics,\ncharts, and other features to help Ray users understand and debug Ray\napplications. It provides a Quick Start guide, user guide, production guide,\nperformance tuning guide, and development workflow guide. It also provides\nan API reference, experimental Java API, and experimental gRPC support.\nFinally, it provides a guide for migrating from 1.x to 2.x.\n\n---\n\nSub-question 2\n\nSub question: How does the Ray blogs present Ray Serve\n\nResponse: \nThe Ray blog presents Ray Serve as a framework for distributed applications\nthat enables users to handle HTTP requests, scale and allocate resources,\ncompose models, and more. It provides a Quick Start guide, a User Guide, and\nAdvanced Guides to help users get started and understand the features of Ray\nServe. Additionally, it provides examples and use cases to help users\nunderstand how to use Ray Serve in their own projects. Conclusion In this example, we showed how you can build a scalable data pipeline and a powerful query engine using LlamaIndex + Ray. We also demonstrated how to deploy LlamaIndex applications using Ray Serve. This allows you to effortlessly ask questions and synthesize insights about Ray across disparate data sources! We used LlamaIndex — a data framework for building LLM applications — to load, parse, embed and index the data. We ensured efficient and fast parallel execution by using Ray. Then, we used LlamaIndex querying capabilities to perform semantic search over a single document, or combine results across multiple documents. Finally, we used Ray Serve to package the application for production use. Implementation in open source, code is available on GitHub:  LlamaIndex-Ray-app What’s next? Visit LlamaIndex  site  and  docs  to learn more about this data framework for building LLM applications. Visit  Ray docs  to learn more about how to build and deploy scalable LLM apps. Join our communities! Join Ray community  on Slack and Ray #LLM channel. You can also join the LlamaIndex  community on discord . We have our  Ray Summit 2023  early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.", "author": "Jerry Liu", "date": "Jun 27, 2023", "tags": ["NLP", "Distributed Systems", "AI", "Large Language Models", "Parallel Computing"]},
{"title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens", "content": "Authors:  Anupam Datta, Shayak Sen, Jerry Liu, Simon Suo Source Link:  https://truera.com/build-and-evaluate-llm-apps-with-llamaindex-and-trulens/ LlamaIndex is a popular open source framework for building LLM apps. TruLens is an open source library for evaluating, tracking, and iterating on LLM apps to improve their quality. The LlamaIndex and TruLens teams are actively collaborating to enable LLM app developers to rapidly build, evaluate, and iterate on their apps. In the latest release of TruLens, we introduce tracing for LlamaIndex based LLM applications that allow you to evaluate and track your experiments with just a few lines of code. This lets you automatically evaluate a number of different components of the application stack including: App inputs and outputs LLM calls Retrieved context chunks from an index Latency Cost and Token Counts (coming soon!) Check out this  notebook  to get started and read along to get a step by step view. How do I actually use this? Build A LlamaIndex App LlamaIndex lets you connect your data to LLMs and rapidly build applications for a number of different use cases. from  llama_index  import  VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader( 'llama_index/data' ).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine() Once you build your app, you can easily query your data: response = query_engine.query( \"What did the author do growing up?\" )\n print (response) And you get an appropriate response. Growing up, the author wrote  short  stories, programmed on an IBM  1401 , and nagged his father to buy him a TRS- 80  microcomputer. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but switched to AI after becoming bored with it. He then took art classes at Harvard and applied to art schools, eventually attending RISD. Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a TruLlama wrapper. This wrapper preserves all LlamaIndex behavior, but traces all of the intermediate steps so that they can be individually evaluated. from  trulens_eval  import  TruLlama\nl = TruLlama(query_engine) The wrapped app can now be queried in the exact same way: response = l.query( \"What did the author do growing up?\" )\n print (response) Except, now the details of the query are logged by TruLens. Add Feedback Functions Now to evaluate the behavior of your models, we can add feedback functions to your wrapped application. Note that as a developer you only need to  add a few lines of code  to start using feedback functions in your apps. You can also easily add functions tailored to the needs of your application. Our goal with feedback functions is to programmatically check the app for quality metrics. The first feedback function checks for language match between the prompt and the response. It’s a useful check since a natural user expectation is that the response is in the same language as the prompt. It is implemented with a call to a HuggingFace API that programmatically checks for language match. The next feedback function checks how relevant the answer is to the question by using an Open AI LLM that is prompted to produce a relevance score. Finally, the third feedback function checks how relevant individual chunks retrieved from the vector database are to the question, again using an OpenAI LLM in a similar manner. This is useful because the retrieval step from a vector database may produce chunks that are not relevant to the question and the quality of the final response would be better if these chunks are filtered out before producing the final response. from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n\n # Initialize Huggingface-based feedback function collection class: \nhugs = feedback.Huggingface()\nopenai = feedback.OpenAI()\n # Define a language match feedback function using HuggingFace. \nf_lang_match = Feedback(hugs.language_match).on_input_output()\n # By default this will check language match on the main app input and main app \n # output. \n\n # Question/answer relevance between overall question and answer. \nf_qa_relevance = Feedback(openai.relevance).on_input_output()\n\n # Question/statement relevance between question and each context chunk. \nf_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n    TruLlama.select_source_nodes().node.text\n).aggregate(np. min )\n\n\nfeedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n\nl = TruLlama(app=query_engine, feedbacks=feedbacks) Explore In Dashboard Every query that is tracked can now be viewed in the TruLens dashboard. After running the feedback functions on a set of records (interactions), you can see the aggregate results of the evaluation on a leaderboard; then drill down into an app version and examine how it is performing on individual records. These steps can help you understand the quality of an app version and its failure modes. In this example, the model is doing fairly well on the relevance and language match feedback evaluations, but seems to be doing poorly on qs_relevance. This can be an indicator that the retrieved chunks are often irrelevant. This can be a significant source of “hallucinations” in retrieval-augmented generative AI apps. We can now drill down and identify specific instances where this may be an issue: Let’s look at a good example first. “What did the author do growing up?” In this example, we retrieved two chunks from the index both of which were fairly relevant to the the question and as a result the LLM summarizes it into a relevant and factually correct answer. On the other hand, let’s look at an example where this didn’t go so well: “Where was the author born?”. In this example, the app confidently provides an incorrect answer. In this example, the two pieces of context retrieved had moderate relevance to the question. Further, neither context contained the answer. Even though our relevance feedback function (which doesn’t check for factual correctness) didn’t detect an issue, because the underlying chunks were not very relevant, this was a strong indicator that something was off. Indeed, this is an example of the model hallucinating on a question that is fairly easy to fact check. Iterate on your App Once you find issues like this with your app, it can be helpful to iterate on your prompts, models and chunking approaches to optimize your app. As you do this, you can track the performance of each version of your model with TruLens. Here is an example of a dashboard with multiple iterations testing against each other.", "author": "Jerry Liu", "date": "Jun 23, 2023", "tags": ["AI", "Mlops", "Large Language Models", "NLP"]},
{"title": "Enriching LlamaIndex Models with GraphQL and Graph Databases", "content": "In this article I wanted to share the process of adding new data loaders to LlamaIndex. First we’ll look at what LlamaIndex is and try a simple example of providing additional context to an LLM query using a simple CSV loader. Then we look at how easy it is to add a new loader for graph databases to LlamaIndex. And lastly we try that new loader and another loader for GraphQL APIs that I added in practice and see how their extra context can help an LLM answer questions better. Background/Context I was listening to the  \"This Week in ML\" (twiml) Podcast  where  Jerry Liu  from LlamaIndex (previously GPT-Index) explained the ideas behind the library to enrich query contexts to LLMs with data from any number of sources. LlamaIndex  is a toolkit to augment LLMs with your own (private) data using in-context learning. It takes care of selecting the right context to retrieve from large knowledge bases. To achieve that it utilizes a number of connectors or loaders (from  LlamaHub ) and data structures (indices) to efficiently provide the pre-processed data as  Documents . Each type of index stores documents in different ways, e.g via embeddings for vector search, as a simple list or graph or tree structure. Those indices are used as query interface to the LLM, transparently embedding the relevant context. Besides the higher quality response from the LLM, you get also the documents returned that have been used to construct the answer. LlamaIndex also allows chain of thought reasoning, compare/contrast queries, and natural language querying of databases. See also this presentation from Jerry: All the code for the blog post is available in this  Colab Notebook . Using a Basic CSV Loader Here is an example of using a basic CSV loader to provide documents for LlamaIndex. In our Notebook we download the  countries.csv  via the  Countries List Project  (MIT) ( raw source ). Our dependencies are  llama-index  and  python-dotenv . !pip install llama-index==0.6.19 python-dotenv We need to provide our OpenAI-api key, to avoid accidentally leaking it in the notebook, I uploaded an  openai.env  file and use the  dotenv  library to load the contents as environment variables. In the next step we load the env file and prepare the OpenAI  ChatGPTLLMPredictor  (using  gpt-3.5-turbo  by default) and add it to the  ServiceContext  . import  os\n from  pathlib  import  Path\n from  llama_index  import  GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext, GPTListIndex\n from  llama_index.llm_predictor.chatgpt  import  ChatGPTLLMPredictor\n from  dotenv  import  load_dotenv\n from  llama_index  import  download_loader\n\nload_dotenv( \"openai.env\" )\n\nllm_predictor = ChatGPTLLMPredictor()\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor) Now we can use the loader to load the CSV and turn it into documents, create an an GPT Index ( VectorStoreIndex  in this case), which LlamaIndex can then use to retrieve the relevant information to pass along in the context to the LLM. Initializing CSV Loader and GPTVectorStoreIndex SimpleCSVReader = download_loader( \"SimpleCSVReader\" )\nloader = SimpleCSVReader(concat_rows= False )\ndocuments = loader.load_data(file=Path( './countries.csv' ))\n\n print (documents)\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context) Documents from the CSV Loader [Document(text='country, capital, type', doc_id='67c30c68-7d9f-4906-945b-9affc96f95d2', embedding=None, doc_hash='3a506ebea9c04655b51406d79fdf5e3a87c3d8ff5b5387aace3e5a79711a21b8', extra_info=None),\nDocument(text='Abkhazia, Sukhumi, countryCapital', doc_id='6e6be4b5-051f-48e0-8774-6d48e0444785', embedding=None, doc_hash='ea387d0eab94cc6c59f98c473ac1f0ee64093901673b43e1c0d163bbc203026e', extra_info=None),\n...] The CSV loader didn’t create one Document per CSV row by default, but only one for the whole document, but you could configure it so that it turned the CSV into one document per row. LlamaIndex supports much more involved setups of different kinds of indexes, allows to chain them and even conditionally select one or the other. Here we just do the bare minimum to demonstrate our loaders. After setting up the indices with the appropriate loaders, and connected indexes, we now can use the index as an LLM query engine and execute our user query. To demonstrate that the LLM still is able to use its world knowledge, we can ask in a mix of English (System), German (Question) and French (requested Answer). queryEngine = index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nProvide the answer in French.\nQuestion: Was ist die Hauptstadt von Albanien?\n\"\"\") As you can see in the response below it doesn’t just answer our question correctly in French  La capitale de l’Albanie est Tirana. , but also provides which documents it used to generate the answer. Response(response=\"La capitale de l'Albanie est Tirana.\", \nsource_nodes=[NodeWithScore(node=Node(text='               &lt; td &gt; Albania &lt; /td &gt; ', doc_id='3decbee1-98cc-4650-a071-ed25cd3e00d5', embedding=None, doc_hash='7d9d85082095471a9663690742d2d49fc37b2ec37cc5acf4e99e006a68a17742', extra_info=None, \nnode_info={'start': 0, 'end': 30, '_node_type':  &lt; NodeType.TEXT: '1' &gt; }, \nrelationships={ &lt; DocumentRelationship.SOURCE: '1' &gt; : '7b6c861f-2c2f-4905-a047-edfc25f7df19'}), score=0.7926356007369129), \nNodeWithScore(node=Node(text='               &lt; td &gt; Algiers &lt; /td &gt; ', doc_id='8111b737-9f45-4855-8cd8-f958d4eb0ccd', embedding=None, doc_hash='8570a02a057a6ebbd0aff6d3f63c9f29a0ee858a81d913298d31b025101d1e44', \nextra_info=None, node_info={'start': 0, 'end': 30, '_node_type':  &lt; NodeType.TEXT: '1' &gt; }, relationships={ &lt; DocumentRelationship.SOURCE: '1' &gt; : '22e11ac6-8375-4d0c-91c6-4750fc63a375'}), score=0.7877589022795918)], extra_info={'3decbee1-98cc-4650-a071-ed25cd3e00d5': None, '8111b737-9f45-4855-8cd8-f958d4eb0ccd': None}) LlamaIndex Loaders The number of existing data sources in  LlamaHub  is impressive, I counted 100+ integrations in  the repository . You can find anything from Google docs, to GitHub, to relational databases. LlamaHub, screenshot by Author But I was missing two of my favorite technologies: GraphQL - the API query language open sourced by Facebook and Graph databases like Neo4j, the best way to store and manage large amounts of connected data, for example in Knowledge Graphs. So I thought: \"How hard can it be to add them :)\" Adding the new loaders Adding new loaders is really straightforward. There is a script in the llama-hub repository to help with adding a new loader. Running  ./add-loader.sh <folder>  added the skeleton files. To get familiar with the existing implementations I looked at the  Databases (relational)  and  MongoDB integrations , the former for the Graph Database and the latter for the GraphQL. It was easy enough, we only needed the requirements for our loader, implement the  base.py  with an straightforward API and a  README.md`  with an explanation and a code example. The main difference my loaders have from the existing ones, is that they don’t use hard-coded field names for extracting the relevant value from the query result, but instead turn the result into YAML. I picked YAML not because I like it, but because it was closest to a textual representation of a nested tree of key-value pairs that a user would write as nested bullet lists. Below is the example code for the Graph Database implementation (the GraphQL one is similar). Adding the Graph Database Loader I added the requirements for the  neo4j  dependency, a Cypher query language over Bolt protocol python driver, that also works with Memgraph and AWS Neptune. Then I added the code for  __init__  to take in a database server URI, database name and credentials to connect and create a driver instance. The  load_data  method takes in the query to run and optional parameters. It’s implemented by calling the driver’s  execute_query  method. Each row of results is mapped into a LlamaIndex  Document  with the  text  being the YAML representation of the results. \"\"\"Graph Database Cypher Reader.\"\"\" \n\n from  typing  import   Dict ,  List ,  Optional \n\n from  llama_index.readers.base  import  BaseReader\n from  llama_index.readers.schema.base  import  Document\n\n import  yaml\n\n class   GraphDBCypherReader ( BaseReader ):\n     \"\"\"Graph database Cypher reader.\n\n    Combines all Cypher query results into the Document type used by LlamaIndex.\n\n    Args:\n        uri (str): Graph Database URI\n        username (str): Username\n        password (str): Password\n\n    \"\"\" \n\n     def   __init__ ( \n        self,\n        uri:  str ,\n        username:  str ,\n        password:  str ,\n        database:  str \n     ) -&gt;  None :\n         \"\"\"Initialize with parameters.\"\"\" \n         try :\n             from  neo4j  import  GraphDatabase, basic_auth\n\n         except  ImportError:\n             raise  ImportError(\n                 \"`neo4j` package not found, please run `pip install neo4j`\" \n            )\n         if  uri:\n             if  uri  is   None :\n                 raise  ValueError( \"`uri` must be provided.\" )\n            self.client = GraphDatabase.driver(uri=uri, auth=basic_auth(username, password))\n            self.database = database\n\n     def   load_data ( \n        self, query:  str , parameters:  Optional [ Dict ] =  None \n     ) -&gt;  List [Document]:\n         \"\"\"Run the Cypher with optional parameters and turn results into documents\n\n        Args:\n            query (str): Graph Cypher query string.\n            parameters (Optional[Dict]): optional query parameters.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\" \n         if  parameters  is   None :\n            parameters = {}\n\n        records, summary, keys = self.client.execute_query(query, parameters, database_ = self.database)\n\n        documents = [Document(yaml.dump(entry.data()))  for  entry  in  records]\n\n         return  documents You’re now ready to start using the data loader. If you want to start using this in your code, simply import `GraphDBCypherReader` from the relevant file and follow the steps below. If you wish to submit the loader on LlamaHub, the process is fairly straightforward. After adding an example to the readme which uses an always-on demo server with StackOverflow data, I was ready to create a  pull request . After a short discussion the PR was quickly merged. Thanks a lot Jerry for the smooth experience. Now let’s see how to use our two loaders. Using the Graph Database Loader The GraphDB Cypher loader, connects to graph databases, which are specialized databases that store data not in tables but in entities ( Nodes ) and their  Relationships . Because they are schema free, you can store real-world knowledge without compromising on richness. Image for “Network Graph” generated by Midjourney by Author Relationships can also hold attributes, which can represent time, weights, costs or whatever defines the concrete relationship. Any node can have as many or as few attributes or relationships as needed. To query a graph database you can use the  Cypher  query language, a pattern based language that expresses those relationships in visual ascii-art patterns. You encircle nodes in parentheses  ()  and draw relationships as arrows  -->  with additional constraints put in square brackets. Otherwise Cypher provides many features known from SQL and also supports many graph operations as well as handling data structures like nested documents, of lists and dicts. Let’s use a movie graph database and ask the LLM a question about  common action movie plots . Setting up the  ServiceContext  and the  ChatGPTLLMPredictor  is the same as before. Then we get the  GraphDBCypherReader  and connect it to our database (with an small example movie graph from  TheMovieDB  with permission). GraphDBCypherReader = download_loader('GraphDBCypherReader')\n\nreader = GraphDBCypherReader(uri = \"neo4j+s://demo.neo4jlabs.com\", \\\n    username = \"recommendations\", password = \"recommendations\", database = \"recommendations\") Then we define our query to the graph database with a parameter of year that allows us to pick more recent movies. When loading the data, each row of results should turn into one  Document  where the  text  property of the document is the YAML representation of the row. query =  \"\"\"\n    MATCH (m:Movie)-[rel:ACTED_IN|DIRECTED|IN_GENRE]-(other)\n    WHERE $year &lt; m.year and m.imdbRating &gt; $rating\n    WITH m, type(rel) as relation, collect(other.name) as names\n    RETURN m.title as title, m.year as year, m.plot as plot, relation, names\n    ORDER BY m.year ASC\n\"\"\" \n\ndocuments = reader.load_data(query, parameters = { \"year\" : 1990 , \"rating\" : 8 })\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\n print ( len (documents))\n print (documents[ 0 : 5 ]) The output will look similar to the following: 829\n[Document(text='names:\\n- Saifei He\\n- Li Gong\\n- Jingwu Ma\\n- Cuifen Cao\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: ACTED_IN\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='782d9a63-251b-4bb8-aa3d-5d8f6d1fb5d2', embedding=None, doc_hash='f9fd966bc5f2234e94d09efebd3be008db8c891f8666c1a364abf7812f5d7a1c', extra_info=None), Document(text='names:\\n- Yimou Zhang\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: DIRECTED\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='2e13caf6-b9cf-4263-a264-7121bc77d1ee', embedding=None, doc_hash='e1f340ed1fac2f1b8d6076cfc2c9e9cb0109d5d11e5dcdbf3a467332f5995cb1', extra_info=None), ...] Now we can use our  index  to run a LLM query to answer the questions we wanted to pose. queryEngine= index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nWhat are the most common plots in action movies?\n\"\"\") The answer shows that the LLM can utilize the inputs, understands the genre \"action movies\" and can summarize their plots. Here is its answer. Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies. Response(response='Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies.',\n\n\nsource_nodes=[NodeWithScore(node=Node(text='names:\\n- Action\\n- Crime\\n- Thriller\\nplot: A group of professional bank robbers start to feel the heat from police when\\n  they unknowingly leave a clue at their latest heist.\\nrelation: IN_GENRE\\ntitle: Heat\\nyear: 1995\\n', doc_id='bb117618-1cce-4cec-bd9b-8645ab0b50a3', embedding=None, doc_hash='4d493a9f33eb7a1c071756f61e1975ae5c313ecd42243f81a8827919a618468b', extra_info=None, node_info={'start': 0, 'end': 215, '_node_type':  &lt; NodeType.TEXT: '1' &gt; }, relationships={ &lt; DocumentRelationship.SOURCE: '1' &gt; : 'dbfffdae-d88c-49e2-9d6b-83dad427a3f3'}), score=0.8247381316731472), NodeWithScore(node=Node(text='names:\\n- Thriller\\n- Sci-Fi\\n- Action\\nplot: A computer hacker learns from mysterious rebels about the true nature of his\\n  reality and his role in the war against its controllers.\\nrelation: IN_GENRE\\ntitle: Matrix, The\\nyear: 1999\\n', doc_id='c4893c61-32ee-4d05-b559-1f65a5197e5e', embedding=None, doc_hash='0b6a080bf712548099c5c8c1b033884a38742c73dc23d420ac2e677e7ece82f4', extra_info=None, node_info={'start': 0, 'end': 227, '_node_type':  &lt; NodeType.TEXT: '1' &gt; }, relationships={ &lt; DocumentRelationship.SOURCE: '1' &gt; : '6c8dea11-1371-4f5a-a1a1-7f517f027008'}), score=0.8220633045996049)], extra_info={'bb117618-1cce-4cec-bd9b-8645ab0b50a3': None, 'c4893c61-32ee-4d05-b559-1f65a5197e5e': None}) Using the GraphQL Loader The GraphQL loader is similarly easy to use. GraphQL  is not a database query language, but an API query language that is based on strict schema expressed in \"type definitions\". There you express your entities, their attributes (fields) both for scalar datatypes as well as object datatypes pointing to other entities. What is GraphQL from GraphQL.org, Screenshot by Author GraphQL itself is a tree based query language, that expresses a nested structure of data that you want to fetch starting from a root query. The fields of every entity returned from that query can be selected and for object fields you can further select fields from the referred entity and so on, almost ad-infinitum (API-Limits apply). There are a number of GraphQL libraries, most notably the JavaScript reference implementation, but also  gql  for python, and also integrations with databases like Hasura, Prisma or the  Neo4j-GraphQL-Library . Several larger projects now provide GraphQL APIs including GitHub, Spotify, Twitter. The demo is similar to our first one. We use a public GraphQL endpoint ( https://countries.trevorblades.com/ ), that provides a structure of continent→country→capital. ( Licensed under MIT ) A subset of the type-definition is here. type  Query  {\n     continent ( code :  ID !):  Continent \n     continents ( filter :  ContinentFilterInput  = {}): [ Continent !]!\n     countries ( filter :  CountryFilterInput  = {}): [ Country !]!\n     country ( code :  ID !):  Country \n     language ( code :  ID !):  Language \n     languages ( filter :  LanguageFilterInput  = {}): [ Language !]!\n}\n\ntype  Continent  {\n     code :  ID !\n     countries : [ Country !]!\n     name :  String !\n}\n\ntype  Country  {\n     awsRegion :  String !\n     capital :  String \n     code :  ID !\n     continent :  Continent !\n     currencies : [ String !]!\n     currency :  String \n     emoji :  String !\n     emojiU :  String !\n     languages : [ Language !]!\n     name ( lang :  String ):  String !\n     native :  String !\n     phone :  String !\n     phones : [ String !]!\n     states : [ State !]!\n     subdivisions : [ Subdivision !]!\n}\n... In our demo, we again define the  ServiceContext  with the  ChatGPTLLMPredictor  as before. Then we get the  GraphQLReader  loader and point it to the URL of the endpoint. You can also provide additional HTTP-Headers, e.g. for authentication. GraphQLReader = download_loader('GraphQLReader')\nreader = GraphQLReader(uri = \"https://countries.trevorblades.com/\", headers = {}) query =  \"\"\"\nquery getContinents {\n  continents {\n    name\n    countries {\n      name\n      capital\n    }\n  }\n}\n\"\"\" \ndocuments = reader.load_data(query, variables = {})\n print ( len (documents))\n print (documents) We see that it finds 7 continents with countries and capitals, each of the root results (continent) is turned into a document 7\n[Document(text='countries:\\n- capital: Luanda\\n  name: Angola\\n- capital: Ouagadougou\\n  name: Burkina Faso\\n- capital: Bujumbura\\n  name: Burundi\\n- capital: Porto-Novo\\n  name: Benin\\n- capital: Gaborone\\n  name: Botswana\\n- capital: Kinshasa\\n  name: Democratic Republic of the Congo\\n- capital: Bangui\\n  name: Central African Republic\\n....',doc_id='b82fec36-5e82-4246-b7ab-f590bf6741ab', embedding=None, doc_hash='a4caa760423d6ca861b9332f386add3c449f1683168391ae10f7f73a691a2240', extra_info=None)] Again we stress the LLM only a little bit by asking it in German, \"Which capitals are in North America\". index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\nqueryEngine= index.as_query_engine()\n\nresponse = queryEngine.query(\"\"\"\nQuestion: Welche Hauptstädte liegen in Nordamerika?\nAnswer:\n\"\"\")\n\nresponse.response I was surprised, as I had only expected a hand-full of countries and cities. But we get 27 countries that are in North America. This shows how our perception is skewed by the western worldview. Die Hauptstädte, die in Nordamerika liegen, sind Ottawa, San Jos\\xE9, Havana, Willemstad, Roseau, Santo Domingo, St. George's, Nuuk, Guatemala City, Tegucigalpa, Port-au-Prince, Kingston, Basseterre, George Town, Castries, Marigot, Fort-de-France, Plymouth, Mexico City, Managua, Panama City, Saint-Pierre, San Juan, San Salvador, Philipsburg, Cockburn Town, Port of Spain, Washington D.C., Kingstown und Road Town. We could also flip the GraphQL query around and then get 250 countries with their respective capitals and continents. query =  \"\"\"\nquery getCountries {\n  countries {\n    name\n    capital\n    continent {\n        name\n    }\n  }\n}\n\"\"\" \ndocuments = reader.load_data(query, variables = {})\n print ( len (documents))\n print (documents) Both document lists should work equally well, but let’s see. This time the answer from the LLM was much more limited. I’m not sure if that was because the index fed the LLM fewer documents to pick from. index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\nqueryEngine= index.as_query_engine()\n\nresponse = queryEngine.query(\"\"\"\nQuestion: Which capitals are in North America?\nAnswer:\n\"\"\")\n\nresponse.response Washington D.C. and Mexico City are in North America. Conclusion It was really smooth to add new data loaders to LlamaHub, thanks a lot to  Jerry Liu  for making it so easy. Please let me know what you’re doing with these loaders and if you have any feedback. If I find time in the next weeks I also want to look into the  KnowledgeGraphIndex  and see if my graph database loader can nicely populate that one.", "author": "Michael Hunger", "date": "Jun 30, 2023", "tags": ["LLM", "GraphQL", "Graph Database", "Llamaindex", "Python"]},
{"title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)", "content": "Overview I had the pleasure of speaking with Sam Charrington on the  TWIML AI podcast  about LlamaIndex, and the episode was just released this past Monday (5/8/23). \n \n I thought it would be a fun experiment to distill some highlights from the podcast! And what better way to do this than using LlamaIndex itself (plus OpenAI Whisper for transcription)? 😉 I did the following: Ran the podcast through  whisper.cpp Did some light manual cleaning of the text, and uploaded it as a  Dropbox file . Get a high-level summary + ask some more targeted questions over the podcast transcript. Uses LlamaIndex 🦙.  Colab notebook here ! We used the our list index to get an overall summary of the podcast, and also our vector index to ask more specific questions. We ask some basic questions ourselves, but also directly ask questions that Sam asked during the podcast in order to extract more concise versions of the responses that I gave (including removing a bunch of filler “you know” words). All the distilled highlights using LlamaIndex are showcased below. Let us know what you think! Distilled Highlights What are three key points described in this podcast? Give a followup of each key point with an explanation. LlamaIndex is a toolkit for connecting language models to data.  It was initially designed as a fun tool to play around with and solve a specific problem, but has since evolved into a set of useful tools and instructions that people can use to build applications on top of. The technique people were playing around with at the time was the idea of prompting and context learning, which is providing the right conditioning variable for the input and using that to send some sort of input prompt to the language model and get back a response. Additionally, LlamaIndex offers more advanced primitives such as decision-making at the top level to route queries to the right sub-data structure, and the ability to synthesize an answer from multiple data sources. It also provides an outer abstraction layer that can automatically reason which structure to use given an outer query request, and can be used as a drop-in module on top of existing data systems without having to worry about complexity. Examples of applications that can be built on top of LlamaIndex include ingesting video and structured data to parse into an audio transcript, running image captioning models, and creating augmented chatbot experiences on top of web scrapers. LlamaIndex is also exploring the idea of automation and unifying everything under a single query interface,  so that users don’t have to specify a different parameter for every use case. This includes optimizing token usage, making queries faster, and reducing costs for the user. Additionally, LlamaIndex is looking into applying automation to the data system world, such as teaching Oracle databases how to spit out natural language prompt responses, and making the data stack more efficient. This includes simplifying the data stack over time, especially as language models take off, and leveraging capabilities of LLM’s and various components of the data landscape to simplify the number of steps it takes from raw data to insight for the user. They are also exploring the idea of inferring the right schemas and writing structured data from unstructured data, as well as automatically building a natural language query interface with a view of the data within the data system. LlamaIndex is also exploring the idea of agents as a layer of automation for decision making over any sort of function that you want to run.  This includes taking in some input and doing reasoning under the hood to decide, make a decision over some input, as well as some access to some context, for instance, over your data or over the set of tools that is able to have access to. Additionally, LlamaIndex is looking into ways to reduce cost and latency, such as using more fine-tuned distilled models that are a bit smaller, and making sure that the more decisions that are chained together, the less errors propagate over time. They are also exploring the idea of observability and evidence across a chain of relatively independent decisions that individual agents are making, as well as the interfaces that these agents might use, such as traditional software and agent worlds. What is the origin story of LlamaIndex? The origin story of LlamaIndex is that it was founded in November by Jerry, who was trying to build a sales bot. He was playing around with GPT-3 and wanted to use it on his internal company data. He wanted to use it to synthesize a to do list for him for the next customer meeting, as he had to spend 20–30 minutes reviewing notes from the previous call transcripts. This led to the idea of stuffing data from Notions, Slack, Salesforce, data lakes, vector databases, and structure databases into language models. This was the impetus for LlamaIndex, which is focused on connecting data to language models and tapping into the capabilities of language models to utilize them on top of private sources of data. What is LlamaIndex doing beyond top-k retrieval? LlamaIndex is offering more advanced primitives on top of basic top-k retrieval in order to provide responses to more complicated questions. These primitives include decision-making at the top level to route queries to the right sub-data structure, synthesizing information from multiple data systems, and providing trade-offs between different approaches. Additionally, LlamaIndex is working on building tooling to help users create customizable indexes and views of their data to allow them to execute different types of queries. This includes connecting to existing data systems, defining metadata on top of each unit of data, providing the building blocks to create different types of indexes, and abstracting away complexity with an outer agent layer that can automatically reason which structure to use given a query request. This allows users to get the best results for a query, while also providing an alternative to something like a langchain or using it as part of building a broader solution. [Sam] It sounds like we’re starting to identify a higher level of abstraction that different use cases will fall under. Is it more the case that there’s some manageable number of these primitives, like 10, 20, or is it that every use case is going to be a little bit different, and there are hundreds of thousands of kind of fundamental ways that people want to work with their documents, and so you need to just give them a very open capability? Jerry’s response is that there are probably a few different use cases that people tend to want to get answers from over their data, and it is possible there is a giant long tail of different tasks. He believes that the complexity of the task scales with the number of steps it requires to execute, and that users need to be given customizable building blocks in order to get the results they want. He also believes that the next natural step is to automate the process and unify everything under a single query interface, so that users don’t have to specify different parameters for every use case. He also believes that this paradigm is displacing more static paradigms like ETL, and that it is applicable to a wide range of applications. He sees this agent type environment becoming fundamental infrastructure that reimagines the entire existing enterprise data stack, and that it can be used to parse unstructured data into structured data, as well as to automatically reason how to best transform data from one place to another. He also believes that this will make the job of the data engineer and data scientist much more efficient, and that it will enable the creation of natural language query interfaces that have a view of the data within the data system. [Sam] When you think about the interface between LLM-based data processing system and the data sources of record, what does that interface evolve to look like? For example, does it evolve to look like the chat GPT plugin model, where we’re going to teach our Oracle databases how to spit out natural language prompt responses, that kind of thing, or do you think that there’s some more efficient way of doing that or is that more efficient? Like, what’s your view of the way these things evolve? I think the way this interface will evolve is that it will become more automated and efficient. We will be able to use language models to understand raw text and extract the relevant information from it, without having to manually enter data into a structured format. We will also be able to use agents to automate decision making and provide a unified query interface, so that users don’t have to specify different parameters for every use case. Additionally, we can use LlamaIndex to structure data in a way that allows us to make use of the limited prompt size of GPT-3, while still being able to achieve the task. We can also use this data stack to infer the right schemas and further write structured data from unstructured data, as well as automatically build a natural language query interface that has a view of the data within the data system. This will enable us to make the job of the data engineer and data scientist much more efficient by having automated reasoning agents over deciding, making decisions at every stage of the data infrastructure stack. Want to ask your own questions over the podcast? If you want to build your own LLM-powered chatbot over our TWIML podcast, check out the resources below! Colab notebook Raw Transcript Podcast on Spotify", "author": "Jerry Liu", "date": "May 10, 2023", "tags": ["Artificial Intelligence", "Machine Learning", "Large Language Models", "NLP"]},
{"title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)", "content": "We had an awesome time at the Berkeley Hackathon two weeks ago (6/17–6/18). The attendance stats were impressive: 1200 hackers 262 submitted projects 2 real-life llamas 🦙 LlamaIndex sponsored a “Best Knowledge-Intensive LLM App” prize series at the hackathon. The criteria was an app that leveraged a knowledge base of custom data to build innovative new application experiences. We announced three prize winners along with an honorable mention. We are excited to feature each project in a special highlight below. In each highlight, the creators describe the project mission and what it solves, the implementation+tech stack, challenges, and future directions. Check it out! 👇 First Prize Winner: Helmet AI Creators: Jaiveer Singh, Devin Mui, Ethan Mehta, Manav Rathod Devpost:  https://devpost.com/software/helmet-ai Introduction In today’s rapidly evolving business landscape, staying ahead of the competition is paramount for success. However, the deluge of information and the ever-changing market dynamics can make it challenging for business leaders to make informed decisions. In this blog post, we introduce Helmet AI, a cutting-edge market intelligence tool designed to empower leadership teams with real-time insights and a competitive edge. Join us as we explore the capabilities, technology stack, and future prospects of Helmet AI. Unveiling Helmet AI Helmet AI is an innovative market intelligence tool that harnesses the power of advanced technologies to provide leaders with actionable insights and an unparalleled understanding of the global business landscape. With its context-aware Ingestion Engine and Insight Extractor powered by OpenAI’s GPT models, Helmet AI offers a comprehensive solution for tracking breaking news, uncovering hidden relationships, and extracting valuable, personalized insights from vast amounts of data. For ease of use, Helmet AI displays these insights in a familiar, Twitter-like “Feed” interface. Additionally, Helmet AI offers a Chat interface for users to ask questions about a particular news story to Helmet’s knowledgeable chat agent. Key Features and Technology Stack Context-Aware Ingestion Engine: Helmet AI’s Ingestion Engine continuously monitors the vast landscape of breaking news and global events. By leveraging techniques such as subscribing to RSS feeds for up to date news data and processing documents with LlamaIndex and LangChain, the engine builds a complete understanding of real-time events and their implications on various user profiles. Embeddings are stored in a Pinecone Vector Database. Insight Extractor with OpenAI’s GPT Models: The Insight Extractor component of Helmet AI utilizes the power of OpenAI’s GPT models to identify and concisely explain the intricate relationships between seemingly disparate topics surfaced in your feed. By transforming raw data into actionable insights with intelligent explanations, leaders can make informed decisions based on an understanding of market trends and complex dynamics. Scalable Infrastructure: Helmet AI is built on Azure’s robust infrastructure, utilizing a range of services such as App Services, a PostgreSQL Database, and Github Actions for orchestrating Deployments. The implementation also incorporates GraphQL for efficient data retrieval and processing. Challenges Overcome and Accomplishments During the development of Helmet AI, our team encountered various challenges, including integrating MindsDB with Azure and overcoming limitations with Gmail authentication. However, we were able to overcome these obstacles and successfully implemented Helmet AI in just 36 hours during the Berkeley AI Hackathon. Additionally, we established a seamless deployment process using GitHub Actions, automating manual service orchestration. The experience was particularly rewarding for the first-time hackers on the team. Key Learnings Throughout the development process, our team gained valuable insights. We discovered the importance of setting up deployment flows early on to reduce stress during crunch time. Embracing best practices in software engineering proved crucial. Furthermore, we realized the potential of leveraging advanced language models as implicit knowledge graphs, expanding their applications beyond traditional embeddings. Future Prospects Looking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the entirety of the web, leveraging technologies like AnyScale. The team plans to collaborate with enterprise business development teams to initiate pilot programs and gather feedback for further refinement. With a solid foundation in place, Helmet AI hopes to have an impact on the way leaders gather insights and make strategic decisions. Conclusion Helmet AI represents a solid attempt at a game-changing solution for business leaders seeking to stay ahead in today’s fast-paced business world. By leveraging cutting-edge technologies, including AI-powered insight extraction and explanation and real-time data analysis, Helmet AI empowers leaders to confidently navigate market challenges and seize emerging opportunities. As the tool continues to evolve and expand its capabilities, the future of market intelligence looks promising. Stay tuned for more updates on Helmet AI’s journey towards transforming the way we approach gathering information and strategic decision-making. Winner: Split Creators: Aditya Ariyur, Nikhil Patel, Ronit Nagarapu Devpost:  https://devpost.com/software/split-pv4hn7 Background/Motivation We wanted to develop an easy-to-use workflow that allowed users to generate personalized emails with the assistance of AI, while retaining the user’s unique writing style and emotion inflections. What It Is Our product learns from your previous emails and trains a custom LLM that will draft emails that sound like you, not like a robot. It learns from your writing style and how you respond to specific people. Then, it generates emails from user prompts that match that style. How We Built It We used the Google API and LlamaIndex to parse through a user’s old emails and develop an LLM model built on OpenAI’s text-davinci-003. Then, we use Hume to understand the user’s tone and emotion in their emails, and associate it with specific subjects and recipients so future\nemails can be fine-tuned to fit the user’s emailing habits. The current interface was developed using React.js for the website and a Flask API to interact with the backend LLM model. Challenges + What We Learned It was quite difficult to get all of the different aspects of our model working together in unison, especially establishing the connection between the parsed emails and Hume emotion tags to the LlamaIndex model. We had to experiment with many different tools and prompt styles to get an accurate email generation. However, with a lot of dedication and troubleshooting, we were able to develop a working model to demonstrate our concept and its potential functionality. We learned how rewarding it was to train our own LLM using LlamaIndex. Base LLMs like ChatGPT are already so powerful, so the functionality of training a custom LLM based on your\nown data unlocks endless possibilities. What’s Next We hope to completely integrate the code and workflow into a Google plugin or extension so users can easily implement it into their daily emailing. We want to ensure the privacy and security of the user’s data, so we want to experiment with methods to reduce how much data is\nsent to third-party services like OpenAI. We also want to dedicate further development to the emotion training, as this could boost the effectiveness of our product and add to our main value proposition of personalized, user-specific email generation. Winner: Prosper AI Creators: Alan Yang, Ashay Changwani, Punit Sai Arani, Vedant Tapadia Devpost:  https://devpost.com/software/prosper-ai Vercel Demo  /  YouTube Video Overview Prosper AI is a trailblazer in utilizing Artificial Intelligence to unlock your full financial potential. It serves as an accessible and smart virtual financial advisor, armed with precise insights and personalized advice. Our mission is to democratize financial expertise. By bridging the resource gap, Prosper AI aims to level the playing field for all. The Genesis of Prosper AI The spark that ignited Prosper AI was a simple observation of the wealth disparity among different social classes. The rich have always had access to knowledge and resources that help in growing and safeguarding their wealth. In contrast, those from modest backgrounds often lack the necessary knowledge and tools to utilize what they earn effectively. Many resort to social media for financial advice, which is often generic and occasionally unreliable as it comes from unqualified influencers. Hiring a financial advisor, on the other hand, could be exorbitant and impractical for those with a limited budget. This is where Prosper AI steps in. We embraced the challenge to develop an innovative solution utilizing state-of-the-art technology and models to help digest and simplify complex financial data. The Prosperity Engine: How Prosper AI Works Prosper AI sources your financial data from any number and type of bank or investment account to provide qualified financial advice that adheres to regulatory policies, to give you personalized tips, advice and explanations. Prosper AI achieves this by leveraging an open finance provider such as Plaid to source users financial information. Then Prosper AI will ask a series of financial goal questions to help contextualize the ideal outcomes for the user. Using this combination of personal financial data and goals, Prosper AI will provide a set of optimal and personalized recommendations on how to achieve these goals. The beauty of Prosper AI lies in its interactivity and support. Users have the liberty to pose questions at any juncture if they find something perplexing. This is particularly invaluable for demystifying complicated charts or financial jargon. Furthermore, Prosper AI goes beyond just answering questions about the current recommendations. It’s like having an expert financial advisor at your beck and call, ready to generate insights, charts, and suggestions for any aspect of your financial landscape. Whether it’s planning for retirement, optimizing investments, or understanding tax liabilities, Prosper AI stands ready to guide users with precision and personalized insights to cultivate financial acumen and empower smarter financial decision-making. The Building Blocks of Prosper AI: A Look into Our Tech Stack Backend:  Our backend, the engine that powers Prosper AI, is written in Python and based on a FastAPI server. We chose Python because of its agility and the vast availability of open-source libraries that expedite the development process. Additionally, Python’s native packages provided by OpenAI and Plaid seamlessly integrate with our backend, ensuring both development and runtime efficiency. One of the cornerstones of Prosper AI’s backend is a powerful prompting pipeline which simulates a fine-tuned model. To achieve this, we tap into the capabilities of OpenAI’s GPT-4, enhanced with function calling, and interlink it with Pinecone’s vector database using additional tools like LlamaIndex. This fusion forges a streamlined yet powerful interface. Frontend:  When we started out, especially during the hackathon phase, we developed the web application frontend using Next.js, which was our comfort zone. However, as we progressed and aimed for higher benchmarks, we recognized the need to migrate to a more performant framework. We decided on SvelteKit, which stands out for its simplicity and performance, significantly accelerating the development process. One of our key objectives is to make Prosper AI accessible and user-friendly. We crafted a minimalist user interface, which declutters the screen while maintaining the essence of information. Moreover, we supplemented this with visualizations, which are crucial in translating complex financial data into understandable and actionable insights for the user. Through this combination of a robust backend and an intuitive frontend, Prosper AI is poised to revolutionize personal financial management. Overcoming Challenges: The Journey of Prosper AI’s Development The primary challenge we encountered during the initial stages was the creation of a pipeline to ingest and process years of financial data analytically. The sheer volume of data was not just overwhelming to handle all at once, but it was also crucial to process it responsibly and meaningfully. To tackle this, we had to design a system that dissected the vast financial data into digestible segments, structuring it in an orderly manner that enabled logical understanding and actionable insights. Although crafting such a system under time pressure was strenuous, it offered us a valuable insight into the magnitude of data we were dealing with. It further emphasized the significance of our mission: to efficiently and comprehensively process such vast data for the benefit of our users. Another demanding task was incorporating the complexities of tax code into our platform. Thousands of pages of tax regulations had to be converted into intelligent code, capable of offering savvy financial suggestions. Despite the enormous effort this task required, it was crucial in creating a comprehensive wealth management system. The result is a platform that delivers an optimized, personalized financial plan tailored to each user’s specific goals and needs, as well as future plans. Our platform not only identifies the type of accounts and the cash flow strategies that would minimize tax liabilities but also charts a roadmap for maximizing net worth growth over the next 30 years. This is the essence of Prosper AI — using technology to simplify complex financial management and facilitate the path towards prosperity. The Road Ahead for Prosper AI As we set our sights on the future, the Prosper AI team is more determined than ever to make strides in revolutionizing personal wealth management. Our immediate focus is to transition into full-time startup mode, which entails delving deeper into the development of feature functionalities and solidifying the foundation of our platform. A key milestone on our roadmap is engaging in pilot use cases with our initial group of customers who have eagerly joined our waitlist. This phase is critical, as it allows us to validate the effectiveness and impact of Prosper AI in real-world scenarios. Through feedback and insights gathered from this initial group, we’ll be able to refine and enhance the platform to ensure it not only meets but surpasses the expectations of our users. But we won’t stop there. The learnings from the pilot phase will serve as the springboard for subsequent developments and innovations. As we continue to harness cutting-edge technology and data analytics, Prosper AI aims to democratize access to financial knowledge and tools that can empower individuals to unlock their financial potential. Stay tuned as Prosper AI embarks on this exciting journey towards transforming the landscape of personal finance, making it more accessible, intelligent, and personalized for all. Together with Prosper AI, let’s cultivate the seeds of financial growth and harvest the fruits of prosperity. Video/screenshots/links to material. Learn more and join our waitlist for a chance to win a $50 Amazon voucher: Unleash The Power of Comparison Tap into Prosper AI, your intelligent sidekick for personalized & optimal financial advice. prosperai.vercel.app https://www.youtube.com/watch?v=_-v0BhFPjAQ", "author": "Jerry Liu", "date": "Jun 30, 2023", "tags": ["Hackathons", "AI", "Large Language Models", "Berkeley", "NLP"]},
{"title": "LlamaIndex Update — 07/11/2023", "content": "Greetings once again, LlamaIndex community! Welcome back to our second installment in the LlamaIndex Update series. In our ongoing commitment to keep you informed and engaged with our rapidly evolving open-source project, this blog post brings you more exciting updates on features, webinars, hackathons, and community events. Building on the foundation of our inaugural post, we will continue to strive to keep both our long-standing contributors and fresh faces synced with our progress. We aim to not just inform but also inspire you to partake in our collective journey towards growth and innovation. Without further delay, let’s delve into the latest happenings in this edition of the LlamaIndex Update. Features And Integrations: LlamaIndex’s partnership with Anyscale uses the Ray platform to boost performance and deployment. It accelerates LlamaIndex’s operations by a factor of ten and streamlines deployment to production servers. The core Ray Distributed Toolkit aids in efficient task parallelization, while Ray Serve ensures easy deployment of query engines to production.\n Blogpost ,  Tweet LlamaIndex enhanced metadata representation in documents. The  extra_info  and  node_info  fields are now replaced with a  metadata  dictionary. This facilitates precise control over data and allows users to exclude metadata keys during embedding or LLM prediction. This boosts LLM and retrieval performance and offers customizable metadata injection, formatting, and template creation.\n Docs ,  Tweet LlamaIndex supports both Text Completion API, involving output parsing and input prompt modification, and Structured API, requiring input function signatures and output conversion. Despite Structured API being easier to use, its limited availability keeps Text Completion API relevant. Both are supported by LlamaIndex’s  PydanticProgram .\n Docs ,  Tweet LlamaIndex now collaborates with Chainlit.io, facilitating swift construction of advanced chat UIs for any LLM app. This integration, beyond providing a basic chat interface, also logs intermediate results and sources.\n Blogpost ,  Tweet LlamaIndex now incorporates the DePlot model for interpreting charts and plots in QA/chatbot applications. Primarily effective for simple charts, such as bar charts and time series, DePlot converts these visuals into text format for easy embedding, indexing, and usage in downstream applications. This functionality is now accessible via the LlamaHub data loader, expanding LlamaIndex’s capabilities for diverse applications.\n Docs ,  Tweet LlamaIndex now incorporates the Github Issues reader, which allows for comprehensive loading and querying of issues from any GitHub repository. Additionally, the Sitemap Loader reader enables users to read all webpages from a specified sitemap.\n Tweet LlamaIndex introduces the  ContextRetrieverOpenAIAgent  feature, which enhances tool picking by incorporating more context from user messages. It performs a retrieval step before the LLM call, ensuring increased reliability and better mapping of queries to the right tools, especially in the presence of domain-specific terms. Unlike a “retrieval tool”, this feature guarantees retrieval before any action is taken.\n Docs ,  Tweet LlamaIndex now features code-based extraction for efficient data extraction from arbitrary text. This feature includes a “Fit” step to generate functions based on training data, and an “Inference” step to run these functions on new data. It offers two versions: DFEvaporateProgram for extracting one value per field from a text, and MultiValueEvaporateProgram for extracting multiple values per field. This feature can be used to extract structured data from raw HTML sources and also offers the ability to identify salient fields in a text given a topic.\n Docs ,  Tweet LlamaIndex has significantly improved its text-to-SQL capabilities, offering a “Default” SQL query engine and an SQL query engine with an object index for handling large table schemas. These upgrades simplify the process, requiring only a SQL database for the default engine and enabling indexing of large table schemas with the ObjectIndex. Additionally, LlamaIndex now also integrates with  duckdb , further enhancing the SQL querying process.\n Docs_SQL ,  Docs_duckdb ,  Tweet LlamaIndex 0.7.0 enhances modularity for LLM app development. It includes native LLM abstractions for platforms like OpenAI and Hugging Face, a standalone Response Synthesis module, and improved Document Metadata Management. These abstractions can be used independently or integrated into indices/query engines. The Response Synthesis module abstracts away context window limitations, while the Document Metadata Management feature allows deep customization of metadata, potentially boosting retrieval performance.\n Blogpost ,  Tweet LlamaIndex introduces Recursive Retrieval, a concept that utilizes the hierarchical nature of knowledge. A Node in LlamaIndex can contain references to other retrievers or query engines. This process starts with a retriever and recursively explores links to others. For instance, structured tables from a PDF can be extracted, each represented as a data frame. These tables can be referenced by  IndexNode  objects embedded with other Nodes. During a query, if an IndexNode is among the top-k nodes, it triggers another retriever or query engine, allowing sophisticated querying overall data.\n Docs ,  Tweet LlamaIndex introduces OpenAI agent streaming for efficient function calling and enhances user experience by providing progress bars during index creation for a real-time understanding of the process duration.\n Tweet LlamaIndex introduces personalized data interaction through system prompts, callback events for SubQuestionQueryEngine, and a streamlined process for Azure OpenAI integration.\n Docs_AOI ,  Notebook_personality ,  Tweet LlamaIndex leverages LLM’s to automatically extract metadata, significantly enhancing the relevance and precision of information retrieval. This is achieved through five key MetadataExtractor modules (SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, MetadataFeatureExtractor) that augment text with rich, context-specific details.\n Docs ,  Tweet Tutorials: Anyscale tutorial  on “How to Build an LLM Query Engine in 10 Minutes using LlamaIndex.” Erika Cardenas tutorial  on how to load data into Weaviate and how to connect LlamaIndex to a Weaviate instance using LlamaIndex. Wenqi Glantz tutorial  on Refreshing Private Data Sources with LlamaIndex Document Management. Michael Hunger tutorial  on   Load in data from  neo4j ,  NebulaGraph , and index/query with LlamaIndex using GraphDB Cypher and GraphQL data loaders. Pradip Nichite video tutorial  and  blogpost  on Mastering LlamaIndex: Create, Save & Load Indexes, Customize LLMs, Prompts & Embeddings. Webinars And Podcasts: Webinar  on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph). Webinar  with Albus — a comprehensive Slackbot for enterprise search,  xpress.ai  — a low-code solution for building LLM workflows + agents and  ImmigrantFirst.ai  — assistant to help immigrants complete their EB-1A/O1 apps more efficiently. Data Exchange Podcast  with Ben Lorica on LlamaIndex Events: Ravi Theja gave talks on “LlamaIndex: Basics To Production” at Accel Partners and Together VC Fund in India.", "author": "Ravi Theja", "date": "Jul 10, 2023", "tags": ["Llamaindex", "AI", "NLP", "Large Language Models", "OpenAI"]},
{"title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB", "content": "Co-authors: Prakul Agarwal — Senior Product Manager, Machine Learning at MongoDB Jerry Liu — co-founder at LlamaIndex Update (6/22/2023):  The preferred way to use LlamaIndex + MongoDB is now with our MongoDBAtlasVectorSearch class. Take a look at our guide here:  https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html Summary Large Language Models (LLMs) like ChatGPT have revolutionized the way users can get answers to their questions. However, the “knowledge” of LLMs is restricted by what they were trained on, which for ChatGPT means publicly available information on the internet till September 2021. How can LLMs answer questions using private knowledge sources like your company’s data and unlock its true transformative power? This blog will discuss how LlamaIndex and MongoDB can enable you to achieve this outcome quickly. The  attached notebook  provides a code walkthrough on how to query any PDF document using English queries. Background Traditionally, AI has been used to analyze data, identify patterns and make predictions based on existing data. The recent advancements have led to AI becoming better at generating new things (rather than just analyzing existing things). This is referred to as Generative AI. Generative AI is powered mainly by machine learning models called Large Language Models (LLM). LLMs are pre-trained on large quantities of publicly available text. There are various proprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of emerging open-source LLMs like Llama, Dolly, etc. There are 2 main scenarios where the knowledge of LLMs falls short: Private data such as your company’s internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and Slack Newer data than when the LLMs were last trained. Example question: Who is the most recent UK prime minister? There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation capabilities of LLMs: Model finetuning and in-context learning. Model Finetuning can be more complex and expensive to operationalize. There are also some open questions like how to delete information from a fine-tuned model to ensure you comply with local laws (ex. GDPR in Europe), and for changing data you need to fine-tune again constantly. In-context learning requires inserting the new data as part of the input prompts to the LLM. To perform this data augmentation in a secure, high performance and cost-effective manner is where tools like LlamaIndex and MongoDB Developer Data Platform can help. Introduction to LlamaIndex LlamaIndex provides a simple, flexible interface to connect LLMs with external data. Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc). Provides indices over the unstructured and structured data for use with LLMs. Structures external information so that it can be used with the prompt window limitations of any LLM. Exposes a query interface which takes in an input prompt and returns a knowledge-augmented output. MongoDB as the Datastore It is effortless to store the ingested documents (i.e. Node objects), index metadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There is an option to store the “documents” as an actual collection in MongoDB using  MongoDocumentStore . There is an option to persist the “Indexes” using the  MongoIndexStore  . Storing LlamaIndex’s documents and indexes in a database becomes necessary in a couple of scenarios: Use cases with large datasets may require more than in-memory storage. Ingesting and processing data from various sources (for example, PDFs, Google Docs, Slack). The requirement to continuously maintain updates from the underlying data sources. Being able to persist this data enables processing the data once and then being able to query it for various downstream applications. MongoDB Atlas MongoDB offers a free forever Atlas cluster in the public cloud service of your choice. This can be accomplished very quickly by following this  tutorial . Or you can get started directly  here . Use of LLMs LlamaIndex uses LangChain’s (another popular framework for building Generative AI applications) LLM modules and allows for customizing the underlying LLM to be used (default being OpenAI’s text-davinci-003 model). The chosen LLM is always used by LlamaIndex to construct the final answer and is sometimes used during index creation as well. The workflow Connect private knowledge sources using LlamaIndex connectors (offered through  LlamaHub ). Load in the Documents. A Document represents a lightweight container around the data source. Parse the Documents objects into Node objects. Nodes represent “chunks” of source Documents (ex. a text chunk). These node objects can be persisted to a MongoDB collection or kept in memory. Construct Index from Nodes. There are various kinds of indexes in LlamaIndex like “List Index” (this stores nodes as Sequential chain), “Vector Store Index” (this stores each node and a corresponding embedding in a vector store). Depending on the type of Index, these indexes can be persisted into a MongoDB collection or a Vector Database. Finally query the index. This is where the the query is parsed, relevant Nodes retrieved through the use of indexes, and provided as an input to a “Large Language Model” (LLM). Different types of queries can use different indexes. LlamaIndex + MongoDB Workflow Diagram Getting questions answered over your private data We want to query the “GPT-4 Technical Report” published by OpenAI in March 2023. This was a  PDF document  with 100 pages. This is a recent publication, so was not included as part of the original ChatGPT training data. Here’s the summary of the various queries we can ask the PDF. Query “How does GPT4 do on the bar exam?” Response “GPT-4 performs well on the Uniform Bar Exam, with a score in the top 10% of test takers (Table 1, Figure 4).” LLM token usage Total embedding token usage: 18 tokens\nTotal LLM token usage: 1889 tokens Query “How much better is GPT-4 in reducing hallucinations over GPT-3.5?” Response “GPT-4 improves on the latest GPT-3.5 model by 19 percentage points, with significant gains across all topics.” Query “What issues were observed after fine-tuning GPT-4 with RHLF??” Response “After fine-tuning GPT-4 with RHLF, issues observed included the model becoming overly cautious in certain ways, such as refusing innocuous requests and excessively hedging or “overrefusing”. Additionally, the model was still quite brittle and sometimes exhibited undesired behaviors based on prompts where instructions to labelers were underspecified.” Query “What is RBRM?” Response “RBRM stands for Rule-Based Reward Model. It is a technique used to provide an additional reward signal to the GPT-4 policy model during PPO fine-tuning on a subset of training prompts. The RBRM takes three things as input: the prompt (optional), the output from the policy model, and a human-written rubric (e.g., a set of rules in multiple-choice style) for how this output should be evaluated. The RBRM then classifies the output based on the rubric.” The screenshots below show how the PDF document is converted into “LlamaIndex nodes” and “LlamaIndex indices” and persisted into MongoDB. Relevant Resources Further details can be found here. Also check out the reference notebook below! Reading data from MongoDB:  link Various Indexes in LlamaIndex:  link Reference Notebook https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing", "author": "Jerry Liu", "date": "May 18, 2023", "tags": ["NLP", "AI", "Machine Learning", "Large Language Models", "Mongodb"]},
{"title": "Building Better Tools for LLM Agents", "content": "Over the past month I’ve been diving into the world of Large Language Model (LLM) Agents and building out LlamaIndex’s library of tools for use with agents. I helped to lead the LlamaHub Tools effort as part of broader  Data Agents launch  last week. In the process of building out LlamaHub Tools I’ve collected some techniques for creating effective and easy to use tools, and want to share some of my thoughts. Context on LlamaHub Tools LlamaHub Tools  allow LLMs like ChatGPT to connect to APIs and act on a user’s behalf to create, read, update and delete data. Examples of tools that we’ve put together include  drafting and sending emails ,  reading and creating Google Calendar invite s,  searching Wikipedia , and that’s just a few of the 15 tools we are releasing on launch. Overview of tool abstractions So how exactly do LlamaHub Tools work? The LlamaHub tool abstractions allow you to easily write Python functions that can be understood and called by Agents. Instead of trying to make an Agent do complicated mathematics for example, we can provide the Agent with a Tool that calls Wolfram Alpha and provides the result to the Agent: from  llama_index.tools.base  import  BaseToolSpec\n\nQUERY_URL_TMPL =  \"http://api.wolframalpha.com/v1/result?appid={app_id}&amp;i={query}\" \n\n # Inherit from the LlamaIndex BaseToolSpec abstraction \n class   WolframAlphaToolSpec ( BaseToolSpec ):\n\n   # Define the functions that we export to the LLM \n    spec_functions = [ \"wolfram_alpha_query\" ]\n\n   # Initialize with our wolfram alpha API key \n     def   __init__ ( self, app_id:  Optional [ str ] =  None ) -&gt;  None :\n         \"\"\"Initialize with parameters.\"\"\" \n        self.token = app_id\n  \n   # Our function to be called by the Agent \n   def   wolfram_alpha_query ( self, query:  str ):\n           \"\"\"\n          Make a query to wolfram alpha about a mathematical or scientific problem.\n  \n          Example inputs:\n              \"(7 * 12 ^ 10) / 321\"\n              \"How many calories are there in a pound of strawberries\"\n  \n          Args:\n              query (str): The query to be passed to wolfram alpha.\n  \n          \"\"\" \n          response = requests.get(QUERY_URL_TMPL. format (app_id=self.token, query=urllib.parse.quote_plus(query)))\n           return  response.text The above code is enough to define a LlamaIndex Tool that allows the Agent to query to Wolfram Alpha. No more incorrect guesses at math problems! We can initialize an instance of the Tool Spec like this: # Initialize an instance of the Tool\nwolfram_spec = WolframAlphaToolSpec(app_id=\"your-key\")\n# Convert the Tool Spec to a list of tools. In this case we just have one tool.\ntools = wolfram_spec.to_tool_list()\n# Convert the tool to an OpenAI function and inspect\nprint(tools[0].metadata.to_openai_function()) Here’s the cleaned up output of the print statement: {\n   'description' :  '\n    Make a query to wolfram alpha about a mathematical or scientific problem.\n  \n          Example inputs:\n              \"(7 * 12 ^ 10) / 321\"\n              \"How many calories are there in a pound of strawberries\"\n  \n          Args:\n              query (str): The query to be passed to wolfram alpha.' ,\n   'name' :  'wolfram_alpha_query' ,\n   'parameters' : {\n     'properties' : { 'query' : { 'title' :  'Query' ,  'type' :  'string' }},\n     'title' :  'wolfram_alpha_query' ,\n     'type' :  'object' \n  }\n} We can see that the  docstring  describing how to use the Tool get passed to the Agent. Additionally, the parameters, type info and function name are passed along to give the Agent a strong idea on how it can use this function. All of this information is essentially acting as the prompt for how the agent understands the tool. Inheriting from the BaseToolSpec class means it’s very simple to write Tools for Agents to use. In fact, the above tool definition is only 9 lines of code, ignoring white space, imports and comments. We can easily get the function ready for Agents to use without any heavy boilerplate or modifications. Let’s look at loading the Tool into an OpenAI Agent: agent = OpenAIAgent.from_tools(tools, verbose=True)\nagent.chat( 'What is (7 * 12 ^ 10) / 321' )\n \"\" \" OUTPUT:\n=== Calling Function ===\nCalling function: wolfram_alpha_query with args: {\n  \" query \": \" ( 7  *  12  ^  10 ) /  14 \"\n}\nGot output: 30958682112\n========================\nResponse(response='The result of the expression (7 * 12 ^ 10) / 14 is 30,958,682,112.', source_nodes=[], metadata=None)\n\" \"\" And we can test out passing this query to ChatGPT without the tools: &gt;  'What is (7 * 12 ^ 10) / 321' \n \"\" \"\nTo calculate the expression (7 * 12^10) / 14, you need to follow the order of operations, which is parentheses, exponents, multiplication, and division (from left to right).\n\nStep 1: Calculate the exponent 12^10.\n12^10 = 619,173,642,24.\n\nStep 2: Multiply 7 by the result from Step 1.\n7 * 619,173,642,24 = 4,333,215,496,68.\n\nStep 3: Divide the result from Step 2 by 14.\n4,333,215,496,68 / 14 = 309,515,392,62.\n\nTherefore, the result of the expression (7 * 12^10) / 14 is 309,515,392,62.\n\" \"\" This example should show how easily you can write new Tools for use with Agents. For the rest of the blog post I’ll be talking about tips and tricks I’ve found to write more functional and effective tools. Hopefully by the end of the blog post you are excited to write and contribute some Tools of your own! Techniques for building better tools Below are a variety of tactics for writing more usable and functional tools to minimize friction when interfacing with the Agent. Not all of the tactics apply to every tool, but usually at least a few of the techniques below will prove valuable. Writing useful tool prompts Here’s an example of the function signature and docstring for a tool that an Agent can call to create a draft email. def   create_draft ( \n         self ,\n         to:   List [str],\n         subject:  str,\n         message:  str\n     ) -&gt;  str: \n         \"\" \"Create and insert a draft email.\n           Print the returned draft's message and id.\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            to (List[str]): The email addresses to send the message to, eg ['adam@example.com']\n            subject (str): The subject for the event\n            message (str): The message for the event\n        \" \"\" This prompt takes advantage of a few different patterns to ensure that the agent can use the tool effectively: Give a concise description of the function and its purpose Inform the Agent on what data will be returned from this function List the arguments that the function accepts, with descriptions and type information Give example values for arguments with a specific format, eg adam@example.com Tool prompts should be concise as to not take up too much length in context, but also informative enough that the agent can use the tool without making mistakes. Making tools tolerant of partial inputs One way to help Agents make fewer mistakes is to write tools that are more  tolerant  of their inputs, for example by making inputs optional when the value can be inferred from somewhere else. Take the example of drafting an email, but this time let’s consider a tool that updates a draft email: def  update_draft (\n        self,\n        draft_id: str,\n        to: Optional[List[str]] = None,\n        subject: Optional[str] = None,\n        message: Optional[str] = None,\n    )  -&gt; str:\n         \"\" \"Update a draft email.\n           Print the returned draft's message and id.\n           This function is required to be passed a draft_id that is obtained when creating messages\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            draft_id (str): the id of the draft to be updated\n            to (Optional[str]): The email addresses to send the message to\n            subject (Optional[str]): The subject for the event\n            message (Optional[str]): The message for the event\n        \" \"\" The Gmail API  requires  all of the above values when updating a draft, however using just the  draft_id  we can fetch the current content of the draft and use the existing values as defaults if the Agent did not provide the values when updating the draft: def   update_draft ( ... ):\n  ...\n  draft = self.get_draft(draft_id)\n  headers = draft[ 'message' ][ 'payload' ][ 'headers' ]\n   for  header  in  headers:\n       if  header[ 'name' ] ==  'To'   and   not  to:\n          to = header[ 'value' ]\n       elif  header[ 'name' ] ==  'Subject'   and   not  subject:\n          subject = header[ 'value' ]\n     elif  header[ 'name' ] ==  'Message'   and   not  message:\n      message = header[ 'values' ]\n  ... By providing the above logic in the  update_draft  function, the Agent can invoke  update_draft  with only one of the fields (and the  draft_id ), and we can update the draft as the user expects. This means that in more circumstances the Agent can complete the task successfully, instead of returning an error or needing to ask for more information. Validating input and Agent error handling Despite best efforts at prompting and tolerance, we can end up in circumstances where the Agent invokes a tool in a way that it can’t complete the task at hand. However, we can detect this and prompt the Agent to recover the error on its own. For example, in the  update_draft  example above, what do we do if the agent calls the function without a  draft_id ? We could simply pass along the null value and return an error from the Gmail API library, but we could also detect that a null  draft_id  will invariably cause an error, and return a prompt for the agent instead: def   update_draft ( ... ):\n   if  draft_id ==  None :\n     return   \"You did not provide a draft id when calling this function. If you previously created or retrieved the draft, the id is available in context\" Now, if the Agent invokes  update_draft  without a  draft_id  , it is made aware of the exact mistake it made and given instructions on how it can correct the issue. In my experience working with this tool, the Agent will often immediately call the  update_draft  function in the correct way when receiving this prompt, or if there is no  draft_id  available, it will inform the user of the issue and ask the user for a  draft_id . Either scenario is much better than crashing or returning an opaque error from a library to the user. Providing simple functions related to the tool Agents can struggle at what would otherwise be simple functions for a computer to calculate. For example, when building a tool for creating events in Google Calendar, a user may prompt the Agent with something like this: Create an event on my Calendar to discuss the Tools PR with  adam@example.com  tomorrow at 4pm Can you see the problem? If we try asking ChatGPT what day it is: agent.chat('what day is it?')\n# > I apologize for the confusion. As an AI language model, I don't have real-time data or access to the current date. My responses are based on the information I was last trained on, which is up until September 2021. To find out the current day, I recommend checking your device's clock, referring to a calendar, or checking an online source for the current date. Agents won’t know what the current date is, and so the Agent would either call the function incorrectly, providing a string like  tomorrow  for the date, hallucinate a date sometime in the past based on when it was trained, or put the burden on the user to tell it the date. All of the above actions cause friction and frustration for the user. Instead, in the Google Calendar Tool Spec we provide a simple deterministic function for the agent to call if it needs to fetch the date: def   get_date ( self ):\n         \"\"\"\n        A function to return todays date.\n        Call this before any other functions if you are unaware of the current date\n        \"\"\" \n         return  datetime.date.today() Now, when the Agent tries to handle the prompt above, it can first call the function to get the date and then create the event as the user requested, inferring the date for “tomorrow” or “a week from now”. No errors, no guesses and no need for further user interaction! Returning prompts from functions that perform mutations Some functions perform mutations to data in a way that it isn’t clear what useful data can be returned from the function, back to the agent. For example, in the Google Calendar tool if an event is successfully created it doesn’t make sense to return the content of the event back to the Agent, as the agent just passed in all of the information and thus has it in context. Generally with functions that are focused on mutations (create, update, delete) we can help the Agent understand its actions better by using the return value of these functions to further prompt the agent. For example, from the Google Calendar  create_event  tool we could do the following: def   create_event ( ... ):\n  ...\n   return   'Event created succesfully! You can move onto the next step.'    This helps the agent register that the action succeeded and encourages it to complete the action it was prompted for, especially if creating the google calendar event is only a single step in a multiple step instruction. We can still return ids as part of these prompts as well: def   create_event ( ... ):\n  ...\n  event = service.events().insert(...).execute()\n   return   'Event created with id {event.id}! You can move onto the next step.' Storing large responses in indices for the Agent to read One consideration when building tools that has been mentioned already is the size of the context window the Agent has. Currently, LLMs tend to have context windows from 4k-16k tokens, however it can certainly be larger or smaller. If the size of the data that a tool would return is larger than the context window, the Agent will be unable to process the data and error out. One consideration when building tools that has been mentioned already is the size of the context window the Agent has. Currently, LLMs tend to have context windows from 4k-16k tokens, however it can certainly be larger or smaller. If the size of the data that a tool would return is larger than the context window, the Agent will be unable to process the data and error out. The only consideration that needs to be made when creating tools that might need to be wrapped by the LoadAndSearchTool, is they need to return a list of LlamaIndex documents. For a tool that returns a string, the only modification you need to make to have it be compatible with the LoadAndSearchTool is wrapping it in a document and an array: from  llama_index.readers.schema.base  import  Document\n\n # Not compatible \n def   large_text_response_function ():\n  ...\n   return  result\n\n # LoadAndSearch compatible \n def   large_text_response_function ():\n  ...\n   return  [Document(text=result)] Verify how the Agent understands the tool A useful technique for debugging tools in development is to  ask the Agent about its own tools : the tools it has available, what arguments the tools accept, what those arguments represent, and what the tool is used for. The responses of the Agent are useful in determining where your prompts might be lacking or helping pinpoint why an Agent is failing to successfully use a tool that you are developing. An example conversation debugging the Google Calendar Tool Spec, assuming you have setup an Agent similar to the example notebook: agent. chat ( 'what tools do you have available' )\n# I have the following tools available\n#    1.   `load_data` :  This  tool allows me to load data  from  your calendar.  It  can retrieve a specified number  of  events starting  from  a given date.\n#    2.   `create_event` :  This  tool allows me to create an event on your calendar.  You  can provide details such  as  the title, description, location, start datetime, end datetime, and attendees.\n#    3.   `get_date` :  This  tool returns today 's date. It can be used to ensure that the correct date is used when working with other tools.\n# Please let me know if you would like more information about any of these tools or if you have any specific requests.\nagent.chat(' what  arguments  does the load_data tool take ')\n# ' The   `load_data`  tool takes the following  arguments :\n#    1.   `number_of_results`  (optional,  default :  100 ):  Specifies  the number  of  events to  return   from  your calendar.\n#    2.   `start_date`  (optional):  Specifies  the start date  from  which to retrieve events.  It  can be provided  as  a string  in   ISO  format (e. g .,  \"2023-07-05\" ) or  as  a  `datetime.date`  object.\n#  You  can use these  arguments  to customize the number  of  events and the start date  for  retrieving data  from  your calendar. Concluding thoughts Building tools for Agents requires you to think critically about how users will interact with the Agent, and to try and anticipate any possible usage patterns. Building well thought out tools that can be resilient to errors and issues requires careful consideration and testing of possible use cases. I hope that these reflections and techniques assist you in creating new tools for Agents, and don’t forget to share your tools on  LlamaHub .", "author": "Adam Hofmann", "date": "Jul 17, 2023", "tags": ["AI", "Llamaindex", "LLM", "Agents", "NLP"]},
{"title": "Data Agents", "content": "Today we’re incredibly excited to announce the launch of a big new capability within LlamaIndex:  Data Agents . Data Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a “read” and “write” function. They are capable of the following: Perform automated search and retrieval over different types of data — unstructured, semi-structured, and structured. Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use. Storing conversation history. Using all of the above to fulfill both simple and complex data tasks. We’ve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Today’s launch consists of the following key components: General Agent/Tool Abstractions :  a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition. LlamaHub Tool Repository :  A  brand-new section within LlamaHub  that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to  community contributions ! See below for full details.  We show you how to build a Gmail agent that’s able to automatically create/send emails in <10 lines of code! Context Our core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define “state” (how to parse/structure your data), and “compute” (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality  beyond that . A lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the “reasoning” capabilities of these query engines. As a result some of our existing query capabilities contain “agent-like” components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities. But there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both “read” and “write” capabilities — the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source. Some existing  services ,  toolkits , and  research   papers  have already demonstrated the possibilities of LLM-powered “agents” that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data. Core Components of Data Agents Building a data agent requires the following core components: A reasoning loop Tool abstractions At a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form). Given an input task, the data agent uses a  reasoning loop  to decide which tools to use, in which sequence, and the parameters to call each tool. The “loop” can conceptually be very simple (a one-step tool selection process), or complex (a multi-step selection process, where a multitude of tools are picked at each step). These components are described in more detail below. Agent Abstraction + Reasoning Loop We have support for the following agents: OpenAI Function agent (built on top of the OpenAI Function API) a ReAct agent (which works across any chat/text completion endpoint). You can use them as the following: from  llama_index.agent  import  OpenAIAgent, ReActAgent\n from  llama_index.llms  import  OpenAI\n\n # import and define tools \n...\n # initialize llm \nllm = OpenAI(model= \"gpt-3.5-turbo-0613\" )\n # initialize openai agent \nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose= True )\n # initialize ReAct agent \nagent = ReActAgent.from_tools(tools, llm=llm, verbose= True )\n # use agent \nresponse = agent.chat( \"What is (121 * 3) + 42?\" ) Each agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task —  chat  and  query . Note that these are the core methods used in our  ChatEngine  and  QueryEngine  respectively. In fact that our base agent class ( BaseAgent ) simply inherits from  BaseChatEngine  and  BaseQueryEngine .  chat  allows the agent to utilize previously stored conversation history, whereas  query  is a stateless call - history/state is not preserved over time. The reasoning loop depends on the type of agent. The OpenAI agent calls the OpenAI function API in a while loop, since the tool decision logic is baked into the function API. Given an input prompt and previous chat history (which includes previous function calls), the function API will decide whether to make another function call (pick a Tool), or return an assistant message. If the API returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. If the API returns an assistant message, then the loop is complete (we assume the task is solved). The ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str → output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this: ...\nYou have access to the following tools:\n{tool_desc}\n\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names})\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n```\nPlease use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, you will receive a response in the following format:\n\n```\nObservation: tool response\n```\n... We implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message. Note:  the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API — the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting. Tool Abstractions Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a single Tool as well as a “ToolSpec” containing a series of functions under the hood. We describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools. Base Tool Abstraction The base tool defines a very generic interface. The  __call__  function can take in any series of arguments, and return a generic  ToolOutput  container that can capture any response. A tool also has metadata containing its name, description, and function schema. @dataclass \n class   ToolMetadata :\n     description:  str\n     name:   Optional [str] =  None \n     fn_schema:   Optional [ Type [ BaseModel ]] =  DefaultToolFnSchema \n\n class   BaseTool :\n     @property \n     @abstractmethod \n     def   metadata ( self ) -&gt;  ToolMetadata :\n        pass\n     @abstractmethod \n     def   __call__ ( self ,  input:   Any ) -&gt;  ToolOutput :\n        pass Function Tool A function tool allows users to easily convert any function into a Tool. It takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface. It can also “auto-infer” the function schema if it isn’t specified beforehand. Our  ToolSpec  classes make use of this  FunctionTool  abstraction to convert functions defined in the tool spec into a set of agent tools (see below). Here’s a trivial example of defining a FunctionTool. from llama_index.tools.function_tool import FunctionTool\n\ndef  multiply (a:  int , b:  int )  -&gt;  int :\n     \"\" \"Multiple two integers and returns the result integer\" \"\" \n     return  a * b\nmultiply_tool = FunctionTool.from_defaults(fn=multiply) QueryEngineTool Of course, we also provide Tool abstractions to wrap our existing query engines. This provides a seamless transition from working on query engines to working on agents. Our query engines can be thought of “constrained” agents meant for the read/write setting and centered around retrieval purposes. These query engines can be used in an overall agent setting. from  llama_index.tools  import  QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=query_engine, \n        metadata=ToolMetadata(\n            name= '&lt;tool_name&gt;' , \n            description= \"Queries over X data source.\" \n        )\n    ),\n ...\n] Tool Specs A  tool spec  is a Python class that represents a full API specification that an agent can interact with, and a tool spec can be converted into a list of tools that an agent can be initialized with. This class allows users to define entire services, not just single tools that perform individual tasks. Each tool spec may contain read/write endpoints that allow an agent to interact with a service in meaningful ways. For instance, a Slack tool spec could allow the user to both read existing messages and channels ( load_data ,  fetch_channels ) as well as write messages ( send_message ). It would be roughly defined as the following: class   SlackToolSpec ( BaseToolSpec ):\n     \"\"\"Slack tool spec.\"\"\" \n    spec_functions = [ \"load_data\" ,  \"send_message\" ,  \"fetch_channels\" ]\n\n     def   load_data ( \n          self,\n          channel_ids:  List [ str ],\n          reverse_chronological:  bool  =  True ,\n       ) -&gt;  List [Document]:\n           \"\"\"Load data from the input directory.\"\"\" \n          ...\n       def   send_message ( \n          self,\n          channel_id:  str ,\n          message:  str ,\n       ) -&gt;  None :\n           \"\"\"Send a message to a channel given the channel ID.\"\"\" \n          ...\n       def   fetch_channels ( \n          self,\n       ) -&gt;  List [ str ]:\n           \"\"\"Fetch a list of relevant channels.\"\"\" \n          ... If a tool spec is initialized, it can be converted into a list of tools that can be fed into an agent with  to_tool_list . For instance, tool_spec = SlackToolSpec()\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True) Defining a tool spec is not that different than defining a Python class. Each function becomes converted into a tool, and by default the docstring for each function gets used as the tool description (though you can customize names/description in  to_tool_list(func_to_metadata_mapping=...) . We also made the intentional choice that the input arguments and return types can be anything. The primary reason is to preserve the generality of the tool interface for subsequent iterations of agents. Even if current iterations of agents expect tool outputs to be in string format, that may change in the future, and we didn’t want to arbitrarily restrict the types of tool interface. LlamaHub Tool Repository A huge component of our launch is a brand-new addition to  LlamaHub : a Tool Repository. The Tool Repository consists of  15+ Tool Specs  that an agent can use. These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. Among others, they include the following specs: Gmail Spec Zapier Spec Google Calendar Spec OpenAPI Spec SQL + Vector Database Spec We also provide a list of  utility tools  that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data. For instance, our Gmail Tool Spec allows an agent to search existing emails, create drafts, update drafts, and send emails. Our Zapier Spec allows an agent to perform any natural language query to Zapier through their  Natural Language Actions  interface. Best of all, you don’t need to spend a lot of time figuring out how to use these tools — we have  10+ notebooks  showing how you can build agents for each service, or even build agents that use a combination of services (e.g. Gmail, Google Calendar, and Search). Example Walkthrough Let’s take a look at a few examples! We initialize an OpenAIAgent with the Gmail Spec. As mentioned above, the spec consists of tools to search emails, create/update drafts, and send emails. Now let’s give the agent a sequence of commands so that it can create an email draft, make a few edits to it, and then send it off. First, let’s create an initial email draft. Note that the agent chooses the  create_draft  tool, which takes in the “to”, “subject”, and “message” parameters. The agent is able to infer the parameters simultaneously while picking the tool. Next, let’s update the draft with a slight modification: Next, let’s show the current state of the draft. Finally, let’s send the email! This is a good start, but this is just the beginning. We are actively working on contributing more tools to this repository, and we’re also opening this up to community contributions. If you’re interested in contributing a Tool to LlamaHub, please feel free to open a PR in this repo. Utility Tools Oftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using). To tackle this, we’ve provided an initial set of “utility tools” in the core LlamaIndex repo — utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that’s returned from any API request. Let’s walk through our two main utility tools below. OnDemandLoaderTool This tool turns any existing LlamaIndex data loader (  BaseReader  class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger  load_data  from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it “on-demand”. All three of these steps happen in a single tool call. Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call. A usage example is given below: from  llama_hub.wikipedia.base  import  WikipediaReader\n from  llama_index.tools.on_demand_loader_tool  import  OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n reader,\n name= \"Wikipedia Tool\" ,\n description= \"A tool for loading data and querying articles from Wikipedia\" \n) LoadAndSearchToolSpec The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements  to_tool_list  , and when that function is called, two tools are returned: a  load  tool and then a  search  tool. The  load  Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The  search  Tool execution would take in a query string as input and call the underlying index. This is helpful for any API endpoint that will by default return large volumes of data — for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows. Example usage is shown below: from  llama_hub.tools.wikipedia.base  import  WikipediaToolSpec\n from  llama_index.tools.tool_spec.load_and_search.base  import  LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n # Get the search wikipedia tool \ntool = wiki_spec.to_tool_list()[ 1 ]\n # Create the Agent with load/search tools \nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose= True \n) This is the output when we run an input prompt agent.chat('what is the capital of poland') Output: ===  Calling   Function  ===\n Calling   function : search_data  with   args : {\n   \"query\" :  \"capital of Poland\" \n}\n Got   output :  Content  loaded!  You  can now search the information using read_search_data\n========================\n===  Calling   Function  ===\n Calling   function : read_search_data  with   args : {\n   \"query\" :  \"What is the capital of Poland?\" \n}\n Got   output : \n The  capital  of   Poland  is  Warsaw .\n========================\n AgentChatResponse (response= 'The capital of Poland is Warsaw.' , sources=[]) Note that the agent figures out that it first needs to first call the “load” tool (denoted by the original name of the tool, “search_data”). This load tool will load the Wikipedia page and index under the hood. The output just mentions that the “content is loaded, and tells the agent that the next step is to use  read_search_data . The agent then reasons that it needs to call the  read_search_data  tool, which will query the index for the right answer. FAQ Should I use Data Agents for search and retrieval, or continue to use Query Engines? Short answer: both are possible. Query engines give you the ability to define your own workflows over your data, in both a constrained reasoning fashion as well as unconstrained fashion. For instance, you may want to define a specific workflow over text-to-SQL with our  NLStructStoreQueryEngine  (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our  SubQuestionQueryEngine  to decompose a question among sub-documents (even less constrained). By default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities — for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too. How are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)? Most of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our “data agents” design, we’ve tried our best to answer the following key questions well: How do we effectively index/query and retrieve data beforehand? How do we effectively index/query and retrieve data on the fly? How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand? How do we properly get sources in citations? Our goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data — these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to “cache” API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner. Can I use Tools with LangChain agents? \n You can easily use any of our tools with LangChain agents as well. tools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool()  for  t  in  tools] See our  tools usage guide  for more details! Conclusion In summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository. Resources We’ve written a comprehensive section in the docs — take a look here:  https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html Take a look at our LlamaHub Tools section:  https://llamahub.ai/ Notebook Tutorials for LlamaHub Tools:  https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks If you have questions, please hop on our Discord:  https://discord.gg/dGcwcsnxhU", "author": "Jerry Liu", "date": "Jul 12, 2023", "tags": ["Llamaindex", "NLP", "AI", "Agents", "Data"]},
{"title": "Building the data framework for LLMs", "content": "Today is an exciting day for LlamaIndex, and a big milestone in my personal journey with generative AI. I’ve followed generative models for most of my academic/professional career — from  my research on GANs/sensor compression  to following  Transformers / GPT   developments . It became increasingly clear that as these models got bigger/better, they were evolving from knowledge generators to intelligent engines that could reason/act over new information. I formalized some of these key intuitions more concretely: LLMs are fantastic reasoning engines, capable of question-answering, summarization, planning, and more. They had the promise of becoming the “neural” compute unit at the core of a new age of AI-enabled software. Yet, LLMs inherently have no awareness of your own data. No one really knew the best practices for feeding your data into the LLM. Models had limited context windows and were expensive to finetune. If we could offer a toolkit to help set up the data architecture for LLM apps, then we could enable anyone to build LLM-powered knowledge workers and transform the way that software is written over private data. LLM-enabled software requires new infrastructure tooling over your data and has significant implications for the modern software data stack. Determined to tackle this challenge, I built GPT Index (which we later rebranded to LlamaIndex), an initial exploratory effort to organize and retrieve information using LLMs. ( first Tweet is here !) It happened at the perfect time. Since last November, there has been an explosion in developer interest in building applications on top of LLMs. Most developers were figuring out ways to leverage the reasoning capabilities of LLMs on top of their own private data. Just two months in, I joined forces with Simon Suo, a brilliant AI technologist and my former colleague, and we evolved LlamaIndex from an exploratory project into a comprehensive framework designed to connect a user’s private data with LLMs. It gained recognition within the AI community, captivating the attention of hackers, developers, and industry experts alike. In just six months, the project garnered an impressive following, with  16K Github Stars ,  20K Twitter followers ,  200K monthly downloads , and  6K active Discord users . Companies like Instabase, Front, and Uber started experimenting with LlamaIndex on top of their data. Some initial stacks started to emerge — for instance a common paradigm for building QA systems and chatbots was using a simple retrieval mechanism (top-k lookup from a vector database) with an LLM. LlamaIndex became viewed as a  critical   data   orchestration   component  of the emerging LLM software landscape. Yet, it became clear that there were still significant technical challenges in the space of LLMs and data, and no one had the right answers. Even with the capable toolkit that we’ve developed, we were just starting to scratch the surface on unlocking value from data. We are thrilled to share that LlamaIndex has secured $8.5 million in seed funding, led by Greylock, to help propel these efforts further. We’re excited to work with Jerry Chen, Saam Motamedi, and Jason Risch on the Greylock team. Joining us in this exciting journey are Jack Altman (CEO of Lattice), Lenny Rachitsky (Lenny’s Newsletter), Mathilde Collin (CEO of Front), Raquel Urtasun (CEO of Waabi), Joey Gonzalez (Berkeley), and many others. Their belief in our vision and the impact of LlamaIndex on the future of AI fuels our passion in solving these data + AI problems. Why LlamaIndex? Calling an LLM API is easy. Setting up a software system that can extract insights from your private data is harder. LlamaIndex is the advanced data framework for your LLM applications. It encompasses essential features allowing you to both manage and query your data. Data Management:  Data ingestion, data parsing/slicing, data storage/indexing. Data Querying:  Data retrieval, response synthesis, multi-step interactions over your data. LlamaIndex allows you to seamlessly integrate individual or enterprise data, including files, workplace apps, and databases, with LLM applications. We also offer an extensive array of integrations with other storage providers and downstream applications. 100+ data loaders 13+ vector database providers Integrations with observability and experimentation frameworks (e.g. prompt tracking and system tracing) Integrations as a  ChatGPT Retrieval Plugin  or with  Poe The end result is that you can build a variety of amazing knowledge-intensive LLM applications. This ranges from a search engine over your data, to chatbot-style interfaces, to structured analytics helpers, to autonomous knowledge agents. What’s next? There are  so  many things that we want to do to more fully realize our vision of unlocking LLM capabilities on top of your data. We’ll broadly break this down into two categories: 1) our continued commitment to the open-source developer community, and 2) solving the data problem at scale for enterprises. Build the best open source data framework and developer community At a high-level, we want to continue iterating on our core feature capabilities, improving reliability, and satisfy both the needs of beginner and advanced users. Handle complex queries:  We want to continue advancing the idea of “querying your data”, whether it’s through leveraging agent-style interactions for data retrieval and synthesis or program synthesis/DSL. Multi-modal data management:  The future of foundation models is multimodal, not just contained to LLMs. There are many types of semi-structured data (e.g. semi-structured data like JSONs, yaml files) as well as “complex” unstructured data (audio, images, video) that we’d love to have native support for. Better evaluation of LLM data systems:  Properly evaluating LLM calls is already tricky (how do you best evaluate the quality of a generated output? Some  libraries  for handling this). This becomes even more tricky when you chain LLM calls within an overall data system. We want to invest efforts into this area to provide greater transparency to our users. Optimization of Latency/Cost:  Users are faced with a plethora of choices when it comes to building a data-driven LLM app: the choice of LLM model, embedding model, vector database, etc. They must choose in accordance to a variety of factors, from latency and cost to privacy. Ease of use for both beginner users and advanced users:  Our goal is to make the utilization of LLM capabilities accessible and user-friendly for individuals at all skill levels. We will develop clear tutorials, examples, and tools to simplify the learning curve and convey the value of all of our features. Solving the data problem at scale for Enterprises As we’re iterating on the open-source project, we also want to identify the surrounding pain points in being able to build and deploy data-powered LLM apps to production. Our solution to this will build upon the success of our open-source project and be a natural evolution to the enterprise setting. Production-ready data ingestion and management:  We want to handle data updates, data consistency, and scalability to larger volumes of data parsing. We also want to continue expanding on the right storage abstractions for multi-modal data. Scale to Large Data Volumes:  Enterprises will typically have orders of magnitude more data than an individual. We want to invest in hosted infrastructure/deployment solutions around our core package so that you don’t have to. Domain-specific LLM solutions:  We want to offer packaged solutions to enable users to easily build LLM apps in different domains, from healthcare to finance to legal. If you’re building LLM apps in the enterprise setting, we’d love to chat and learn more about pain points + desired features! Check out our  form here . Join the Llama Gang! 🦙 Join the Llama(Index) gang as we embark on this journey to solve problems at the intersection of LLMs and data. We are not just building tools for ML practitioners/researchers; the emerging LLM + data architecture stacks have implications for  all  of software development. As a result, we are operating at the intersection of incredibly fun and challenging problems from a variety of different fields: Foundation Model Development Information Retrieval + Recommendation Systems Data Systems MLOps DevOps Interested in checking out the project? Find our project on  Github  and check out our  Docs Check out our brand new landing page:  https://llamaindex.ai Join our  Discord  or Follow our  Twitter Also, we’re hiring! We’re looking for founding engineers — experience in one or more of AI, data systems, and full-stack/front-end is nice to have but not a requirement. If you’re interested,  fill out our form here .", "author": "Jerry Liu", "date": "Jun 6, 2023", "tags": ["AI", "Fundraising", "Seed Investment", "Machine Learning", "NLP"]},
{"title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation", "content": "Summary In this article, we showcase a powerful new query engine (  SQLAutoVectorQueryEngine  ) in LlamaIndex that can leverage both a SQL database as well as a vector store to fulfill complex natural language queries over a combination of structured and unstructured data. This query engine can leverage the expressivity of SQL over structured data, and join it with unstructured context from a vector database. We showcase this query engine on a few examples and show that it can handle queries that make use of both structured/unstructured data, or either. Check out the full guide here:  https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html . Context Data lakes in enterprises typically encompass both  structured  and  unstructured  data. Structured data is typically stored in a tabular format in SQL databases, organized into tables with predefined schemas and relationships between entities. On the other hand, unstructured data found in data lakes lacks a predefined structure and does not fit neatly into traditional databases. This type of data includes text documents, but also other multimodal formats such as audio recordings, videos, and more. Large Language Models (LLMs) have the ability to extract insights from both structured and unstructured data. There have been some initial tooling and stacks that have emerged for tackling both types of data: Text-to-SQL (Structured data):  Given a collection of tabular schemas, we convert   natural language into a SQL statement which can then be executed against the database. Semantic Search with a Vector Database (Unstructured Data):  Store unstructured documents along with their embeddings in a vector database (e.g. Pinecone, Chroma, Milvus, Weaviate, etc.). During query-time, fetch the relevant documents by embedding similarity, and then put into the LLM input prompt to synthesize a response. Each of these stacks solves particular use cases. Text-to-SQL Over Structured Data In the structured setting, SQL is an extremely expressive language for operating over tabular data — in the case of analytics, you can get aggregations, join information across multiple tables, sort by timestamp, and much more. Using the LLM to convert natural language to SQL can be thought as a program synthesis “cheat code” — just let the LLM compile to the right SQL query, and let the SQL engine on the database handle the rest! Use Case:  Text-to-SQL queries are well-suited for analytics use cases where the answer can be found by executing a SQL statement. They are not suited for cases where you’d need more detail than what is found in a structured table, or if you’d need more sophisticated ways of determining relevance to the query beyond simple constructs like  WHERE  conditions. Example queries suited for Text-to-SQL: “What is the average population of cities in North America”? “What are the largest cities and populations in each respective continent?” Semantic Search over Unstructured Data In the unstructured setting, the behavior for retrieval-augmented generation systems is to first perform retrieval and then synthesis. During retrieval, we first look up the most relevant documents to the query by embedding similarity. Some vector stores support being able to handle additional metadata filters for retrieval. We can choose to manually specify the set of required filters, or have the LLM “infer” what the query string and metadata filters should be (see our  auto-retrieval modules  in LlamaIndex or LangChain’s  self-query module ). Use Case:  Retrieval Augmented Generation is well suited for queries where the answer can be obtained within some sections of unstructured text data. Most existing vector stores (e.g. Pinecone, Chroma) do not offer a SQL-like interface; hence they are less suited for queries that involve aggregations, joins, sums, etc. Example queries suited for Retrieval Augmented Generation “Tell me about the historical museums in Berlin” “What does Jordan ask from Nick on behalf of Gatsby?” Combining These Two Systems For some queries, we may want to make use of knowledge in  both structured tables as well as vector databases/document stores  in order to give the best answer to the query. Ideally this can give us the best of both worlds: the analytics capabilities over structured data, and semantic understanding over unstructured data. Here’s an example use case: You have access to a collection of articles about different cities, stored in a vector database You also have access to a structured table containing statistics for each city. Given this data collection, let’s take an example query: “Tell me about the arts and culture of the city with the highest population.” The “proper” way to answer this question is roughly as follows: Query the structured table for the city with the highest population. SELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1 Convert the original question into a more detailed question: “Tell me about the arts and culture of Tokyo.” Ask the new question over your vector database. Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer. Let’s think about some of the high-level implications of such a sequence: Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first “retrieval” step. We want to make sure that we can somehow “join” the results from the SQL query with the context stored in the vector database. There is no existing language to “join” information between a SQL and vector database. We will have to implement this behavior ourselves. Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population. A Query Engine to Combine Structured Analytics and Semantic Search We have created a brand-new query engine (  SQLAutoVectorQueryEngine  ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer. The  SQLAutoVectorQueryEngine  is initialized through passing in a SQL query engine (  GPTNLStructStoreQueryEngine  ) as well as a query engine that uses our vector store  auto-retriever module  (  VectorIndexAutoRetriever  ). Both the SQL query engine and vector query engines are wrapped as “Tool” objects containing a  name  and  description  field. Reminder: the  VectorIndexAutoRetriever  takes in a natural language query as input. Given some knowledge of the metadata schema of the vector database, the auto retriever first  infers  the other necessary query parameters to pass in (e.g. top-k value, and metadata filters), and executes a query against the vector database with all the query parameters. Diagram of the flow for SQLAutoVectorQueryEngine During query-time, we run the following steps: A selector prompt (similarly used in our  RouterQueryEngine  , see  guide ) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the  RetrieverQueryEngine  with  VectorIndexAutoRetriever  . If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output. A  query transformation  is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is “Tell me about the arts and culture of the city with the highest population.”, and the SQL query returns Tokyo as the city with the highest population, then the new query is “Tell me about the arts and culture of Tokyo.” The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response. The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a  VectorIndexAutoRetriever  module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like  query_str=\"arts and culture\"  and  filters={\"title\": \"Tokyo\"}  . The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer. Taking a step back, here are some general comments about this approach: Using our auto-retrieval module is our way of  simulating  a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with. This also implies that there doesn’t need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and document store metadata though; that way we don’t need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters. Experiments So how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection. Setup Our experiment setup is very simple. We have a SQL table called  city_stats  which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin. We also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate “Node” object; each chunk also contains a  title  metadata attribute containing the city name. We then derive the  VectorIndexAutoRetriever  and  RetrieverQueryEngine  from the Pinecone vector index. from  llama_index.indices.vector_store.retrievers  import  VectorIndexAutoRetriever\n from  llama_index.vector_stores.types  import  MetadataInfo, VectorStoreInfo\n from  llama_index.query_engine.retriever_query_engine  import  RetrieverQueryEngine\n\n\nvector_store_info = VectorStoreInfo(\n    content_info= 'articles about different cities' ,\n    metadata_info=[\n        MetadataInfo(\n            name= 'city' , \n             type = 'str' , \n            description= 'The name of the city' ),\n    ]\n)\nvector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    vector_auto_retriever, service_context=service_context\n) You can also get the SQL query engine as follows sql_query_engine = sql_index.as_query_engine() Both the SQL query engine and vector query engine can be wrapped as  QueryEngineTool  objects. sql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        'Useful for translating a natural language query into a SQL query over a table containing: '\n        'city_stats, containing the population/country of each city'\n    )\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=query_engine,\n    description=f'Useful for answering semantic questions about different cities',\n) Finally, we can define our  SQLAutoVectorQueryEngine query_engine = SQLAutoVectorQueryEngine(\n    sql_tool,\n    vector_tool,\n    service_context=service_context\n) Results We run some example queries. Query 1 query_engine.query(\n  'Tell me about the arts and culture of the city with the highest population'\n) Intermediate steps: Final Response: Tokyo is the city with the highest population, with 13.96 million people. It is a vibrant city with a rich culture and a wide variety of art forms. From traditional Japanese art such as calligraphy and woodblock prints to modern art galleries and museums, Tokyo has something for everyone. There are also many festivals and events throughout the year that celebrate the city's culture and art. Some popular festivals and events in Tokyo include the Sannō Festival at Hie Shrine, the Sanja Festival at Asakusa Shrine, and the biennial Kanda Festivals. These events often feature parades with elaborately decorated floats and thousands of people. Additionally, an enormous fireworks display over the Sumida River takes place annually on the last Saturday of July, attracting over a million viewers. During spring, when cherry blossoms bloom, many residents gather in Ueno Park, Inokashira Park, and the Shinjuku Gyoen National Garden for picnics under the blossoms. This query runs through the full flow of the  SQLAutoVectorQueryEngine  . It first queries the SQL database for the city with the highest population (“Tokyo”), and then queries the vector database with the new query. The results are combined into a final response. Query 2 query_engine.query(\"Tell me about the history of Berlin\") Intermediate Steps: Final Response: Berlin's history dates back to the early 13th century when it was founded as a small settlement. In 1618, the Margraviate of Brandenburg entered into a personal union with the Duchy of Prussia, and in 1701, they formed the Kingdom of Prussia with Berlin as its capital. The city grew and merged with neighboring cities, becoming a center of the Enlightenment under the rule of Frederick the Great in the 18th century.\n\nThe Industrial Revolution in the 19th century transformed Berlin, expanding its economy, population, and infrastructure. In 1871, it became the capital of the newly founded German Empire. The early 20th century saw Berlin as a hub for the German Expressionist movement and a major world capital known for its contributions to science, technology, arts, and other fields.\n\nIn 1933, Adolf Hitler and the Nazi Party came to power, leading to a decline in Berlin's Jewish community and the city's involvement in World War II. After the war, Berlin was divided into East and West Berlin, with the former under Soviet control and the latter under the control of the United States, United Kingdom, and France. The Berlin Wall was built in 1961, physically and ideologically dividing the city until its fall in 1989. Following the reunification of Germany in 1990, Berlin once again became the capital of a unified Germany and has since continued to grow and develop as a major global city. This query only requires the vector database and not the SQL database. The initial selector correctly identifies that we should just query the vector database and return the result. Query 3 query_engine.query('Can you give me the country corresponding to each city?') Intermediate Steps Final Response  Toronto is in Canada, Tokyo is in Japan, and Berlin is in Germany. This query can be answered by just querying the SQL database, it does not need additional information from the vector database. The query transform step correctly identifies “None” as the followup question, indicating that the original question has been answered. Conclusion So far, the stacks around LLMs + unstructured data and LLMs + structured data have largely been separate. We’re excited about how combining LLMs on top of both structured and unstructured data can unlock new retrieval/query capabilities in novel and interesting ways! We’d love for you to try out the  SQLAutoVectorQueryEngine  and let us know what you think. The full notebook walkthrough can be found  in this guide  ( associated notebook ).", "author": "Jerry Liu", "date": "May 28, 2023", "tags": ["Artificial Intelligence", "Machine Learning", "NLP", "Data Science", "Database"]},
{"title": "Introducing LlamaIndex.TS", "content": "We are beyond excited to announce v0.0.1 of  LlamaIndex.TS , a Typescript first library focused on helping you use your private data with large language models. What is LlamaIndex? Our core goal for LlamaIndex is to help developers easily integrate their data with Large Language Models (LLMs). LLMs, like ChatGPT, have been a revolution in the way we think about handling textual input and data, but all of them have the limitation in what data they have access to. In addition to the “knowledge cutoff” (we are nearing the 2 year anniversary for when ChatGPT’s latest data was trained) LLMs can’t access data from your companies, from your personal analyses, or from the data your users generate. With LlamaIndex.TS, we look to achieve that goal by meeting developers at their (my) language of choice, in this case Typescript. We are committed to making this library the easiest to use, most robust solution out there for using data with LLMs. Backstory It was at the Emergency ChatGPT Hackathon hosted by Pete Huang and Rachel Woods that I met Jerry. Having worked in the JS world for the last 8 years, my first question was “why don’t you build this in Javascript?” After he demurred, he very patiently guided me through setting up the Python dev environment. (I think it took us 20 minutes before we figured it all out!) So, when Jerry offered to let me build LlamaIndex.TS I obviously couldn’t turn him down. Can’t wait to see what you build with it! Design At a high level, LlamaIndex.TS first takes the file inputs, loads them into a standardized format, and creates an Index (knowledge base). We then retrieve the relevant information from the index and use that in our query to the LLM to generate more a grounded response. Check out  our docs  for a more in depth explanation! Playground We are building an open source playground for LlamaIndex.TS. Please check it out at  https://llama-playground.vercel.app/  PRs are welcome here!  https://github.com/run-llama/ts-playground Main Differences from LlamaIndex Python All function names are 🐪 camel cased. The prompt interface is much simpler and uses native javascript template literals. We do not ship non-async versions of functions. Please use await or .then callbacks. We use interfaces and POJOs in lieu of classes where it makes sense. For example, ChatEngine, a base class in Python is an interface in JS. ServiceContext, a class in Python is an interface/POJO in JS. Runtimes Currently, we support NodeJS v18 and up. Lots of plans on this front though. Stay tuned! Contributing Only the core features are built out so far, so there is a lot of work that needs to be done on the loader and integration side. If you’re interested in contributing, please send us a message or even better a PR! https://github.com/run-llama/LlamaIndexTS", "author": "Yi Ding", "date": "Jul 24, 2023", "tags": ["Artificial Intelligence", "Large Language Models", "ChatGPT", "Chatbots", "Deep Learning"]},
{"title": "LlamaIndex and Transformers Agents", "content": "Summary Agents are a popular use-case for Large Language Models (LLMs), typically provide a structure that enables LLMs to make decisions, use tools, and accomplish tasks. These agents can take many forms, like the fully-autonomous versions seen with  Auto-GPT , to more controlled implementations like  Langchain  Agents. With the recent release of  Transformers Agents , we showcase how  LlamaIndex  continues to be a useful tool for agents, by augmenting their existing image-generator tool. Using an vector index created from 10K  DiffusionDB  prompts, the Text2Image Prompt Assistant tool we created can re-write prompts to generate more beautiful images. Full source code is available in the  Hugging Face Space for the tool , and a  colab notebook  is available as a usage walkthrough. Creating the Tool Transformers Agents come with a variety of per-configured tools that leverage the vast amounts of open-source models hosted on Hugging Face-Hub. Furthermore, additional tools can be created and shared by simply publishing a new Hugging Face Space with the proper tool setup. To create a tool, your code simply needs a  tool_config.json  file that describes the tool, as well as a file containing the implementation of your tool. While the documentation was a little fuzzy for this part, we eventually were able to use  the implementation of existing custom tools  as the framework for our own. To enable LlamaIndex to write text-to-image prompts, we need a way to show the LLM what examples of good prompts look like. To do this, we indexed 10K random text-to-image prompts from DiffusionDB. from  datasets  import  load_dataset\n from  llama_index  import  VectorStoreIndex, Document\n\n # downloads a LOT of data \ndataset = load_dataset( 'poloclub/diffusiondb' ,  '2m_random_10k' )\n\ndocuments = []\n for  sample  in  dataset[ 'train' ]:\n    documents.append(Document(sample[ 'prompt' ]))\n\n # create index \nindex = VectorStoreIndex.from_documents(documents)\n\n # store index \nindex.storage_context.persist(persist_dir= \"./storage\" ) To get LlamaIndex to write prompts using examples, we need to customize the prompt templates a bit. You can see the final prompt templates and how to use them below: text_qa_template =  Prompt (\n     \"Examples of text-to-image prompts are below: \\n\" \n     \"---------------------\\n\" \n     \"{context_str}\" \n     \"\\n---------------------\\n\" \n     \"Given the existing examples of text-to-image prompts, \" \n     \"write a new text-to-image prompt in the style of the examples, \" \n     \"by re-wording the following prompt to match the style of the above examples: {query_str}\\n\" \n)\n\n\nrefine_template =  Prompt (\n     \"The initial prompt is as follows: {query_str}\\n\" \n     \"We have provided an existing text-to-image prompt based on this query: {existing_answer}\\n\" \n     \"We have the opportunity to refine the existing prompt \" \n     \"(only if needed) with some more relevant examples of text-to-image prompts below.\\n\" \n     \"------------\\n\" \n     \"{context_msg}\\n\" \n     \"------------\\n\" \n     \"Given the new examples of text-to-image prompts, refine the existing text-to-image prompt to better \" \n     \"statisfy the required style. \" \n     \"If the context isn't useful, or the existing prompt is good enough, return the existing prompt.\" \n)\n\nquery_engine = index. as_query_engine (\n    text_qa_template=text_qa_template, \n    refine_template=refine_template\n)\n\nresponse = query_engine. query ( \"Draw me a picture of a happy dog\" ) Snag #1 One main drawback of Transformers Agents currently is that they will only pick one tool to solve each prompt. So if we want to augment the image-generator tool, we need to replace it! In our tool implementation, we actually load the original image-generator tool and call it after running LlamaIndex to generate a new text-to-image prompt. Snag #2 The next bump in our journey is how Hugging Face downloads tools from the space. Initially, it only downloading the  tool_config.json  file and the source code for the tool. But we also need to download the prompts we spent time indexing! To get around this, during the  setup()  of the tool, we call  hf_hub_download()  to download the files we need to load the index. Back on Track With the index created and the general processes figured out, the actual tool implementation is fairly straightforward. class   Text2ImagePromptAssistant ( Tool ):\n    \n    inputs = [ 'text' ]\n    outputs = [ 'image' ]\n    description = PROMPT_ASSISTANT_DESCRIPTION\n    \n     def   __init__ ( self, *args, openai_api_key= '' , model_name= 'text-davinci-003' , temperature= 0.3 , verbose= False , **hub_kwargs ):\n         super ().__init__()\n        os.environ[ 'OPENAI_API_KEY' ] = openai_api_key\n         if  model_name ==  'text-davinci-003' :\n            llm = OpenAI(model_name=model_name, temperature=temperature)\n         elif  model_name  in  ( 'gpt-3.5-turbo' ,  'gpt-4' ):\n            llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n         else :\n             raise  ValueError(\n                 f\" {model_name}  is not supported, please choose one \" \n                 \"of 'text-davinci-003', 'gpt-3.5-turbo', or 'gpt-4'.\" \n            )\n        service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n        set_global_service_context(service_context)\n        \n        self.storage_path = os.path.dirname(__file__)\n        self.verbose = verbose\n        self.hub_kwargs = hub_kwargs\n\n     def   setup ( self ):\n        hf_hub_download(repo_id= \"llamaindex/text2image_prompt_assistant\" , filename= \"storage/vector_store.json\" , repo_type= \"space\" , local_dir=self.storage_path)\n        hf_hub_download(repo_id= \"llamaindex/text2image_prompt_assistant\" , filename= \"storage/index_store.json\" , repo_type= \"space\" , local_dir=self.storage_path)\n        hf_hub_download(repo_id= \"llamaindex/text2image_prompt_assistant\" , filename= \"storage/docstore.json\" , repo_type= \"space\" , local_dir=self.storage_path)\n        \n        self.index = load_index_from_storage(StorageContext.from_defaults(persist_dir=os.path.join(self.storage_path,  \"storage\" )))\n        self.query_engine = self.index.as_query_engine(similarity_top_k= 5 , text_qa_template=text_qa_template, refine_template=refine_template)\n        \n         # setup the text-to-image tool too \n        self.text2image = load_tool( 'huggingface-tools/text-to-image' )\n        self.text2image.setup()\n\n        self.initialized =  True \n\n     def   __call__ ( self, prompt ):\n         if   not  self.is_initialized:\n            self.setup()\n\n        better_prompt =  str (self.query_engine.query(prompt)).strip()\n        \n         if  self.verbose:\n             print ( '==New prompt generated by LlamaIndex==' , flush= True )\n             print (better_prompt,  '\\n' , flush= True )\n\n         return  self.text2image(better_prompt) Running the Tool With the tool setup, we can now test it with an actual agent! For testing, we used an  OpenAIAgent  with the  text-davinci-003  model. When asked to draw a picture of a mountain, this is what we got: from  transformers  import  OpenAiAgent\nagent = OpenAiAgent(model= \"text-davinci-003\" , api_key= \"your_api_key\" )\n\nagent.run( \"Draw me a picture a mountain.\" ) The initial picture of mountains that the agent created. As you can see, the picture looks alright. But, text-to-image prompts are somewhat of an art. To use our new tool, we just need to replace the existing image-generator tool: from  transformers  import  load_tool\nprompt_assistant = load_tool(\n     \"llamaindex/text2image_prompt_assistant\" ,\n    openai_api_key= \"your_api_key\" ,\n    model_name= 'text-davinci-003' ,\n    temperature= 0.3 ,   # increase or decrease this to control variation \n    verbose= True \n)\n\n from  transformers  import  OpenAiAgent\nagent = OpenAiAgent(model= \"text-davinci-003\" , api_key= \"your_api_key\" )\n\n # replace the existing tool \nagent.toolbox[ 'image_generator' ] = prompt_assistant\n\nagent.run( \"Draw me a picture a mountain.\" ) Using Our new LlamaIndex Prompt Assistant tool, we get a much more stylized result. In the terminal, we see the prompt was re-written as “a majestic mountain peak, surrounded by lush greenery, with a stunning sunset in the background,” which resulted in the following image: Image generated by our Text2Image Prompt Assistant tool. Looks great! With the temperature variable, we can control how varied the generated prompts become. With a temperature above zero, each prompt generated by LlamaIndex with the same agent prompt will be brand new! Conclusion In conclusion, we have demonstrated how LlamaIndex can be used to augment LLM agents, by implementing a Text2Image Prompt Assistant tool with a Transformers Agent. Using a vector database created from DiffusionDB, LlamaIndex can suggest better prompts when generating images. Custom tools in Transformers Agents are easily distributed and shared using Hugging Face Spaces, and we are excited to see what other people build and share!", "author": "Logan Markewich", "date": "Jun 8, 2023", "tags": ["Llamaindex", "LLM", "Hugging Face", "Generative Ai", "AI"]},
{"title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs", "content": "(co-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co-founder at Metaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs) We’re incredibly excited to launch an  integration  between LlamaIndex and  Metaphor : combine the capabilities of LlamaIndex data agents with Metaphor as a  native  LLM search tool to enable knowledge workers capable of answering any question over any data, no matter how recent or complex. We provide a deeper overview of Metaphor and the LlamaIndex integration below. We also walk through our  example notebook  to showcase how they can be combined. Background/Context State-of-the art large language models (LLMs) such as ChatGPT, GPT-4, Claude 2 have incredible reasoning capabilities that unlock a wide variety of use cases — from insight extraction to question-answering to general workflow automation. Yet they are limited in their abilities to retrieve contextually relevant information. A popular stack that has emerged is to setup a retrieval-augmented generation (RAG) system, which combines LLMs with external storage solutions over a static knowledge source. Frameworks such as LlamaIndex provide a variety of tools to setup both simple and complex RAG systems. Yet even this is not the complete picture. LLMs should ideally be able to dynamically search and retrieve information from the external world, not just depend on a static source of knowledge. This would allow them to fulfill a more general set of tasks and not only perform search/retrieval, but perform actions as well. To do this well, we need two core components: General abstractions that allow LLMs to intelligently perform various tasks over your data, in both a “read” and “write” fashion A good search engine tailored for LLM use LlamaIndex  data agent abstractions  help to satisfy the first core component. A complete data agent consists of both a reasoning loop as well as a set of Tools. These tools can be interfaces for search/retrieval or more generally any external API. Given a query, the agent will execute its reasoning loop and dynamically figure out the set of Tools it will need to fulfill the task at hand. Data agents have access to a rich set of Tools offered on  LlamaHub  — these range from Gmail API, to a SQL db API, to a basic tool in the form of Bing search. We’ve shown that they are capable of e2e tasks from  sending emails ,  scheduling meetings , to automating  custom support insight extraction . Yet there has never been a tool tailored for LLM use. Overview of Metaphor The Metaphor API is designed to connect your LLM to the internet. It allows you to perform fully neural, highly semantic searches over the Internet and also get clean, HTML content from the results. Metaphor was trained to predict links on the internet, given how people talk about things on the Internet. For example, someone might post about a great article they read like this: Found an amazing article I read about the history of Rome’s architecture: [LINK] By training a model to predict these links given how people talk about them, the end result is a totally different way to search the internet —  search as if you’re about to share the link you want . While a little unintuitive at first, searching this way can return extremely high quality results. But for the purposes of LlamaIndex, you won’t need to worry about this because by default, queries will be converted into Metaphor prompts. Why would you use Metaphor Search over Bing/Google? There are 3 main reasons: You can search fully semantically, for instance with feelings or complex descriptors. You can search only for the type of entity that you want. Companies, articles, people. You can find content that Google simply doesn’t surface well, maybe because keywords aren’t the right tool or maybe just because Google doesn’t care about returning good results for that type of content. To learn more, you can read the full Metaphor API  blog post . Integration Details The  Metaphor Tool Spec in LlamaHub  is an API interface that consists of 5 tools that an agent can use. Search:  The entrypoint to Metaphor — allows an agent to pass a natural language query that will then be passed to the Metaphor search engine. This endpoint also contains some additional parameters, such as the number of results, domains to include/exclude, and a date filter. Retrieve Documents:  This will retrieve the content of a set of documents given IDs. These ids are returned as part of the results from the search endpoint above. Search and Retrieve Documents:  This is a convenience endpoint that combines the functionality of `search` and `retrieve_documents`. Find Similar:  This directly calls an endpoint offered by Metaphor, which will return a list of documents similar to a given URL. Current Date:  This is a convenience function that returns the current date. On its own it is unrelated to Metaphor’s API, but may be called beforehand to figure out the right date filters to pass to some of Metaphor’s endpoints. In the next section, let’s walk through how a data agent can make use of these endpoints through various use cases. Example Walkthrough Let’s walk through our example notebook showing how LlamaIndex data agents can be used with Metaphor. Testing the Metaphor Tools The first step is to import the Metaphor tool spec. # Set up Metaphor tool \n from  llama_hub.tools.metaphor.base  import  MetaphorToolSpec\nmetaphor_tool = MetaphorToolSpec(\napi_key= 'your-key' ,\n)\n # convert tool spec to a list of tools \nmetaphor_tool_list = metaphor_tool.to_tool_list()\n for  tool  in  metaphor_tool_list:\n print (tool.metadata.name) In this walkthrough, we make use of all of the tools. But you’re free to pick and choose to use specific tools if you want to define a more custom workflow and restrict the agent action space. We can play around with the set of tools before defining our agent. All of our Metaphor tools make use of the `AutoPrompt` option where Metaphor will pass a query through an LLM to refine and improve the query. Example input: metaphor_tool.search('machine learning transformers', num_results=3) Example output: [{'title': 'On the potential of Transformers in Reinforcement Learning',\n'url': 'https://lorenzopieri.com/rl_transformers/',\n'id': 'ysJlYSgeGW3l4zyOBoSGcg'},\n{'title': 'Transformers: Attention in Disguise',\n'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n{'title': 'Transformers in Computer Vision: Farewell Convolutions!',\n'url': 'https://towardsdatascience.com/transformers-in-computer-vision-farewell-convolutions-f083da6ef8ab?gi=a1d0a9a2896c',\n'id': 'kX1Z89DdjSvBrH1S1XLvwg'}] The notebook also contains examples of us playing around with the other endpoints:  retrieve_documents ,  find_similar ,  search_and_retrieve_documents . Setting up an OpenAI Function Calling Agent with Metaphor We can create an agent with access to all of the above tools and start testing it out: from  llama_index.agent  import  OpenAIAgent\n # We don't give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools \nagent = OpenAIAgent.from_tools(\n  metaphor_tool_list,\n  verbose= True ,\n) That’s it in terms of setup! Let’s try giving an example query: print (agent.chat( 'What are the best restaurants in toronto?\")) We walk through the execution trace of this agent to see how it is interacting with the Metaphor tool. ===  Calling   Function  ===\n Calling   function : search  with   args : {\n   \"query\" :  \"best restaurants in Toronto\" \n}\n[ Metaphor   Tool ]  Autoprompt   string :  Here 's a link to the best restaurant in Toronto:\nGot output: [{' title ': ' Via   Allegro   Ristorante  -  Toronto   Fine   Dining   Restaurant ', ' url ': ' https : //viaallegroristorante.com/', 'id': 'EVlexzJh-lzkVr4tb2y_qw'}, {'title': 'The Senator – Home', 'url': 'https://thesenator.com/', 'id': 'dA3HVr5P8E0Bs7nH2gH7ZQ'}, {'title': 'Home - The Rushton', 'url': 'https://therushton.com/', 'id': '6Je-igG-i-ApqISC5XXmGQ'}, {'title': 'Location', 'url': 'https://osteriagiulia.ca/', 'id': 'HjP5c54vqb3n3UNa3HevSA'}, {'title': 'StockYards | Stockyards Toronto', 'url': 'https://www.thestockyards.ca/', 'id': 'Pffz-DQlOepqVgKQDmW5Ig'}, {'title': 'Select A Restaurant', 'url': 'https://www.torontopho.com/', 'id': 'DiQ1hU1gmrIzpKnOaVvZmw'}, {'title': 'Home | Kit Kat Italian Bar &amp; Grill', 'url': 'http://www.kitkattoronto.com/', 'id': 'kdAcLioBgnwzuHyd0rWS1w'}, {'title': 'La Fenice', 'url': 'https://www.lafenice.ca/', 'id': 'M-LHQZP6V40V81fqLFAQxQ'}, {'title': 'Le Phénix', 'url': 'https://www.lephenixto.com/', 'id': 'spCTcFr0GHlFUTzyngfRVw'}, {'title': 'ITALIAN, INSPIRED.', 'url': 'https://figotoronto.com/', 'id': 'OvBcTqEo1tCSywr4ATptCg'}] \n========================\n Here  are some  of  the best restaurants  in   Toronto :\n\n 1.  [ Via   Allegro   Ristorante ]( https : //viaallegroristorante.com/) \n 2.  [ The   Senator ]( https : //thesenator.com/) \n 3.  [ The   Rushton ]( https : //therushton.com/) \n 4.  [ Osteria   Giulia ]( https : //osteriagiulia.ca/) \n 5.  [ Stockyards ]( https : //www.thestockyards.ca/) \n 6.  [ Toronto   Pho ]( https : //www.torontopho.com/) \n 7.  [ Kit   Kat   Italian   Bar  &amp;  Grill ]( http : //www.kitkattoronto.com/) \n 8.  [ La   Fenice ]( https : //www.lafenice.ca/) \n 9.  [ Le   Ph énix]( https : //www.lephenixto.com/) \n 10.  [ Figo ]( https : //figotoronto.com/) \n\n You  can visit their websites  for  more information.  Enjoy  your dining experience  in   Toronto ! The execution trace shows that the agent is simply calling the `search` endpoint with “best restaurants in Toronto”, and returning that as a list of dictionaries representing the search results. Note that we can ask a followup question as well: print (agent.chat( 'tell me more about Osteria Giulia' )) And we get the following result (note: we truncate some of the intermediate output): ===  Calling   Function  ===\n Calling   function : retrieve_documents  with   args : {\n \"ids\" : [ \"HjP5c54vqb3n3UNa3HevSA\" ]\n}\n Got   output : […]\n========================\n Osteria   Giulia  is a restaurant located at  134   Avenue   Road   in   Toronto ,  Ontario .  You  can contact them at  416.964 .8686  or via email at info@osteriagiulia. ca  ( for  general inquiries only, no reservation requests via email).\n The  restaurant 's operating hours are from Monday to Saturday, from 5:00pm to 11:00pm. On Sundays, the restaurant is available for private bookings.\nParking is available on Avenue Road and Davenport Road.\nYou can follow Osteria Giulia on Instagram [@osteriagiulia](https://www.instagram.com/osteriagiulia). They also have a sister restaurant called Giulietta, which you can visit at [giu.ca](https://giu.ca) or on Instagram [@giulietta972](https://www.instagram.com/giulietta972).\nPlease note that the information provided is based on the available document and may be subject to change. It is recommended to visit their official website or contact them directly for the most up-to-date information. Since “Osteria Giulia” is in the agent conversation history, the agent now knows to call the `retrieve` endpoint to return more information about the relevant search result. Advanced: Avoiding Context Window Issues One issue with using  retrieve  is that the content can be quite long. If the content is naively appended to the conversation history and dumped into the LLM context window, then we may run into context window limitations. LlamaIndex offers tool abstractions to help deal with this. Our  LoadAndSearchToolSpec  wraps any given tool that may return a large amount of data, and it splits it into two tools: a load tool that will dynamically store the data in an index, and a search tool that allows for search over that index. On the Metaphor side, this is also where we define a  search_and_retrieve_documents  endpoint that combines  search  and  retrieve . This allows the agent to make a single query to retrieve a large number of documents, which when combined with the  LoadAndSearchToolSpec  will get directly stored within an index. If the agent were to call  search  and  retrieve  separately, then it would both take longer and consume more tokens to write the search results to conversation history, and then passing that into the prompt again to call  retrieve  over all document IDs. Creating the  LoadAndSearchToolSpec : from  llama_index.tools.tool_spec.load_and_search.base  import  LoadAndSearchToolSpec\n # The search_and_retrieve_documents tool is the third in the tool list, as seen above \nwrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n  metaphor_tool_list[ 2 ],\n) Now let’s walk through a full execution example: # Just pass the wrapped tools and the get_date utility \nagent = OpenAIAgent.from_tools(\n  [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[ 4 ]],\n  verbose= True ,\n)\n print (agent.chat( 'Can you summarize everything published in the last month regarding news on superconductors' )) The output here shows that the agent calls multiple tools in succession to get the right answer. ===  Calling   Function  ===\n Calling   function : current_date  with   args : {}\n Got   output :  2023 - 08 - 20 \n========================\n===  Calling   Function  ===\n Calling   function : search_and_retrieve_documents  with   args : {\n   \"query\" :  \"superconductors\" ,\n   \"start_published_date\" :  \"2023-07-20\" ,\n   \"end_published_date\" :  \"2023-08-20\" \n}\n[ Metaphor   Tool ]  Autoprompt :  \"Here is an interesting article about superconductors:\nGot output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n========================\n=== Calling Function ===\nCalling function: read_search_and_retrieve_documents with args: {\n  \" query \": \" superconductors \"\n}\nGot output: \nSuperconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. The agent used the  get_date  tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling  search . It then loaded the documents using  retrieve_documents  and read them using  read_retrieve_documents . Conclusion As shown above, the integration between LlamaIndex data agents + Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We’re excited to continue exploring this further in future blog posts. We encourage you to play around with the notebook — make sure to check it out! Resources: Notebook:  https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb LlamaHub:  https://llamahub.ai/l/tools-metaphor Metaphor:  https://platform.metaphor.systems/ Metaphor API Docs:  https://docs.metaphor.systems/", "author": "Jerry Liu", "date": "Aug 21, 2023", "tags": ["Search", "LLM", "Agents", "NLP", "Llamaindex"]},
{"title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications", "content": "Llama 2  is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the  Hugging Face Open LLM Leaderboard . Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents. Context: Llama-2–7B is Not Good at Text-to-SQL A downside of the smallest Llama 2 model (7B parameters), however, is that it’s not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template: You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response: Here we plugged in a sample entry from the  sql-create-context dataset . input: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR) Meanwhile, here is the generated output vs. correct output: Generated output: SELECT * FROM `table_name_8` WHERE  '1980'  = YEAR AND TEAM =  \"Boston Celtics\"  ORDER BY OVERALL_PICK DESC LIMIT  1 ;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year =  1981  AND overall_pick =  \"148\" This is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs. This is exactly where fine-tuning comes in — given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g.  how LoRA works ). Once the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on “in-context learning” and “retrieval-augmentation” use cases — freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started. Tutorial Overview In this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of  LlamaIndex . Here is the stack that we use: b-mc2/sql-create-context  from Hugging Face datasets  as the training dataset OpenLLaMa   open_llama_7b_v2  as the base model PEFT for efficient finetuning Modal  for handling all cloud compute/orchestration for finetuning. And also for the excellent reference  doppel-bot repo . LlamaIndex  for text-to-SQL inference against any SQL database. Special mention to the awesome  Llama 2 tutorial from Anyscale that helped to inspire this project . All of our materials can be found in our Github repo:  https://github.com/run-llama/modal_finetune_sql  (again emphasizing that this is adapted from  doppel-bot ). Also, the full tutorial can be found in our  Jupyter notebook guide . Make sure to check it out! As mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don’t cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview. There are also certainly higher-level APIs that we could’ve used (e.g. OpenAI, Lamini) in order to achieve this task. There’s plenty of room for followup tutorials to cover these topics! Step 1: Loading Training Data for Finetuning LLaMa The first step here is to open up the  Jupyter notebook . The notebook is organized into a series of runnable scripts that each perform the steps needed to load data. Our code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don’t contain Python blocks of their own. First we use Modal to load in the  b-mc2/sql-create-context  dataset. This is a simple task that just loads in the dataset and formats it into a  .jsonl  file. modal run src.load_data_sql --data-dir \"data_sql\" As we can see, under the hood the task is quite straightforward: # Modal stubs allow our function to run remotely \n @stub.function( \n    retries=Retries( \n        max_retries= 3 ,\n        initial_delay= 5.0 ,\n        backoff_coefficient= 2.0 ,\n     ),\n    timeout= 60  *  60  *  2 ,\n    network_file_systems={VOL_MOUNT_PATH.as_posix( ): output_vol},\n    cloud= \"gcp\" ,\n ) \n def   load_data_sql ( data_dir:  str  =  \"data_sql\" ):\n     from  datasets  import  load_dataset\n\n    dataset = load_dataset( \"b-mc2/sql-create-context\" )\n\n    dataset_splits = { \"train\" : dataset[ \"train\" ]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents= True , exist_ok= True )\n\n     for  key, ds  in  dataset_splits.items():\n         with   open (out_path,  \"w\" )  as  f:\n             for  item  in  ds:\n                newitem = {\n                     \"input\" : item[ \"question\" ],\n                     \"context\" : item[ \"context\" ],\n                     \"output\" : item[ \"answer\" ],\n                }\n                f.write(json.dumps(newitem) +  \"\\n\" ) Step 2: Run Finetuning Script The next step is to run our finetuning script on the parsed dataset. modal run src.finetune_sql --data-dir \"data_sql\" --model-dir \"model_sql\" The finetuning script performs the following steps. Splits the dataset into training and validation splits train_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt) Formats each split into tuples of (input prompt, label):  The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt — this allows the model to train on next-token prediction. def   generate_and_tokenize_prompt ( data_point ):\n  full_prompt = generate_prompt_sql(\n      data_point[ \"input\" ],\n      data_point[ \"context\" ],\n      data_point[ \"output\" ],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n   if   not  train_on_inputs:\n       raise  NotImplementedError( \"not implemented yet\" )\n   return  tokenized_full_prompt The input prompt is the exact same as what was given at the top of this blog. When the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not specified). Step 3: Evaluation The model has been finetuned and can be served from the cloud. We can run some basic evaluations using sample data from sql-create-context to compare the performance of the finetuned model vs. the baseline Llama 2 model. modal run src.eval_sql::main The results demonstrate a massive improvement for the finetuned model: Input  1 : { 'input' :  'Which region (year) has Abigail at number 7, Sophia at number 1 and Aaliyah at number 5?' ,  'context' :  'CREATE TABLE table_name_12 (region__year_ VARCHAR, no_5 VARCHAR, no_7 VARCHAR, no_1 VARCHAR)' ,  'output' :  'SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"sophia\" AND\nno_5 = \"aaliyah\"' }\nOutput  1  (finetuned model): SELECT region__year_ FROM table_name_12 WHERE no_7 =  \"abigail\"  AND no_1 =  \"aaliyah\"  AND no_5 =  \"sophia\" \nOutput  1  (base model): SELECT * FROM table_name_12 WHERE region__year =  '2018'  AND no_5 =  'Abigail'  AND no_7 =  'Sophia'  AND no_1 =  'Aaliyah' ;\n\n\nInput  2 : { 'input' :  'Name the result/games for 54741' ,  'context' :  'CREATE TABLE table_21436373_11 (result_games VARCHAR, attendance VARCHAR)' ,  'output' :  'SELECT result_games FROM table_21436373_11 WHERE attendance = 54741' }\nOutput  2  (finetuned model): SELECT result_games FROM table_21436373_11 WHERE attendance =  \"54741\" \nOutput  2  (base model): SELECT * FROM table_21436373_11 WHERE result_games =  'name'  AND attendance &gt;  0 ; Whereas the base model produces wrongly formatted outputs, or incorrect SQL statements, the finetuned model is able to produce outputs that are much closer to that of the expected output. Step 4: Integrating the Finetuned Model with LlamaIndex We can now use this model in LlamaIndex for text-to-SQL over any database. We first define a test SQL database that we can then use to test the inference capabilities of the model. We create a toy  city_stats  table that contains city name, population, and country information, and populate it with a few sample cities. db_file =  \"cities.db\" \nengine =  create_engine (f \"sqlite:///{db_file}\" )\nmetadata_obj =  MetaData ()\n# create city  SQL  table\ntable_name =  \"city_stats\" \ncity_stats_table =  Table (\n    table_name,\n    metadata_obj,\n     Column ( \"city_name\" ,  String ( 16 ), primary_key= True ),\n     Column ( \"population\" ,  Integer ),\n     Column ( \"country\" ,  String ( 16 ), nullable= False ),\n)\nmetadata_obj. create_all (engine) This is stored in a  cities.db  file. We can then use Modal to load both the finetuned model and this database file into the  NLSQLTableQueryEngine  in LlamaIndex - this query engine allows users easily start performing text-to-SQL over a given database. modal run src.inference_sql_llamaindex::main --query  \"Which city has the highest population?\"  --sqlite-file-path  \"nbs/cities.db\"  --model-dir  \"model_sql\"  --use-finetuned-model True We get a response like the following: SQL   Query :  SELECT   MAX (population)  FROM  city_stats  WHERE  country =  \"United States\" \n Response : [( 2679000 ,)] Conclusion And that’s basically it! This tutorial provides a very high-level way for you to get started finetuning a Llama 2 model on generating SQL statements, and showcases end-to-end how you can plug it into your text-to-SQL workflows with LlamaIndex. Resources For the sake of completeness we’re linking all of our resources again here. Tutorial repo:  https://github.com/run-llama/modal_finetune_sql  (adapted from  doppel-bot ). Jupyter notebook guide . Stack: [b-mc2/sql-create-context  from Hugging Face datasets]( https://huggingface.co/datasets/b-mc2/sql-create-context ) OpenLLaMa PEFT Modal  (+  doppel-bot repo ). LlamaIndex Special mention:  Llama 2 tutorial from Anyscale .", "author": "Jerry Liu", "date": "Aug 17, 2023", "tags": ["Llamaindex", "Fine Tuning", "Large Language Models", "AI", "NLP"]},
{"title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews", "content": "Introduction E-commerce platforms, such as Amazon and Walmart, are teeming with products that attract a multitude of reviews every single day. These reviews are crucial touchpoints that reflect consumer sentiments about products. But how can businesses sift through vast databases to derive meaningful insights from these reviews? The answer lies in combining SQL with RAG (Retrieval Augmented Generation) through LlamaIndex. Let’s deep dive into this! Sample Dataset of Product Reviews For the purpose of this demonstration, we’ve generated a sample dataset using GPT-4 that comprises reviews for three products: iPhone13, SamsungTV, and an Ergonomic Chair. Here’s a sneak peek: iPhone13: “Amazing battery life and camera quality. Best iPhone yet.” SamsungTV: “Impressive picture clarity and vibrant colors. A top-notch TV.” Ergonomic Chair: “Feels really comfortable even after long hours.” Here is a sample dataset. rows = [\n    # iPhone13 Reviews\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The iPhone13 is a stellar leap forward. From its sleek design to the crystal-clear display, it screams luxury and functionality. Coupled with the enhanced battery life and an A15 chip, it's clear Apple has once again raised the bar in the smartphone industry.\"},\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"This model brings the brilliance of the ProMotion display, changing the dynamics of screen interaction. The rich colors, smooth transitions, and lag-free experience make daily tasks and gaming absolutely delightful.\"},\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The 5G capabilities are the true game-changer. Streaming, downloading, or even regular browsing feels like a breeze. It's remarkable how seamless the integration feels, and it's obvious that Apple has invested a lot in refining the experience.\"},\n\n    # SamsungTV Reviews\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"Samsung's display technology has always been at the forefront, but with this TV, they've outdone themselves. Every visual is crisp, the colors are vibrant, and the depth of the blacks is simply mesmerizing. The smart features only add to the luxurious viewing experience.\"},\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"This isn't just a TV; it's a centerpiece for the living room. The ultra-slim bezels and the sleek design make it a visual treat even when it's turned off. And when it's on, the 4K resolution delivers a cinematic experience right at home.\"},\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"The sound quality, often an oversight in many TVs, matches the visual prowess. It creates an enveloping atmosphere that's hard to get without an external sound system. Combined with its user-friendly interface, it's the TV I've always dreamt of.\"},\n\n    # Ergonomic Chair Reviews\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"Shifting to this ergonomic chair was a decision I wish I'd made earlier. Not only does it look sophisticated in its design, but the level of comfort is unparalleled. Long hours at the desk now feel less daunting, and my back is definitely grateful.\"},\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"The meticulous craftsmanship of this chair is evident. Every component, from the armrests to the wheels, feels premium. The adjustability features mean I can tailor it to my needs, ensuring optimal posture and comfort throughout the day.\"},\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"I was initially drawn to its aesthetic appeal, but the functional benefits have been profound. The breathable material ensures no discomfort even after prolonged use, and the robust build gives me confidence that it's a chair built to last.\"},\n] Setting up an In-Memory Database To process our data, we’re using an in-memory SQLite database. SQLAlchemy provides an efficient way to model, create, and interact with this database. Here’s how our  product_reviews  table structure looks: id  (Integer, Primary Key) category  (String) product_name  (String) review  (String, Not Null) Once we’ve defined our table structure, we populate it with our sample dataset. engine = create_engine( \"sqlite:///:memory:\" )\nmetadata_obj = MetaData()\n\n # create product reviews SQL table \ntable_name =  \"product_reviews\" \ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column( \"id\" , Integer(), primary_key= True ),\n    Column( \"category\" , String( 16 ), primary_key= True ),\n    Column( \"product_name\" , Integer),\n    Column( \"review\" , String( 16 ), nullable= False )\n)\nmetadata_obj.create_all(engine)\n\nsql_database = SQLDatabase(engine, include_tables=[ \"product_reviews\" ])\n\n for  row  in  rows:\n    stmt = insert(city_stats_table).values(**row)\n     with  engine.connect()  as  connection:\n        cursor = connection.execute(stmt)\n        connection.commit() Analysing Product Reviews — Text2SQL + RAG Deriving insights from data often requires intricate questioning. SQL + RAG in LlamaIndex simplifies this by breaking it into a three-step process: Decomposition of the Question: Primary Query Formation: Frame the main question in natural language to extract preliminary data from the SQL table. Secondary Query Formation: Construct an auxiliary question to refine or interpret the results of the primary query. 2. Data Retrieval : Run the primary query using the Text2SQL LlamaIndex module to obtain the initial set of results. 3. Final Answer Generation:  Use List Index to further refine the results based on the secondary question, leading to the conclusive answer. Let’s start doing it step by step. Decomposing User Query into Two Phases When working with a relational database, it’s often helpful to break down user queries into more manageable parts. This makes it easier to retrieve accurate data from our database and subsequently process or interpret this data to meet the user’s needs. We’ve designed an approach to decompose queries into two distinct questions by giving an example to  gpt-3.5-turbo  model to generate two distinct questions. Let’s apply this to the query “Get the summary of reviews of Iphone13” and our system would generate: Database Query: “Retrieve reviews related to iPhone13 from the table.” Interpretation Query: “Summarize the retrieved reviews.” This approach ensures that we cater to both the data retrieval and data interpretation needs, resulting in more accurate and tailored responses to user queries. def generate_questions(user_query: str) - &gt;  List[str]:\n  system_message = '''\n  You are given with Postgres table with the following columns.\n\n  city_name, population, country, reviews.\n\n  Your task is to decompose the given question into the following two questions.\n\n  1. Question in natural language that needs to be asked to retrieve results from the table.\n  2. Question that needs to be asked on the top of the result from the first question to provide the final answer.\n\n  Example:\n\n  Input:\n  How is the culture of countries whose population is more than 5000000\n\n  Output:\n  1. Get the reviews of countries whose population is more than 5000000\n  2. Provide the culture of countries\n  '''\n\n  messages = [\n      ChatMessage(role=\"system\", content=system_message),\n      ChatMessage(role=\"user\", content=user_query),\n  ]\n  generated_questions = llm.chat(messages).message.content.split('\\n')\n\n  return generated_questions\n\nuser_query = \"Get the summary of reviews of Iphone13\"\n\ntext_to_sql_query, rag_query = generate_questions(user_query) Data Retrieval — Executing the Primary Query When we decompose a user’s question into its constituent parts, the first step is to convert the “Database Query in Natural Language” into an actual SQL query that can be run against our database. In this section, we’ll use the LlamaIndex’s  NLSQLTableQueryEngine  to handle the conversion and execution of this SQL query. Setting up the NLSQLTableQueryEngine: The  NLSQLTableQueryEngine  is a powerful tool that takes natural language queries and converts them into SQL queries. We initiate this by providing the necessary details: sql_database : This represents our SQL database connection details. tables : We specify which table(s) our query will be run against. In this scenario, we're targeting the  product_reviews  table. synthesize_response : When set to  False , this ensures we receive raw SQL responses without additional synthesis. service_context : This is an optional parameter, which could be used to provide service-specific settings or plugins. sql_query_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[ \"product_reviews\" ],\n    synthesize_response= False ,\n    service_context=service_context\n) Executing the natural language Query: After setting up the engine, the next step is executing our natural language query against it. The engine’s  query()  method is used for this purpose. sql_response = sql_query_engine.query(text_to_sql_query) Processing the SQL Response: The result of our SQL query is usually a list of rows (with each row represented as a list of reviews). To make it more readable and usable for the third step of processing summarizing reviews, we convert this result into a single string. sql_response_list = ast.literal_eval(sql_response.response)\ntext = [ ' ' .join(t)  for  t  in  sql_response_list]\ntext =  ' ' .join(text) You can check the generated SQL query in  sql_response.metadata[\"sql_query\"]. By following this process, we’re able to seamlessly integrate natural language processing with SQL query execution. Let’s go with the last step in this process for getting a summary of the reviews. Refining and Interpreting the reviews with ListIndex: After obtaining the primary set of results from the SQL query, there are often situations where further refinement or interpretation is required. This is where  ListIndex  from LlamaIndex plays a crucial role. It allows us to execute the secondary question on our obtained text data to get a refined answer. listindex = ListIndex([Document(text=text)])\nlist_query_engine = listindex.as_query_engine()\n\nresponse = list_query_engine.query(rag_query)\n\n print (response.response) Now let’s wrap everything under a function and try out a few interesting examples: \"\" \"Function to perform SQL+RAG\" \"\" \n\n def   sql_rag ( user_query:  str ) -&gt;  str: \n  text_to_sql_query, rag_query = generate_questions(user_query)\n\n  sql_response = sql_query_engine.query(text_to_sql_query)\n\n  sql_response_list = ast.literal_eval(sql_response.response)\n\n  text = [ ' ' .join(t)  for  t  in  sql_response_list]\n  text =  ' ' .join(text)\n\n  listindex =  ListIndex ([ Document (text=text)])\n  list_query_engine = listindex.as_query_engine()\n\n  summary = list_query_engine.query(rag_query)\n\n   return  summary.response Examples: sql_rag(\"How is the sentiment of SamsungTV product?\") The sentiment of the reviews for the Samsung TV product is generally positive. Users express satisfaction with the picture clarity, vibrant colors, and stunning picture quality. They appreciate the smart features, user-friendly interface, and easy connectivity options. The sleek design and wall-mounting capability are also praised. The ambient mode, gaming mode, and HDR content are mentioned as standout features. Users find the remote control with voice command convenient and appreciate the regular software updates. However, some users mention that the sound quality could be better and suggest using an external audio system. Overall, the reviews indicate that the Samsung TV is considered a solid investment for quality viewing. sql_rag(\"Are people happy with Ergonomic Chair?\") The overall satisfaction of people with the Ergonomic Chair is high. You can play around with the approach and dataset in the Google Colab Notebook —  here . Conclusion In the era of e-commerce, where user reviews dictate the success or failure of products, the ability to rapidly analyze and interpret vast swaths of textual data is paramount. LlamaIndex, through its ingenious integration of SQL and RAG, offers businesses a powerful tool to glean actionable insights from such datasets. By seamlessly blending structured SQL queries with the abstraction of natural language processing, we’ve showcased a streamlined approach to transform vague user queries into precise, informative answers. With this approach, businesses can now efficiently sift through mountains of reviews, extract the essence of user sentiments, and make informed decisions. Whether it’s about gauging the overall sentiment for a product, understanding specific feature feedback, or even tracking the evolution of reviews over time, the Text2SQL+RAG methodology in LlamaIndex is the harbinger of a new age of data analytics.", "author": "Ravi Theja", "date": "Aug 12, 2023", "tags": ["Llamaindex", "AI", "LLM", "Sql", "Information Retrieval"]},
{"title": "Fine-Tuning Embeddings for RAG with Synthetic Data", "content": "UPDATE 9/10/2023:  We’ve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our  embedding fine-tuning guides  in the core documentation. We’ve created a  comprehensive, end-to-end guide  showing you how to fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!). The result is a  5–10% performance increase in retrieval evaluation metrics  — our finetuned  bge  model almost reaches  text-embedding-ada-002  levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole. This tutorial is helpful to  anyone  building RAG systems: If you’re new to finetuning, no problem! We have  step by step notebooks  walking through the key steps. Simply substitute the file links for your own data, and just run every cell. Finetuning embedding models is lightweight and doesn’t require a GPU. These notebooks were tested on an M2 Macbook Pro. Resources Repo:  https://github.com/run-llama/finetune-embedding Notebooks:  Dataset Generation ,  Finetuning ,  Evaluation Background/Context The Current RAG Stack RAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not present in its training corpus. It pairs a  retrieval model  over a knowledge bank with the LLM through its input prompt space. RAG stacks typically look like the following: Indexing : Prepare a corpus of unstructured text, parse/chunk it. Then  embed  each chunk and put in a vector database. Query-time:  Retrieve  context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space. (Of course RAG can be much more advanced than this, and LlamaIndex provides tools for both  simple and advanced RAG ) Unfortunately RAG is easy to prototype by cobbling together the different components, but hard to productionize. The simple stack has many failure modes and oftentimes the issue lies with bad retrieval — if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad. How Can We Make Retrieval Better? We can try more sophisticated retrieval algorithms (e.g. hybrid search, reranking). An  insight  from our recent  production RAG webinar , however, is that the embeddings themselves may not live in an optimal latent space for your data. Embeddings generated by pre-trained models may be close/far from each other based on the pre-training objective, but may not completely align with your own retrieval objective. For instance, if you’re building search over ML ArXiv papers, you may want the embeddings to align semantically with specific ML concepts (e.g. “LLMs”, “NLP”) and not filler words “This paper is…”). Finetuning is a way to solve that. The concept of finetuning has become increasingly popular in the LLM space, with  technological advancements  as well as  easy-to-use services . In this tutorial, we focus on  finetuning the embedding model.  We show how finetuning the embedding model can lead to better retrieval performance. Challenges/Considerations When you finetune embeddings, you need training examples. In the case of embeddings, this typically means that you have both “positive” and “negative” examples — pairs of texts that should be close to each other and far from each other. An issue is that we don’t have these positive or negative examples apriori. Given a dataset of unstructured text, is it possible to  automatically  generate these example pairs? With LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions from unstructured text chunks. These (question, chunk) pairs are then used as positive examples as training signals for the model (negative examples are randomly sampled across other chunks). The next section shows a full walkthrough across all of our modules. Walkthrough At a high-level, we do the following: Generating synthetic dataset for training and evaluation ( Notebook ) Finetuning an opensource embedding model ( Notebook ) Evaluating the embedding model ( Notebook ) Generating synthetic dataset for training and evaluation The key idea here is that we can leverage an LLM to generate hypothetical questions that are best answered by a given piece of context. This allows us to generate synthetic positive pairs of (query, relevant documents) in a scalable way without requiring human labellers. More concretely, we first process the given documents into a corpus of text chunks. We do this with the  SimpleNodeParser  module in LlamaIndex: parser =  SimpleNodeParser ()\nnodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\ncorpus = {\n  node. node_id:  node.get_content(metadata_mode= MetadataMode . NONE ) \n   for  node  in  nodes\n} Then for each text chunk, we use LLM to generate a few hypothetical questions that can be answered with information form that text chunk. The example prompt is shown below as well. prompt_template = prompt_template  or   \"\" \"\\\n  Context information is below.\n  \n  ---------------------\n  {context_str}\n  ---------------------\n  \n  Given the context information and not prior knowledge.\n  generate only questions based on the below query.\n  \n  You are a Teacher/ Professor. Your task is to setup \\\n  {num_questions_per_chunk} questions for an upcoming \\\n  quiz/examination. The questions should be diverse in nature \\\n  across the document. Restrict the questions to the \\\n  context information provided.\" \n   \"\" \"\n\n# for a given node, extract questions (do this over all nodes in outer loop)\nquery = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\nresponse = llm.complete(query)\n\nresult = str(response).strip().split(\" \\n \")\nquestions = [\n    re.sub(r\" ^\\d+[\\).\\s] \", \" \", question).strip() for question in result\n]\nquestions = [question for question in questions if len(question) &gt; 0]\n Finally, we collect all pairs of questions and text chunks as the dataset. Example query, chunk, and mapping is shown below. \n# example query\nf331640a-b407-4028-8db8-4b8db691dd34: \"What is the market value of Lyft's common stock held by non-affiliates as of June 30, 2021, based on the closing sales price of the Class A common stock on that date?\"\n\n# example corpus\nd5554f3e-cdaf-41d7-ac49-8f0ffe3f5759:\"UNITED STATESSECURITIES AND...\"\n\n# example mapping\nf331640a-b407-4028-8db8-4b8db691dd34: d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759 Finetuning an opensource embedding model We leverage the high-level model fitting API from  sentencetransformers  to very easily setup a training process. We use  MultipleNegativesRankingLoss  as the training object and  InformationRetrievalEvaluator  as the evaluator during training. Also, we use BAAI/bge-small-en  on Hugging Face as the base model and train for a small number of epochs. # define model \nmodel_id =  \"BAAI/bge-small-en\" \nmodel = SentenceTransformer(model_id)\n\n...\n\n # define loss \n from  sentence_transformers  import  losses\nloss = losses.MultipleNegativesRankingLoss(model)\n\n # define evaluator \n from  sentence_transformers.evaluation  import  InformationRetrievalEvaluator\n # define over validation dataset \n...\nevaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n\n # run training \n...\nmodel.fit(\n    train_objectives=[(loader, loss)],\n    epochs=EPOCHS,\n    warmup_steps=warmup_steps,\n    output_path= 'exp_finetune' ,\n    show_progress_bar= True ,\n    evaluator=evaluator, \n    evaluation_steps= 50 ,\n) Evaluating the embedding model We compare the finetuned model against the base model, as well as the OpenAI embedding model  text-embedding-ada-002  . We evaluate with two main metrics: Hit-rate metric:  For each (query, relevant_doc) pair, we retrieve the top-k documents with the query. It’s a  hit  if the results contain relevant_doc. InformationRetrievalEvaluator  from sentence_transformers. This provides a comprehensive suite of metrics such as cosine similarity accuracy, precision, recall at different top-k values. Results In terms of hit-rate metric, the base model gets 78% hit-rate on the validation dataset, and the fine-tuned model gets 84%.  text-embedding-ada-002  gets 87%, which means that our fine-tuned model is only 3% off! Hit-rate for `text-embedding-ada-002`, base model, finetuned model The InformationRetrievalEvaluator shows a similar improvement across an entire suite of metrics. The fine-tuned model increases evaluation metrics by 5–10% compared to the base-model. Evaluation suite from `InformationRetrievalEvaluator` Conclusion We successfully finetuned an embedding model over unlabeled, unstructured data to give better retrieval performance for downstream RAG systems. We show a 5–10% improvement across all metrics! Resources (copied from intro) Repo:  https://github.com/run-llama/finetune-embedding Notebooks:  Dataset Generation ,  Finetuning ,  Evaluation", "author": "Jerry Liu", "date": "Aug 25, 2023", "tags": ["Fine Tuning", "LLM", "Llamaindex", "NLP", "Embedding"]},
{"title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases", "content": "Introduction: In the world of IT and Software Development, knowledge transfer (KT) stands out as a big challenge. Whether it’s new hires trying to understand their roles, folks on their notice periods aiming for a smooth handover, or the daily tasks of developers and product specialists adapting to ever-changing projects — the KT process often leads to stress and worry. This gets more complicated with information spread out everywhere, the mix of new and old tech, and the fast pace of IT and Software Development projects. In this situation, broken bits of knowledge become the norm, causing delays, misunderstandings, and making learning harder. But amidst these challenges, might there be a beacon of optimism shining through? Vibhav  and I have developed a system that seamlessly organizes KT sessions. By leveraging personal images, we generate video explanations that are paired with individual code snippets, making the code far more comprehensible. Our innovative approach was recognized when we secured the First Prize at the Google Cloud, Searce, and LifeSight hackathon. With the combined strengths of LlamaIndex and D-ID, our aim is not just to consolidate information but also to simplify tasks and elevate the KT process. In doing so, we’re transforming a daunting industry challenge into a straightforward and manageable endeavor. Want to see how LlamaIndex plays a key role in this change? Let’s dive in together! Solution: The solution has four stages: Code Parsing: Break down the code base into individual code snippets or blocks. Summary and Explanation Generation with LlamaIndex: Produce a comprehensive summary of the entire code base. Create detailed explanations for each individual code block using LlamaIndex. Video Creation with D-ID: Generate videos using text-to-speech capabilities provided by D-ID. Video-Code Integration: Seamlessly stitch together the individual code blocks with their corresponding generated videos. Let’s dive into each stage in detail. \n\n\n\n\n \n   \n \n\n 1. Code Parsing: Breaking Down the Code \n \n   \n   Code Parser \n \n \n  Understanding a code base starts with a high-level summary, but the true depth\n  lies in individual snippets or blocks. However, using entire code bases for\n  explanations can overwhelm language models like LLMs, causing them to either\n  exceed token limits or miss key details.\n \n \n  Our approach is simple yet efficient: break the code into digestible sections\n  like import statements, classes, initializer functions, and methods without\n  losing the code’s flow. This segmentation is done through a dependency graph\n  approach, utilizing Python’s\n   ast  library. By analyzing the code's structure, we can extract\n  classes, their docstrings, initializers, and other methods. This method not\n  only captures the essence of each segment but is also flexible, allowing for\n  further rules to extract additional code components.\n \n \n  The  code_parser  class embodies this strategy. It navigates the\n  code, distinguishing module-level functions from class-nested ones, and\n  arranges them systematically. The result? A granular yet comprehensive view of\n  the code, paving the way for precise and context-rich explanations.\n \n 2. Summary and Explanation Generation with LlamaIndex \n \n   Producing a Comprehensive Summary: \n \n \n  The initial step in understanding a code base is to grasp its overall essence.\n  This is achieved by generating a concise summary that gives a bird’s-eye view\n  of the entire code. LlamaIndex’s Summary Index \n  has been tailored for this exact task. In SummaryIndex, each block of code is\n  treated as a node. By inputting the structured blocks obtained from our code\n  parsing phase into SummaryIndex, we can produce a comprehensive snapshot that\n  serves as a summary of the entire code base.\n \n\n \n   \n \n \n   Detailed Explanations for Individual Code Blocks: \n \n \n  With a general understanding established, the next step is to delve into the\n  finer details. Starting from import statements, progressing to functions, and\n  eventually diving into classes and initializer functions, every block gets its\n  due attention. Here, LlamaIndex’s\n   accumulate response mode is a valuable asset, providing in-depth\n  explanations for each block.\n \n \n  However, a challenge arises. While\n   accumulate  provides in-depth insights into each block, it can\n  occasionally miss the broader context offered by preceding blocks. To address\n  this limitation, we’ve adopted a two-pronged approach. As depicted in the\n  subsequent architecture, we employ two SummaryIndices for this endeavor.\n \n \n   \n    We utilize the first SummaryIndex to generate a concise summary for each\n    block, treating each block as a Node in SummaryIndex.\n   \n   \n    For the second SummaaryIndex in the stack, we feed the summarized context\n    from one node into the next. This ensures every node benefits from the\n    context of its predecessor. We then harness the\n     accumulate  mode to provide detailed explanations, making\n    certain that every segment of the code is explained comprehensively,\n    preserving the broader perspective. The outcome? A deep, contextually rich\n    interpretation of each code section.\n   \n \n \n  Note: We utilized Google’s PaLM API in conjunction with LlamaIndex to generate\n  summaries and explanations. Alternatively, models like GPT-3.5, GPT-4, or\n  other LLM’s can be employed for this purpose.\n \n \n   \n \n 3. Video Creation with D-ID: \n \n  After carefully crafting summaries and detailed explanations for each code\n  block, it’s essential to convey this information in a captivating and\n  accessible manner. Videos, given their dynamic appeal, have the power to make\n  intricate code explanations clearer and more engaging. This is where D-ID\n  comes into play.\n \n \n  With the prowess of D-ID’s cutting-edge technology, we’re able to create\n  realistic videos where avatars — whether they’re of us or another chosen\n  figure — articulate each code block. Now, what brings these avatars to life?\n  The answer lies in Microsoft’s text-to-speech synthesizer. This tool takes our\n  detailed textual explanations and transforms them into natural, fluent speech.\n  Thus, with D-ID, we’re not just generating video but also integrating audio,\n  culminating in a comprehensive and fluid video explanation.\n \n To see this in action, let’s take a look at a sample output. \n \n   \n \n 4. Video-Code Integration: \n \n  After generating insightful videos with avatars elucidating the code and\n  having our individual code snippets ready, the next crucial step is to marry\n  these two elements. This fusion ensures that viewers receive an immersive\n  visual experience, where they can simultaneously watch the explanation and\n  observe the related code.\n \n \n  To achieve this, we employed the\n   carbon  library, which transforms our code snippets into visually\n  appealing images. These images, when presented side-by-side with our\n  explanatory videos, offer a clearer understanding of the code in focus. The\n  final touch is added with the  moviepy  library, which seamlessly\n  stitches the video and code images together, ensuring a smooth and integrated\n  visual flow. Below, you'll find a sample illustrating this compelling\n  combination.\n \n \n   \n \n Final Automatic Knowledge Transfer (KT) Generated Video \n \n  Following our detailed process, we’ve crafted a KT video where Jerry explains\n  the ChatEngine code base of LlamaIndex. Watch the video below to see it all\n  come together!\n \n \n \n  Code Repository:\n   https://github.com/ravi03071991/KT_Generator \n \n Conclusion \n \n  Through this post, we’ve showcased the transformative potential of LlamaIndex\n  in creating Knowledge Transfer (KT) Videos for code bases. It’s genuinely\n  remarkable to envision the advancements we’re making in this space. The\n  methodology we’ve adopted is language-neutral, allowing flexibility in\n  adapting to various code bases. With some tweaks to the code parsing phase, we\n  believe it’s feasible to scale this to cover expansive code repositories\n  within organizations. Imagine a platform akin to YouTube, perhaps\n   KodeTube(KT) , where an organization’s entire codebase is\n  cataloged through explanatory videos. The horizon is bright with the\n  opportunities LlamaIndex brings, and we’re thrilled about the journey ahead.", "author": "Ravi Theja", "date": "Aug 29, 2023", "tags": ["Large Language Models", "Llamaindex", "AI", "NLP", "Software Development"]},
{"title": "Introducing Airbyte sources within LlamaIndex", "content": "Authored by Joe Reuter, Software Engineer at Airbyte (cross-posted from the Airbyte blog; check it out  here !) Content It’s now possible to utilize the Airbyte sources for  Gong ,  Hubspot ,  Salesforce ,  Shopify ,  Stripe ,  Typeform  and  Zendesk Support  directly within your LlamaIndex-based application, implemented as  data loaders . For example, to load the Stripe invoices for a user, you can use the AirbyteStripeLoader. Installing it is super simple, when you have LlamaIndex installed locally you only need to install the source you are interested in, and you are ready to go: pip install airbyte-source-stripe\npip install llama-hub After that, simply download the loader and pass in configuration and the stream you want to load: from  llama_hub.airbyte_stripe.base  import  AirbyteStripeReader\n\nconfig = {\n   \"client_secret\" :  \"&lt;secret key&gt;\" ,\n   \"account_id\" :  \"&lt;account id&gt;\" ,\n   \"start_date\" :  \"&lt;date from which to start retrieving records from in ISO format, e.g. 2020–10–20T00:00:00Z&gt;\" \n}\nreader = AirbyteStripeReader(config=config)\ndocuments = reader.load_data(stream_name= \"invoices\" ) Why does this matter? This is the beginning of making Airbyte’s  300+ sources  available as data loaders in LlamaHub. Airbyte can move data from just about any source to your warehouse or vector database to power your LLM use case (check out this  tutorial  for setting up such a data pipeline!). This is normally done by using Airbyte Cloud or a local Airbyte instance, setting up a connection, and running it on a schedule (or via API trigger) to make sure your data stays fresh. But if you are just getting started and are running everything locally, using a full Airbyte instance (including the UI, scheduling service, scale-out capabilities, etc..) may be overkill. With this release, it’s easier than ever to run any Python-based source in LlamaIndex directly within your Python runtime — no need to spin up an Airbyte instance or make API calls to Airbyte Cloud. Moving between hosted and embedded Airbyte Since the same code is running under the hood, every Airbyte-built loader is compatible with the respective source in the Airbyte service. This means it’s trivial to lift your embedded loading pipeline into your self-hosted Airbyte installation or your Airbyte Cloud instance. The schema of the loader configuration object and that of the output records is 100% compatible. Running syncs on hosted Airbyte means: UI to keep track of running pipelines Event notifications including alerting on failing syncs or running post-sync operations Easily running pipelines on a schedule Scale-out capabilities API to power programmatic use cases Out-of-the-box state management of your connections Support And more Running syncs with LlamaIndex loaders means: No overhead for running yet another service Full control over timing and pipeline execution Combining Airbyte loaders with indices and query engines As Airbyte loaders are behaving like regular loaders, they can easily be combined with all LlamaIndex utilities to build powerful LLM-based applications: relevant_keys = [\"customer_name\", \"total\", \"currency\"]\nreader = AirbyteStripeReader(\n    config=strip_config,\n    record_handler=lambda record, id: Document(\n        doc_id=id,\n        text=record.data[\"description\"] or \"\",\n        extra_info={\n            key: record.data[key] for key in relevant_keys if key in record.data\n        },\n    ),\n)\n\nindex = ListIndex.from_documents(reader.load_data(stream_name=\"invoices\"))\nquery_engine = index.as_query_engine()\nquestion = input(\"What do you want to know about your customers?\")\nprint(query_engine.query(question)) Incremental loads Since your python application is basically acting as the Airbyte platform, you have full control over how the “sync” is executed. For example you can still benefit from  incremental syncs  if your stream supports it by accessing the “last_state” property of the loader. This allows you to load only documents that changed since the last time you loaded, allowing you to update an existing vector database effectively: import airbyte_cdk.models.airbyte_protocol import AirbyteMessage\nwith open('stripe_sync_checkpoint.json', 'w') as file:\n  file.write(reader.last_state.json())\n\n# later\nwith open('stripe_sync_checkpoint.json', 'r') as file:\n  current_state = AirbyteStateMessage.parse_raw(file.read())\nnew_docs = reader.load_data(stream_name=\"invoices\", state=current_state) Mapping Airbyte records to LlamaIndex documents By default, each record gets mapped to a Document as part of the loader, with all the various fields in the record becoming a part of the `extra_info` property of the Document (the `extra_info` represents structured metadata for each document) . The text portion of the document is set to the JSON representation of the record. By default, any metadata defined on the Document will be concatenated with the text in downstream modules, so all the fields in the record will be used for embedding and synthesis purposes within a LlamaIndex app. You can pass in a record handler to customize this behavior to build the text part of a record depending on the data: def   handle_record ( record,  id ):\n   return  Document(doc_id= id , text=record.data[ \"title\" ], extra_info=record.data)\nreader = AirbyteGongReader(config=gong_config, record_handler=handle_record) Custom sources For now, the following Airbyte sources are available as pip packages (with more to come): Gong  pip install airbyte-source-gong Hubspot  pip install airbyte-source-hubspot Salesforce  pip install airbyte-source-salesforce Shopify  pip install airbyte-source-shopify Stripe  pip install airbyte-source-stripe Typeform  pip install airbyte-source-typeform Zendesk Support  pip install airbyte-source-zendesk-support However, if you have implemented your own custom Airbyte sources, it’s also possible to integrate them by using the AirbyteCDKReader base class that works with the Source interface of the Airbyte CDK: from  llama_index  import  download_loader\n from  my_source.source  import  MyCustomSource  # plug in your own source here \nAirbyteCDKReader = download_loader(AirbyteCDKReader)\nconfig = {\n   # your custom configuration \n}\nreader = AirbyteCDKReader(source_class=MyCustomSource, config=config)\ndocuments = reader.load_data(stream_name= \"my-stream\" ) You can also install sources from the main Airbyte repository by installing directly via git — for example, to fetch the Github source, simply run pip install  \"source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github\" After that, the source is available to be plucked into the AirbyteCDKLoader: from  source_github.source  import  SourceGithub\nissues_loader = AirbyteCDKReader(source_class=SourceGithub, config=config)\ndocuments = reader.load_data(stream_name= \"issues\" ) Check out  the connector development documentation  for how to get started writing your own sources — it’s easy to get started with them and will allow you to move from local embedded loaders to using a hosted Airbyte instance seamlessly depending on your needs. Any questions? We would love to hear from you If you are interested in leveraging Airbyte to ship data to your LLM-based applications,  please take a moment  to fill out our survey so we can make sure to prioritize the most important features. If you have questions or are interested in other existing sources being exposed as loaders this way, do not hesitate to reach out on our  community slack channel  or in the  integrations channel  on the LlamaIndex discord server.", "author": "Jerry Liu", "date": "Aug 29, 2023", "tags": ["Airbyte", "Llamaindex", "AI", "LLM", "Etl"]},
{"title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development", "content": "A few months ago, we launched LlamaIndex 0.6.0, which included a massive rewrite of our codebase to make our library more modular, customizable, and accessible to both beginner and advanced users: We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines). We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system. Today, we’re excited to launch LlamaIndex 0.7.0. Our latest release continues the theme of improving modularity/customizability at the lower level to enable  bottoms-up development of LLM applications over your data.  You now have even more control over using key abstractions: the LLM, our response synthesizer, and our Document and Node objects. We’ve created  standalone LLM abstractions  (OpenAI, HuggingFace, PaLM). We’ve made our  response synthesis module an independent module  you can use completely independently of the rest of our abstractions — get rid of the prompt boilerplate of trying to figure out how to fit context within a context window. We’ve added  extensive metadata management capabilities  to our Document/Node objects — now you have complete control over context you decide to inject into your documents. Below, we describe each section more in detail. We also outline a full list of breaking changes at the bottom. Standalone LLM Abstractions We’ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM. These abstractions can be used on their own, or as part of an existing LlamaIndex system (query engines, retrievers). High-level Motivation We did this for multiple reasons: Cleaner abstractions in the codebase. Before, our  LLMPredictor  class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize. Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use “text-davinci-003”, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it’s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain’s LLMs if you wish. Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It’s reflective of our bigger push in 0.7.0 to let users compose their own workflows. Using on their own Our LLM abstractions support both  complete  and  chat  endpoints. The main difference is that  complete  is designed to take in a simple string input, and output a  CompletionResponse  (containing text output + additional fields).  chat  takes in a  ChatMessage  and outputs a  ChatResponse  (containing a chat message + additional fields). These LLM endpoints also natively support streaming via  stream_complete  and  stream_chat . Here’s on how you can use the LLM abstractions on their own: from  llama_index.llms  import  OpenAI\n\n # using complete endpoint \nresp = OpenAI().complete( 'Paul Graham is ' )\n print (resp)\n # get raw object \nresp_raw = resp.raw\n # using chat endpoint \n from  llama_index.llms  import  ChatMessage, OpenAI\nmessages = [\n    ChatMessage(role= 'system' , content= 'You are a pirate with a colorful personality' ),\n    ChatMessage(role= 'user' , content= 'What is your name' )\n]\nresp = OpenAI().chat(messages)\n print (resp)\n # get raw object \nresp_raw = resp.raw\n # using streaming endpoint \n from  llama_index.llms  import  OpenAI\nllm = OpenAI()\nresp = llm.stream_complete( 'Paul Graham is ' )\n for  delta  in  resp:\n     print (delta, end= '' ) Here’s how you can use the LLM abstractions as part of an overall LlamaIndex system. from  llama_index.llms  import  OpenAI\n from  llama_index.indices.service_context  import  ServiceContext\n from  llama_index  import  VectorStoreIndex\n\nllm = OpenAI(model= 'gpt-3.5-turbo' , temperature= 0 )\nservice_context = ServiceContext.from_defaults(llm=llm)\nindex = VectorStoreIndex.from_documents(docs, service_context=service_context)\nresponse = index.as_query_engine().query( \"&lt;question&gt;\" ) Note: Our top-level  LLMPredictor  still exists but is less user-facing (and we might deprecate in the future). Also, you can still use LangChain LLMs through our  LangChainLLM  class. Resources All of our notebooks have by default been updated to use our native OpenAI LLM integration. Here’s some resources to show both the LLM abstraction on its own as well as how it can be used in the overall system: OpenAI LLM Using LLM in LLMPredictor Changing LLM within Index/Query Engine Defining a custom LLM Model Standalone Response Synthesis Modules Context In any RAG system, there is retrieval and there is synthesis. The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM. Fundamentally, the synthesis module needs to synthesize a response over  any  context list, regardless of how long that context list is. This is essentially “boilerplate” that an LLM developer /  “AI engineer”  must write. We had this as an internal abstraction in LlamaIndex before (as a  ResponseSynthesizer ), but the external-facing UX was unfriendly to users. The actual piece that gathered responses (the  ResponseBuilder  ) was hard to customize, and the  ResponseSynthesizer  itself was adding an extra unnecessary layer. Now we have a set of standalone modules that you can easily import. Previously, when you set the  response_mode  in the query engine, these were being setup for you. Now they are more directly available and user-facing. Here’s a list of all the new  Response Synthesiszer  modules available from  llama_index.response_synthesizer : Refine  - Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk. Accumulate  - Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses Compact  - The same as  Refine , but puts as much text as possible into each LLM call CompactAndAccumulate  - The same as  Accumulate , but puts as much text as possible TreeSummarize  - Create a bottom-up summary from the provided text chunks, and return the root summary SimpleSummarize  - Combine and truncate all text chunks, and summarize in a single LLM call Usage As detailed above, you can directly set a response synthesizer in a query engine, or let the  response_mode  fetch the relevant response synthesizer. Furthermore though, you can directly call and use these synthesizers as low level modules. Here’s a small example: from  llama_index  import  ServiceContext\n from  llama_index.response_synthesizers  import  CompactAndRefine\n\n # you can also configure the text_qa_template, refine_template,  \n # and streaming toggle from here \nresponse_synthesizer = CompactAndRefine(\n  service_context=service_context.from_defaults()\n)\nresponse = response_synthesizer.get_response(\n  \"What skills does Bob have?\" ,\n  text_chunks=[ \" ...\" ]   # here would be text, hopefully about Bob's skills \n) Resources Here are some additional notebooks showing how to use  get_response_synthesizer  : Low-level API Usage Pattern Custom Retrievers Metadata Management Capabilities If you want to have good performance in any LLM application over your data (including a RAG pipeline), you need to make sure that your documents actually contain relevant context for the query. One way to do this is to add proper metadata, both at the document-level and after the documents have been parsed into text chunks (into Nodes). We allow you to define metadata fields within a Document, customize the ID, and also customize the metadata text/format for LLM usage and embedding usage. Defining Metadata Fields document = Document(\n    text='text', \n    metadata={\n        'filename': ' &lt; doc_file_name &gt; ', \n        'category': ' &lt; category &gt; '\n    }\n) Customizing the ID The ID of each document can be set multiple ways Within the constructor:  document = Document(text=\"text\", doc_id_=\"id\") After constructing the object:  document.doc_id = \"id\" Automatically using the  SimpleDirectoryReader  :  SimpleDirectoryReader(filename_as_id=True).load_data() Customizing the Metadata Text for LLMs and Embeddings As seen above, you can set metadata containing useful information. By default, all the metadata will be seen by the embedding model and the LLM. However, sometimes you may want to only include data to bias embeddings, or only include data as extra information for the LLM! With the new  Document  objects, you can configure what each metadata field is used for: document = Document(\n    text='text', \n    metadata={\n        'filename': ' &lt; doc_file_name &gt; ', \n        'category': ' &lt; category &gt; '\n    },\n    excluded_llm_metadata_keys=['filename', 'category'],\n    excluded_embed_metadata_keys=['filename']\n) Customizing the Metadata Format Template When the metadata is inserted into the text, it follows a very specific format. This format is configurable at multiple levels: from llama_index.schema import MetadataMode\n\ndocument = Document(\n  text='text',\n  metadata={\"key\": \"val\"},\n  metadata_seperator=\"::\",\n    metadata_template=\"{key}= &gt; {value}\",\n    text_template=\"Metadata: {metadata_str}\\\\n-----\\\\nContent: {content}\"\n)\n# available modes are ALL, NONE, LLM, and EMBED\nprint(document.get_content(metadata_mode=MetadataMode.ALL))\n# output:\n# Metadata: key= &gt; val\n# -----\n# text Please check out this guide for more  details ! Full List of Breaking Changes Response Synthesis + Node Postprocessors The  ResponseSynthesizer  object class has been removed, and replaced with  get_response_synthesizer  . In addition to this, node post processors are now handled by the query engine directly, and the old  SentenceEmbeddingOptimizer  has been switched to become a node post processor instance itself. Here is an example of the required migration to use all moved features. Old from  llama_index  import  (\n    VectorStoreIndex,\n    ResponseSynthesizer,\n)\n from  llama_index.indices.postprocessor  import  SimilarityPostprocessor\n from  llama_index.optimizers  import  SentenceEmbeddingOptimizer\n from  llama_index.query_engine  import  RetrieverQueryEngine\n\ndocuments = ...\n # build index \nindex = VectorStoreIndex.from_documents(documents)\n # configure retriever \nretriever = index.as_retriever(\n   similarity_top_k= 3 \n)\n # configure response synthesizer \nresponse_synthesizer = ResponseSynthesizer.from_args(\n   response_mode= \"tree_summarize\" ,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff= 0.7 ),\n        SentenceEmbeddingOptimizer(percentile_cutoff= 0.5 )\n    ]\n)\n # assemble query engine \nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n) New from llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.indices.postprocessor import (\n    SimilarityPostprocessor,\n    SentenceEmbeddingOptimizer\n)\n\ndocuments = ...\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n   response_mode=\"tree_summarize\",\n)\n# assemble query engine\nquery_engine = index.as_query_engine(\n  similarity_top_k=3,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7),\n        SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n    ]\n) LLM Predictor While introducing a new LLM abstraction, we cleaned up the LLM Predictor and removed several deprecated functionalities: Remove  ChatGPTLLMPredictor  and  HuggingFaceLLMPredictor  (use  OpenAI  and  HuggingFaceLLM  instead, see  migration guide ) Remove support for setting  cache  via  LLMPredictor  constructor. Removed  llama_index.token_counter.token_counter  module (see  migration guide ). Now, the LLM Predictor class is mostly a lightweight wrapper on top of the  LLM  abstraction that handles: conversion of prompts to the string or chat message input format expected by the LLM logging of prompts and responses to a callback manager We advice users to configure the  llm  argument in  ServiceContext  directly (instead of creating LLM Predictor). Chat Engine We updated the  BaseChatEngine  interface to take in a  List[ChatMessage]]  for the  chat_history  instead of tuple of strings. This makes the data model consistent with the input/output of the  LLM  , also more flexibility to specify consecutive messages with the same role. Old engine = SimpleChatEngine. from_defaults (\n\tchat_history=[( \"human message\" ,  \"assistant message\" )],\n)\nresponse = engine. chat ( \"new human message\" ) New engine = SimpleChatEngine.from_defaults(\n    service_context=mock_service_context,\n    chat_history=[\n        ChatMessage(role=MessageRole.USER, content=\"human message\"),\n        ChatMessage(role=MessageRole.ASSISTANT, content=\"assistant message\"),\n    ],\n)\nresponse = engine.chat(\"new human message\") We also exposed  chat_history  state as a property and supported overriding  chat_history  in  chat  and  achat  endpoints. Prompt Helper We removed some previously deprecated arguments:  max_input_size ,  embedding_limit ,  max_chunk_overlap Conclusion At a high-level, we hope that these changes continue to enable bottoms-up development of LLM applications over your data. We first encourage you to play around with our new modules on their own to get a sense what they do and where they can be used. Once you’re ready to use them in more advanced workflows, then you can figure out how to use our outer components to setup a sophisticated RAG pipeline. As always, our  repo  is here and our  docs  are here. If you have thoughts/comments, don’t hesitate to hop in our  Discord !", "author": "Jerry Liu", "date": "Jul 4, 2023", "tags": ["Llamaindex", "AI", "Machine Learning", "LLM", "NLP"]},
{"title": "LlamaIndex Update — 08/01/2023", "content": "Greetings once again, LlamaIndex community! Welcome to the third installment of our LlamaIndex Update series. Your active participation continues to drive our open-source community forward. We appreciate every contribution whether you’re an experienced LlamaIndex contributor or a newcomer! In our latest edition, we’ve prepared an assortment of updates for you. From advancements in Data Agents and LlamaIndex TS, benchmarking, and a host of inspiring events, webinars, blog posts, and demos, we’ve got plenty in store. Without more ado, let’s dive into these updates. New Features: We heard you! LlamaIndex has completely revamped our documentation. The update includes new clearer documents on high-level concepts, detailed module guides, comprehensive tutorials, and an all-inclusive API reference.  Docs ,  Tweet LlamaIndex launched Data Agents, an innovative feature that combines AI agents with data. This launch introduces components like an agent reasoning loop and tool abstractions. Accompanied by an extensive upgrade to LlamaHub, the new feature offers more than 15 tool specs for easy integration. Data Agents enhance query capabilities and are designed to handle varied data applications.  Docs ,  Tweet ,  Blog Post LlamaIndex launched LlamaIndex.TS, a lean Typescript package for building robust Retrieval Augmented Generation (RAG) systems. It simplifies tasks like document parsing and tackling context window limitations. LlamaIndex.TS is ideal for quickly building apps like using frameworks like Next.JS to chat over your data.  Docs ,  Tweet ,  Blogpost LlamaIndex teams up with Zapier Natural Language API (NLA), reducing the cognitive load on the data agent when handling APIs with multiple parameters. Zapier NLA translates complex third-party APIs into simpler interfaces using a single natural language parameter: instruction. This helps the data agent concentrate on tool selection and action orchestration.  Tweet ,  Blogpost LlamaIndex’s  ContextChatEngine addresses the issue of conversational agents hallucinating information by ensuring retrieval of context with every user interaction. This feature, compatible with all ReAct and OpenAI Function agent types, prepends retrieved-context as a system message.  Docs ,  Tweet This month marked the launch of two new exciting LLMs. First off was Anthropic Claude 2.0. We launched with day 1 support of the new model.  Docs ,  Tweet . The other one was Llama2, and LlamaIndex now offers best-in-class integration with the Llama2 model on Replicate.  Docs ,  Tweet LlamaIndex is day one compatible with Chroma v0.4.0, enhancing support for in-memory, persisted, and self-hosted databases. This upgrade simplifies the use of Chroma within LlamaIndex, making database handling easier and more efficient.  Tweet LlamaIndex’s newly launched Data Agents can automatically interact with any API defined via an OpenAPI spec. It handles indexing/loading of large data from API specs and facilitates easy integration of the OpenAPI Tool, enhancing the ability to call web services.  Docs ,  Tweet LlamaIndex now utilizes the  rebel-large  model for high-speed relation extraction. Combined with CUDA, you can generate knowledge graphs from your text data.  Tweet LlamaIndex introduced a code interpreter tool. This feature equips any LLM with the ability to analyze data and generate visualizations, expanding their capabilities similar to those of ChatGPT.  Tweet LlamaIndex now integrates with Eduardo Reis’s Llama 2 functions API at  llama-api.com .  Tweet LlamaIndex TS now supports integration with OpenAI Whisper.  Docs ,  Tweet LlamaIndex now seamlessly integrates with  Kùzudb , allowing users to directly store extracted knowledge graphs/triples for advanced processing, querying, and visualization.  Docs ,  Tweet LlamaIndex combines data agents with text-to-image models enhancing user prompts with relevant context from a knowledge base. This integration allows for more advanced multimodal reasoning by merging LLM RAG systems with text-to-image tools.  Docs ,  Tweet Benchmarking: LlamaIndex now supports BEIR, an Information Retrieval benchmark. Users can define custom retrievers within LlamaIndex, apply the vector index, or implement reranking steps, and then easily evaluate their methods using any dataset from BEIR.  Tweet LlamaIndex’s Llama2 agents have shown promising performance in our agent task benchmark. Especially notable is their capability to appropriately use tools within a ReACT loop. However, the tasks’ difficulty varies, with both 13B and 70B models notably refraining from dialing a phone number, underlining the AI’s limitations.  Tweet LlamaIndex now has integration with the HotpotQA benchmark! This enables rigorous testing of LLM’s multi-hop reasoning capabilities by providing the full context to the models, helping you evaluate their performance more accurately. Perfect for stress-testing LLMs like ChatGPT, Claude 2, PaLM, and more. Plus, explore how context reordering can simplify tasks for your LLMs.  Tweet LlamaIndex now supports over 20 vector databases, each with unique features and capabilities. To help understand their differences, we have compiled a comprehensive comparison table, guiding the choice of the optimal database for the use case.  Tweet Tutorials: We were excited to see so many people making tutorials for LlamaIndex this month! Adam Hofmann ’s blog post on  Building Better Tools for LLM Agents . Weav iate’s  tutorial  on using the Llama2 model with LlamaIndex and Weaviet on external data. Erika ’s  tutorial  on VectorStore Index, List Index, and Tree Index. James Maslek ’s  tutorial  on Breaking Barriers with OpenBB and LlamaIndex: Simplifying data access to 100+ trusted sources. Ayush Thakur ’s tutorial on  Building Advanced Query Engine and Evaluation with LlamaIndex and W&B . Trulens ’s  tutorial  on using LlamaIndex Yelp agent to answer queries using Yelp data, and evaluate it for definitiveness and accuracy using custom feedback functions, compare its performance against a standalone LLM. Airbyte ’s  tutorial  on Chat with your data warehouse without writing SQL. Anil Chandra Naidu ’s tutorial on  Retrievers  and  QueryEngines . Wenqi Glantz ’s tutorial on  Exploring Snowflake and Streamlit With LlamaIndex Text-to-SQL . And from the LlamaIndex team: Logan ’s  tutorial  on a comprehensive understanding of embedding models, their benchmarking, and their implementation in LlamaIndex, with a focus on OpenAI and Instructor embeddings, enabling semantic search through numerical text representations. Logan ’s  tutorial  on the evaluation of query engines using LlamaIndex, learn to handle uncontrolled outputs and runtime costs while measuring performance with GPT-4. Ravi Theja ’s  tutorial  on Key Components to build QA Systems. Webinars: Webinar  with Didier Lopes, CEO/Co-Founder at OpenBB on LLMs for Investment Research. Webinar  on Building & Evaluating an Advanced Query Engine Over Your Data with Weights and Biases. Webinar  with  Jason  Liu on From Prompt to Schema Engineering with Pydantic. Events: LlamaIndex and Arize  workshop  on LLM Search & Retrieval Systems with Arize and LlamaIndex: Powering LLMs on Your Proprietary Data. LlamaIndex and TruLens  workshop  on building an LLM App. TPF  (The Product Folks)  workshop session  on Building QA Systems With LlamaIndex by  Ravi Theja . Ravi Theja   talk  at the  Speciale VC  GenAI meetup in Chennai on Beyond the Basics: Leveraging LlamaIndex from Concept to Production. Data Agents session at TPF X Nexus VC  Buildathon  by  Ravi Theja . Demos: Tali.AI  at the Augment hackathon dove into the future of support roles by developing an Autonomous Support Bot using LlamaIndex.  Tweet SuperAGI  integrated with LlamaIndex which enables AI agents to process a wide variety of data from both structured and unstructured sources including Docx, PDF, CSV files, videos, and images.  Tweet", "author": "Ravi Theja", "date": "Aug 1, 2023", "tags": ["Large Language Models", "Llamaindex", "AI", "NLP", "Chat"]},
{"title": "Data Agents + Zapier NLA", "content": "Joint blog by LlamaIndex team & Zapier NLA team Wouldn’t it be great to have a personal assistant that can  access your data  and  perform tasks for you ? Introducing LlamaIndex data agents, now more powerful with Zapier NLA.  Within 5 lines of code , you can access the 5,000+ third party apps and over 30,000 actions on Zapier. from  llama_hub.tools.zapier.base  import  ZapierToolSpec\n from  llama_index.agent  import  OpenAIAgent\n\nzapier_spec = ZapierToolSpec(api_key= \"sk-ak-your-key\" )\nagent = OpenAIAgent.from_tools(zapier_spec.to_tool_list(), verbose= True )\n\nagent.chat( 'Can you summarize the unread emails and send it to me on Slack?' )", "author": "Simon Suo", "date": "Jul 25, 2023", "tags": ["Agents", "Llamaindex", "Artificial Intelligence", "API", "Zapier"]},
{"title": "Zep and LlamaIndex: A Vector Store Walkthrough", "content": "Editor’s Note: This article was written by  Daniel  at Zep for the LlamaIndex blog. Zep is a  long-term memory store for LLM applications . With Zep, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Document and chat history storage, embedding, enrichment, and more are taken care of by the Zep service. In this article, we demonstrate how to use Zep’s new Document Vector Store with the (also new) ZepVectorStore for LlamaIndex. Installing Zep and some important concepts Zep is open source  and may be  installed via Docker , or to Kubernetes and hosting platforms such as Render. SDKs are available for Python and TypeScript, and frameworks such as LangChain and LlamaIndex ship with support for Zep. Zep stores documents in Collections, with the document text, embeddings, and metadata all colocated. This enables hybrid semantic search over a collection, with results filtered by JSONPath queries against document metadata. When using Zep with LlamaIndex, LlamaIndex filters are translated for use by Zep. A document or document chunk is equivalent to a LlamaIndex TextNode or NodeWithEmbedding. Collections can be optionally set to automatically embed texts using a service such as OpenAI or locally using an embedding model of your choice. However, when using Zep with LlamaIndex, we rely on LlamaIndex’s integrations with embedded services and libraries. Creating a ZepVectorStore and Document Collection You will need to have  installed Zep  and have your API URL and, optionally, authentication key handy. from  llama_index.vector_stores  import  ZepVectorStore\nzep_api_url =  \"http://localhost:8000\" \nzep_api_key =  \"&lt;optional_jwt_token&gt;\" \ncollection_name =  \"babbage\"   # The name of a new or existing collection \nembedding_dimensions =  1536   # the dimensions of the embedding model you intend to use \nvector_store = ZepVectorStore(\n  api_url=zep_api_url,\n  api_key=zep_api_key,\n  collection_name=collection_name,\n  embedding_dimensions=embedding_dimensions\n) The collection name is a unique identifier for your vector index and should only contain alphanumeric characters. If the collection does not exist, Zep will automatically create one for you. Creating and populating an Index Below we’ll use a common LlamaIndex pattern for loading content and adding it to an index. After loading the text data, we create a StorageContext backed by the ZepVectorStore. We then create the index using our loaded documents and Zep-backed storage context. from  llama_index  import  VectorStoreIndex, SimpleDirectoryReader\n from  llama_index.storage.storage_context  import  StorageContext\n\ndocuments = SimpleDirectoryReader( \"./babbages_calculating_engine/\" ).load_data()\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nquery =  \"the sun and stars\" \n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\n\n\n print ( str (response)) But one of the most signal examples of this kind, of which we are aware, is related by Mr Baily. The catalogue of stars published by the Astronomical Society was computed by two separate and independent persons, and was afterwards compared and examined with great care and attention by Mr Stratford. On examining this catalogue, and recalculating a portion of it, Mr Baily discovered an error in the case of the star Finally, we run a simple text query against the index and print the resulting node’s text. Hybrid search with metadata filters As mentioned above, Zep also supports associating rich metadata with documents. This metadata can be an arbitrarily deep JSON structure. When working with LlamaIndex, we currently support filtering on top-level keys in the map. The code below demonstrates running a vector search over an index and filtering on metadata using LlamaIndex’s MetadataFilters. We print the result and the normalized cosine similarity for the matching result. from  llama_index.schema  import  TextNode\n from  llama_index.vector_stores.types  import  ExactMatchFilter, MetadataFilters\n\nnodes = [\n   TextNode(\n       text= \"Not aware that tables of these squares existed, Bouvard, who calculated the tides for Laplace, underwent the labour of calculating the square of each individual sine in every case in which it occurred.\" ,\n       metadata={\n            \"topic\" :  \"math\" ,\n            \"entities\" :  \"laplace\" ,\n       },\n   ),\n   TextNode(\n       text= \"Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.\" ,\n       metadata={\n            \"topic\" :  \"astronomy\" ,\n            \"entities\" :  \"moon\" ,\n       },\n   ),\n]\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n\nfilters = MetadataFilters(filters=[ExactMatchFilter(key= \"topic\" , value= \"astronomy\" )])\n\n\nretriever = index.as_retriever(filters=filters)\nresult = retriever.retrieve( \"What is the structure of our galaxy?\" )\n\n\n for  r  in  result:\n    print ( \"\\n\" , r.node.text, r.score) Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.  0.6456785674 Summing it up Zep offers a single API for vector search over documents and chat history, allowing developers to populate prompts with both forms of long-term memory. LlamaIndex makes it extremely easy to populate Zep with content from a broad set of documents and data sources and query these sources when building prompts and other functionality for LLM apps. Next Steps Read the  Zep Quick Start Guide Zep and LlamaIndex Walkthrough Notebook Getting Started with LllamaIndex", "author": "Jerry Liu", "date": "Aug 11, 2023", "tags": ["AI", "Llamaindex", "Machine Learning", "Vector Database", "NLP"]},
{"title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications", "content": "Authors: Avthar Sewrathan, Matvey Arye, Jerry Liu, Yi Ding Introducing the  Timescale Vector  integration for LlamaIndex. Timescale Vector enables LlamaIndex developers to build better AI applications with PostgreSQL as their vector database: with faster vector similarity search, efficient time-based search filtering, and the operational simplicity of a single, easy-to-use cloud PostgreSQL database for not only vector embeddings but an AI application’s relational and time-series data too. PostgreSQL is the world’s most loved database, according to the  Stack Overflow 2023 Developer Survey . And for a good reason: it’s been battle-hardened by production use for over three decades, it’s robust and reliable, and it has a rich ecosystem of tools, drivers, and connectors. And while pgvector, the open-source extension for vector data on PostgreSQL, is a wonderful extension (and all its features are offered as part of Timescale Vector), it is just one piece of the puzzle in providing a production-grade experience for AI application developers on PostgreSQL. After speaking with numerous developers at nimble startups and established industry giants, we saw the need to enhance pgvector to cater to the performance and operational needs of developers building AI applications. Here’s the TL;DR on how Timescale Vector helps you build better AI applications with LlamaIndex: Faster similarity search on millions of vectors:  Thanks to the introduction of a new search index inspired by the DiskANN algorithm,  Timescale Vector achieves 3X faster search speed at ~99% recall than a specialized database  and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a  10x index space savings compared to pgvector . Timescale Vector also offers pgvector’s Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms. Efficient similarity search with time-based filtering:  Timescale Vector optimizes time-based vector search queries, leveraging the automatic time-based partitioning and indexing of  Timescale’s hypertables  to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses. Simplified AI infra stack:  By combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale. Simplified metadata handling and multi-attribute filtering:  Developers can leverage all PostgreSQL data types to store and filter metadata and JOIN vector search results with relational data for more contextually relevant responses. In future releases, Timescale Vector will further optimize rich multi-attribute filtering, enabling even faster similarity searches when filtering on metadata. On top of these innovations for vector workloads, Timescale Vector provides a robust, production-ready cloud PostgreSQL platform with flexible pricing, enterprise-grade security, and free expert support. In the rest of this post, we’ll dive deeper (with code!) into the unique capabilities Timescale Vector enables for developers wanting to use PostgreSQL as their vector database with LlamaIndex: Faster similarity search with DiskANN, HNSW and IVFFlat index types. Efficient similarity search when filtering vectors by time. Retrieval Augmented Generation (RAG) with time-based context retrieval. (If you want to jump straight to the code, explore  this tutorial ). 🎉  LlamaIndex Users Get 3 Months of Timescale Vector for Free We’re giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won’t be charged for any cloud PostgreSQL databases you spin up during your trial period.  Try Timescale Vector for free today . Faster Vector Similarity Search in PostgreSQL Timescale Vector speeds up Approximate Nearest Neighbor (ANN) search on large-scale vector datasets, enhancing pgvector with a state-of-the-art ANN index inspired by the  DiskANN  algorithm. Timescale Vector also offers pgvector’s HNSW and IVFFlat indexing algorithms, giving developers the flexibility to choose the right index for their use case. Our performance benchmarks using the  ANN benchmarks  suite show that Timescale Vector achieves between 39.43% and 1,590.33% faster search speed at ~99% recall than all existing PostgreSQL search indexes, and 3X faster search speed at ~99% recall than specialized vector databases on a dataset of one million OpenAI embeddings. You can  read more about the performance benchmark methodology, the databases compared and results here . Timescale Vector’s new DiskANN-inspired index outperforms all existing PostgreSQL index types when performing approximate nearest neighbor searches at 99 % recall on 1 million OpenAI embeddings. Using Timescale Vector’s DiskANN, HNSW, or IVFFLAT indexes in LlamaIndex is incredibly straightforward. Simply create a Timescale Vector vector store and add the  data nodes  you want to query as shown below: from  llama_index.vector_stores  import  TimescaleVectorStore\n\n # Create a timescale vector store with specified params \nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name= \"your_table_name\" ,\n   time_partition_interval= timedelta(days= 7 ),\n)\nts_vector_store.add(nodes) Then run: # Create a timescale vector index (DiskANN)\nts_vector_store.create_index() This will create a timescale-vector index with the default parameters. We should point out that the term “index” is a bit overloaded. For many VectorStores, an index is the thing that stores your data (in relational databases this is often called a table), but in the PostgreSQL world an index is something that speeds up search, and we are using the latter meaning here. We can also specify the exact parameters for index creation in the  create_index  command as follows: # create new timescale vector index (DiskANN) with specified parameters \nts_vector_store. create_index ( \"tsv\" , max_alpha= 1.0 , num_neighbors= 50 ) Advantages to this Timescale Vector’s new DiskANN-inspired vector search index include the following: Faster vector search at 99% accuracy in PostgreSQL. Optimized for running on disks, not only in memory use. Quantization optimization compatible with PostgreSQL, reducing the vector size and consequently shrinking the index size ( by 10x in some cases !) and expediting searches. Efficient hybrid search or filtering additional dimensions. For more on how Timescale Vector’s new index works,  see this blog post . Pgvector is packaged as part of Timescale Vector, so you can also access pgvector’s HNSW and IVFFLAT indexing algorithms in your LlamaIndex applications. The ability to conveniently create ANN search indexes from your LlamaIndex application code makes it easy to create different indexes and compare their performance: # Create an HNSW index\n# Note: You don't need to specify m and ef_construction parameters as we set smart defaults.\nts_vector_store.create_index(\"hnsw\", m=16, ef_construction=64)\n\n# Create an IVFFLAT index\n# Note: You don't need to specify num_lists and num_records parameters as we set smart defaults.\nts_vector_store.create_index(\"ivfflat\", num_lists=20, num_records=1000) Add Efficient Time-Based Search Functionality to Your LlamaIndex AI Application Timescale Vector optimizes time-based vector search,   leveraging the automatic time-based partitioning and indexing of  Timescale’s hypertables  to efficiently search vectors by time and similarity. Time is often an important metadata component for vector embeddings. Sources of embeddings, like documents, images, and web pages, often have a timestamp associated with them, for example, their creation date, publishing date, or the date they were last updated, to name but a few. We can take advantage of this time metadata in our collections of vector embeddings to enrich the quality and applicability of search results by retrieving vectors that are not just semantically similar but also pertinent to a specific time frame. Here are some examples where time-based retrieval of vectors can improve your LlamaIndex applications: Finding recent embeddings:  Finding the most recent embeddings that are semantically similar to a query vector. For example, finding the most recent news, documents, or social media posts related to elections. Search within a time-range:  Constraining similarity search to only vectors within a relevant time range. For example, asking time-based questions about a knowledge base (“What new features were added between January and March 2023?”). Chat history:  Storing and retrieving LLM response history. For example, chatbot chat history. Let’s take a look at an example of performing time-based searches on a  git log dataset . In a git log, each entry has a timestamp, an author, and some information about the commit. To illustrate how to use TimescaleVector’s time-based vector search functionality, we’ll ask questions about the git log history for TimescaleDB. Each git commit entry has a timestamp associated with it, as well as a message and other metadata (e.g., author). We’ll illustrate how to create nodes with a time-based UUID and how to run similarity searches with time range filters using the Timescale Vector vector store.. Create nodes from each commit in the gitlog First, we load the git log entries from the  demo CSV file  using Pandas: import  pandas  as  pd\n from  pathlib  import  Path\n\n\n # Read the CSV file into a DataFrame \nfile_path = Path( \"../data/csv/commit_history.csv\" )\ndf = pd.read_csv(file_path) Next, we’ll create nodes of type `TextNode` for each commit in our git log dataset, extracting the relevant information and assigning it to the node’s text and metadata, respectively. from  llama_index.schema  import  TextNode, NodeRelationship, RelatedNodeInfo\n # Create a Node object from a single row of data \n def   create_node ( row ):\n   record = row.to_dict()\n   record_name = split_name(record[ \"author\" ])\n   record_content =  str (record[ \"date\" ]) +  \" \"  + record_name +  \" \"  +  str (record[ \"change summary\" ]) +  \" \"  +  str (record[ \"change details\" ])\n   node = TextNode(\n       id_=create_uuid(record[ \"date\" ]),\n       text= record_content,\n       metadata={\n            'commit' : record[ \"commit\" ],\n            'author' : record_name,\n            'date' : create_date(record[ \"date\" ]),\n       }\n   )\n    return  node\n\nnodes = [create_node(row)  for  _, row  in  df.iterrows()] Note:  The code above references two helper functions to get things in the right format (`split_name()` and `create_date()`), which we’ve omitted for brevity. The full code is included in the tutorial linked in the Resources section at the end of this post. Create UUIDs for each node based on the date of each git commit We will take a closer look at a helper function we use to create each node’s  id_ . For time-based search in LlamaIndex, Timescale Vector uses the ‘datetime’ portion of a UUID v1 to place vectors in the correct time partition.  Timescale Vector’s Python client library  provides a simple-to-use function named `uuid_from_time` to create a UUID v1 from a Python DateTime object, which we’ll then use as our `ids` for the TextNodes. from  timescale_vector  import  client\n # Function to take in a date string in the past and return a uuid v1 \n def   create_uuid ( date_string:  str ):\n    if  date_string  is   None :\n        return   None \n   time_format =  '%a %b %d %H:%M:%S %Y %z' \n   datetime_obj = datetime.strptime(date_string, time_format)\n   uuid = client.uuid_from_time(datetime_obj)\n    return   str (uuid) Since we are dealing with timestamps in the past, we take advantage of the `uuid_from_time` function to help generate the correct UUIDs for each node. If you want the current date and time associated with your Nodes (or Documents) for time-based search, you can skip this step. A UUID associated with the current date and time will be automatically generated as the nodes are added to the table in Timescale Vector by default. Let’s take a look at the contents of a node: print(nodes[0].get_content(metadata_mode=\"all\")) commit: 44e41c12ab25e36c202f58e068ced262eadc8d16\nauthor: Lakshmi Narayanan Sreethar\ndate: 2023-09-5 21:03:21+0850\n\nTue Sep 5 21:03:21 2023 +0530 Lakshmi Narayanan Sreethar Fix segfault in set_integer_now_func When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037 Create vector embeddings for the text of each node Next, we’ll create vector embeddings of the content of each node so that we can perform similarity searches on the text associated with each node. We’ll use the `OpenAIEmbedding` model to create the embeddings. # Create embeddings for nodes \n from  llama_index.embeddings  import  OpenAIEmbedding\nembedding_model = OpenAIEmbedding()\n\n for  node  in  nodes:\n   node_embedding = embedding_model.get_text_embedding(\n       node.get_content(metadata_mode= \"all\" )\n   )\n   node.embedding = node_embedding Load nodes into Timescale Vector vector store Next, we’ll create a `TimescaleVectorStore` instance and add the nodes we created to it. # Create a timescale vector store and add the newly created nodes to it\nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name=\"li_commit_history\",\n   time_partition_interval= timedelta(days=7),\n)\nts_vector_store.add(nodes) To take advantage of Timescale Vector’s efficient time-based search, we need to specify the `time_partition_interval` argument when instantiating a Timescale Vector vector store. This argument represents the length of each interval for partitioning the data by time. Each partition will consist of data that falls within the specified length of time. In the example above, we use seven days for simplicity, but you can pick whatever value makes sense for the queries used by your application — for example, if you query recent vectors frequently, you might want to use a smaller time delta like one day, or if you query vectors over a decade-long time period, then you might want to use a larger time delta like six months or one year. As a rule of thumb, common queries should touch only a couple of partitions and at the same time your full dataset should fit within a 1000 partitions, but don’t stress too much — the system is not very sensitive to this value. Similarity search with time filters Now that we’ve loaded our nodes that contain vector embeddings data and metadata into a Timescale Vector vector store, and enabled automatic time-based partitioning on the table our vectors and metadata are stored in, we can query our vector store with time-based filters as follows: # Query the  vector  database\nvector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k= 5 )\n\n# Time filter variables  for  query\nstart_dt = datetime( 2023 ,  8 ,  1 ,  22 ,  10 ,  35 ) # Start date =  1  August  2023 ,  22 : 10 : 35 \nend_dt = datetime( 2023 ,  8 ,  30 ,  22 ,  10 ,  35 ) # End date =  30  August  2023 ,  22 : 10 : 35 \n\n # return most similar vectors to query between start date and end date date range \n # returns a VectorStoreQueryResult object \nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, end_date = end_dt) Let’s take a look at the date and contents of the nodes returned by our query: # for each node in the query result, print the node metadata date \n for  node  in  query_result.nodes:\n    print ( \"-\"  *  80 )\n    print (node.metadata[ \"date\" ])\n    print (node.get_content(metadata_mode= \"all\" )) --------------------------------------------------------------------------------\n2023-08-3 14:30:23+0500\ncommit:  7aeed663b9c0f337b530fd6cad47704a51a9b2ec\nauthor: Dmitry Simonenko\ndate: 2023-08-3 14:30:23+0500\n\nThu Aug 3 14:30:23 2023 +0300 Dmitry Simonenko Feature flags for TimescaleDB features This PR adds..\n--------------------------------------------------------------------------------\n2023-08-29 18:13:24+0320\ncommit:  e4facda540286b0affba47ccc63959fefe2a7b26\nauthor: Sven Klemm\ndate: 2023-08-29 18:13:24+0320\n\nTue Aug 29 18:13:24 2023 +0200 Sven Klemm Add compatibility layer for _timescaledb_internal functions With timescaledb 2.12 all the functions present in _timescaledb_internal were…\n--------------------------------------------------------------------------------\n2023-08-22 12:01:19+0320\ncommit:  cf04496e4b4237440274eb25e4e02472fc4e06fc\nauthor: Sven Klemm\ndate: 2023-08-22 12:01:19+0320\n\nTue Aug 22 12:01:19 2023 +0200 Sven Klemm Move utility functions to _timescaledb_functions schema To increase schema security we do not want to mix…\n--------------------------------------------------------------------------------\n2023-08-29 10:49:47+0320\ncommit:  a9751ccd5eb030026d7b975d22753f5964972389\nauthor: Sven Klemm\ndate: 2023-08-29 10:49:47+0320\n\nTue Aug 29 10:49:47 2023 +0200 Sven Klemm Move partitioning functions to _timescaledb_functions schema To increase schema security…\n--------------------------------------------------------------------------------\n2023-08-9 15:26:03+0500\ncommit:  44eab9cf9bef34274c88efd37a750eaa74cd8044\nauthor: Konstantina Skovola\ndate: 2023-08-9 15:26:03+0500\n\nWed Aug 9 15:26:03 2023 +0300 Konstantina Skovola Release 2.11.2 This release contains bug fixes since the 2.11.1 release… Success! Notice how only vectors with timestamps within the specified start and end date ranges of 1 August, 2023, and 30 August, 2023, are included in the results. Here’s some intuition for why Timescale Vector’s time-based partitioning speeds up ANN queries with time-based filters. Timescale Vector partitions the data by time and creates ANN indexes on each partition individually. Then, during search, we perform a three-step process: Step 1: filter our partitions that don’t match the time predicate. Step 2: perform the similarity search on all matching partitions. Step 3: combine all the results from each partition in step 2, rerank, and filter out results by time. Timescale Vector leverages  TimescaleDB’s hypertables , which automatically partition vectors and associated metadata by a timestamp. This enables efficient querying on vectors by both similarity to a query vector and time, as partitions not in the time window of the query are ignored, making the search a lot more efficient by filtering out whole swaths of data in one go. When performing a vector similarity search on `TimescaleVectorStore`, rather than specifying the start and end dates for our search, we can also specify a time filter with a provided start date and time delta later: # return most similar vectors to query from start date and a time delta later\nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, time_delta = td) And we can specify a time filter within a provided end_date and time delta earlier. This syntax is very useful for filtering your search results to contain vectors before a certain date cutoff. # return most similar vectors to query from end date and a time delta earlier\nquery_result = ts_vector_store.query(vector_store_query, end_date = end_dt, time_delta = td) Powering Retrieval Augmented Generation With Time-Based Context Retrieval in LlamaIndex Applications With Timescale Vector Let’s put everything together and look at how to use the TimescaleVectorStore to power RAG on the git log dataset we examined above. To do this, we can use the TimescaleVectorStore as a  QueryEngine . When creating the query engine, we use TimescaleVector’s time filters to constrain the search to a relevant time range by passing our time filter parameters as `vector_strore_kwargs`. from  llama_index  import  VectorStoreIndex\n from  llama_index.storage  import  StorageContext\n\nindex = VectorStoreIndex.from_vector_store(ts_vector_store)\nquery_engine = index.as_query_engine(vector_store_kwargs = ({ \"start_date\" : start_dt,  \"end_date\" :end_dt}))\n\nquery_str =  \"What's new with TimescaleDB functions? When were these changes made and by whom?\" \nresponse = query_engine.query(query_str)\n print ( str (response)) We asked the LLM a question about our gitlog, namely, “What’s new with TimescaleDB functions. When were these changes made and by whom?” Here’s the response we get, which synthesizes the nodes returned from semantic search with time-based filtering on the Timescale VectorStore: TimescaleDB functions have undergone changes recently. These changes include the addition of several GUCs (Global User Configuration) that allow for enabling or disabling major TimescaleDB features. Additionally, a compatibility layer has been added for the \"_timescaledb_internal\" functions, which were moved into the \"_timescaledb_functions\" schema to enhance schema security. These changes were made by Dmitry Simonenko and Sven Klemm. The specific dates of these changes are August 3, 2023, and August 29, 2023, respectively. This is a simple example of a powerful concept — using time-based context retrieval in your RAG applications can help provide more relevant answers to your users. This time-based context retrieval can be helpful to any dataset with a natural language and time component. Timescale Vector uniquely enables this thanks to its efficient time-based similarity search capabilities, and taking advantage of it in your LlamaIndex application is easy thanks to the Timescale Vector integration. Resources and next steps Now that you’ve learned how Timescale Vector can help you power better AI applications with PostgreSQL, it’s your turn to dive in. Take the next step in your learning journey by following one of the tutorials or reading one of the blog posts in the resource set below: Up and Running Tutorial :  learn how to use Timescale Vector in LlamaIndex using a real-world dataset. You’ll learn how to use Timescale Vector as a Vectorstore, Retriever, and QueryEngine and perform time-based similarity search on vectors. Timescale Vector explainer : learn more about the internals of Timescale Vector. Timescale Vector website :  learn more about Timescale Vector and Timescale’s AI Launch Week. 🎉 And a reminder:  LlamaIndex Users get Timescale Vector free for 90 days We’re giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won’t be charged for any cloud PostgreSQL databases you spin up during your trial period.  Try Timescale Vector for free today .", "author": "Jerry Liu", "date": "Sep 27, 2023", "tags": ["Llamaindex", "NLP", "LLM", "AI", "Timescaledb"]},
{"title": "ChatGPT’s Knowledge is Two Years Old: What to do if you’re building applications?", "content": "It’s official: as of today, ChatGPT’s knowledge cutoff is 2 years old. Happy 2nd birthday to ChatGPT's knowledge cutoff! 🎂  pic.twitter.com/O1cgRPSP3l — Yi Ding -- prod/acc (@yi_ding)  September 1, 2023   Why doesn’t OpenAI just update it? There are some fundamental reasons for this: training new LLMs is an expensive — at least tens of millions of dollars — and not guaranteed process. Cleaning new data sets for training is also expensive. What should I do if I’m building an application that needs more recent data? You may be tempted to just send ChatGPT the entire wikipedia pages for 2022 and 2023:  https://en.wikipedia.org/wiki/2022  You’ll soon run into two limits: 1. there is a limit on the number of words you can send to a large language model (LLM). This is called the “context window.” 2. LLM APIs charge you by the word, so the more you send it, the more expensive your API calls become. The standard technique is one called “Retrieval Augmented Generation” or RAG. What it is, boiled down very simply, is a process of searching for the right context, giving that context to the LLM, and then getting better results back. What’s Retrieval Augmented Generation? Search, Give, Get. For those of us coming from a traditional software development background RAG can sound intimidating, but it really is a simple concept: Search for the relevant data Give the data to GPT Get a better response Of course,… — Yi Ding -- prod/acc (@yi_ding)  July 28, 2023   At  LlamaIndex  we are the RAG experts, but there is a whole community of open source projects that are tackling this problem. We have integrated with over 20 open source vector databases and there are other open source tools like LangChain, Semantic Kernel, DSPy, Axilla and others (put your favorites in the comments!) that are attacking the problem in different ways. Another technique is called fine tuning. Here, you essentially create a new custom model on top of an existing LLM. While LlamaIndex does support fine tuning, it often requires much more work and data: We are big fans of fine tuning and custom models but knowing when to use RAG and when to use fine tuning, and how to use them in combination, is essential. Watch this space!  https://t.co/vTpWauhj3C — LlamaIndex 🦙 (@llama_index)  August 18, 2023   What if I don’t need more recent data? That’s totally OK! Not every application needs data that’s more recent than 2021. Before LlamaIndex, I worked on an open source reading education tool, and phonics have definitely not changed in the last two years. If you’re building something to write bedtime stories (❤️ Kidgeni  https://kidgeni.com/ ) or raps (check out TextFX!  https://textfx.withgoogle.com/ ) your application What if I just want to use ChatGPT with more recent information? There are a lot of chatbots that use Retrieval Augmented Generation currently. A few of the ones I’ve personally tried are Metaphor  https://metaphor.systems/ , Perplexity  https://www.perplexity.ai/  and Medisearch  https://medisearch.io/ , and of course Google Bard and BingGPT.", "author": "Yi Ding", "date": "Sep 1, 2023", "tags": ["Llamaindex", "ChatGPT", "AI", "Fine Tuning", "NLP"]},
{"title": "LlamaIndex update 2023–10–10", "content": "Here’s our weekly look at developments across the LLM space and RAG (Retrieval Augmented Generation) in particular, as well as the latest news and features from your favorite open source library. If you’ve got a project (or a blog post, or a video) that you think people should hear about, we’re happy to feature it in here! Drop us a line at  news@llamaindex.ai . This update is now available in handy email form! Just head to our  home page  and enter your email to sign up. 🤩  First, the highlights: Full observability with Arize AI Phoenix : we launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local.  Notebook ,  Tweet . RetrieverEvaluator : new in the library, “RetrieverEvaluator” allows enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.  Docs ,  Tweet . HuggingFace Embeddings : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format.  Docs ,  Tweet . Multi-Document Agents : we’ve introduced v0 experimental support for multi-document agents for advanced QA, beyond typical top-k RAG. It supports diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency.  Docs ,  Tweet . 🏆  Congratulations to our Streamlit Hackathon Winners! We love seeing people build amazing things with LlamaIndex! NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkutx FinSight by Vishwas Gowda:  https://buff.ly/3PzOnyC ✨  Feature Releases   and Enhancements: Multi-Document Agents : we introduced multi-document agents (V0) for advanced QA, beyond typical top-k RAG. They support diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency.  Docs ,  Tweet . Ensemble Retriever:  we’re addressing the RAG challenge of determining chunk size by experimenting with diverse document chunking and ensembling for retrieval.  Docs ,  Tweet . HuggingFace Embeddings : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format.  Docs ,  Tweet . OpenAI Function Calling fine-tuning:  we’re using OpenAI’s latest function calling fine-tuning which enhanced structured data extraction, optimizing gpt-3.5-turbo for improved extraction in RAG.  Docs ,  Tweet . Metadata Extraction : we’re making metadata extraction efficient by extracting a complete Pydantic object from a document with just one LLM call.  Docs ,  Tweet . Structured RAG Outputs : we now efficiently structure RAG pipeline outputs with native Pydantic outputs from all queries without the need for an additional LLM parsing call.  Docs ,  Tweet . Streamlined  secinsights.ai  deployment : Our open-sourced  secinsights.ai  offers a RAG app template, now enhanced with GitHub Codespaces and Docker for swift cloud deployment without setup hassles.  Tweet . LongContextReorder:  We introduced LongContextReorder****,**** Zeneto’s approach to reposition vital context in RAG systems, addressing the challenge of over-retrieving which can obscure essential details.  Docs ,  Tweet . RA-DIT:  We drew inspiration from the RA-DIT paper, which introduced LLM fine-tuning for retrieval-augmented input prompts to improve RAG systems. This method fosters enhanced utilization of context and more effective answer synthesis, even in the presence of suboptimal context.  Docs ,  Tweet . Blockchain:  LlamaIndex data agents can be now used to analyze any blockchain subgraph using natural language queries.  Tweet . 🔎  RAG Evaluation Enhancements: RetrieverEvaluator : We introduced “RetrieverEvaluator” for enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.  Docs ,  Tweet . SemanticSimilarityEvaluator : We introduced a new semantic similarity evaluator — SemanticSimilarityEvaluator for LLM/RAG outputs, comparing embedding similarity between reference and generated answers.  Docs ,  Tweet . 📚  Tutorials: Guide  on building RAG from scratch with open-source modules. Dstack   tutorial  on implementing RAG with OSS LLMs using LlamaIndex and Weaviate. Wenqi Glantz   turorial  on Exploring ReAct Agent for Better Prompting in RAG Pipeline. Javier Torres   tutorial  on building a multi-document chatbot. Erika Cardenas   tutorial  on RAG techniques in LlamaIndex covering SQL Router Query Engine, Sub Question Query Engine, Recursive Retriever Query Engine, Self-Correcting Query Engine. Wenqi Glantz   tutorial  on 7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex. Ravi Theja   tutorial  on Evaluating the Ideal Chunk Size for RAG using LlamaIndex. ⚙️  Integrations & Collaborations: Arize AI Phoenix : We launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local.  Notebook ,  Tweet . Neo4j : We introduced an API spec for LLM-agent interaction with Neo4j, offering beyond just “text-to-cypher” with full agent reasoning.  Docs ,  Tweet . TimescaleDB : We integrated with TimescaleDB for enhanced time-based retrieval in RAG systems, offering time filters and cost-effective storage solutions.  Blogpost ,  Tweet . BraintrustData : We integrated with BraintrustData, enabling seamless RAG pipeline construction, evaluations, and easy public URL sharing for results.  Notebook ,  Tweet . LocalAI : We integrated LocalAI_API LLM support for on-prem runs or as an alternative to OpenAI LLM.  Tweet . HoneyHiveAI : We integrated with HoneyHiveAI for enhanced multi-step RAG/agent pipeline monitoring. Log traces, gather user feedback, and utilize it for precise fine-tuning and evaluations.  Docs ,  Tweet . UnstructuredIO : We integrated with UnstructuredIO to tackle the RAG challenge of querying embedded tables in 10-K filings. Now, seamlessly query any tabular data or text within a 10-K document.  Notebook ,  Tweet . Clarifai : We integrated with Clarifai, offering access to 40+ LLMs and various embedding models.  Tweet . 🎥  Webinars: Webinar  by SingleStoreDB on How to Build a GenAI App with LlamaIndex. Webinar  on projects built during the SuperAGI Autonomous Agents Hackathon featuring evo.ninja, RicAI, Atlas and MunichAI. 🎈  Events: Jerry Liu and Simon  conducted  a workshop on RAG + Evaluation at RaySummit. Yi Ding  spoke on ‘LLM Quirks Mode’ at MLOps community event. Jerry Liu spoke on Evals/ Benchmarking  and Advanced RAG techniques  at AIConf 2023. Ravi Theja  conducted a workshop on Mastering RAG with LlamaIndex at PyCon India, 2023. Ravi Theja  presented a  poster  on Automatic Knowledge Transfer(KT) Video generation on code bases using LlamaIndex at PyCon India, 2023.", "author": "LlamaIndex", "date": "Oct 10, 2023", "tags": ["Newsletter", "LLM", "Llamaindex", "AI"]},
{"title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex", "content": "Introduction Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the  chunk_size . How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex  Response Evaluation  comes in handy. In this blog post, we'll guide you through the steps to determine the best  chunk size  using LlamaIndex’s  Response Evaluation  module. If you're unfamiliar with the  Response  Evaluation module, we recommend reviewing its  documentation  before proceeding. Why Chunk Size Matters Choosing the right  chunk_size  is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways: Relevance and Granularity : A small  chunk_size , like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the  similarity_top_k  setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively. Response Generation Time : As the  chunk_size  increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial. In essence, determining the optimal  chunk_size  is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset. For a practical evaluation in choosing the right  chunk_size , you can access and run the following setup on this  Google Colab Notebook . Setup Before embarking on the experiment, we need to ensure all requisite modules are imported: import  nest_asyncio\n\nnest_asyncio.apply()\n\n from  llama_index  import  (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\n from  llama_index.evaluation  import  (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\n from  llama_index.llms  import  OpenAI\n\n import  openai\n import  time\nopenai.api_key =  'OPENAI-API-KEY' Download Data We’ll be using the Uber 10K SEC Filings for 2021 for this experiment. !mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf' Load Data Let’s load our document. documents = SimpleDirectoryReader(\"./data/10k/\").load_data() Question Generation To select the right  chunk_size , we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various  chunk_sizes . The  DatasetGenerator  will help us generate questions from the documents. data_generator = DatasetGenerator.from_documents(documents)\neval_questions = data_generator.generate_questions_from_nodes() Setting Up Evaluators We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators,  FaithfulnessEvaluator  and  RelevancyEvaluator , are initialised with the  service_context  . Faithfulness Evaluator  — It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. Relevancy Evaluator  — It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. # We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4) Response Evaluation For A Chunk Size We evaluate each chunk_size based on 3 metrics. Average Response Time. Average Faithfulness. Average Relevancy. Here’s a function,  evaluate_response_time_and_accuracy , that does just that which has: VectorIndex Creation. Building the Query Engine. Metrics Calculation. # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size \n # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it. \n def   evaluate_response_time_and_accuracy ( chunk_size, eval_questions ):\n     \"\"\"\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n    \n    Parameters:\n    chunk_size (int): The size of data chunks being processed.\n    \n    Returns:\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n    \"\"\" \n\n    total_response_time =  0 \n    total_faithfulness =  0 \n    total_relevancy =  0 \n\n     # create vector index \n    llm = OpenAI(model= \"gpt-3.5-turbo\" )\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n     # build query engine \n    query_engine = vector_index.as_query_engine()\n    num_questions =  len (eval_questions)\n\n     # Iterate over each question in eval_questions to compute metrics. \n     # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html), \n     # we're using a loop here to specifically measure response time for different chunk sizes. \n     for  question  in  eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n     return  average_response_time, average_faithfulness, average_relevancy Testing Across Different Chunk Sizes We’ll evaluate a range of chunk sizes to identify which offers the most promising metrics. chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\n\n for  chunk_size  in  chunk_sizes:\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\n   print ( f\"Chunk size  {chunk_size}  - Average Response time:  {avg_response_time: .2 f} s, Average Faithfulness:  {avg_faithfulness: .2 f} , Average Relevancy:  {avg_relevancy: .2 f} \" ) Bringing It All Together Let’s compile the processes: import  nest_asyncio\n\nnest_asyncio.apply()\n\n from  llama_index  import  (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\n from  llama_index.evaluation  import  (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\n from  llama_index.llms  import  OpenAI\n\n import  openai\n import  time\n\nopenai.api_key =  'OPENAI-API-KEY' \n\n # Download Data \n!mkdir -p  'data/10k/' \n!wget  'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'  -O  'data/10k/uber_2021.pdf' \n\n # Load Data \nreader = SimpleDirectoryReader( \"./data/10k/\" )\ndocuments = reader.load_data()\n\n # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages. \neval_documents = documents[: 20 ]\ndata_generator = DatasetGenerator.from_documents()\neval_questions = data_generator.generate_questions_from_nodes(num =  20 )\n\n # We will use GPT-4 for evaluating the responses \ngpt4 = OpenAI(temperature= 0 , model= \"gpt-4\" )\n\n # Define service context for GPT-4 for evaluation \nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n # Define Faithfulness and Relevancy Evaluators which are based on GPT-4 \nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n\n # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size \n def   evaluate_response_time_and_accuracy ( chunk_size ):\n    total_response_time =  0 \n    total_faithfulness =  0 \n    total_relevancy =  0 \n\n     # create vector index \n    llm = OpenAI(model= \"gpt-3.5-turbo\" )\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n\n    query_engine = vector_index.as_query_engine()\n    num_questions =  len (eval_questions)\n\n     for  question  in  eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n     return  average_response_time, average_faithfulness, average_relevancy\n\n # Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size. \n for  chunk_size  in  [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\n  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n   print ( f\"Chunk size  {chunk_size}  - Average Response time:  {avg_time: .2 f} s, Average Faithfulness:  {avg_faithfulness: .2 f} , Average Relevancy:  {avg_relevancy: .2 f} \" ) Result The above table illustrates that as the chunk size increases, there is a minor uptick in the Average Response Time. Interestingly, the Average Faithfulness seems to reach its zenith at  chunk_size of 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy. Conclusion Identifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. With LlamaIndex’s  Response Evaluation  module, you can experiment with various sizes and base your decisions on concrete data. When building a RAG system, always remember that  chunk_size  is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.", "author": "Ravi Theja", "date": "Oct 5, 2023", "tags": ["Llamaindex", "AI", "LLM", "OpenAI", "Retrieval"]},
{"title": "Fine-Tuning a Linear Adapter for Any Embedding Model", "content": "We’ve added capabilities in LlamaIndex allowing you to fine-tune a linear adapter on top of embeddings produced from  any  model ( sentence_transformers , OpenAI, and more). This allows you to transform your embedding representations into a new latent space that’s optimized for retrieval over your specific data and queries. This can lead to small increases in retrieval performance that in turn translate to better performing RAG systems. A nice bonus: you do  not  need to re-embed your documents by using this adapter! Simply transform the query instead. We have a  full end-to-end guide  showing how you can generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance. Context The concept of fine-tuning your embedding model is powerful. In fact, we were inspired to both add a  full example repository  /  blog post  as well as  native abstractions in LlamaIndex  showing how you can fine-tune a sentence_transformers model over any unstructured text corpus (with our  SentenceTransformersFinetuneEngine ). However, this approach has some limitations: The  SentenceTransformersFinetuneEngine  is limited to fine-tuning  sentence_transformers  models. After finetuning the embedding model, you will need to re-embed your document corpus. During our  Finetuning + RAG webinar  last Friday, Jo (Vespa) mentioned the exact same problem: fine-tuning the embeddings model requires you to reindex your documents. However, his work with Vespa  explored the concept of “freezing” document embeddings using a foundation model , and instead training a transformation on the query embedding. This inspired us to explore a similar embedding fine-tuning approach that was simultaneously more general but also allowed us to freeze existing document embeddings. Approach Our brand-new  EmbeddingAdapterFinetuneEngine  fine-tunes a  linear adapter  on top of query embeddings produced by any model. The  linear adapter  is simply a linear transformation that specifically transforms the query embedding  while keeping document embeddings fixed . The linear adapter can be used on top of any existing embeddings model: SBERT embeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you can just plug this in on top of any embedding model that you’re already using! Since document embeddings are unchanged, this means that you can always fine-tune this linear adapter  after  you’ve generated embeddings for your documents. You can choose to arbitrarily re-train this adapter on top of changing data distributions, without needing to re-embed all your documents. Technical Details As mentioned above, the linear adapter simply performs a linear transformation on top of the query embedding while keeping the Document embeddings fixed (with a weight matrix W + bias term b): And that’s it! If document embeddings can be represented as a (n x d) matrix D, where n is number of documents and d is the embedding dimension, then embedding similarity is just measured by The linear adapter is trained using a similar loss term as the  MultipleNegativesRankingLoss  function in  sentence_transformers  — given a batch of positive (question, context) examples, the function uses cross-entropy loss under the hood to penalize the ground-truth (question, context) pairs for being far apart and swapped pairs for being too close. Additional Notes:  We ended up writing the bulk of this fine-tuning logic in plain PyTorch, but taking heavy inspiration from the  sentence_transformers   source code . We couldn’t use sentence_transformers directly since we take in embeddings as inputs rather than raw text. You can take a look at some of our training code here. Notebook Walkthrough In this notebook walkthrough, we follow a similar set of steps as our  previous blog post on embedding fine-tuning : Generate a synthetic question-context dataset for both training and evaluation. Fine-tuning our linear adapter on top of an existing model (e.g. SBERT) Getting the embedding model, and evaluating it. As with the previous post, we use the UBER and LYFT 10K as example data. We use Lyft to generate our training dataset and Uber to generate our evaluation dataset. The full guide is here:  https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html Generate a Synthetic Dataset for Trraining and Evaluation We use our helper abstractions,  generate_qa_embedding_pairs  , to generate our training and evaluation dataset. This function takes in any set of text nodes (chunks) and generates a structured dataset containing (question, context) pairs. from llama_index.finetuning import (\n    generate_qa_embedding_pairs,\n    EmbeddingQAFinetuneDataset,\n)\n\n# generate\ntrain_dataset = generate_qa_embedding_pairs(train_nodes)\nval_dataset = generate_qa_embedding_pairs(val_nodes)\n\n# save\ntrain_dataset.save_json(\"train_dataset.json\")\nval_dataset.save_json(\"val_dataset.json\")\n\n# load \ntrain_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\nval_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\") Fine-tuning our Linear Adapter We then fine-tune our linear adapter on top of an existing embedding model. We import our new EmbeddingAdapterFinetuneEngine  abstraction, which takes in an existing embedding model and a set of training parameters. In this example we use the  bge-small-en  sentence-transformers model, but we can also use any embedding model in LlamaIndex/LangChain. from  llama_index.finetuning  import  EmbeddingAdapterFinetuneEngine\n from  llama_index.embeddings  import  resolve_embed_model\n import  torch\n\nbase_embed_model = resolve_embed_model( \"local:BAAI/bge-small-en\" )\n # alternative: use OpenAI \n # from llama_index.embeddings import OpenAIEmbedding \n # openai = OpenAIEmbedding() \n\nfinetune_engine = EmbeddingAdapterFinetuneEngine(\n    train_dataset,\n    base_embed_model,\n    model_output_path= \"&lt;model_output_path&gt;\" ,\n    epochs= 4 ,\n    verbose= True ,\n     # can optionally pass along any parameters that go into `train_model` \n     # optimizer_class=torch.optim.SGD, \n     # optimizer_params={\"lr\": 0.01} \n) We can then call fine-tune to kick off the fine-tuning job. Training a linear model is quite straightforward and doesn’t require heavy machinery — this can easily run on a Macbook. finetune_engine.finetune() Getting the Embedding Model, and Evaluating it Once the fine-tuning job is then, we can then fetch our embedding model. We can either directly fetch it from our  finetune_engine  , or import our new  LinearAdapterEmbeddingModel  and construct it in a more manual fashion. Option 1: embed_model = finetune_engine.get_finetuned_model() Option 2: from  llama_index.embeddings  import  LinearAdapterEmbeddingModel\n\nembed_model = LinearAdapterEmbeddingModel(base_embed_model,  \"&lt;model_output_path&gt;\" ) The next step is to evaluate it. We compare the fine-tuned model against the base model, as well as against  text-embedding-ada-002 . We evaluate with two ranking metrics: Hit-rate metric:  For each (query, context) pair, we retrieve the top-k documents with the query. It’s a  hit  if the results contain the ground-truth context. Mean Reciprocal Rank : A slightly more granular ranking metric that looks at the “reciprocal rank” of the ground-truth context in the top-k retrieved set. The reciprocal rank is defined as 1/rank. Of course, if the results don’t contain the context, then the reciprocal rank is 0. Some additional comments: We ran with 4 epochs over the Lyft documents We used Adam as an optimizer with the default learning rate (we tried SGD and it didn’t work as well) Results Quantiative metrics (hit-rate and MRR) for ada, bge, and our fine-tuned model In terms of hit-rate, the base model gets 78.7% hit-rate on the validation dataset, and the fine-tuned model gets 79.8%. In the meantime  text-embedding-ada-002  gets 87.0%. In terms of MRR, the base model gets 64.3%, and the fine-tuned model gets 66%.  text-embedding-ada-002  gets 68.4%. There is some performance bump from the fine-tuned model, though admittedly it is small — it is smaller than the performance bump gained through fine-tuning sentence_transformers directly on the latest dataset. That said, a performance bump is still a performance bump, and it’s very cheap for you to spin up and try yourself! So you can decide whether or not this would make sense for you. Conclusion We created a brand-new module in LlamaIndex that allows you fine-tune a linear adapter on top of any embedding model. It can help you eke out some marginal improvement in retrieval metrics; importantly, it allows you to keep document embeddings fixed and only transform the query. Resources Guide:  https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html Training code (if you want to take a look for yourself):  https://github.com/jerryjliu/llama_index/blob/main/llama_index/finetuning/embeddings/adapter_utils.py", "author": "Jerry Liu", "date": "Sep 6, 2023", "tags": ["Fine Tuning", "Embedding", "Llamaindex", "NLP", "AI"]},
{"title": "LlamaIndex Newsletter 2023–10–17", "content": "Hello Llama Enthusiasts 🦙! Another week has flown by, and we’re back with a jam-packed newsletter filled with updates on hackathons, guides, integrations, features, webinars, tutorials, blogs, and demos. If you have a project, blog post, or video that deserves a spotlight, we’d love to feature it! Just reach out to us at  news@llamaindex.ai . Bonus: You can now get all these updates straight to your inbox! Simply visit our  homepage  and sign up for our email updates. 🤩  First, the highlights: AI.Engineer Summit : At the AI.Engineer Summit, Jerry Liu discussed RAG applications, while Simon led a workshop on RAG app optimization (Jerry’s  slides , Simon’s  slides ) Text to pgVector : we launched PGVectorSQLQueryEngine for combined SQL and vector queries on PostgreSQL. ( Docs ,  Tweet ) Hugging Face Integration : Integrated with HuggingFace’s text-embeddings-inference server for high-speed, large-scale BERT model serving. ( Docs ,  Tweet ) Multi-Document Agents : New V1 agents support advanced multi-document retrieval and async query planning. ( Docs ,  Tweet ) Unstructured Parsing : Unveiled UnstructuredElementNodeParser, a hierarchical parser for embedded tables/text using UnstructuredIO. ( Docs ,  Tweet ) LLM Compatibility : We have charted LLM performances on various tasks and found that the Zephyr-7b-alpha model stands out as the top-performing 7B model in advanced RAG tasks. ( Docs ) 🏆 Congratulations to our AGI House Hackathon Winners! We love seeing people build amazing things with LlamaIndex! Build: Demostify Stick with Fit ,  SafeQuery , Cherry Break: Fuzzy Access Test: X-Ray Insight Honorable Mentions: KindleGPT PenTest 🎤 LlamaIndex at  AI.Engineer Summit : Jerry Liu  gave a talk on Building production-ready RAG applications.  Slides . Simon  conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Production with LlamaIndex.  Slides ,  Code . 🗺️ Guides: LLM Compatibility Tracking:  We’ve charted LLM performances on various tasks, revealing zephyr-7b-alpha as the only current 7B model excelling in advanced RAG/ Agentic tasks.  Docs . Evaluations:  Adjusting chunk size is essential for RAG apps. Having more chunks isn’t necessarily better, and re-ranking might be counterproductive. To fine-tune, experiment with different chunk sizes and top-k values. The Arize AI team has provided a guide to help you evaluate using Arize AI Phoenix and Llama Index.  Slides ,  Notebook . ✍️ Tutorials: Shahul’s   tutorial  demonstrates how to choose the best embeddings for your data, emphasizing that retriever performance and embedding quality are crucial for a RAG system’s efficacy using the LlamaIndex and RAGAS libraries. Wenqi Glantz ’s  tutorial  on Evaluation Driven Development for RAG Pipelines. Wenqi Glantz ’s  tutorial  on Masking PII Data in the RAG Pipeline. Ofer Mendelevitch’s from  Vectara  has a  tutorial  on Retrieval Augmented Generation with LlamaIndex on comparing Vectara’s new Boomerang model to OpenAI and Cohere. Patrick Loeber  from AssemblyAI has a  tutorial  on Build LlamaIndex Audio Apps. Pradip Nichite  made a  tutorial  on NL2SQL with LlamaIndex: Querying Databases Using Natural Language. Mayo Oshin  has a  tutorial  on How to Compare Multiple Large PDF Files. Sudarshan Koirala  made a  tutorial  on Chat With Documents with LlamaIndex and Pinecone. 💡 Demos: Siva Surendira  built  YC Bot  to get instant startup advice from your favorite YC mentors. ✨ Feature Releases and Enhancements: Text to pgVector:  We introduced the PGVectorSQLQueryEngine, which allows you to query a PostgreSQL database using both full SQL and vector search simultaneously.  Docs ,  Tweet . Multi-Document Agents:  We introduce Multi-Document Agents (V1) that can now retrieve across multiple docs and plan queries asynchronously, offering a superior analysis compared to standard RAG.  Docs ,  Tweet . UnstructuredIO:  We’ve partnered with UnstructuredIO to enhance LLM/RAG applications. By extracting tables from PDFs, we’ve improved query methods beyond basic vector indexing, enabling hybrid queries and cross-document comparisons, especially for tabular questions.  Docs ,  Tweet . UnstructuredElementNodeParser:  Going beyond basic text-splitting, we introduce the UnstructuredElementNodeParser. It models embedded tables/text hierarchically in a data graph using UnstructuredIO.  Docs ,  Tweet . Cross-Encoder Fine-Tuning:  Cross-encoders enhance RAG by refining post-embedding search results. With LlamaIndex, you can now fine-tune cross-encoders on any document, boosting performance.  Docs ,  Tweet . ⚙️ Integrations & Collaborations: Assembly AI:  We introduced a new data reader for audio data integration with AssemblyAI. This integration allows effortless audio loading and facilitates building vector store indices and query engines for inquiries.  Docs ,  Tweet . Nougat — MetaAI:  We integrated Nougat, an exceptional OCR tool from Meta, that excels in interpreting scientific papers, notably mathematical notations, and LaTeX as a loader in LlamaHub, allowing streamlined processing of ArXiv papers within the RAG pipeline.  Docs ,  Tweet . Hugging Face-Text Embeddings Inference:  We integrated with the new text-embeddings-inference server from HuggingFace offering production-scale serving with distributed tracing for all BERT models at impressive speeds.  Docs ,  Tweet . 🎥 Webinars And Podcast: Webinar  with Timescale on Time-based retrieval for RAG. Webinar  with Omar Khattab and Thomas Joshi on DSPy — a framework for LLMs that emphasizes programming over prompting. Jerry Liu’s  podcast  with Latent Space on LlamaIndex’s origin story, fine-tuning, and more.", "author": "LlamaIndex", "date": "Oct 17, 2023", "tags": ["LLM", "Programming", "AI", "Llamaindex", "Newsletter"]},
{"title": "LlamaIndex + Vectara", "content": "(co-authored by Ofer Mendelevitch, head of Developer Relations at Vectara, and Logan Markewich, founding engineer at LlamaIndex) Introduction Vectara  is a trusted GenAI platform. Exposing a set of easy to use  APIs , Vectara’s platform reduces the complexity involved in developing  Grounded Generation  (aka retrieval-augmented-generation) applications, and managing the LLM infrastructure that’s required to deploy them at scale in production. Today we’re happy to announce Vectara’s integration with  LlamaIndex  via a new type of Index: the  Managed Index . In this blog post, we’ll dig deeper into how a  ManagedIndex  works, and show examples of using Vectara as a Managed Index. What is Vectara? Vectara is an end-to-end platform that offers powerful generative AI capabilities for developers, including: Data processing.  Vectara supports various file types for ingestion including markdown, PDF, PPT, DOC, HTML and many others. At ingestion time, the text is automatically extracted from the files, and chunked into sentences. Then a vector embedding is computed for each chunk, so you don’t need to call any additional service for that. Vector and text storage.  Vectara hosts and manages the vector store (where the document embeddings are stored) as well as the associated text. Developers don’t need to go through a long and expensive process of evaluation and choice of vector databases. Nor do they have to worry about setting up that Vector database, managing it in their production environment, re-indexing, and many other DevOps considerations that become important when you scale your application beyond a simple prototype. Query flow.  When issuing a query, calculating the embedding vector for that query and retrieving the resulting text segments (based on similarity match) is fully managed by Vectara. Vectara also provides a robust implementation of hybrid search and re-ranking out of the box, which together with a state of the art embedding model ensures the most relevant text segments are returned in the retrieval step. Security and Privacy.  Vectara’s API is fully encrypted in transit and at rest, and supports customer-managed-keys (CMK). We never train on your data, so you can be sure your data is safe from privacy leaks. Figure 1:  Vectara’s API platform for “Grounded Generation” The nice thing is that all this complexity is fully managed by Vectara, taking a lot of the heavy lifting off of the developer’s shoulders, so that they don’t have to specialize in the constantly evolving skills of large language models, embedding models, vector stores and MLOps. From VectorStoreIndex to ManagedIndex LlamaIndex is a data framework for building LLM applications. It provides a set of composable modules for users to define a data pipeline for their application. This consists of data loaders, text splitters, metadata extractors, and vector store integrations. A popular abstraction that users use is the VectorStoreIndex, providing integrations with different vector databases. However, a challenge here is that users still need to carefully define how to load data, parse it, as well as choose an embedding model and a vector DB to use. Since Vectara abstracts away this complexity, the Vectara and LlamaIndex teams jointly came up with a new abstraction: The ManagedIndex. As shown in figure 2, when ingesting data into a VectorStoreIndex, data is processed locally taking advantage of multiple components like Data Connectors and Node parsers. Figure 2:  typical flow of document processing in LlamaIndex for a VectorStoreIndex With Vectara (figure 3), this whole flow is replaced by a single “indexing” API call , and all this processing is instead performed in the backend by the Vectara platform. Figure 3:  pre-processing with the VectaraIndex simplifies the complex ingest flow to a single step. How does the VectaraIndex work? Let’s take a look at a simple question-answering example using VectaraIndex, in this case asking questions from one of Paul Graham’s Essays. Step 1: Setup your Vectara account and Index To get started, follow our  quickstart  guide:  signup  for a free Vectara account, create a corpus (index), and generate your API key. Then setup your Vectara customer_id, corpus_id and api_key as environment variables, so that the VectaraIndex can access those easily, for example: VECTARA_CUSTOMER_ID=<YOUR_CUSTOMER_ID>\nVECTARA_CORPUS_ID=<YOUR_CORPUS_ID>\nVECTARA_API_KEY=\"zwt_RbZfGT…\" Step 2: Create a VectaraIndex instance with LlamaIndex Building the Vectara Index is extremely simple: from  llama_index  import  SimpleDirectoryReader\n from  llama_index.indices  import  VectaraIndex\nFrom pprint Import pprint\n\ndocuments = SimpleDirectoryReader( \"paul_graham\" ).load_data()\nindex = VectaraIndex.from_documents(documents) Here we load Paul Graham’s Essay using LlamaIndex’s SimpleDirectoryReader into a single document. The from_documents() constructor is then used to generate the VectaraIndex instance. Unlike the common flow that uses LlamaIndex tools like data connectors, parsers and embedding models to process the input data, with VectaraIndex the documents are sent directly to Vectara via the  Indexing API . Vectara’s platform then processes, chunks, encodes and stores the text and embeddings into a Vectara corpus, making it available instantly for querying. Step 3: Query After the data is fully ingested, you can take advantage of the rich set of query constructs built into LlamaIndex. For example let’s use the index to retrieve the top-k most relevant nodes: retriever = index.as_retriever(similarity_top_k= 7 )\n # docs should contain the 7 most relevant documents for the query \ndocs = retriever.retrieve(“What  is  the IBM  1401 ?”)\npprint(docs[ 0 ].node.text) (‘My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called “data processing.” This was in 9th grade, so I was 13 or 14. The school district’s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it.’) Here we printed out the top matching Node given the query “what is the IBM 1401?” This in turn results in a call to Vectara’s  Search API  that returns the top-k matching document segments. Those are transformed into NodeWithScore objects and thus can be used as usual with the rest of the LlamaIndex querying tools. For example we can use LlamaIndex’s query_engine() to convert the retrieved matching document segments (nodes) into a comprehensive response to our question: # Get an answer to the query based on the content of the essay \nresponse = index.as_query_engine().query( \"What can the 1401 do?\" )\n print (response) The 1401 was used for “data processing” and could load programs into memory and run them. It had a card reader, printer, CPU, disk drives, and used an early version of Fortran as the programming language. The only form of input to programs was data stored on punched cards. Why Use VectaraIndex with LlamaIndex? By adding the concept of a “Managed Index” and the VectaraIndex to LlamaIndex, users can continue to take advantage of the tools and capabilities offered by the LlamaIndex library while integrating with a generative AI platform like Vectara. Retrievers and Query Engines are just the tip of the iceberg. Using a managed index with Vectara, developers have full access to advanced utilities like routers, advanced query engines, data agents, chat engines, and more! Being able to retrieve context using Vectara empowers developers to build these complex applications using LlamaIndex components. For example, in the following code we use the chat engine in LlamaIndex to quickly create a chat interaction using our VectaraIndex: chat = index.as_chat_engine(chat_mode='context')\nres = chat.chat(\"When did the author learn Lisp?\")\nprint(res.response) “The author learned Lisp in college.” A follow up question retains the chat history for context, as you might expect: chat.chat(\"and was it helpful for projects?\").response “Yes, learning Lisp was helpful for the author’s projects. They used Lisp in both Viaweb and Y Combinator, indicating its usefulness in their work.” chat.chat(\"what was a distinctive characteristic of that programming language?\").response “A distinctive characteristic of Lisp is that its core is a language defined by writing an interpreter in itself. It was originally designed as a formal model of computation and an alternative to the Turing machine. This self-referential nature of Lisp sets it apart from other programming languages.” For more information on how to use chat-engines, check out the  documentation , and for more information on other query capabilities with LlamaIndex, check out the full documentation  here . Summary LlamaIndex makes it super easy to populate VectaraIndex with content from any document or data source, while utilizing the Vectara service for managing the document processing, chunking, embedding and making all of this data available for advanced retrieval in query time using the LlamaIndex library. VectaraIndex is based on the new LlamaIndex Managed Index abstraction, which better supports GenAI platforms like Vectara, and enables additional vendors who also provide end-to-end platforms to join in. To get started with Vectara and LlamaIndex you can follow the Vectara quickstart  guide  to setup your account, and the examples above with your own data.", "author": "Jerry Liu", "date": "Sep 12, 2023", "tags": ["Llamaindex", "NLP", "AI", "LLM", "Search"]},
{"title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)", "content": "Introduction Large Language Models (LLMs) improve performance by accessing external data for background knowledge tasks related. However, existing approaches require costly modifications during LM’s pre-training or integrating the data store after the model has been trained. On the downside, both strategies lead to suboptimal performance. To address this problem an  AI Research team at Meta  has proposed a method called  RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING  that allows any LLM to be upgraded to include retrieval features. In this blog post, we will explore RA-DIT capabilities to have better performance on Retrieval Augmentation Generation (RAG) through building the dataset and fine-tuning the models. The RA-DIT approach involves two distinct fine-tuning steps: Update a pre-trained LM to better use retrieved information. Update the retriever to return more relevant results How it works The RA-DIT approach separately fine-tunes the LLM and the retriever. The LLM is updated to maximize the probability of the correct answer given the retrieval-augmented instructions, while the retriever is updated to minimize how much the document is semantically similar (relevant) to the query. Below we are going through each step from generating the fine-tuning dataset, fine-tuning the language model for better predictions, and refining the retrieval search process. Fine-tuning Dataset The fine-tuning dataset is tailored to enhance the language model’s ability to leverage knowledge and boost its contextual awareness during prediction generation. Generating Q/A pairs, summarizing data, and incorporating chain-of-thought reasoning can lead to improved results when integrated with the models. Following our  LamaIndex implementation , we retrieve the top_k nodes, generate Q/A pairs from the documents, and then augment the data. We use the Q/A pairs through the QueryResponseDataset module, which returns a (query, response) pair for the fine-tuning dataset. While the retrieval fine-tuning data set is created on Q/A pairs data. Language Model Fine-tuning With our fine-tuning dataset in hand, we can refine our LLM to achieve two main benefits: Adapt the LLM to better utilization of relevant background knowledge and train the LLM to produce accurate predictions even with incorrectly retrieved chunks, empowering the model to rely on its own knowledge. Retriever Fine-tuning The retriever is fine-tuned using the LM-Supervised Retrieval (LSR) method. In this approach, the LLM assesses the information fetched by the retriever. If the LLM finds the information misaligned with the given query, it sends feedback to the retriever. Using this feedback, the retriever refines its search process, ensuring it fetches data that the LLM can effectively use. This collaboration enhances the overall quality of the answers provided. Evaluation To assess the suggested method, the authors employed specific datasets and metrics. Let’s delve into each of these to grasp the experimental results better. Metrics An “exact match” (EM) metric was used to measure how closely the model’s prediction matches the ground truth answer. Dataset The methodology was tested on two distinct tasks: Knowledge-intensive tasks. Commonsense reasoning. Let’s explore the datasets utilized for both of these tasks. Knowledge-intensive dataset For knowledge-intensive tasks the selected datasets predominantly focus on the model’s capacity to access, understand, and relay deep and specific knowledge. They encompass questions rooted in facts, general trivia, and complex domain-specific queries; The datasets used are MMLU, Natural Questions (NQ), TriviaQA, and a subset of tasks from the KILT benchmark. Commonsense reasoning dataset Commonsense reasoning datasets challenge the model’s ability to reason and infer based on general knowledge and everyday scenarios. They contain questions and scenarios that typically don’t rely on deep domain knowledge but rather on intuitive and general world understanding. The datasets used are BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-E, ARC-C, OBQA. For a better understanding of how these datasets were utilized you can check the  paper  for better understanding. Results In a comparative analysis of model performance on knowledge-intensive (Table 2 below) and commonsense reasoning tasks (Table 3 below), three models were considered: LLAMA 65B LLAMA 65B REPLUG (only retrieval augmentation) RA-DIT 65B Knowledge Intensive Tasks Evaluations are conducted in 0-shot, 5-shot, and 64-shot fine-tuning settings. 0-shot Analysis: RA-DIT 65B demonstrated superior performance with an average EM score of 50.5 across all tasks. It outperformed LLAMA 65B REPlug (43.1 average) and significantly surpassed LLAMA 65B (32.9 average). 5-shot Analysis: RA-DIT 65B maintained its lead with an average EM score of 55.2. LLAMA 65B REPlug followed closely with 52.7, while LLAMA 65B achieved an average of 45.0. In a separate evaluation for 64-shot fine-tuning, two models were analyzed: ATLAS and RA-DIT 65B. 64-shot Fine-tuning: RA-DIT 65B achieved an average performance of 60.9 across all tasks, slightly surpassing ATLAS, which obtained an average score of 56.8 Commonsense reasoning RA-DIT 65B  was benchmarked in order to evaluate the impact of retrieval-augmented instruction tuning on the LLMs parametric knowledge and reasoning capabilities. In this experiment without retrieval augmentation,  RA-DIT  showed improvements over base  LLAMA  65B models on 7 of 8 evaluation datasets, indicating that the parametric knowledge and reasoning capabilities of the LLM component are in general preserved. In summary, RA-DIT 65B consistently delivered great results, surpassing its competitors in multiple scenarios, underscoring its proficiency and aptitude in knowledge-intensive tasks while showing that the parametric knowledge and reasoning capabilities of the LLM are still preserved. Conclusion The RA-DIT approach provides a structured method to enhance how Large Language Models utilize external data. Through the dual fine-tuning of both the model and the retriever, we target better accuracy and context-awareness in responses. The incorporation of the LSR technique fosters a more efficient data retrieval process, ensuring that the generated answers are both relevant and informed, the final results show that RA-DIT surpasses un-tuned RALM approaches like REPLUG showing competitive results. You can explore more about the LLamaIndex implementation at:  https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-tuning-with-retrieval-augmentation References RA-DIT paper:  https://arxiv.org/abs/2310.01352 Connect with me on  Twitter  and  Linkedin", "author": "Emanuel Ferreira", "date": "Oct 18, 2023", "tags": ["AI", "Large Language Models", "Machine Learning", "Fine Tuning", "Llamaindex"]},
{"title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser", "content": "Despite recent motivation to utilize NLP for wider range of real world\n  applications, most NLP papers, tasks and pipelines assume raw, clean texts.\n  However, many texts we encounter in the wild, including a vast majority of\n  legal documents (e.g., contracts and legal codes), are not so clean, with many\n  of them being visually structured documents (VSDs) such as PDFs. PDFs are\n  versatile, preserving the visual integrity of documents, but they often pose a\n  significant challenge when it comes to extracting and manipulating their\n  contents.\n \n \n  In this discussion, our focus will primarily be on text-only layered PDFs, a\n  category often regarded by many as a resolved issue.\n \n\n Complexity of Parsing PDFs \n \n   \n     Layout Complexity : PDFs can contain complex layouts, such\n    as multi-column text, tables, images, and intricate formatting. This layout\n    diversity complicates the extraction of structured data.\n   \n   \n     Font encoding issue s: PDFs use a variety of font encoding\n    systems, and some of these systems do not map directly to Unicode. This can\n    make it difficult to extract the text accurately.\n   \n   \n     Non-linear text storage:  PDFs do not store text in the\n    order it appears on the page. Instead, they store text in objects that can\n    be placed anywhere on the page. This means that the order of the text in the\n    underlying code may not match the order of the text as it appears visually.\n   \n   \n     Inconsistent use of spaces : In some PDFs, spaces are not\n    used consistently or are not used at all between words. This can make it\n    difficult to even identify word boundaries.\n   \n \n\n Do we need an efficient parser? \n \n   \n    In the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n    Process Entire PDFs?\n   \n \n \n  This question gains relevance if the answer to this next question is “Yes”.\n \n \n   Do we need Retrieval-Augmented Generation (RAG)? \n \n \n  While LLMs are powerful, they have certain limitations in terms of the amount\n  of text they can process at once and the scope of information they can\n  reference. Further recent research have suggested LLM performance is often\n  highest when relevant information occurs at the beginning or end of the input\n  context, and significantly degrades when models must access relevant\n  information in the middle of long contexts. Techniques like RAG help overcome\n  these limitations, enabling more effective and efficient processing of large\n  documents and broader information retrieval.\n \n \n   Still Skeptical? Let’s ask an LLM for confirmation. \n \n \n   \n \n \n  Now that we’ve established the importance of an efficient parser, it becomes\n  instrumental in constructing an effective Retrieval-Augmented Generation (RAG)\n  pipeline to address the limitations of an LLM. Let’s explore how we are\n  achieving this today. It’s crucial to remember that the quality of the context\n  fed to an LLM is the cornerstone of an effective RAG, as the saying goes,\n  ‘ Garbage In — Garbage Out .’\n \n \n  In the context of building LLM-related applications,\n   chunking  is the process of breaking down large pieces of text\n  into smaller segments. It’s an essential technique that helps optimize the\n  relevance of the content we get back from a database once we use the LLM to\n  embed content. Some of the strategies involved are\n \n \n   \n     Fixed-size chunking . This is the most common and\n    straightforward approach to chunking: we simply decide the number of tokens\n    in our chunk and, optionally, whether there should be any overlap between\n    them. Easy to implement & most commonly used, but never makes it to a\n    production setting because the output is satisfactory in a Proof of Concept\n    (POC) setup, but its accuracy degrades as we conduct further testing.\n   \n   \n     “Content-aware” chunking . Set of methods for taking\n    advantage of the nature of the content we’re chunking and applying more\n    sophisticated chunking to it. Challenging to implement due to the reasons\n    mentioned above, but if tackled correctly, it could be the most ideal\n    building block for a production-grade Information Retrieval (IR) engine.\n   \n \n\n Where’s This Article Headed, Anyway? \n \n  Certainly, let’s put an end to the historical and background details, shall\n  we?\n \n \n  Introducing\n   LayoutPDFReader   for “ Context-aware ” chunking.\n   LayoutPDFReader \n  can act as the most important tool in your RAG arsenal by parsing PDFs along\n  with hierarchical layout information such as:\n \n \n   \n    Identifying sections and subsections, along with their respective hierarchy\n    levels.\n   \n   Merging lines into coherent paragraphs. \n   Establishing connections between sections and paragraphs. \n   \n    Recognizing tables and associating them with their corresponding sections.\n   \n   Handling lists and nested list structures with precision. \n \n \n  The first step in using the\n   LayoutPDFReader \n  is to provide a URL or file path to it (assuming it’s already been installed)\n  and get back a document object.\n \n from llmsherpa.readers import LayoutPDFReader llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\" pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf pdf_reader = LayoutPDFReader(llmsherpa_api_url) doc = pdf_reader.read_pdf(pdf_url) \n Vector search and RAG with Smart Chunking \n \n  LayoutPDFReader employs intelligent chunking to maintain the cohesion of\n  related text:\n \n \n   \n    It groups all list items together, along with the preceding paragraph.\n   \n   Items within a table are chunked together. \n   \n    It incorporates contextual information from section headers and nested\n    section headers.\n   \n \n \n  As a quick example, the following code snippet generates a\n   LlamaIndex \n  query engine from the document chunks produced by LayoutPDFReader.\n \n from llama_index.readers.schema.base import Document from llama_index import VectorStoreIndex index = VectorStoreIndex([]) for chunk in doc.chunks():     index.insert(Document(text=chunk.to_context_text(), extra_info={})) query_engine = index.as_query_engine() # Let's run one query response = query_engine.query(\"list all the tasks that work with bart\") print(response) \n We get the following response: \n BART works well for text generation, comprehension tasks, abstractive dialogue, question answering, and summarization tasks. \n Key Considerations: \n \n   \n    LLMSherpa leverages a cost-free and open API server. Your PDFs are not\n    retained beyond temporary storage during the parsing process.\n   \n   \n    LayoutPDFReader has undergone extensive testing with a diverse range of\n    PDFs. However, achieving flawless parsing for every PDF remains a\n    challenging task.\n   \n   \n    Please note that OCR (Optical Character Recognition) functionality is\n    presently unavailable. The tool exclusively supports PDFs equipped with a\n    text layer.\n   \n   \n    For inquiries regarding private hosting options, OCR support, or tailored\n    assistance with particular PDF-related concerns, feel free to reach out to\n     contact@nlmatics.com \n    or to\n     me \n    directly.\n   \n \n \n  If you have any questions, please leave them in the comments section, and I\n  will try to respond ASAP.\n \n \n   Connect? \n \n \n  If you want to get in touch, feel free to shoot me a message on\n   LinkedIn  or via\n   email .\n \n References \n \n   https://github.com/nlmatics/llmsherpa \n \n \n   Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser \n \n \n   Lost in the Middle: How Language Models Use Long Contexts", "author": "Kiran Neelakanda Panicker", "date": "Oct 18, 2023", "tags": ["NLP", "Large Language Models", "Llamaindex", "Pdf", "Rag"]},
{"title": "LlamaIndex newsletter 2023–10–24", "content": "Hello Llama Fans 🦙! Welcome back to our newsletter covering new features, guides, integrations, webinars, tutorials, and more. Got a project, blog, or video you’re proud of? Let’s spotlight it! Contact us at  news@llamaindex.ai . Plus, for direct updates in your email, just head to  our homepage  and subscribe to our newsletter. 🤩  First, the highlights: QueryFusionRetriever  Launch:  Inspired by  Adrian Raudaschl’s  RAG-Fusion, enhancing multiple query generation with LLMs.  Tweet ,  Docs . Router Fine-Tuning:  Our innovative router fine-tuning approach has achieved an outstanding 99% match rate, outpacing both the gpt-3.5’s 65% and the base model’s 12%.  Tweet ,  Docs . Fusion Retriever Guide:  Guide on building an advanced Fusion Retriever from scratch.  Docs Amazon Bedrock LLMs and AI21 Labs LLMs:  We have expanded our LLM compatibility, now seamlessly integrating with both Amazon Bedrock and AI21 Labs models. ✨ Feature Releases and Enhancements: QueryFusionRetriever : We introduced the  QueryFusionRetriever , inspired by  Adrian Raudaschl’s  work on RAG-Fusion. This retriever allows users to generate multiple queries with LLMs, run various retrieval methods, and apply reciprocal rank fusion for improved results.  Tweet ,  Docs . Router Fine-Tuning:  We introduced router fine-tuning (V0) for improved LLM automated decision-making. Our approach achieved a 99% match rate, outperforming gpt-3.5’s 65% and the base model’s 12%.  Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a RAG pipeline setup over SQL databases for structured table node retrieval and response synthesis.  Tweet ,  Docs . 🗺️ Guides: Tutorial  guide on  Building an Advanced Fusion Retriever from Scratch. ✍️ Tutorials: Saurav Joshi ’s  tutorial  on Complex Query Resolution through LlamaIndex Utilizing Recursive Retrieval, Document Agents, and Sub Question Query Decomposition. Greg Loughnane  and  Chris Alexiuk   tutorial  on tackling domain-specific fine tuning using LlamaIndex. Vishwas Gowda ’s  blog post  on Streamlit LLM Hackathon winning app — FinSight using LlamaIndex. Emanuel Ferreira ’s blog post on the RA-DIT paper and its implementation in LlamaIndex. Yujian Tang’s  blog post  on Chat with Towards Data Science using LlamaIndex. Sudarshan Koirala   tutorial  on Chat with documents with Pinecone and LlamaIndex. Sudarshan Koirala   tutorial  on Combined Text-TO-SQL + Semantic Search with LlamaIndex. PromptEngineer   tutorial  on building LLM-powered financial analyst with LlamaIndex. ⚙️ Integrations & Collaborations: Gradient AI:  We introduce a collaboration with Gradient AI to easily integrate fine-tuned LLMs into your LlamaIndex RAG pipeline.  Tweet ,  Blogpost . PrivateGPT:   PrivateGPT  partners with LlamaIndex allowing private document interactions using default or custom integrations.  Tweet . VectorFlow & LlamaHub Collaboration:  VectorFlow’s open-source vector-embedding pipeline now leverages LlamaHub for data connectors to streamline code and reduce maintenance.  Tweet . Amazon Bedrock & AI21 Labs LLMs:  We’ve broadened our LLM compatibility range by integrating with Amazon Bedrock LLMs and AI21 Labs LLMs. DashVector : We have introduced an integration with DashVector, a robust, fully-managed vectorDB service. Tencent Cloud:  We’ve integrated with Tencent Cloud VectorDB. PGVectorStore within LlamaIndex has been enhanced to support custom Postgres schemas. This facilitates better index management and promotes easy schema-based versioning. We now accommodate custom models that align with the OpenAI-compatible API. 🎥 Webinars: Wenqi Glantz  workshop webinar on Evaluation-Driven Development (EDD). Webinar  showcasing the winning projects from the recent AGI House hackathon: “Build, Test, and Launch LLM Apps”. This event was co-sponsored by LlamaIndex, TruEra, and Pinecone.", "author": "LlamaIndex", "date": "Oct 24, 2023", "tags": ["LLM", "AI", "Llamaindex", "Newsletter", "AWS"]},
{"title": "LlamaIndex Update — 09/03/2023", "content": "Hello LlamaIndex Community! We’re thrilled to bring you the latest edition of our LlamaIndex Update series. Whether you’ve been a part of our journey from the start or have just recently joined us, your engagement and input are invaluable to us. In this update, we’re excited to unveil some significant advancements. We’ve got comprehensive updates on new features for both the Python and TypeScript versions of LlamaIndex. In addition, we’re offering some expert insights on RAG tips that you won’t want to miss. To keep you ahead of the curve, we’ve also curated a selection of webinars, tutorials, events, and demos. So without further ado, let’s delve into the latest developments. New Features: LlamaIndex LlamaIndex introduces the Sweep AI code splitter for RAG apps, addressing the challenges of traditional code splitting. This tool features recursive splitting combined with CSTs across 100+ languages, enhancing the LlamaIndex experience.  BlogPost ,  Tweet . LlamaIndex now supports streaming data ETL, enhancing structured data extraction with the OpenAI Function API. By inputting a Pydantic object class in LlamaIndex, users can receive streamed data objects from OpenAI individually.  Docs ,  Tweet . LlamaIndex has teamed up with Neo4j to amplify knowledge graph capabilities with LLM’s. This integration not only allows for storing any knowledge graph created in LlamaIndex directly in Neo4j but also introduces a specialized text-to-cypher prompt for Neo4j users.  Docs ,  Tweet . LlamaIndex, in collaboration with Mendable AI and Nomic AI, unveils a Nomic Atlas visual map detailing user questions from the Mendable AI bot. This innovative tool groups similar questions, providing insights for improved app deployment, prompt control, language support, and documentation. New users can find the helpful Mendable AI bot on LlamaIndex’s documentation site.  Tweet . LlamaIndex, in collaboration with Predibase, offers an optimal way to operationalize LLMs. Experience top-tier RAG by privately hosting open-source LLMs on managed infrastructure right within your VPC.  Docs ,  Tweet . The LlamaIndex playground  app  enhances the RAG experience. Updates include new Temperature and Top P options, along with intuitive tooltips offering plain language explanations. LlamaIndex Tip💡: Boost your RAG systems by adding structured data to raw text. This allows for easier metadata filtering and optimal embedding biases. Dive into our guide on harnessing the HuggingFace span marker for targeted entity extraction.  Docs ,  Tweet . LlamaIndex now has the Semantic Scholar Loader. With it, users can swiftly set up citation-based Q&A systems.  Docs ,  Tweet . LlamaIndex highlights the significance of text chunk size in LLM QA systems. To determine the best chunk size without human intervention, we suggest ensembling different sizes and using a reranker for context relevance during queries. This method involves simultaneous queries across retrievers of various sizes and consolidating results for reranking. Though experimental, this approach aims to discern the optimal chunk size strategy.  Docs ,  Tweet . LlamaIndex’s customer support bot seamlessly interfaces with Shopify’s 50k-line GraphQL API Spec. Through smart tools and LlamaIndex features, it offers quick insights like  refunded orders  despite the vast spec size. Efficient indexing ensures precise user query responses.  Docs ,  Tweet . LlamaIndex’s integration with Xinference enables users to effortlessly expand models like llama 2, chatglm, and vicuna to incorporate RAG and agents.  Docs ,  Tweet . LlamaIndex introduces  One-click Observability . With just a single code line, integrate LlamaIndex with advanced observability tools from partners like Weights & Biases, ArizeAI, and TruEra, simplifying LLM app debugging for production.  Docs ,  Tweet . LlamaIndex has updated the LLM default temperature value to 0.1.  Tweet . LlamaIndex integration with Zep, enhancing the memory layer of LLM apps. It’s not just about storage but also enriching data with summaries, metadata, and more.  BlogPost ,  Tweet . LlamaIndex has revamped its defaults! Now, gpt-3.5-turbo is the go-to LLM, with enhanced prompts and a superior text splitter. Additionally, if OpenAI’s key isn’t set, it has backup options with llama.cpp. New embedding features have also been added.  Tweet . LlamaIndex now seamlessly integrates with FastChat by  lmsysorg . Elevate your LLM deployments like Vicuna and Llama 2, serving as an alternative to OpenAI.  Tweet . LlamaIndex provides a seamless integration with Azure AI Services. Dive into a richer ecosystem of AI tools from Computer Vision, Translation, and speech enhancing your multi-modal AI interactions.  Docs1 ,  Docs2 ,  Docs3 ,  Tweet . LlamaIndex unveils  Graph RAG  — an approach to enhance LLMs with context from graph databases. Extract valuable subgraphs from any knowledge graph for superior question-answering capabilities.  Docs ,  Tweet . LlamaIndex has expanded native async support, enhancing the scalability of full-stack LLM apps. We now offer async agents, tool execution, and callback support, and have introduced async methods in vector stores.  Tweet . LlamaIndex enhances debugging with data agent trace observability. Additionally, system prompts can now be added to any query engine and we have begun the transition of LLM and embedding modules to Pydantic.  Docs ,  Tweet . LlamaIndex’s  Recursive Document Agents  enhance RAG by retrieving based on summaries and adjusting chunk retrieval per need. This boosts querying across varied documents, offering both question-answering and summarization within a document.  Docs ,  Tweet . LlamaIndex integrates with Metaphor to supercharge data agents. This integration offers a specialized search engine tailored for LLMs, allowing dynamic data lookup beyond just RAG, and answering a broader range of questions.  BlogPost ,  Tweet . LlamaIndex now supports integration with OpenAI’s fine-tuned models via their new endpoint. Seamlessly integrate these models into your RAG pipeline.  Docs ,  Tweet . LlamaIndex introduces the  OpenAIFineTuningHandler  to streamline data collection for fine-tuning gpt-3.5-turbo with GPT-4 outputs. Run RAG with GPT-4 and effortlessly generate a dataset to train a more cost-effective model.  Notebook ,  Tweet . LlamaIndex presents the  Principled Development Practices  guide, detailing best practices for LLM app development Observability, Evaluation, and Monitoring.  Docs ,  Tweet . LlamaIndex introduces a refined Prompt system. With just three core classes:  PromptTemplate ,  ChatPromptTemplate , and  SelectorPromptTemplate , users can effortlessly format as chat messages or text and tailor prompts based on model conditions.  Docs ,  Tweet . LlamaIndex delves into  chunk dreaming  a concept inspired by  Thomas H. Chapin IV . By auto-extracting metadata from a text chunk, it can identify potential questions and provide summaries over neighboring nodes. This enriched context boosts RAG’s performance.  Docs ,  Tweet . LlamaIndex is integrated with BagelDB, enabling developers to effortlessly tap into vector data stored on BagelDB.  Tweet . LlamaIndex now lets the LLM choose between vector search for semantic queries or our BM25 retriever for keyword-specific ones.  Docs ,  Tweet . LlamaIndex introduces the  AutoMergingRetriever , crafted with insights from  Jason  and ChatGPT. This technique fetches precise context chunks and seamlessly merges them, optimizing LLM responses. Using the HierarchicalNodeParser, we ensure interconnected chunks for enhanced context clarity.  Docs ,  Tweet . LlamaIndex introduces embedding finetuning for optimized retrieval performance. Beyond enhancing RAG, we’ve simplified retrieval evaluations with automatic QA dataset generation from text, streamlining both finetuning and evaluation processes.  Docs ,  Tweet . LlamaIndex now integrates directly with Airbyte sources including Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform, and Zendesk Support. Easily enhance your LlamaIndex application with these platforms implemented as data loaders.  BlogPost ,  Tweet . LlamaIndex integrates with DeepEval, a comprehensive library to evaluate LLM and RAG apps. Assess on four key metrics: Relevance, Factual Consistency, Answer Similarity, and Bias/Toxicity.  Docs ,  Tweet . LlamaIndex recommends evaluating LLM + RAG step-by-step, especially retrieval. Create synthetic retrieval datasets from text chunks using LLMs. This method not only evaluates retrieval but also fine-tunes embeddings.  Docs ,  Tweet . LlamaIndex unveils a managed index abstraction simplifying RAG’s ingestion and storage processes with Vectara.  Docs ,  Tweet . LlamaIndex has significantly enhanced its callback handling support, encompassing features like tracebacks, LLM token counts, templates, and detailed agent tool information. These advancements pave the way for smoother integrations with evaluation and observability applications.  Tweet . LlamaIndex has integrated with AskMarvinAI, enabling automated metadata extraction from text corpora. Just annotate a Pydantic model and effortlessly log metadata from all associated text chunks.  Docs ,  Tweet . LlamaIndex is integrated with RunGPT by JinaAI, an outstanding framework for one-click deployment of various open-source models such as Llama, Vicuna, Pythia, and more. Coupled with LlamaIndex’s innate chat/streaming capabilities, users can now deploy and utilize powerhouse models like Llama-7B seamlessly.  Docs ,  Tweet . LlamaIndex.TS LITS has Full Azure OpenAI integration.  Tweet . LITS Enhanced Llama2 support, new default temperature (0.1), and GPT chat integration.  Tweet . LITS helps to use  fromDocuments  without repeat checks; auto SHA256 comparison.  Tweet . LITS now supports OpenAI v4, Anthropic 0.6, & Replicate 0.16.1., CSV loader, Merged NodeWithEmbeddings & BaseNode.  Tweet . LITS now supports PapaCSVLoader for math.  Tweet . LITS is now integrated with LiteLLM.  Tweet . LITS now has additional session options for proxy server support, Default timeout reset to 60 seconds for OpenAI.  Tweet . LITS now has Pinecone integration.  Tweet . LITS has Optimized ChatGPT prompts, fixed metadata rehydration issues, and OpenAI Node v4.1.0 with fine-tuned model support.  Tweet . LITS has introduced enhanced text-splitting features, including a specialized tokenizer for Chinese, Japanese, and Korean, and refinements to the SentenceSplitter for handling decimal numbers.  Tweet . LITS has a Markdown loader and metadata support in the response synthesizer.  Tweet . LITS revamped usability:  ListIndex  is now  SummaryIndex  for clarity, and prompts have been made typed and customizable to enhance user control and experience.  Tweet . LITS has Notion Reader. Now, users can effortlessly import their documents directly into their RAG or Data Agent application in LITS.  Tweet . RAG Tips: LlamaIndex shares  four tactics  to boost your RAG pipeline: 1️⃣ Use summaries for retrieval, and a broader context for synthesis. 2️⃣ Use metadata for structured retrieval over large docs. 3️⃣ Deploy LLMs for dynamic retrieval based on tasks. 4️⃣ Fine-tune embeddings for better retrieval. Tutorials: Jason's   tutorial  on adding Image Responses to GPT knowledge retrieval apps. Wenqi Glantz   tutorial  on Building Production-Ready LLM Apps with LlamaIndex: Document Metadata for Higher Accuracy Retrieval Streamlit  tutorial  on Building a chatbot with custom data sources, powered by LlamaIndex. Wenqi Glantz   tutorial  on Building Production-Ready LLM Apps With LlamaIndex: Recursive Document Agents for Dynamic Retrieval. Erika Cardenas  covers the usage of  LlamaIndex in building an RAG app . Argilla  blog post on  Fine-tuning and evaluating GPT-3.5 with human feedback for RAG using LlamaIndex . KDNuggests  blog post on  Build Your Own PandasAI with LlamaIndex . From the LlamaIndex team: Jerry Liu ’s  tutorial  on fine-tuning Llama 2 for Text-to-SQL Applications. Jerry Liu's   tutorial  on Fine-Tuning Embeddings for RAG with Synthetic Data. Ravi Theja’s   tutorial  on combining Text2SQL and RAG with LlamaIndex to analyze product reviews. Ravi Theja’s  tutorial on different  Indicies, Storage Context, and Service Context of LlamaIndex. Ravi Theja’s  tutorial on  Custom Retrievers and Hybrid Search in LlamaIndex. Adam's  tutorial on  Introduction to Data Agents for Developers . Ravi Theja’s  tutorial on creating  Automatic Knowledge Transfer (KT) Generation for Code Bases using LlamaIndex. Webinars: Webinar  with members from Docugami on Document Metadata and Local Models for Better, Faster Retrieval. Webinar  with Shaun and Piaoyang on building Personalized AI Characters with RealChar. Webinar  with Bob (Weaviet), Max (sid.ai), and Tuana (HayStack) on making RAG Production-Ready. Workshop  by Wey Gu on Building RAG with Knowledge Graphs. Webinar  with Jo Bergum and Shishir Patil on fine-tuning and RAG. Events: Jerry Liu  spoke about LlamaIndex at the  NYSE Floor Talk . Ravi Theja  spoke about LlamaIndex at the  Fifth Elephant conference  in Bengaluru, India. Ravi Theja  conducted a  workshop  on LlamaIndex in Bengaluru, India. Demos And Papers: The paper titled  Performance of ChatGPT, human radiologists, and context-aware ChatGPT in identifying AO codes from radiology reports  is an intriguing medical research. It leverages both LlamaIndex and ChatGPT to pinpoint AO codes within radiology reports, enhancing fracture classification. A fantastic fusion of tech and medicine! SEC Insights AI  does SEC document analysis using LlamaIndex is on Product Hunt as the 5th product of the day. RentEarth : an agent to build your own startup with an amazing 3D interface and LlamaIndex. In wrapping up this edition of our LlamaIndex Update series, we’re reminded of the power of collaboration and innovation. From new features to integrations and tutorials, our mission to revolutionize the AI realm marches forward. To every member of our community, thank you for your unwavering support and enthusiasm. Let’s continue to elevate the world of AI together!", "author": "Ravi Theja", "date": "Sep 6, 2023", "tags": ["Large Language Models", "Llamaindex", "OpenAI", "AI", "NLP"]},
{"title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex — Hackathon winning app", "content": "We’re excited to share with you the thought process and solution design of  NewsGPT  ( Neotice ) — Clickbait Buster , a Streamlit LLM hackathon-winning app powered by LlamaIndex, Streamlit, and Qdrant. In this article, we’ll define the problem we’re trying to solve and discuss how we approached it. Lastly, we offer a workaround to enable LlamaIndex streaming on Streamlit Chat Bot and the all the code can be found  here . We hope you’ll find our insights helpful and informative. Introduction Problem Statement It’s evident that people’s habits of consuming information have changed over time. Previously, we read lengthy articles content and watched long videos, such as newspapers and YouTube videos. However, we currently prefer reading titles and consuming short-form content, such as TikTok and YouTube shorts. Although this shift has made it easier to get more information in less time, it has also led to clickbait headlines that often contain incorrect information. When we started developing NewsGPT, our primary focus was to solve the above-stated pain points and provide a solution that 1)  provides accurate information  and 2)  saves time for users . Neotice We are excited to announce that the beta version of the  Neotice  app, which is the production version of NewsGPT, is now available for users to try out! We are grateful to the Streamlit Hackathon for showcasing our prototype and philosophy. With the help of this platform, we are confident that our app will revolutionize the way people consume news. Why NewsGPT Stands Out NewsGPT has four main components: Reliable News Sources, Tailored News Recommendations, Efficient Information Retrieval, and Time Saver. ✅  Reliable News Sources: We’ve established a dynamic data pipeline designed to ingest daily news, ensuring our information is up-to-date and relevant. Sophisticated  Named-Entity Recognition, Text Embedding with OpenAI API,  and  asynchronous article embedding processes  are incorporated. This data is systematically stored in the  Qdrant Vector Database, promoting accuracy and efficiency. ✅  Tailored News Recommendations: Our system does more than just present news; it learns from you. By analyzing your reading habits, we leverage article embeddings to curate a personal news feed tailored to your interests. A versatile  search bar  is always at your disposal, letting users explore any news topics that capture their interest. ✅  Efficient Information Retrieval: With just a single click on an article of interest, NewsGPT gets to work. It collates  similar news from multiple sources (3–5)  and activates a Streamlit chatbot. Your engagement begins immediately: the first query is autonomously forwarded to our chatbot to fetch a concise news summary. For ease of user experience, we display  predefined prompts  as clickable buttons. This means users can receive information without the need for manual input. Curiosity welcomed: any questions users may have about the news article will be addressed as long as the answers are detailed within the source articles. ✅  Time-Saving Reminder & Category Distribution Chart: To keep you informed, our sidebar displays the time saved using NewsGPT and visually represents news category distribution. Delving Deep into Architecture Data Pipeline We start with a reliable and sustainable data pipeline to support the users to get fresh news with two powerful libraries,  pygooglenews,  and  newspaper3k. pip install pygooglenews --upgrade\npip install newspaper3k By utilizing  Spark batch processing , we efficiently process data with  NER(Named-Entity Recognition)  and create embeddings via  OpenAI API (Ada model) . After the preprocessing of the data, we collect the metadata, including keywords, NER results, summary, body, title, author, and so on, in the payload and push the payload with embedding to the Qdrant Vector Database. We will skip the part on how to create embeddings with the OpenAI Ada model, as there are many existing tutorials available. To perform Named Entity Recognition, we utilize the  dslim/bert-base-NER  model from HuggingFace. from  transformers  import  pipeline\n from  transformers  import  AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained( \"dslim/bert-base-NER\" )\nmodel = AutoModelForTokenClassification.from_pretrained( \"dslim/bert-base-NER\" )\nnlp = pipeline( \"ner\" , model=model, tokenizer=tokenizer, batch_size=batch_size)\nner_results = nlp(texts) Personalization When accessing NewsGPT, we have two options: we can either create a new account or use a guest login to test the service. It should be noted, however, that using the guest login will not provide personalization. If we choose to sign up, we will be asked about our preferred news categories, which will help the service make initial recommendations during the cold start. Recommendation Pipeline for Personalization After users sign up and log in, we query the preferences and activities from Google Firebase, if any; otherwise, use the favorite categories for recommendation cold start. If the activities and preferences are available, we will call the recommendation API hosted on AWS Lambda and generate personalized articles for the users. The activities related to the user reading history. Users can indicate their preference for articles by clicking thumbs-up or down buttons on the article page. Some of the recommendation logic can be found in the  code . Besides the personalized feeds, users can also choose different categories or search for specific topics using the search bar. Chat with Article Powered by LlamaIndex LlamaIndex Pipeline Unlike other news aggregator apps, NewsGPT offers a unique service by providing users with a summary and discrepancy key takeaway of a topic from various news sources. This feature not only saves users time by eliminating the need to read multiple articles but also helps identify which information can be trusted by comparing discrepancies. What is under the hood is when the user clicks on the “chat with article” button, the system first uses a search API hosted on AWS Lambda to find related articles from various sources. Then, the system utilizes the  LlamaIndex  library to create a vector store and a query engine, which can be integrated with the  Streamlit  chat component to create an  RAG  application for information retrieval. Streaming Output with LlamaIndex and Streamlit Thanks to the powerful library, S treamlit-Extras , which provides additional functionality not officially supported. To enhance the user experience and make it more like chatting with ChatGPT, we use the  streaming_write  functions from the Streamlit-Extras library. Additionally, we set  streaming=True  for the  query_engine  to ensure a smoother experience. Let’s take a look at the code. pip install streamlit-extras To begin with, we set up the  service_context . from  llama_index  import  ServiceContext\n\nst.session_state[ \"service_context\" ] = ServiceContext.from_defaults(\n            llm=OpenAI(\n                model= \"gpt-3.5-turbo\" ,\n                temperature= 0.2 ,\n                chunk_size= 1024 ,\n                chunk_overlap= 100 ,\n                system_prompt= \"As an expert current affairs commentator and analyst,\\\n                               your task is to summarize the articles and answer the questions from the user related to the news articles\" ,\n            ),\n            chunk_size= 256 , \n            chunk_overlap= 20 \n        ) Next, we create a text splitter using  TokenTextSplitter  and a node parser using  SimpleNodeParser  to parse multiple articles. from  llama_index.text_splitter  import  TokenTextSplitter\n from  llama_index.node_parser  import  SimpleNodeParser\n\ntext_splitter = TokenTextSplitter(separator= \" \" , chunk_size= 256 , chunk_overlap= 20 )\n #create node parser to parse nodes from document \nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\nnodes = node_parser.get_nodes_from_documents(documents) In the third step, we create an index using  VectorStoreIndex . To enable streaming capability, ensure to set  streaming=True  while setting up the  query_engine . from  llama_index  import  VectorStoreIndex\n\nindex = VectorStoreIndex(\n        nodes=nodes,\n        service_context=st.session_state[ \"service_context\" ]\n    )\nst.session_state[ \"chat_engine\" ] = index.as_query_engine(streaming= True ) To add streaming capability to Streamlit chat components, we took inspiration from this  code . Instead of using  st.write()  from the regular chat implementation, we replaced it with the  write  function from  streaming_write . response = st.session_state[ \"chat_engine\" ].query(prompt)\n def   stream_example ():\n     for  word  in  response.response_gen:\n         yield  word\nwrite(stream_example) Streaming Demo Predefined Prompts Three different prompts are predefined in the article chat page, allowing users to select from a drop-down menu to ask questions without typing. The prompts are 5W1H, Similar Viewpoints, and Discrepancy Viewpoints. prompt_content = {\n     \"5W1H\" :  'Summarize the content details in the \"5W1H\" approach (Who, What, When, Where, Why, and How) in bullet points' ,\n     \"Similar Viewpoints\" :  \"Compare between the articles and provide the similar viewpoints in bullet points\" ,\n     \"Discrepency Viewpoints\" :  \"Compare between the articles and provide the discrepency viewpoints in bullet points\" \n} What’s Next Huge thanks to LlamaIndex and Streamlit for generously providing a massive platform that allows more people to gain awareness of the organic news digest and save valuable time through  NewsGPT . If you enjoyed reading the article and agree with our concept, please do not hesitate to leave a clap for the article and join  Neotice , the production app of NewsGPT, to support us. We are confident in our mission and look forward to having you on board with us. Thank you! You can also connect us on LinkedIn:   Kang-Chi Ho ,  Jian-An Wang 🎉 Click Here to Join Neotice 👉  Neotice", "author": "Tim Ho", "date": "Oct 27, 2023", "tags": []},
{"title": "Boosting RAG: Picking the Best Embedding & Reranker models", "content": "UPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let’s first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on. Now that we’ve established the scope and familiarized ourselves with the metrics, it’s time to dive into the experiment. For a hands-on experience, you can also follow along using our  Google Colab Notebook Setting Up the Environment !pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf Setting Up the Keys openai_api_key =  'YOUR OPENAI API KEY' \ncohere_api_key =  'YOUR COHEREAI API KEY' \nanthropic_api_key =  'YOUR ANTHROPIC API KEY' \nopenai.api_key = openai_api_key Download the Data We will use Llama2 paper for this experiment. Let’s download the paper. !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\" Load the Data Let’s load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix. This data was then parsed by converted to nodes, which represent chunks of data we’d like to retrieve. We did use chunk_size as 512. documents = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents) Generating Question-Context Pairs: For evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs. Let’s initialize a prompt template to generate question-context pairs. # Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\" llm = Anthropic(api_key=anthropic_api_key)\nqa_dataset = generate_question_context_pairs(\n    nodes, llm=llm, num_questions_per_chunk= 2 \n) Function to filter out sentences such as —  Here are 2 questions based on provided context # function to clean the dataset \n def   filter_qa_dataset ( qa_dataset ):\n     \"\"\"\n    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n    the filtered data.\n\n    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n    \"\"\" \n\n     # Extract keys from queries and relevant_docs that need to be removed \n    queries_relevant_docs_keys_to_remove = {\n        k  for  k, v  in  qa_dataset.queries.items()\n         if   'Here are 2'   in  v  or   'Here are two'   in  v\n    }\n\n     # Filter queries and relevant_docs using dictionary comprehensions \n    filtered_queries = {\n        k: v  for  k, v  in  qa_dataset.queries.items()\n         if  k  not   in  queries_relevant_docs_keys_to_remove\n    }\n    filtered_relevant_docs = {\n        k: v  for  k, v  in  qa_dataset.relevant_docs.items()\n         if  k  not   in  queries_relevant_docs_keys_to_remove\n    }\n\n     # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data \n     return  EmbeddingQAFinetuneDataset(\n        queries=filtered_queries,\n        corpus=qa_dataset.corpus,\n        relevant_docs=filtered_relevant_docs\n    )\n\n # filter out pairs with phrases `Here are 2 questions based on provided context` \nqa_dataset = filter_qa_dataset(qa_dataset) Custom Retriever: To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base  VectorIndexRetriever . Upon retrieving the nodes, we then introduce a reranker to further refine the results. It’s worth noting that for this particular experiment, we’ve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with  OpenAIEmbedding , please refer to the  notebook  for code with other embeddings. embed_model = OpenAIEmbedding()\nservice_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)\nvector_index = VectorStoreIndex(nodes, service_context=service_context)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10) class   CustomRetriever ( BaseRetriever ):\n     \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\" \n\n     def   __init__ ( \n        self,\n        vector_retriever: VectorIndexRetriever,\n     ) -&gt;  None :\n         \"\"\"Init params.\"\"\" \n\n        self._vector_retriever = vector_retriever\n\n     def   _retrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n         \"\"\"Retrieve nodes given query.\"\"\" \n\n    retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n     if  reranker !=  'None' :\n      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n        else :\n          retrieved_nodes = retrieved_nodes[: 5 ]\n         \n        return  retrieved_nodes\n\n     async   def   _aretrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n         \"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\" \n         return  self._retrieve(query_bundle)\n\n     async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n         if   isinstance (str_or_query_bundle,  str ):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n         return   await  self._aretrieve(str_or_query_bundle)\n\ncustom_retriever = CustomRetriever(vector_retriever) Evaluation: To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics: retriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\n)\neval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset) Results: We put various embedding models and rerankers to the test. Here are the models we considered: Embedding Models : OpenAI Embedding Voyage Embedding CohereAI Embedding  (v2.0/ v3.0) Jina Embeddings  (small/ base) BAAI/bge-large-en Google PaLM Embedding Rerankers : CohereAI bge-reranker-base bge-reranker-large It’s worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools. bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere’s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better. Foundation is Key : Choosing the right embedding for the initial search is essential; even the best reranker can’t help much if the basic search results aren’t good. Working Together:  To get the best out of retrievers, it’s important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing.", "author": "Ravi Theja", "date": "Nov 3, 2023", "tags": ["Embedding", "LLM", "OpenAI", "Search", "Llamaindex"]},
{"title": "LlamaIndex Update — 20/09/2023", "content": "Hello LlamaIndex Enthusiasts! Welcome to the fifth edition of our LlamaIndex Update series. Most Important Takeaways: We’ve open-sourced  SECInsights.ai  — your gateway to the production RAG framework. Replit templates — kickstart your projects with zero environment setup hassles. Build RAG from scratch and get hands-on with our processes. But wait, there’s more! Feature Releases and Enhancements Fine-Tuning Guides Retrieval Tips for RAG Building RAG from Scratch Guides Tutorials Integration with External Platforms Events Webinars So, let’s embark on this journey together. Dive in and explore the offerings of the fifth edition of the LlamaIndex Update series! Feature Releases and Enhancements Open-Sourced RAG Platform : LlamaIndex open-sourced  http://secinsights.ai , accelerating RAG app development with chat-based Q&A features.  Tweet Linear Adapter Fine-Tuning : LlamaIndex enables efficient fine-tuning of linear adapters on any embedding without re-embedding, enhancing retrieval/RAG across various models.  Tweet ,  Docs ,  BlogPost Hierarchical Agents : By structuring LLM agents in a parent-child hierarchy, we enhance complex search and retrieval tasks across diverse data, offering more reliability than a standalone agent.  Tweet SummaryIndex : We’ve renamed ListIndex to SummaryIndex to make it clearer what its main functionality is. Backward compatibility is maintained for existing code using ListIndex.  Tweet Evaluation:  LlamaIndex’s new RAG evaluation toolkit offers async capabilities, diverse assessment criteria, and a centralized BaseEvaluator for easier developer integrations.  Tweet ,  Docs . Hybrid Search for Postgres/pgvector : LlamaIndex introduces a hybrid search for Postgres/pgvector.  Tweet ,  Docs . Replit Templates:  LlamaIndex partners with Replit for easy LLM app templates, including ready-to-use Streamlit apps and full Typescript templates.  Tweet ,  Replit Templates . LlamaIndex.TS: Launches with MongoDBReader and type-safe metadata.  Tweet . Launches with chat history, enhanced keyword index, and Notion DB support.  Tweet . Fine-Tuning Guides: OpenAI Fine-Tuning:  LlamaIndex unveils a fresh guide on harnessing OpenAI fine-tuning to embed knowledge from any text corpus. In short: generate QA pairs with GPT-4, format them into a training dataset, and proceed to fine-tuning.  Tweet ,  Docs . Embedding Fine-Tuning:  LlamaIndex has a more advanced embedding fine-tuning feature, enabling complex NN query transformations on any embedding, including custom ones, and offering the ability to save intermediate checkpoints for enhanced model control.  Tweet ,  Docs . Retrieval Tips For RAG: Use references (smaller chunks or summaries) instead of embedding full text. Results in 10–20 % improvement. Embeddings decoupled from main text chunks. Smaller references allow efficient LLM synthesis. Deduplication applied for repetitive references. Evaluated using synthetic dataset; 20–25% MRR boost. Tweet Building RAG from Scratch Guides: Build Data Ingestion from scratch.  Docs . Build Retrieval from scratch.  Docs . Build Vector Store from scratch.  Docs . Build Response Synthesis from scratch.  Docs . Build Router from scratch.  Docs . Build Evaluation from scratch.  Docs . Tutorials: Wenqi Glantz   tutorial  on Fine-Tuning GPT-3.5 RAG Pipeline with GPT-4 Training Data with LlamaIndex fine-tuning abstractions. Wenqi Glantz   tutorial  on Fine-Tuning Your Embedding Model to Maximize Relevance Retrieval in RAG Pipeline with LlamaIndex. Tutorials from the LlamaIndex Team. Sourabh   tutorial  on SEC Insights, End-to-End Guide on  secinsights.ai Adam’s   tutorial  on Custom Tools for Data Agents. Logan   tutorial  on retrieval/reranking, covering Node Parsing, AutoMergingRetriever, HierarchicalNodeParser, node post-processors, and the setup of a RouterQueryEngine. Integrations with External Platforms Integration with PortkeyAI : LlamaIndex integrates with PortkeyAI, boosting LLM providers like OpenAI with features like auto fallbacks and load balancing.  Tweet,   Documentation Collaboration with Anyscale : LlamaIndex collaborates with anyscalecompute, enabling easy tuning of open-source LLMs using Ray Serve/Train.  Tweet,   Documentation Integration with Elastic : LlamaIndex integrates with Elastic, enhancing capabilities such as vector search, text search, hybrid search models, enhanced metadata handling, and es_filters.  Tweet,   Documentation Integration with MultiOn : LlamaIndex integrates with MultiOn, enabling data agents to navigate the web and handle tasks via an LLM-designed browser.  Tweet,   Documentation Integration with Vectara : LlamaIndex collaborates with Vectara to streamline RAG processes from loaders to databases.  Tweet,   Blog Post Integration with LiteLLM : LlamaIndex integrates with LiteLLM, offering access to over 100 LLM APIs and features like chat, streaming, and async operations.  Tweet,   Documentation Integration with MonsterAPI : LlamaIndex integrates with MonsterAPI, allowing users to query data using LLMs like Llama 2 and Falcon.  Tweet,   Blog Post Events: Jerry Liu  spoke on  Production Ready LLM Applications  at the Arize AI event. Ravi Theja  conducted a  workshop  at LlamaIndex + Replit Pune Generative AI meetup. Jerry Liu   session  on Building a Lending Criteria Chatbot in Production with Stelios from MQube. Webinars : Webinar  on How to Win an LLM Hackathon by Alex Reibman, Rahul Parundekar, Caroline Frasca, and Yi Ding. Webinar  on LLM Challenges in Production with Mayo Oshin, AI Jason, and Dylan.", "author": "Ravi Theja", "date": "Sep 21, 2023", "tags": ["Llamaindex", "LLM", "OpenAI", "AI", "NLP"]},
{"title": "LlamaIndex news special edition: OpenAI developer day!", "content": "Hello Llama fans! Yesterday was a big day in the world of LLMs; OpenAI held their  developer day conference  and there were a slew of new features. The team were all hands on deck to bring support for these features to the library as fast as possible — which is to say,  the same day ! In case you missed our tweet about it, if you install the  latest build  of LlamaIndex you’ll get everything below: Support for two new models released today gpt-4-1106-preview  , aka  GPT-4 Turbo , the latest GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and a 128,000 token context window gpt-4-vision-preview  , aka  GPT 4 Turbo with vision  with long-awaited multimodal support, has the ability to understand images in addition to all the other GPT-4 Turbo capabilities. You can use these models just as you would any other OpenAI model: from  llama_index.llms  import  OpenAI\n from  llama_index  import  VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\nllm = OpenAI(model= \"gpt-4-1106-preview\" )\nservice_context = ServiceContext.from_defaults(llm=llm)\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n) Azure OpenAI endpoints Check out the  OpenAI Azure notebook  for examples. New embeddings abstractions Including Azure embeddings. Function calling Check out  our notebook  for examples. SEC insights Our demo of the power of retrieval-augmented generation for financial filings,  SEC Insights , has been updated to use the latest version of GPT-4! Watch as you instantly get deeper insights and more relevant responses. Look out for more OpenAI updates soon! Our regular newsletter will also be posted tomorrow.", "author": "Laurie Voss", "date": "Nov 7, 2023", "tags": ["LLM", "AI", "Llamaindex", "OpenAI"]},
{"title": "LlamaIndex Newsletter 2023-11–07", "content": "Hi again Llama Fans! 🦙 We hope you enjoyed our  OpenAI Dev Day special edition  yesterday! Here’s our wrap-up of everything else that happened last week. As always, if you’ve got a project, article, or video that’s turning heads? We’re all ears! Drop us a line at  news@llamaindex.ai . And for all this goodness delivered directly to you, don’t forget to subscribe to our newsletter via our  website . 🤩  First, the highlights: LlamaIndex Chat:  We unveiled a customizable LLM chatbot template with system prompts and avatars, all within an open-source MIT-licensed framework using LlamaIndex for TypeScript. Explore the  Demo  or check the  Tweet . Evaluator Fine-Tuning:  We launched a method to enhance LLM output assessment by distilling GPT-4 into GPT-3.5, optimizing both cost and speed. See our  Tweet . ParamTuner:  We introduced a new hyperparameter tuning abstraction to refine RAG pipeline performance, featuring objective functions, grid search, and Ray Tune integration. Check out the  Notebook  and  Tweet . CohereAI Embed v3 & Voyage AI Integration:  We strengthened the LlamaIndex RAG pipeline with two powerful embedding model additions: the latest Embed v3 from CohereAI and the high-performing embedding model from Voyage AI.  Tweet  and  tweet . ✨ Feature Releases and Enhancements: We introduced LlamaIndex Chat, a new feature allowing users to create and share custom LLM chatbots tailored to their data, complete with personalized system prompts and avatars. Additionally, we’re proud to share that it’s a fully open-source template under the MIT license, crafted using LlamaIndexTS for a seamless start to LLM application development.  Demo ,  Tweet . We introduced a method for fine-tuning an Evaluator to distill GPT-4 into GPT-3.5, enhancing LLM output assessment while reducing costs and improving speed.  Tweet . We introduced  ParamTuner , a hyperparameter tuning abstraction for LlamaIndex RAG, streamlining the process with objective functions and support for grid search, including integration with Ray Tune for enhanced optimization.  Notebook ,  Tweet . 🎥  Demos: GPTDiscord is a versatile LLM-powered Discord bot with over 20 features, including multi-modal image understanding and advanced data analysis. It boasts an infinite conversational memory and the ability to interact with various file types and internet services.  Tweet . 🗺️ Guides: We shared a  guide  for integrating Activeloop’s Deep Memory with LlamaIndex, a module that enhances your embeddings at ingestion and can improve RAG metrics by 15%, all while seamlessly fitting into LlamaIndex’s automated dataset and vector store features. We shared a  guide  inspired by  Chengrun Yang  and GoogleDeepMind’s  Optimization by Prompting  paper, demonstrating how to automate prompt tuning in LlamaIndex RAG pipelines using meta-prompting, boosting evaluation performance while acknowledging the experimental nature of this technique. We shared a  guide  on how to implement Emotion Prompting in LlamaIndex, allowing you to enhance your RAG pipeline with various emotional stimuli and evaluate their impact on task performance. We showcased MongoDB  starter  kit, a comprehensive LlamaIndex RAG setup with Flask backend, Next frontend, and easy deployment to Render. ✍️ Tutorials: Wenqi Glantz  made a  blog post  on deploying the HuggingFace  text-embeddings-inference  server on an AWS EC2 GPU instance, enhancing LlamaIndex RAG pipeline's performance and results. Sophia Yang’s   tutorial  on Zephyr-7b-beta showcases its leading capabilities in LLM technology, including how it’s benchmarked with LlamaIndex for diverse AI tasks. Sudarshan Koirala  gave a  tutorial  on how to build a multi-modal retrieval system with LlamaIndex, Qdrant, and bge/CLIP embeddings. Sophia Yang’s  gave another  tutorial , this time on Small-to-Big Retrieval with LlamaIndex in building advanced RAG systems. Ravi Theja’s   tutorial  on the Router Query Engine that helps you to set up multiple indices/ query engines for your dataset, allowing the LLM to choose the most suitable one for each specific question. ⚙️ Integrations & Collaborations: We integrated the  Tavily AI  research API into the LlamaIndex RAG pipeline, offering a robust tool for web research to enhance LLM agent automation.  Notebook ,  Tweet . We integrated  Noam Gat ’s LLM Enforcer into the LlamaIndex RAG pipeline to ensure structured outputs for various models.  Docs ,  Tweet . We integrated the latest Embed v3 model from CohereAI, enhancing document retrieval quality within the LlamaIndex RAG pipeline.  Notebook ,  Tweet . We integrated the new Voyage AI embedding model, a top-performing option for RAG pipelines.  Notebook ,  Tweet .", "author": "LlamaIndex", "date": "Nov 8, 2023", "tags": ["LLM", "Llamaindex", "AI", "Newsletter"]},
{"title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas", "content": "Hi, I’m Laurie, and today is my first day as VP of Developer Relations at LlamaIndex! Quick background on me: I started my career 27 years ago as a web developer, founded a couple of companies including  npm Inc. , and have always been about talking to developers about the state of the development world and how we fit into it, whether that’s on my  personal site , in  conference talks , or in big  community surveys . I wrote last week about why  LLMs are the future of software . To summarize that post: until now computers have been very good at ingesting, sorting, and transmitting data, but understanding what they were working with was beyond them. Very recently, with tools like  GPT-4  and  Llama 2  that threshold has been crossed. Software can now read, summarize, and make novel connections within arbitrarily large sets of data. It can write software, it can use tools, it can generate text, images and music. A huge new set of capabilities have been unlocked. The last time I saw a shift this big in the technological landscape was probably the original iPhone. Suddenly, everybody who was doing everything needed to also do it for mobile devices. You sell real estate? Now you do it with an app. You run a dating site? Now you do it with an app. There was a gigantic rush as everyone who did everything in software suddenly saw huge benefits from moving onto a new platform. But the most interesting applications that platform shift enabled were the ones that couldn’t have existed before. Uber relies on everyone — drivers as well as customers — already owning a piece of mobile hardware that can connect to the Internet and use GPS to locate them. Prior to the mobile revolution, that would have been an absurd business plan. After it happened, it was taken for granted. So in the same way, I’m especially interested in seeing what the applications of LLMs are that were simply impossible before. All of which is why I’m delighted to be at a company at the center of the LLM universe. If you’re new to LlamaIndex, it’s a Python and JavaScript framework that lets you quickly put together totally customizable, production-class applications that use LLMs. We let you ingest data from dozens of sources and use any model you want, and we make it simple to link everything together. The team at LlamaIndex is absolutely amazing and I am looking forward to working with Jerry, Simon and Yi. Yi will be focusing more on partnerships. Want to see a demo? Our  SEC Insights  app lets you ingest regulatory documents from major corporations and then ask questions about them. Want to dive and build your own? Get started in  Python  or  JavaScript ! P.S. If you’re wondering “why an alpaca?” it’s because my  personal mascot  is an alpaca and has been for years. Alpacas and llamas are a great match!", "author": "Laurie Voss", "date": "Oct 2, 2023", "tags": ["Llamaindex", "Hires"]},
{"title": "LlamaIndex turns 1!", "content": "It’s our birthday! One year ago, Jerry pushed his  first commit  to GPT Index, the project that would become LlamaIndex. It worked with GPT-3, the state of the art model available at the time. That initial version was very simple, but the problem statement — and the solution — remain the same: one fundamental limitation of GPT-3 is the context size […] the ability to feed “knowledge” to GPT-3 is mostly limited to this limited prompt size […] But what if GPT-3 can have access to potentially a much larger database of knowledge[…]? Twelve months have passed and there’s been a tsunami of new developments in the world of generative AI and LLMs, but the reason LlamaIndex was invented remains: even the most sophisticated model isn’t trained on  your  data, which can be locked behind an API or in a SQL database, and even the latest GPT-4-Turbo context size of 128,000 tokens isn’t enough to hold even a relatively modest dataset. Retrieval-Augmented Generation (RAG) is here to stay. Big numbers At just 1 year old, LlamaIndex has gotten very big. How big? Here’s some numbers: Over  450 contributors  to our open-source library! Nearly 3,000 open-source projects  depend on LlamaIndex ! Nearly 4,000 members in our Discord ( come join us! ) 47,000 lines of Python in the library! (Don’t worry, it’s still just  0.5MB to download ) Nearly  900,000 downloads every month ! RAG deployed among  popular   open-source   projects , as well as in  production  in  enterprise   settings . Big thanks But big numbers aside, the thing we’re proudest of is our community: we have users in (nearly) every country in the world, from single hobby developers to Fortune 500 companies and everyone in between. LlamaIndex’s founder, Jerry Liu, says: Our community is everything at LlamaIndex. We love seeing the amazing things people are building every day! It’s what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what developers can do with GenAI. And we’re especially grateful to the developers who give back by pushing PRs, issues and bug reports. They’re what makes the open source world go round. Big milestones What’s happened in a year? Well, everything! But here’s some highlights: November 2022: Launched  GPT Tree Index , a way of organizing information into a tree. Based on the initial interest/traction, we expanded this into a List Index and Keyword Index. Then ChatGPT launched in November December 2022: Some big feature releases: support for  indexing embeddings + vector stores , and initial data loaders for Notion, Slack, and Google Drive January 2023: LlamaIndex hits Github trending for the first time! February 2023: We launched  LlamaHub  with Jesse Zhang, containing an initial repository of data loaders for users to access. We ran a sweepstakes with OctoML and got 50+ data loader submissions! March 2023:  ChatGPT API launched  and then  Plugins . We scrambled to support the new API +  Plugin  integrations. April 2023: We incorporated! May 2023: At the end of April, we launched  0.6.0 , where we completely rewrote the entire framework from the ground-up for greater modularity and composability for different levels of abstraction. June 2023: We announced that we raised $8.5M in funding! July 2023: We launched  Data Agents  + Agent Tools on LlamaHub. We also launched a  Typescript package August 2023: We integrated with  OpenAI fine-tuning and launched a variety of LLM and embedding fine-tuning abstractions . September 2023: We  launched   secinsights.ai  — a production-ready full-stack application October 2023: We launched  LlamaIndex Chat  — a full-stack Typescript template. November 2023: Went  fully multi-modal  with the release of GPT-4-vision! Big plans With all that growth and all those features, what’s next for us? Stay tuned!", "author": "LlamaIndex", "date": "Nov 13, 2023", "tags": ["Llamaindex", "AI", "LLM", "Rag"]},
{"title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex", "content": "In the ever-evolving landscape of AI, OpenAI’s ChatGPT with vision capabilities has opened a new chapter. It’s an exciting time for developers and creators as we explore the fusion of visual understanding with conversational AI. Inspired by this innovation, I set out to build my own multi-modal prototype, not just as a replica but as a launchpad for more advanced and tailored visual-language applications. The tools at our disposal are nothing short of extraordinary.  KOSMOS-2  a true powerhouse in painting vivid narratives from mere pixels, making image captioning seem almost magical. Then there’s the Google  PaLM API , bringing a level of conversational depth that truly understands and responds with relevance. And of course, there’s  LlamaIndex  - the brains of the operation, orchestrating these elements with such finesse that the interaction flows as naturally as a conversation between old friends. Features Overview The outcome of my curiosity and coding is a Streamlit app — a prototype that stands as an homage and alternative to ChatGPT’s vision capabilities. Here’s what it brings to the table: Real-Time Image Interaction:  Upload your images and instantly dive into a dialogue about them. Automatic Captioning with KOSMOS-2:  Microsoft’s AI model offers a descriptive base for the conversation. Conversational Depth with PaLM:  Google’s language model ensures the chat is as rich and nuanced as the images themselves. User-Friendly Interface:  Streamlit powers an intuitive and clean UI, making it easy for anyone to navigate and interact. Deep Dive into the Tech Stack The project is a symphony of technologies, each playing a crucial role: Microsoft AI  KOSMOS-2  via Replicate breathes life into images by providing them a narrative. Google  PaLM API  adds the layer of linguistic intelligence, making the conversation about the images insightful and engaging. LlamaIndex  acts as the maestro, coordinating the models to work in harmony. Unveiling  app.py : The Core of the Application The  app.py  script is the heart of the app, where we bring together KOSMOS-2 and PaLM with Llamaindex to create a seamless multimodal experience. Let’s walk through it, from start to finish. 1. Initial Setup We start by importing the necessary libraries and setting up our Streamlit page. Here, we lay the groundwork for image processing and conversation management. import  streamlit  as  st\n import  extra_streamlit_components  as  stx\n import  requests\n from  PIL  import  Image\n from  transformers  import  AutoProcessor, AutoModelForVision2Seq\n from  io  import  BytesIO\n import  replicate\n from  llama_index.llms.palm  import  PaLM\n from  llama_index  import  ServiceContext, VectorStoreIndex, Document\n from  llama_index.memory  import  ChatMemoryBuffer\n import  os\n import  datetime\n\nst.set_page_config(layout= \"wide\" )\nst.write( \"My version of ChatGPT vision. You can upload an image and start chatting with the LLM about the image\" ) 2. User Interface Next, we craft the sidebar and the main area, ensuring that the user knows who created the app and has access to other projects, enhancing credibility and engagement. # Sidebar\nst.sidebar.markdown('## Created By')\nst.sidebar.markdown(\"[Harshad Suryawanshi](https://www.linkedin.com/in/harshadsuryawanshi/)\")\nst.sidebar.markdown('## Other Projects')\n# ...sidebar content continues 3. Image Upload and Processing Upon uploading an image, the app not only displays it but also invokes the  get_image_caption  function to generate a relevant caption. This function, decorated with  @st.cache for caching, uses the KOSMOS-2 model through Replicate to provide a brief description of the uploaded image. The description is then used as the basis for the initial conversation with the user. @st.cache \n def   get_image_caption ( image_data ):\n    input_data = {\n         \"image\" : image_data,\n         \"description_type\" :  \"Brief\" \n    }\n    output = replicate.run(\n         \"lucataco/kosmos-2:3e7b211c29c092f4bcc8853922cc986baa52efe255876b80cac2c2fbb4aff805\" ,\n         input =input_data\n    )\n     # Split the output string on the newline character and take the first item \n    text_description = output.split( '\\n\\n' )[ 0 ]\n     return  text_description 4. Conversational Flow with PaLM and Llamaindex With the image caption in hand, the  create_chat_engine  function is called to set up the chat engine. This function is crucial as it establishes the context for the conversation and initializes the PaLM API for interaction. @st.cache_resource \n def   create_chat_engine ( img_desc, api_key ):\n    llm = PaLM(api_key=api_key)\n    service_context = ServiceContext.from_defaults(llm=llm)\n    doc = Document(text=img_desc)\n    index = VectorStoreIndex.from_documents([doc], service_context=service_context)\n    chatmemory = ChatMemoryBuffer.from_defaults(token_limit= 1500 )\n    \n    chat_engine = index.as_chat_engine(\n        chat_mode= \"context\" ,\n        system_prompt=(\n             f\"You are a chatbot, able to have normal interactions, as well as talk. \" \n             \"You always answer in great detail and are polite. Your responses always descriptive. \" \n             \"Your job is to talk about an image the user has uploaded. Image description: {img_desc}.\" \n        ),\n        verbose= True ,\n        memory=chatmemory\n    )\n     return  chat_engine The  create_chat_engine  function builds the infrastructure for our app's conversation capabilities. It starts by instantiating a PaLM object with the provided API key, setting up the service context, and creating a document with the image description. This document is then indexed to prepare it for Llamaindex’s context chat engine. Finally, the chat engine is configured with a prompt that instructs the AI on how to engage in the conversation, referencing the image description and defining the chatbot's behavior. 5. User Interaction and Message Handling The application ensures an engaging and controlled user experience by limiting the number of messages to 20 per session in the demo version. If this limit is reached, it gracefully notifies the user and disables further input to manage resources effectively. if  message_count &gt;=  20 :\n    st.error( \"Notice: The maximum message limit for this demo version has been reached.\" )\n     # Disabling the uploader and input by not displaying them \n    image_uploader_placeholder = st.empty()   # Placeholder for the uploader \n    chat_input_placeholder = st.empty()       # Placeholder for the chat input However, when the message count is within the limit, the application provides a clear chat option and handles the image upload process. Upon uploading, it immediately processes the image to get a caption, sets up the chat engine, and updates the user interface to reflect the successful upload. else :\n     # Add a clear chat button \n     if  st.button( \"Clear Chat\" ):\n        clear_chat()\n\n     # Image upload section \n    image_file = st.file_uploader( \"Upload an image\" ,  type =[ \"jpg\" ,  \"jpeg\" ,  \"png\" ], key= \"uploaded_image\" , on_change=on_image_upload)\n     # ...code for image upload and display For each user input, the message is added to the chat history, and the chat engine is queried for a response. The app ensures that each message — whether from the user or the assistant — is displayed in the chat interface, maintaining a coherent conversation flow. # ...code for handling user input and displaying chat history \n\n # Call the chat engine to get the response if an image has been uploaded \n if  image_file  and  user_input:\n     try :\n         with  st.spinner( 'Waiting for the chat engine to respond...' ):\n             # Get the response from your chat engine \n            response = chat_engine.chat(user_input)\n         # ...code for appending and displaying the assistant's response \n     except  Exception  as  e:\n        st.error( f'An error occurred.' )\n         # ...exception handling code Wrapping Up This app is the foundation, a springboard for more complex visual-language applications. The potential is limitless, and your insights can shape its future. I invite you to dive into the demo, tinker with the code, and join me in pushing the envelope of what AI can do. Link to GitHub Repo Connect with Me on LinkedIn LinkedIn Post :", "author": "Harshad Suryawanshi", "date": "Nov 8, 2023", "tags": ["Llamaindex", "ChatGPT", "Google Palm", "Kosmos", "Streamlit"]},
{"title": "How I built the Streamlit LLM Hackathon winning app — FinSight using LlamaIndex.", "content": "In this article, we’ll dive deep into the world of LLM app development and take a closer look at my journey of building the Streamlit LLM hackathon-winning app  FinSight — Financial Insights At Your Fingertips . This article covers the entire process from ideation to execution, along with code snippets and snapshots. \n Introduction \n A use case for LLMs in finance \n \n  One fascinating use case for LLMs in finance is to use them on company annual\n  reports (10-K form). These reports are publicly available information that\n  pretty much every portfolio manager, financial analyst, and shareholder uses\n  regularly to make informed decisions.\n \n \n  However reading, understanding, and assessing these reports, especially for\n  multiple companies can be tedious and time-consuming. Hence, using LLMs on\n  annual reports to extract insights and summarize would solve a lot of problems\n  and save valuable time.\n \n \n  When the\n   Streamlit LLM Hackathon \n  was, announced I thought this was the best time to explore this idea. And\n  that’s how\n   FinSight \n  came into existence.\n \n How does FinSight work? \n \n   \n\n   A small Demonstration \n \n \n  FinSight has two main features called Annual Report Analyzer and Finance\n  Metric Review, but for this blog post, we will be concentrating on the former.\n \n \n  Annual Report Analyzer is a RAG(Retrieval Augmented Generation) based feature,\n  which means that the LLM will be generating insights based on the information\n  in a knowledge base (which in this case is a company’s annual report). Here’s\n  how it works behind the scenes:\n \n\n \n   \n   RAG pipeline for Annual Report Analyzer \n \n \n  While this is a basic representation of the architecture, we will be doing a\n  deep dive into the importance of each of these components and how they work.\n \n Setup \n \n  In case you want to refer the code to the app:\n   Repo \n \n \n  We will use\n   LlamaIndex \n  to build the knowledge base and to query it using an LLM (gpt-4 is the best\n  suited). LlamaIndex is a simple, flexible data framework for connecting custom\n  data sources to large language models.\n \n \n  For the front end,\n   Streamlit \n  is the most convenient tool to build and share web apps.\n \n \n   Clone Repository \n \n git clone https://github.com/vishwasg217/finsight.git cd finsight \n 2. Setup Virtual Environment \n # For macOS and Linux: python3 -m venv venv # For Windows: python -m venv venv \n 3. Activate Virtual Environment \n # For macOS and Linux: source venv/bin/activate # For Windows: .\\venv\\Scripts\\activate \n 4. Install Required Dependencies: \n pip install -r requirements.txt \n 5. Set up the Environment Variables: \n # create directory mkdir .streamlit # create toml file touch .streamlit/secrets.toml \n \n  You can get your API keys here:\n   AlphaVantage ,\n   OpenAI ,\n \n # Add the following API keys av_api_key = \"ALPHA_VANTAGE API KEY\" openai_api_key = \"OPEN AI API KEY\" \n Document Loading, Indexing, and Storage \n \n  Although LlamaIndex has its own set of data connectors to read PDFs, we still\n  need to write a small function\n   process_pdf() to load the PDFs since we are doing it through\n  Streamlit.\n \n from pypdf import PdfReader from llama_index.schema import Document def process_pdf(pdf):     file = PdfReader(pdf)     text = \"\"     for page in file.pages:         text += str(page.extract_text())              doc = Document(text=text)     return [doc] \n \n  The next step is to ingest, index, and store this document in a vector\n  database. In this case, we will use FAISS DB, as we require in an in-memory\n  vector database. FAISS is also very convenient to use. Hence, we write a\n  function called\n   get_vector_index() to do exactly that.\n \n \n  In case you’re interested in checking out other vector DB options, you read\n  can\n   this .\n \n from llama_index.llms import OpenAI from llama_index import VectorStoreIndex, ServiceContext, StorageContext from llama_index.vector_stores import FaissVectorStore def get_vector_index(documents):     llm = OpenAI(OPENAI_API_KEY)     faiss_index = faiss.IndexFlatL2(d)     vector_store = FaissVectorStore(faiss_index=faiss_index)     storage_context = StorageContext.from_defaults(vector_store=vector_store)     service_context = ServiceContext.from_defaults(llm=llm)      index = VectorStoreIndex.from_documents(documents,          service_context=service_context,         storage_context=storage_context     )         return index \n \n   ServiceContext()  and  StorageContext()  are used to\n  set the configurations for the vector store. Using\n   VectorStoreIndex.from_documents() \n  we ingest, index, and store the document as vector embeddings in the FAISS DB.\n \n # Calling the functions through streamlit frontend import streamlit as st if \"index\" not in st.session_state:   st.session_state.index = None if \"process_doc\" not in st.session_state:         st.session_state.process_doc = False if st.sidebar.button(\"Process Document\"):         with st.spinner(\"Processing Document...\"):             documents = process_pdf(pdfs)             st.session_state.index = get_vector_index(documents)             st.session_state.process_doc = True   st.toast(\"Document Processsed!\") \n Query Tools and Engines \n \n  Now that we have our knowledge base ready, it’s time to build a mechanism to\n  query it.\n \n index = get_vector_index(documents) engine = index.as_query_engine() query = \"How has Microsoft performed in this fiscal year?\" response = engine(query) \n \n  Ideally, the above code should have been enough to query and synthesize a\n  response from the information in the vector DB. However, the response wouldn't\n  be comprehensive and detailed enough, especially for such open-ended\n  questions. We need to develop a better mechanism that allows us to break down\n  a query into more detailed questions and retrieve context from multiple parts\n  of the vector DB.\n \n def get_query_engine(engine):     query_engine_tools = [         QueryEngineTool(             query_engine=engine,             metadata=ToolMetadata(                 name=\"Annual Report\",                 description=f\"Provides information about the company from its annual report.\",             ),         ),     ]     s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)     return s_engine index = get_vector_index(documents) engine = index.as_query_engine() s_engine = get_query_engine(engine) \n \n  Let’s break the above function down. The\n   QueryEngineTool  module wraps around the  engine  and\n  helps provide context and metadata to the engine. This is especially useful\n  when you have more than one engine and you want to provide context to the LLM\n  as to which one to use for a given query.\n \n Here’s what that would look like: \n # example for multiple query engine tools query_engine_tools = [     QueryEngineTool(         query_engine=sept_engine,         metadata=ToolMetadata(             name=\"sept_22\",             description=\"Provides information about Uber quarterly financials ending September 2022\",         ),     ),     QueryEngineTool(         query_engine=june_engine,         metadata=ToolMetadata(             name=\"june_22\",             description=\"Provides information about Uber quarterly financials ending June 2022\",         ),     ) ] \n \n  You can read more about the tools available in LlamaIndex\n   here .\n \n However, we’re currently sticking to just one QueryEnginerTool for now. \n \n  The  SubQuestionQueryEngine  module breaks down a complex query\n  into many sub-questions and their target query engine for execution. After\n  executing all sub-questions, all responses are gathered and sent to a response\n  synthesizer to produce the final response. Using this module is essential\n  because generating insights from annual reports requires complex queries that\n  need to retrieve information from multiple nodes within the vector DB.\n \n\n \n   \n   SubQuestionQueryEngine at work \n \n\n Prompt Engineering \n \n  Prompt engineering is essential to the entire process mainly for two reasons:\n \n \n   \n    To provide clarity to the agent as to what it needs to retrieve from the\n    vector DB by writing precise and relevant queries\n   \n   \n    And then control the quality of the output generated from the retrieved\n    context by providing a structure and description for the output to be\n    generated.\n   \n \n \n  Both these points are handled by using\n   PromptTemplate  and  PydanticOutputParser  module in\n   langchain  .\n \n \n  Using the  PydanticOutputParser  we write the description for the\n  different sections of the insights to be generated. After having a few\n  conversations with finance experts, I concluded generating insights for these\n  4 sections: different sections: Fiscal Year Highlights, Strategic Outlook and\n  Future Direction, Risk Management, Innovation and R&D. Now let’s write the\n   pydantic  class for these sections:\n \n from pydantic import BaseModel, Field class FiscalYearHighlights(BaseModel):     performance_highlights: str = Field(..., description=\"Key performance metrics and financial stats over the fiscal year.\")     major_events: str = Field(..., description=\"Highlight of significant events, acquisitions, or strategic shifts that occurred during the year.\")     challenges_encountered: str = Field(..., description=\"Challenges the company faced during the year and, if and how they managed or overcame them.\") class StrategyOutlookFutureDirection(BaseModel):     strategic_initiatives: str = Field(..., description=\"The company's primary objectives and growth strategies for the upcoming years.\")     market_outlook: str = Field(..., description=\"Insights into the broader market, competitive landscape, and industry trends the company anticipates.\") class RiskManagement(BaseModel):     risk_factors: str = Field(..., description=\"Primary risks the company acknowledges.\")     risk_mitigation: str = Field(..., description=\"Strategies for managing these risks.\") class InnovationRnD(BaseModel):     r_and_d_activities: str = Field(..., description=\"Overview of the company's focus on research and development, major achievements, or breakthroughs.\")     innovation_focus: str = Field(..., description=\"Mention of new technologies, patents, or areas of research the company is diving into.\") \n \n   Note: These sections and their description are for generic use cases. They\n    can be changed to suit your particular needs. \n \n \n  These pydantic classes will provide the format and description for each\n  section to the prompt. So let’s write a function that allows us to plug in any\n  pydantic class to a prompt:\n \n from langchain.prompts import PromptTemplate from langchain.output_parsers import PydanticOutputParser prompt_template = \"\"\" You are given the task of generating insights for {section} from the annual report of the company.  Given below is the output format, which has the subsections. Must use bullet points. Always use $ symbol for money values, and round it off to millions or billions accordingly Incase you don't have enough info you can just write: No information available --- {output_format} --- \"\"\" def report_insights(engine, section, pydantic_model):     parser = PydanticOutputParser(pydantic_object=pydantic_model)     prompt_template = PromptTemplate(         template=prompt_template,         input_variables=[\"section\"],         partial_variables={\"output_format\": parser.get_format_instructions()}     )     formatted_input = prompt_template.format(section=section)     response = engine.query(formatted_input)     parsed_response = parser.parse(response.response)     return parsed_response \n \n   PromptTemplate  plugs in all the values such as\n   section  and  output_format  into the prompt template.\n   PydanticOutputParser  converts the pydantic class into a format\n  that is readable to the LLM. The response generated will be in string format,\n  hence we use the  parser.parse()  function to parse the response\n  and get a structured output.\n \n # calling the function in streamlit frontend if st.session_state.process_doc:     if st.button(\"Analyze Report\"):         engine = get_query_engine(st.session_state.index.as_query_engine(similarity_top_k=3))         with st.status(\"**Analyzing Report...**\"):             st.write(\"Fiscal Year Highlights...\")             st.session_state.fiscal_year_highlights = report_insights(engine, \"Fiscal Year Highlights\", FiscalYearHighlights)             st.write(\"Strategy Outlook and Future Direction...\")             st.session_state.strategy_outlook_future_direction = report_insights(engine, \"Strategy Outlook and Future Direction\", StrategyOutlookFutureDirection)             st.write(\"Risk Management...\")             st.session_state.risk_management = report_insights(engine, \"Risk Management\", RiskManagement)                          st.write(\"Innovation and R&D...\")             st.session_state.innovation_and_rd = report_insights(engine, \"Innovation and R&D\", InnovationRnD) # displaying the generated insights   if st.session_state.fiscal_year_highlights:                  with tab1:             st.write(\"## Fiscal Year Highlights\")             st.write(\"### Performance Highlights\")             st.write(st.session_state.fiscal_year_highlights.performance_highlights)             st.write(\"### Major Events\")             st.write(st.session_state.fiscal_year_highlights.major_events)             st.write(\"### Challenges Encountered\")             st.write(st.session_state.fiscal_year_highlights.challenges_encountered)             st.write(\"### Milestone Achievements\")             st.write(str(st.session_state.fiscal_year_highlights.milestone_achievements))     if st.session_state.strategy_outlook_future_direction:         with tab2:             st.write(\"## Strategy Outlook and Future Direction\")             st.write(\"### Strategic Initiatives\")             st.write(st.session_state.strategy_outlook_future_direction.strategic_initiatives)             st.write(\"### Market Outlook\")             st.write(st.session_state.strategy_outlook_future_direction.market_outlook)             st.write(\"### Product Roadmap\")             st.write(st.session_state.strategy_outlook_future_direction.product_roadmap)     if st.session_state.risk_management:         with tab3:             st.write(\"## Risk Management\")             st.write(\"### Risk Factors\")             st.write(st.session_state.risk_management.risk_factors)             st.write(\"### Risk Mitigation\")             st.write(st.session_state.risk_management.risk_mitigation)     if st.session_state.innovation_and_rd:         with tab4:             st.write(\"## Innovation and R&D\")             st.write(\"### R&D Activities\")             st.write(st.session_state.innovation_and_rd.r_and_d_activities)             st.write(\"### Innovation Focus\")             st.write(st.session_state.innovation_and_rd.innovation_focus) \n \n  You can find the complete code Annual Report Analyzer\n   here \n \n Upcoming Features \n \n   \n    Select and Store Insights: I’ve been working on a feature that allows the\n    user to select any insight needed and also save it into the user’s account\n   \n   \n    Adding more profession-specific insights: Currently, the insight works well\n    for generic purposes. However, different professions use annual reports\n    differently, so naturally I need to create a different set of insights based\n    on the user’s use case.\n   \n   \n     PandasQueryEngine  Module for querying financial statements:\n    Using this module, the LLM will be able to extract better insights from\n    financial statements which are typically in a structured format.\n   \n \n Conclusion \n \n  In summary, FinSight’s Annual Report Analyzer makes financial analysis easier\n  and more insightful by harnessing the power of LLMs. It’s a valuable tool for\n  portfolio managers, financial analysts, and shareholders, saving time and\n  improving decision-making. While the core pipeline remains consistent, note\n  that our deployed app code might evolve to incorporate upgrades and enhanced\n  features, ensuring ongoing improvements.\n \n \n  Big thanks to\n   LlamaIndex \n  for helping me make FinSight a reality. No other framework is as advanced in\n  making RAG-based tools.\n \n \n  If you like what you’ve read, please do leave a clap for me, and also show\n  some love to\n   FinSight . You can check out the GitHub repo\n   here .\n \n \n  You connect with me on\n   LinkedIn \n  and\n   Twitter", "author": "Vishwas Gowda", "date": "Oct 17, 2023", "tags": []},
{"title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex", "content": "Introduction: Achieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is heavily dependent on robust retrieval performance. As we explored in our previous  blog post , rerankers have a significant impact on boosting retrieval performance. But what if we could take it a step further? What if our reranker was not just any reranker, but one tuned specifically to our domain or dataset? Could this specialization enhance the retrieval performance even more? To answer these questions, we turn to CohereAI’s beta release of fine-tuning reranker(Custom reranker) models. By integrating these with LlamaIndex, we now offer the ability to build your very own Cohere custom reranker using our streamlined process. In this blog post, we’ll guide you through the steps to create a Cohere custom reranker with LlamaIndex and evaluate the retrieval performance. For a hands-on walkthrough, you can follow the tutorial on  Google Colab Notebook . Let’s start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex. N OTE: This is a guide for fine-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. We suggest experimenting with your dataset and various parameters before deciding to incorporate it into your RAG pipeline. Setting Up the Environment !pip install llama-index cohere pypdf Setting Up the Keys openai_api_key = 'YOUR OPENAI API KEY'\ncohere_api_key = 'YOUR COHEREAI API KEY'\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\nos.environ[\"COHERE_API_KEY\"] = cohere_api_key Download the Data We will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC Filings for evaluation. !mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf' Load the Data lyft_docs = SimpleDirectoryReader(input_files=['./data/10k/lyft_2021.pdf']).load_data()\nuber_docs = SimpleDirectoryReader(input_files=['./data/10k/uber_2021.pdf']).load_data() Data Curation Create Nodes. The  documentation  mentions that Query + Relevant Passage/ Query + Hard Negatives should be less than 510 tokens. To accommodate that we limit  chunk_size to 400 tokens. (Each chunk will eventually be treated as a Relevant Passage/ Hard Negative) # Limit chunk size to 400\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=400)\n\n# Create nodes\nlyft_nodes = node_parser.get_nodes_from_documents(lyft_docs)\nuber_nodes = node_parser.get_nodes_from_documents(uber_docs) We will use gpt-4 to create questions from chunks. llm = OpenAI(api_key=openai_api_key, temperature=0, model='gpt-4') Prompt to generate questions from each Node/ chunk. # Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\" It expects a minimum of 256 (Query + Relevant passage) pairs with or without hard negatives for training and 64 pairs for validation. Please note that the validation is optional. Training:  We use the first 256 nodes from Lyft for creating training pairs. Validation:  We will use the next 64 nodes from Lyft for validation. Testing:  We will use the first 150 nodes from Uber. # Training dataset \nqa_dataset_lyft_train = generate_question_context_pairs(\n    lyft_nodes[: 256 ], llm=llm, num_questions_per_chunk= 1 , qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n # Save [Optional] \nqa_dataset_lyft_train.save_json( \"lyft_train_dataset.json\" )\n\n # Validation dataset \nqa_dataset_lyft_val = generate_question_context_pairs(\n    lyft_nodes[ 257 : 321 ], llm=llm, num_questions_per_chunk= 1 , qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n # Save [Optional] \nqa_dataset_lyft_val.save_json( \"lyft_val_dataset.json\" )\n\n # Testing dataset \nqa_dataset_uber_val = generate_question_context_pairs(\n    uber_nodes[: 150 ], llm=llm, num_questions_per_chunk= 1 , qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n # Save [Optional] \nqa_dataset_uber_val.save_json( \"uber_val_dataset.json\" ) Now that we have compiled questions from each chunk, we will format the data according to the specifications required for training and validation. Data Format and Requirements For both training and validation, it currently accepts data in the format of triplets, every row should have the following query:  This represents the question or target. relevant_passages:  This represents a list of documents or passages that contain information that answers the query. For every query, there must be at least one relevant_passage hard_negatives:  This represents chunks or passages that don’t contain answers for the query. It should be noted that Hard negatives are optional but providing at least ~5 hard negatives will lead to meaningful improvement. You can check the  documentation  for more details. We need to have an embedding model for creating hard negatives with a cosine similarity approach. # Initialize the Cohere embedding model which we use it for creating Hard Negatives.\nembed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n) Let’s create 3 datasets. Dataset without hard negatives. Dataset with hard negatives selected at random. Dataset with hard negatives selected based on cosine similarity. # Train and val datasets without hard negatives.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    finetune_dataset_file_name = \"train.jsonl\"\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    finetune_dataset_file_name = \"val.jsonl\"\n)\n\n# Train and val datasets with hard negatives selected at random.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"train_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"val_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\n# Train and val datasets with hard negatives selected based on cosine similarity.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"train_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"val_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n) Fine-tuning Reranker (Custom Reranker) With our training and validation datasets ready, we’re set to proceed with the training process. Be aware that this training is expected to take approximately 25 to 45 minutes. # Reranker model with 0 hard negatives.\nfinetune_model_no_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train.jsonl\",\n    val_file_name=\"val.jsonl\",\n    model_name=\"lyft_reranker_0_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n    api_key = cohere_api_key\n)\nfinetune_model_no_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected at random\nfinetune_model_random_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_random.jsonl\",\n    val_file_name=\"val_5_random.jsonl\",\n    model_name=\"lyft_reranker_5_random_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_random_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected based on cosine similarity\nfinetune_model_cosine_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_cosine_similarity.jsonl\",\n    val_file_name=\"val_5_cosine_similarity.jsonl\",\n    model_name=\"lyft_reranker_5_cosine_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_cosine_hard_negatives.finetune() Once the jobs are submitted, you can check the training status in the  models  section of the  dashboard . You can check the status of the job in the dashboard and you should see an image something similar to the following one. You then need to get the Cohere Reranker model for testing. reranker_base = CohereRerank(top_n= 5 )\nreranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(\n    top_n= 5 \n)\nreranker_model_5_random = (\n    finetune_model_random_hard_negatives.get_finetuned_model(top_n= 5 )\n)\nreranker_model_5_cosine = (\n    finetune_model_cosine_hard_negatives.get_finetuned_model(top_n= 5 )\n) Testing We will conduct tests on the first 150 nodes from Uber using the following different rerankers. Without Reranker. Cohere Reranker. Fine-tuned reranker (Custom reranker) without hard negatives. Fine-tuned reranker (Custom reranker) with hard negatives selected at random. Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity. Let’s define the rerankers. RERANKERS = {\n    \"WithoutReranker\": \"None\",\n    \"CohereRerank\": reranker_base,\n    \"CohereRerank_0\": reranker_model_0,\n    \"CohereRerank_5_random\": reranker_model_5_random,\n    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n} Create an Index and Retriever for evaluation purposes. # Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\nindex_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)\n\nquery_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n)\n\nservice_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\nservice_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n\nvector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query) Define a function to display the results def   display_results ( embedding_name, reranker_name, eval_results ):\n     \"\"\"Display results from evaluate.\"\"\" \n\n    metric_dicts = []\n     for  eval_result  in  eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[ \"hit_rate\" ].mean()\n    mrr = full_df[ \"mrr\" ].mean()\n\n    metric_df = pd.DataFrame(\n        { \"Embedding\" : [embedding_name],  \"Reranker\" : [reranker_name],  \"hit_rate\" : [hit_rate],  \"mrr\" : [mrr]}\n    )\n\n     return  metric_df Loop over different rerankers and evaluate retrieval performance using Custom Retriever. results_df = pd.DataFrame()\n\nembed_name =  'CohereEmbedding' \n\n # Loop over rerankers \n for  rerank_name, reranker  in  RERANKERS.items():\n\n     print ( f\"Running Evaluation for Reranker:  {rerank_name} \" )\n\n     # Define Retriever \n     class   CustomRetriever ( BaseRetriever ):\n         \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\" \n\n         def   __init__ ( \n            self,\n            vector_retriever: VectorIndexRetriever,\n         ) -&gt;  None :\n             \"\"\"Init params.\"\"\" \n\n            self._vector_retriever = vector_retriever\n\n         def   _retrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n             \"\"\"Retrieve nodes given query.\"\"\" \n\n            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n             if  reranker !=  'None' :\n                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n             else :\n                retrieved_nodes = retrieved_nodes[: 5 ]\n\n             return  retrieved_nodes\n\n         async   def   _aretrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n             \"\"\"Asynchronously retrieve nodes given query.\n            \"\"\" \n             return  self._retrieve(query_bundle)\n\n         async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n             if   isinstance (str_or_query_bundle,  str ):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n             return   await  self._aretrieve(str_or_query_bundle)\n\n    custom_retriever = CustomRetriever(vector_retriever)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n        [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\n    )\n    eval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n\n    current_df = display_results(embed_name, rerank_name, eval_results)\n    results_df = pd.concat([results_df, current_df], ignore_index= True ) Results: From the above table (1- without reranker, 2 — with base cohere reranker, 3–5: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It’s crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker. Summary: In this blog post, we’ve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community’s use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.", "author": "Ravi Theja", "date": "Nov 16, 2023", "tags": ["Llamaindex", "AI", "LLM", "Cohere", "Fine Tuning"]},
{"title": "NVIDIA Research: RAG with Long Context LLMs", "content": "Introduction Why Long Context Matters and How Retrieval Augmentation Steps In: In the dynamic landscape of LLMs, two methods have gained traction and seem to be taking center stage: expanding the context window of Large Language Models (LLMs) and enhancing these models with retrieval capabilities. The continued evolution of GPU technology, coupled with breakthroughs in attention mechanisms, has given rise to long-context LLMs. Simultaneously, the concept of retrieval — where LLMs pick up only the most relevant context from a standalone retriever — promises a revolution in efficiency and speed. In the midst of these evolving narratives, some interesting questions emerge: Retrieval-augmentation versus long context window, which one is better for downstream tasks? Can both methods be combined to get the best of both worlds? To dissect these questions, in this blog post we turn to  NVIDIA’s recent study , which harnesses the power of two powerful LLMs: the proprietary GPT — 43B and LLaMA2–70B, the research strives to provide actionable insights for AI practitioners. Prior Research and the NVIDIA Divergence: Interestingly, while NVIDIA’s findings are interesting in many respects, Another recent work by  Bai et al. (2023)  also ventured into similar territory, although with differing outcomes. Their work explored the impact of retrieval on long context LLMs, evaluating models like GPT-3.5-Turbo-16k and Llama2–7B-chat-4k. However, their findings diverge from NVIDIA’s in crucial ways.  Bai et al.  discerned that retrieval was beneficial only for the Llama2–7B-chat-4k with a 4K context window, but not for extended context models like GPT-3.5-Turbo-16k. One hypothesis for this difference centers on the challenges tied to experiments using black-box APIs and the smaller white-box LLMs they employed, which potentially had limited capability to integrate context through retrieval. NVIDIA’s work distinguishes itself by tapping into much larger LLMs, yielding results that not only match top-tier models like ChatGPT-3.5 but even indicate further enhancements when incorporating retrieval methods. Models, Datasets, and Evaluation Metrics Large Language Models (LLMs) Explored: The researchers delved deep into the potential of large language models for tasks like generative QA and summarization. Specifically, two models were the primary focus: Nemo GPT-43B:  A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings. LLaMA2–70B:  A publicly available 70B parameter model trained on 2T tokens, primarily in English. It’s structured with 80 layers and also utilizes RoPE embeddings. Context Window Extension: To enhance the models’ capability to process longer contexts, their initial 4K context window length was augmented. The GPT-43B was modified to handle 16K, while the LLaMA2–70B was expanded to both 16K and 32K, employing the position interpolation method. Instruction Tuning: To optimize the LLMs for the tasks at hand, instruction tuning was implemented. A diverse dataset blend, comprising sources like Soda, ELI5, FLAN, and others, was created. A consistent format template was adopted for multi-turn dialogue training, and the models were meticulously fine-tuned to accentuate the answer segment. Retrieval Models Tested: Three retrieval systems were put to the test: Dragon:  A state-of-the-art dual encoder model for both supervised and zero-shot information retrieval. Contriever:  Utilizes a basic contrastive learning framework and operates unsupervised. OpenAI embedding:  The latest version was used, accepting a maximum input of 8,191 tokens. The retrieval approach entailed segmenting each document into 300-word sections, encoding both questions and these chunks, and then merging the most pertinent chunks for response generation. Datasets Used for Evaluation: The study employed seven diverse datasets, sourced from the Scroll benchmark and LongBench. A snapshot of these datasets includes: QMSum:  A query-based summarization dataset, QMSum consists of transcripts from diverse meetings and their corresponding summaries, built upon contextual queries. Qasper:  A question-answering dataset centered on NLP papers, Qasper offers a mix of abstractive, extractive, yes/no, and unanswerable questions from the Semantic Scholar Open Research Corpus. NarrativeQA:  Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources. QuALITY:  A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration. MuSiQue:  Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts. HotpotQA:  Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers. MultiFieldQA-en:  Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students. Evaluation Metrics: The research team used a wide range of metrics suited to each dataset. The geometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY, and F1 scores for others were the primary metrics. Results Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated. With retrieval, performance for 4K models like LLaMA2–70B-4K and GPT-43B-4K significantly improved. HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models. Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks. There exists a unique “U-shaped” performance curve for LLMs due to the  lost in the middle  phenomenon, making them better at utilizing information at the beginning or end of the input. The study presents a contrasting perspective to LongBench’s findings, emphasizing that retrieval is beneficial for models regardless of their context window size. Comparing to OpenAI Models: The LLaMA2–70B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks. Comparison of Different Retrievers: Retrieval consistently enhances the performance across different retrievers. Public retrievers outperformed proprietary ones like OpenAI embeddings. Comparing with the number of retrieved chunks: The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn’t offer additional benefits and can even degrade performance. The deterioration in performance when adding more chunks could be due to the  lost-in-the-middle  phenomenon or the model being sidetracked by non-relevant information. Conclusion As we delved deep into understanding how retrieval augmentation and long-context extension interact when applied to leading language models fine-tuned for long-context question-answering and summarization tasks. Here are some things to be noted: Boost in Performance with Retrieval : Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts. Efficiency of 4K Models with Retrieval : 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process. Best Model Performance : After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2–70B-32k-ret (LLaMA2–70B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003. References: Retrieval meets long context, large language models. Longbench: A bilingual, multitask benchmark for long context understanding. We trust that this blog post on the review of the paper on retrieval augmentation with long-context LLMs has furnished you with meaningful insights. We’re keen to hear if your experiments align with our findings or present new perspectives — divergent results always make for interesting discussions and further exploration.", "author": "Ravi Theja", "date": "Oct 22, 2023", "tags": ["Llamaindex", "LLM", "Nvidia", "Research Paper", "Llama 2"]},
{"title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.", "content": "Introduction GPT-4V has amazed us with its ability to analyze images and even generate website code from visuals. This blog post investigates GPT-4V’s proficiency in interpreting bar charts, scatter plots, and tables. We aim to assess whether specific questioning and chain of thought prompting can yield better responses compared to broader inquiries. Our demonstration seeks to determine if GPT-4V can exceed these known limitations with precise questioning and systematic reasoning techniques. We observed in these experiments that asking specific questions, rather than general ones, yields better answers. Let’s delve into these experiments. You can also follow along with this blog post in our  Google Colab Notebook . N OTE: This blog post aims to inform the community about GPT-4V’s performance, though the results might not be universally applicable. We strongly advise conducting tests with similar questions on your own dataset before drawing conclusions. Experiment: What we’ll do is take an image of each of a bar chart, scatter plot, and table and analyze them by asking three types of questions. General Question: Simply ask, “Analyse the image.” Specific Question: Specific Question the performance of a certain category by providing more details. Chain of Thought Prompting: Use a step-by-step reasoning method to walk through the analysis. These guidelines aim to test how different questioning techniques might improve the precision of the information we gather from the images. Data for experiments: To test out we selected the following images from  Llama2  and  MistralAI  papers. Violation percentage of safety with different LLMs across categories. (Llama2 paper) Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper) Performances of different LLMs across various NLP tasks. (Llama2 paper) Let’s go through each of these data points one by one. Image 1 — Violation percentage of safety with different LLMs across categories. General Question: Question:  Analyse the Image. Answer: The image you’ve provided is a bar chart displaying data grouped into three categories of online violations: hateful and harmful, illicit and criminal activity, and unqualified advice. Each of these categories has a number in parentheses, indicating the sample size of reported incidents for each type of violation (395, 728, and 311, respectively). The x-axis of the chart specifies various social media platforms or services, such as “Video sharing”, “Social networking”, “Gaming”, “Dating”, “Forums & boards”, “Commercial Websites”, “Media sharing”, “P2P/File sharing”, “Wiki”, and “Other”. It appears to measure how often these categories of violations occur on each type of platform. The y-axis measures the percentage of violations reported, ranging from 0% to 60%. Each platform/service has three bars corresponding to the three violation categories, showing their respective percentages. The bars also have error bars, which typically represent the variability of the data, such as standard deviation, standard error, or confidence interval, indicating the precision of the estimates. The chart has a legend indicating the color corresponding to each of the three categories of violations. This visual representation helps to compare the prevalence of different types of violations across the different types of online services. Without specific numbers, it’s not possible to provide exact percentages, but we can observe trends, such as: — “Forums & boards” and “Social networking” platforms have notably higher percentages across all three types of violations compared to other platforms. — “Commercial Websites” seem to have lower percentages of reported hateful and harmful activities and unqualified advice but higher percentages of illicit and criminal activities when compared to other platforms like “Wiki” or “P2P/File sharing”. — “Gaming” appears to have a moderate percentage of hateful and harmful violations, lower levels of illicit and criminal activity, and relatively higher levels of unqualified advice. Overall, this chart is a visual tool that likely aims to inform about the prevalence of certain types of online violations across different digital platforms. Observation: As you can see though the categories hateful and harmful, illicit and criminal activity, and unqualified advice but it hallucinated with x-axis values with — “Video sharing”, “Social networking”, “Gaming”, “Dating”, “Forums & boards”, “Commercial Websites”, “Media sharing”, “P2P/File sharing”, “Wiki”, and “Other”. Specific Questions: Question:  Which model among llama2 and vicuna models does better in terms of violation percentages in Hateful and harmful category. Answer: From the provided bar chart, we can analyze the violation percentage for both the Llama2 and Vicuna models in the “Hateful and harmful” category. To determine which model does better (i.e., has a lower violation percentage), you would look at the height of the bars corresponding to each model within that category. In the “Hateful and harmful” category (which is the first group of bars on the left), you can compare the blue bar (representing Llama2) to the light blue bar (representing Vicuna). The model with the shorter bar in this category will have a lower violation percentage, and hence, better performance with respect to minimizing hateful and harmful content. Please note that I cannot physically see the chart, so I’m unable to specify which model has the lower violation percentage. If you provide the actual percentages or describe the relative bar lengths for Llama2 and Vicuna in the “Hateful and harmful” category, I could then tell you which model performs better in that respect. Observation: It failed to accurately capture the information, mistakenly identifying the light blue bar as representing Vicuna when, in fact, it is the light blue bar that represents Llama2. Let us give more information on the question and see if it can improve the answer. Question:  In the image provided to you depicts about the violation rate performance of various AI models across Hateful and harmful, Illicit and criminal activity, Unqualified advice categories. Hateful and harmful category is in first column. Bars with light blue are with Llama2 model and dark blue are with Vicuna models. With this information, Can you compare about Llama2 and Vicuna models in Hateful and harmful category. Answer: In the image you provided, which shows a bar graph for the violation rate performance of various AI models in the Hateful and harmful category, we can analyze the light blue bars that represent the Llama2 model and the dark blue bars that represent the Vicuna model. Based on the visual information given, I can compare the two models within this category by observing the heights of the light blue bars (Llama2) versus the heights of the dark blue bars (Vicuna) for each subsection within the category. A lower bar indicates a lower violation rate, which could be interpreted as better performance in minimizing violations for the given criteria. Since I can’t give you the exact numbers or percentages, I would describe their relative performances. It appears that for some subsections, the Llama2 bars are shorter than the Vicuna bars, suggesting that the Llama2 model could have a lower violation rate in those areas. Conversely, in other subsections, the Vicuna bars might be shorter than the Llama2 bars, indicating a lower violation rate for the Vicuna model in those areas. The exact subsections where one model outperforms the other would depend on their relative bar heights, which should be compared individually for the given information. Observation: It did answer the question correctly. Chain Of Thought Prompting: Question:  Based on the image provided. Follow the steps and answer the query — which model among llama2 and vicuna does better in terms of violation percentages in ‘Hateful and harmful’. Examine the Image: Look at the mentioned category in the query in the Image. Identify Relevant Data: Note the violation percentages. Evaluate: Compare if there is any comparison required as per the query. Draw a Conclusion: Now draw the conclusion based on the whole data. Answer: Examine the Image: The image is a bar chart depicting violation percentages in three categories: “Hateful and harmful,” “Illicit and criminal activity,” and “Unqualified advice.” Each category has two bars next to each other representing two models named “llama2” and “vicuna.” Identify Relevant Data: We are specifically interested in the “Hateful and harmful” category, which is the first set of bars on the left. The two bars indicate the violation percentages for “llama2” and “vicuna.” Evaluate: By visually inspecting the bar corresponding to “Hateful and harmful,” we see that the blue bar (indicating “llama2”) has a lower percentage than the red bar (indicating “vicuna”). The exact percentages are not clear, but we can discern the relative performance between the two. Draw a Conclusion: From the data visible, “llama2” has a lower violation percentage than “vicuna” in the “Hateful and harmful” category, thereby doing better in that respect according to the given image. Observation: With chain of thought prompting it did hallicunate with bar colours but answered correctly saying Llama2 has lower violation compared to vicuna in Hateful and harmful though for a section Llama2 has higher violation compared to vicuna. Image2 — Llama2 vs Mistral model performances across various NLP tasks. General Question: Question:  Analyze the Image. Answer: The image appears to be a set of four line graphs that compare the performance of different natural language processing (NLP) models across four different metrics: Natural Language Understanding (NLU), Reasoning, Knowledge, and Commonsense. The models compared are LLaMA 2, LLaMA 13B, Mistral, and GPT-3. Each graph plots the performance metric (on the y-axis) against the model size measured in billions of effective parameters (on the x-axis). In general, these plots demonstrate that as the model size increases, the performance on each metric improves. Here are specific observations for each graph: 1. Natural Language Understanding (Top left): All models show an increase in NLU performance as the model size increases. Mistral is depicted as the highest-performing model at each size benchmark. 2. Reasoning (Top right): Again, larger models perform better on reasoning tasks, with Mistral consistently outperforming the other models mentioned. 3. Knowledge (Bottom left): Similar to the NLU and reasoning tasks, the trend is for better performance with larger models, with Mistral leading across the different sizes. 4. Commonsense (Bottom right): This graph follows the same trend with the Mistral model performing better at each size level compared to the other models. Based on these graphs, Mistral appears to be the top-performing model in all four metrics, suggesting that it might be more efficient or effective than LLaMA 2, LLaMA 13B, and GPT-3, at least within the scope of the parameters and tasks represented here. On a technical note, it’s important to mention that the y-axis shows different scales for the metrics (ranging from 45–75% for NLU, Knowledge, and Commonsense, and from 50–72% for Reasoning), which suggests that these tasks might have different levels of difficulty or different benchmarks for success. Observation: It did answer the query but hallucinated with the NLU task which is the MMLU task and assumed Mistral is available across all different model parameters. Specific Questions: Question:  How well does mistral model compared to llama2 model? Answer: The image you’ve provided contains four graphs, each plotting different performance metrics for evaluating language models. It compares two models: Mistral and LLaMA2 across four different aspects — MMLU (Multi-tasking multiple-choice), Reasoning, Knowledge, and Commonsense. In each graph, the performance metric is plotted on the y-axis while the effective model size in billion parameters is plotted on the x-axis. Here’s a summary based on the trends observed in the graphs: 1. **MMLU (Top Left Graph)**: LLaMA2 shows a steeper performance curve as the model size increases, starting at around 61% for the smallest size and reaching upwards of 66% for the largest model. Mistral also shows performance gains with model size but plateaus earlier, reaching a level just shy of 62%. 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement curve, starting from about 61% and surpassing 72% for the largest model. Mistral, while improving, seems to plateau near 70% for the largest model. 3. **Knowledge (Bottom Left Graph)**: This graph reflects a similar trend to the previous ones, with LLaMA2 beginning at a lower performance around 46% and eclipsing 52%. Mistral starts higher at around 48% and appears to plateau near 52%. 4. **Commonsense (Bottom Right Graph)**: Here, LLaMA2 starts its performance at approximately 62% and reaches just above 66%. Mistral seems to start at a slightly higher point than LLaMA2 but ends at a similar level to LLaMA2’s largest model. Overall, the LLaMA2 model appears to show a greater degree of improvement in performance as the model size increases compared to Mistral across these metrics. Meanwhile, Mistral starts at a higher performance for some metrics but tends to plateau earlier, suggesting that LLaMA2 may scale better with size in terms of performance gains. Observation: Incorrect answers and percentages are not accurate enough and again assumed mistral is available across all parameter models. Question:  Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model? Answer: This image appears to be a set of four graphs, each comparing the performance of three different language models on various tasks: Mistral, LLaMA-2, and a hypothetical “Size 7B” model. The graphs are labeled with task-specific performance metrics: “MLM U”, “Reasoning”, “Knowledge”, and “Commonsense”. The x-axes on the graphs represent model size in terms of the number of parameters, with three points that likely correspond to the sizes of the models being compared. The y-axes represent performance as a percentage, which could mean accuracy, precision, recall, or another relevant performance metric depending on the specific task. The graphs appear to show that Mistral performs better than LLaMA-2 and the Size 7B model across all metrics. This indicates that within the context of these measurements and tasks, Mistral is a stronger model. The exact nature of the tasks or what “MLM U”, “Reasoning”, “Knowledge”, and “Commonsense” specifically refer to are not detailed in the image, but they likely correspond to standard NLP tasks designed to test understanding of language, ability to reason, knowledge recall, and commonsense reasoning, respectively. The performance improvements are depicted as increasingly significant with larger model sizes. Observation: Now by giving the detail that mistral is available in 7B series, it is able to answer correctly. Chain of thought prompting: Question:  Based on the image provided. Follow the steps and answer the query — Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?. Examine the Image: Look at the mentioned category in the query in the Image. Identify Relevant Data: Note the respective percentages. Evaluate: Compare if there is any comparision required as per the query. Draw a Conclusion: Now draw the conclusion based on the whole data. Answer: Examine the Image: The image contains four graphs, each graph compares the performance of three different models — Llama 2, Mistral, and an unnamed third model — across different metrics: mAP@L (mean Average Precision at L), Reason@L (presumably a reasoning score at L), Knowledge@L, and Comprehension@L. Each graph shows performance as a function of model size (in terms of billion parameters). Identify Relevant Data: We need to focus on the Mistral and Llama 2 models across all four graphs to extract the relevant data. For mAP@L: — Llama 2 reaches above 65% when reaching 70 billion parameters. — Mistral starts at around 5 billion parameters with about 48% and reaches almost 65% by the time it gets to 34 billion parameters. For Reason@L: — Llama 2 reaches about 68% when reaching 70 billion parameters. — Mistral starts at around 5 billion parameters with about 52% and reaches almost 68% by the time it gets to 34 billion parameters. For Knowledge@L: — Llama 2 starts at about 45% with 1.3 billion parameters and reaches just slightly under 60% by 70 billion parameters. — Mistral is not plotted in this graph, so no direct comparison can be made. For Comprehension@L: — Llama 2 starts just above 40% with 1.3 billion parameters and reaches just under 60% by 70 billion parameters. — Mistral starts around 13 billion parameters with about 52% and continues above 62% by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral and Llama 2 are very comparable in performance in the metrics where both are shown. However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that particular metric. Draw a Conclusion: In the graphs where both models can be compared (mAP@L, Reason@L, and Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching similar levels of performance at smaller parameter sizes relative to the plotted sizes for Llama 2. This suggests that Mistral could be more parameter-efficient. However, since there is no data for Mistral in the Knowledge@L graph, we cannot draw a complete comparison Observation: There is a hallucination with a number of model parameters and percentage points though the final conclusion is partially correct. Image 3 — Performances of different LLMs across various NLP tasks. General Question: Question:  Analyze the Image. Answer: The image appears to be a table containing numerical data, which seems to be a comparative analysis of various models across multiple parameters or tests. Each row represents a different model or configuration as indicated by names such as “MPT” and “Falcon” followed by a size specification like “70B,” “7B,” or some other parameter. The columns are labeled with test names or evaluation metrics, such as “Size,” “Avg,” “AQuA-RAT,” “LogiQA,” “L-SAT-AR,” “L-SAT-IC,” “L-SAT-RC,” “SAT-en (w/o Psg.),” and “SAT-math.” The data is likely related to performance scores of these models on these tests, where higher numbers probably indicate better performance. Without additional context, it is difficult to provide a comprehensive analysis of this data, but it seems clear that it is intended to provide a performance comparison between different models on various tasks, possibly in the field of machine learning or artificial intelligence evaluations, where such models are typically assessed on reasoning, comprehension, or problem-solving capabilities. The “70B” and “7B” demarcations could refer to the size of the model in terms of the number of parameters, commonly used in assessing language models. A deeper evaluation would require further insights into the specific nature of these tests and models, along with the intended use-case for which they were being compared. Observation: It did not analyze the image specifically but understood the overall data present in the image to some extent. Specific Questions: Question:  which model has higher performance in SAT-en? Answer: In the table you’ve provided, the models are compared based on their performance on several benchmarks, including SAT-en (SAT Analogies, or “SAT” in the table). To determine which model has higher performance specifically on the SAT-en benchmark, you’ll need to look at the corresponding column. In the SAT-en column (second from the right), the two models with the highest scores are: — LLaMA1 65B: with a score of 57.9 — LLaMA2 70B: with a score of 63.4 Between these two, the LLaMA2 model with 70 billion parameters shows the higher performance on the SAT-en benchmark with a score of 63.4. Observation: It did answer correctly but the numbers are being hallucinated. Question:  which model has higher performance in SAT-en in 7B series models? Answer: The table you’ve provided shows performance benchmark scores for different model sizes across various AI models such as MPT, Falcon, and LLaMA on different tasks, such as Aqua-RAT, LogiQA, LastAR, SAT-en, and SAT-math. For the SAT-en task specifically, you asked which model in the 7B series has the highest performance. To find out, we need to look at the column labeled “SAT-en (w/o Ps_8).” In the 7B series of models, here are the scores: — MPT 7B: 37.1 — Falcon 7B: 37.3 — LLaMA 7B: 63.9 — Model2 7B: 37.4 The LLaMA 7B model outperforms the other 7B models on the SAT-en (w/o Ps_8) task with a score of 63.9. Observation: Incorrect answer and recognized Llama series of models and values incorrectly. Chain of thought prompting: Question:  Based on the image provided. Follow the steps and answer the query — which model has higher performance in SAT-en in 7B series models? Examine the Image: Look at the mentioned category in the query in the Image. Identify Relevant Data: Note the respective percentages. Evaluate: Compare if there is any comparison required as per the query. Draw a Conclusion: Now draw the conclusion based on the whole data. Answer: To answer which model has higher performance in SAT-en in the 7B series models, I will follow the provided steps: Examine the Image: The categories in the image include model names and sizes, and performance scores for various tasks, including the SAT-en category. Identify Relevant Data: The relevant data for the SAT-en category are the percentages listed under it for each 7B series model. Evaluate: I will compare the SAT-en percentages of each 7B series model. Draw a Conclusion: The SAT-en scores for the 7B series models are as follows: — MPT 7B: 63.1% — Falcon 7B: 73.4% — LLama 1 7B: No data present for this category. — LLama 2 7B: 76.6% Based on the data, the LLama 2 7B model has the highest SAT-en performance among the 7B series models with a score of 76.6%. Observation: With chain of thought prompting we are able to get the right conclusion though it should be noted that it picked up wrong values. Final Observations: Observations made based on experiments on Hallucination and correctness. (Please note that these observations are specific to the images used and cannot be generalized, as they vary depending on the images.) Summary In this blog post, we have showcased experiments ranging from general inquiries to systematic questions and chain of thought prompting techniques and observed Hallucination and correctness metrics. However, it should be noted that the outputs from GPT-4V can be somewhat inconsistent, and the levels of hallucination are slightly elevated. Therefore, repeating the same experiment could result in different answers, particularly with generalized questions.", "author": "Ravi Theja", "date": "Nov 17, 2023", "tags": ["Llamaindex", "Gpt 4", "LLM", "Vision", "OpenAI"]},
{"title": "Evaluating Multi-Modal Retrieval-Augmented Generation", "content": "A few days ago, we published a blog on  Multi-Modal RAG  (Retrieval-Augmented Generation) and our latest (still in beta) abstractions to help enable and simplify building them. In this post, we now go over the important topic of how one can sensibly evaluate Multi-Modal RAG systems. A natural starting point is to consider how evaluation was done in traditional, text-only RAG and then ask ourselves how this ought to be modified to suit the multi-modal scenario (e.g., in text-only RAG, we use an LLM, but in multi-modal RAG we require a Large Multi-Modal Model or LMM for short). This is exactly what we’ll do next and as you’ll see, the overarching evaluation framework stays the same as it was in the text-only RAG, requiring only a few additions and modifications in order to make it more multi-modal appropriate. Primer: Multi-Modal RAG vs Text-Only RAG Illustration of text-only RAG versus multi-modal RAG. In multi-modal RAG, images modality can show up in the user query, the retrieved context, as well as the final answer. Let’s consider the main differences between multi-modal and text-only RAG. Below are two tables that describe the RAG build considerations as well as query-time pipeline and compares and contrasts multi-modal and text-only cases against them. Table 1: Build considerations for RAG systems and how they differ text-only versus multi-modal scenarios. Table 2: The pipeline for querying a RAG and how they differ text-only versus multi-modal scenarios. Evaluation Of Text-Only RAG For text-only RAG, the standard approach is to separately consider the evaluation of two stages: Retrieval and Generation. Retriever Evaluation:  are the retrieved documents relevant to the user query? Some of the more popular metrics for retrieval evaluation include  recall, hit rate ,  mean reciprocal rank, mean average precision, and normalized discounted cumulative gain.  The first two of these metrics recall and hit rate, don’t consider the position (or ranking) of the relevant documents, whereas all the others do in their own respective ways. Generator Evaluation:  does the response use the retrieved documents to sufficiently answer the user query? In abstractive question-answering systems, like the kinds we’re talking about in this blog, measuring the generated response is made more tricky due to the fact that there isn’t just one way to sufficiently answer a query in written language — there’s plenty! So, in this case, our measurement relies on subjective judgement, which can be performed by humans, though this is costly and unscalable. An alternative approach, is to use an LLM judge to measure things like  relevancy  and  faithfulness . Relevancy: considers textual context and evaluates how much the generated response matches the query. Faithfulness: evaluates how much the generated response matches the retrieved textual context. For both of these, the retrieved context as well as the query and generated response are passed to the LLM judge. (This pattern of using an LLM to judge the responses has been termed by some researchers of the space as LLM-As-A-Judge ( Zheng et al., 2023 ).) Currently, the  llama-index (v0.9.2)  library supports hit-rate and mean reciprocal rank for retrieval evaluation, as well as relevancy, faithfulness and a few others for generator evaluation. (Check out our Evaluation guides in our  docs !). Evaluation Of Multi-Modal RAG For the multi-modal case, the evaluation can (and should) still be carried out with respect to the different stages of retrieval and generation. Separating Out Retrieval Evaluation For Text and Image Modalities Now that retrieved documents can come in two forms, it would seem most sensible to consider computing the usual retrieval evaluation metrics separately for images and text. In this way, you have more knowledge as to which aspect of the multi-modal retriever is working well and what isn’t. One can then apply a desired weighting scheme to establish a single aggregated retrieval score per metric. Hit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75 Table 3: Retrieval evaluation in multi-modal scenario. Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge) Multi-modal models (i.e., LMMs) like OpenAI’s GPT-4V or open-source alternatives like LLaVA are able to take in both input and image context to produce an answer the user query. As in text-only RAG, we are also concerned about the “relevancy” and “faithfulness” of these generated answers. But in order to be able to compute such metrics in the multi-modal case, we would need a judge model that is also able to take in the context images and text data. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in order to compute relevancy and faithfulness as well as other related metrics! Relevancy (multi-modal): considers  textual and visual context  and evaluates how much the generated response matches the query. Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved  textual and visual context . If you want to test these out, then you’re in luck as we’ve recently released our beta Multi-Modal Evaluator abstractions! See the code snippet below for how one can use these abstractions to perform their respective evaluations on a generated response to a given query. from  llama_index.evaluation.multi_modal  import  (\n\tMultiModalRelevancyEvaluator,\n\tMultiModalFaithfulnessEvaluator\n)\n from  llama_index.multi_modal_llm  import  OpenAIMultiModal\n\nrelevancy_judge = MultiModalRelevancyEvaluator(\n    multi_modal_llm=OpenAIMultiModal(\n        model= \"gpt-4-vision-preview\" ,\n        max_new_tokens= 300 ,\n    )\n)\n\nfaithfulness_judge = MultiModalRelevancyEvaluator(\n    multi_modal_llm=OpenAIMultiModal(\n        model= \"gpt-4-vision-preview\" ,\n        max_new_tokens= 300 ,\n    )\n)\n\n # Generated response to a query and its retrieved context information \nquery = ...\nresponse = ...\ncontexts = ...   # retrieved text contexts \nimage_paths = ...   # retrieved image contexts \n\n # Evaluations \nrelevancy_eval = relevancy_judge.evaluate(\n query=query,\n response=response,\n contexts=contexts,\n image_paths=image_paths\n)\n\nfaithfulness_eval = faithfulness_judge.evaluate(\n query=query,\n response=response,\n contexts=contexts,\n image_paths=image_paths\n) A Few Important Remarks First, it is worth mentioning that using LLMs or LMMs to judge generated responses has its drawbacks. These judges are generative models themselves and can suffer from hallucinations and other inconsistencies. Though studies have shown that strong LLMs can align to human judgments at a relatively high rate ( Zheng et al., 2023 ), using them in production systems should be handled with higher standards of care. At time of writing, there has no been study to show that strong LMMs can also align well to human judgements. Secondly, the evaluation of a generator touches mostly on the evaluation of its knowledge and reasoning capabilities. There are other important dimensions on which to evaluate LLMs and LMMs, including Alignment and Safety — see  Evaluating LMMs: A Comprehensive Survey  for more information. Go forth and evaluate In this post, we covered how evaluation can be performed on multi-modal RAG systems. We believe that separating out the retrieval evaluations per modalities for increased visibility as well as the LMM-As-A-Judge represent a sensible extension of the evaluation framework for text-only RAG. We encourage you to check out our practical notebook guides as well as docs for more information on how you can not only build Multi-Modal RAGs but also adequately evaluate them! Notebook guide for evaluating Multi-Modal RAG systems with LlamaIndex Intro to Multi-Modal RAG Docs/guides on Multi-Modal Abstractions", "author": "LlamaIndex", "date": "Nov 16, 2023", "tags": ["LLM", "AI", "Evaluation", "Multimodal", "Llamaindex"]},
{"title": "LlamaIndex Newsletter 2023–10–31", "content": "Greetings Llama Enthusiasts 🦙! Another week has zoomed past, and here we are with our latest roundup of updates, features, tutorials, and so much more. Have a noteworthy project, article, or video to share? We’d love to feature it! Reach out to us at  news@llamaindex.ai . Want these updates straight to your inbox? Simply subscribe to our newsletter on our  homepage . 🤩  First, the highlights: Revamped Documentation:  Overhauled  docs  for smoother LLM/RAG app development. Contribution Board:  Our new  board  welcomes community-driven LlamaIndex enhancements. Zephyr-7b-beta Insights:   Tested and verified  for unmatched ReAct agent task efficiency on LlamaIndex. Image Captioning Boost For RAG:  LLaVa’s outputs are now supercharged with knowledge-based augmentation.  Notebook ,  Tweet ✨ Feature Releases and Enhancements: We introduced Retrieval-Augmented Image Captioning, enhancing LLaVa multi-modal model outputs with knowledge base insights.  Notebook ,  Tweet . We introduced the ability to view and set prompts for LlamaIndex modules in just two lines of code.  Docs ,  Tweet . We introduced the integration of our  OpenAILike  class, allowing users to tap into various open-source LLM projects with OpenAI-compatible APIs, irrespective of the model provider.  Tweet . We introduced Prompt Compression for RAG: with LongLLMLingua, which helps to cut token usage and latency by up to 20x.  Notebook ,  Tweet . We introduced a method to refine open-source LLMs like llama2 for structured data outputs. Using LlamaIndex, transform llama2–7b to produce Pydantic objects without PyTorch. Our guide covers synthetic dataset creation, fine-tuning, and RAG pipeline integration.  Notebook ,  Tweet . 🎥  Demos: Harshad Suryawanshi did a  demo  on equity research report generator using LlamaIndex and Streamlit. Bharat Ramanathan  built  Wandbot , a live RAG app enabling chat over Weights & Biases documentation, integrated with Discord and SlackHQ. Key features include periodic data ingestion, custom document and code parsing, model fallback, and logging with Weights and biases. 🗺️ Guides: We introduced a revamped documentation structure tailored to guide users from prototyping to production of LLM/RAG apps using LlamaIndex. Dive into our 200+ guides to enhance your app.  Docs ,  Tweet . We unveil our new Request For Contribution Github board  here . It’s your guide to contribute to LlamaIndex, streamlining community suggestions. We released the  guide  on using the Jina 8k open-source text embedding model with LlamaIndex. We introduce our comprehensive survey of llama2-chat models across varying capacities in LlamaIndex. The major insight: While reasoning is enhanced with more parameters, structured outputs remain a challenge.  Tweet . We share a  guide  to test the newly released HuggingFace Zephyr-7b-beta model on LlamaIndex RAG/agent tasks, it stood out as the only 7B LLM capable of handling ReAct agent tasks over data. We share a new  guide  on Advanced Prompt Engineering for RAG. Learn about understanding, customizing, and extending RAG prompts, from QA templates to few-shot examples and context/query transformations.  Tweet . ✍️ Tutorials: Kiran  made a  blog post  on Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser. Wenqi Glantz  gave us an excellent  blog post  on Optimizing Text Embeddings with HuggingFace’s text-embeddings-inference Server and LlamaIndex. Ravi Theja’s   blog post  delves into NVIDIA Research on RAG vs Long Context LLMs, questioning the necessity of RAG in the presence of long-context LLMs. Sudarshan Koirala  has a tutorial on Extracting Tables + Texts from .htm pages for RAG Using LlamaIndex. Wenqi Glantz  also made a second  blog post  on Multimodal Retrieval with Text Embedding and CLIP Image Embedding for Backyard Birds. ⚙️ Integrations & Collaborations: We introduced our new cookbooks in partnership with Gradient AI, enabling effortless fine-tuning of open-source LLMs like Llama 2 and integration into your LlamaIndex RAG pipeline.  Docs ,  Tweet . We introduced integration with HuggingFace Inference API which gives access to over 150,000 models. Now you can plugin any  conversational ,  text_generation ,  feature_extraction  endpoints into your LlamaIndex app.  Docs ,  Tweet . 🎥 Webinars: Mayo Oshin  and  Jerry Liu  gave a  webinar  on Unlocking ChatGPT for Business. 📚Workshops: Jerry Liu and Simon conducted a Multipart LlamaIndex workshop in collaboration with Anyscale. Ravi Theja conducted a day-long  workshop  on Retrieval Augmented Generation with LlamaIndex.", "author": "LlamaIndex", "date": "Oct 31, 2023", "tags": ["LLM", "AI", "Newsletter", "Llamaindex"]},
{"title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression", "content": "In  the RAG, after the retrieval phase, it’s necessary to perform Re-ranking + Fine-Grained Prompt Compression + Subsequence Recovery to enhance LLM’s perception of key information, which is LongLLMLingua. TL;DR: While Retrieval-Augmented Generation (RAG) is highly effective in various scenarios, it still has drawbacks such as 1) Performance drop, like the “Lost in the middle” issue, 2) High costs, both financially and in terms of latency, and 3) Context windows limitation. LongLLMLingua offers a solution to these problems in RAG or Long Context scenarios via prompt compression. It can boost accuracy by as much as 21.4% while only using ¼ of the tokens. In long context situations, it can save $28 for every 1000 examples. See real-world cases on the  project page . We previously wrote a  blog post  introducing the design of  LLMLingua , which started from the perspective of  designing a special language for LLMs . This time, our focus will be on the scenarios involving RAG. Retrieval-Augmented Generation is currently the most reliable and proven technique for creating AI-agents that are grounded on any specific collection of text. Frameworks like  LlamaIndex  provide comprehensive RAG solutions to help users utilize specialized data in LLMs more conveniently. A common misunderstanding is that retrieving as many relevant documents as possible during the RAG process and stitching them together to form a long retrieved prompt is beneficial, especially as more and more LLMs support longer context windows. However, this method can introduce more  noise  into the prompt and  weaken   the LLM’s perception of key information , leading to issues such as ‘ lost in the middle ’[1]. These issues become more apparent in real-world scenarios involving RAG. Better retrieval mechanisms can introduce higher quality noise documents, which can more easily lead to a drop in performance. Re-ranking is an intuitive concept. One intuitive idea is to reposition the most relevant information to the sides of the prompt through re-ranking. This concept of re-ranking has already been implemented in frameworks such as  LlamaIndex  and  LangChain . However, according to our experiments, it’s difficult for an embedding model to serve as a ‘good’ re-ranker. The underlying reason is the lack of an interaction process between the query and the document. The dual-tower structure of embeddings is not suitable for re-ranking in general scenarios, although it may be effective after fine-tuning. Using LLMs directly as a re-ranker may also lead to misjudgments due to  hallucinations . Recently, some re-ranking models have been extended from embedding models, such as  bge-rerank . However, such re-ranking models generally have context window limitations. To address the above issues, we propose a Question-aware Coarse-Grained prompt compression method. This method evaluates the relevance between the context and the question based on the perplexity corresponding to the question. To mitigate the hallucination problem in smaller LLMs, we append a restrictive statement, specifically “ We can get the answer to this question in the given documents ”, after the question to limit the latent space caused by related hallucinations. Figure 1.  The accuracy of different methods for ranking documents from Multi-documemnt QA dataset, which increases from top to bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods. You can find the script in  this link . Results show that this approach significantly outperforms both embedding models and re-ranking models. We’ve added some recently released embedding and reranking models. As you can see, the performance of  bge-rerank-large  is very close to that of  LongLLMLingua . Reranking models generally perform better than embedding models. Currently,  Jina  is the best performing method among the embedding models. Compress unrelated and unimportant information Besides recalling as many relevant documents as possible, another approach is to  compress  irrelevant or unimportant contexts as much as possible. Previous work on long context has focused on how to extend LLMs to support longer context windows. However, almost no work has explored whether this can actually improve the performance of downstream tasks. Some previous studies have shown that the presence of more noise in the prompt, as well as the position of key information in the prompt, can affect the performance of LLMs. From the perspective of prompt compression, Selective Context[2] and LLMLingua[3] estimate the importance of elements by using a small language model to calculate the mutual information or perplexity of the prompt. However, in scenarios like RAG or long context scenarios, this method can easily lose key information because it cannot perceive the question information. In recent submissions to ICLR’24, there have been some similar practices. For example, Recomp[4] reduces the use of tokens in RAG scenarios by jointly training compressors of two different granularities. RAG in Long Context[5] decomposes the long context into a series of chunks and uses retrieval methods for compression, which is actually the retrieval-based method implemented in the LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also designed a hierarchical summarization tree to enhance the LLM’s perception of key information. Question-aware Fine-grained Prompt Compression In order to make token-level prompt compression also perceive the information of the question, we propose a  contrastive perplexity , which compares the difference between the perplexity distribution corresponding to the document and the perplexity distribution corresponding to the document with the question. An intuitive feeling is that when the question serves as context, the perplexity corresponding to the relevant tokens in the document will decrease. This decrease in magnitude represents the importance of the tokens in the document relative to the question. Figure 3.  Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt QA dataset. The document with the ground truth is located on the left side of the dashed line. Figure 3 shows the distribution difference in extracting key tokens between perplexity and contrastive perplexity. How to reduce the loss in the middle Since Coarse-grained Prompt compression far exceeds other retrieval methods in terms of accuracy, it is a very natural idea to use this ranking information to rearrange the documents that are more related to the question to the beginning and end of the prompt. However, through our testing, we found that rearranging to the beginning of the prompt is more effective than evenly distributing at both ends. So, we choose to reorder the most related document to the beginning of the prompt. How to achieve adaptive granular control during compression? In order to better use the information from the two grained compressions, in the fine-grained prompt compression, we dynamically allocate different compression ratios to different documents based on the rank information obtained from the coarse-grained compression, thereby preserving more important information from important documents. How to improve the integrity of key information? Since LongLLMLingua is a token-level prompt compression, it will inevitably delete some tokens of the word, which may result in some retrieval-related tasks not getting complete results. But this can actually be recovered through a simple subsequence matching method. Specifically, there is a subsequence relationship between the original prompt, compressed prompt and response. By establishing the mapping relationship between the response subsequence that appears in the compressed prompt and the subsequence of the original prompt, the original prompt content can be effectively recovered. Experiments To evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in Multi-document QA (RAG) and two long Context benchmarks. Particularly, the dataset chosen for Multi-document QA is very close to the actual RAG scenario (e.g. Bing Chat), where  Contriever  (one of the state-of-the-art retrieval systems) is used to recall 20 relevant documents including one ground-truth. The original documents have a high semantic relevance with the question. As can be seen, compared to Retrieval-based methods and compression-based methods, LongLLMLingua improves performance more in the RAG scenario, and can increase up to 21.4 points at a 4x compression rate, avoiding the original “lost in the middle” situation. The results of the two benchmarks, LongBench and ZeroScrolls, also reached similar conclusions. LongLLMLingua is better at retaining key information related to the question in long context scenarios. Besides, LongLLMLingua is very efficient and can speed up the end-to-end inference process. Used in LlamaIndex Thank  Jerry Liu  for your help with the LongLLMLingua project. Now you can use LongLLMLingua as a  NodePostprocessor  in this widely used RAG framework. For specific usage, you can refer to the  example 1 ,  example 2  and the following code. from  llama_index.query_engine  import  RetrieverQueryEngine\n from  llama_index.response_synthesizers  import  CompactAndRefine\n from  llama_index.indices.postprocessor  import  LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str= \"Given the context, please answer the final question\" ,\n    target_token= 300 ,\n    rank_method= \"longllmlingua\" ,\n    additional_compress_kwargs={\n         \"condition_compare\" :  True ,\n         \"condition_in_question\" :  \"after\" ,\n         \"context_budget\" :  \"+100\" ,\n         \"reorder_context\" :  \"sort\" ,   # enable document reorder \n         \"dynamic_context_compression_ratio\" :  0.4 ,  # enable dynamic compression ratio \n    },\n) References [1] Lost in the Middle: How Language Models Use Long Contexts. Nelson F. Liu etc.\n[2] Compressing Context to Enhance Inference Efficiency of Large Language Models. Yucheng Li etc.\n[3] LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. Huiqiang Jiang, Qianhui Wu etc.\n[4] RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. Fangyuan Xu etc.\n[5] Retrieval meets Long Context Large Language Models. Peng Xu etc.\n[6] Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. Howard Chen etc.", "author": "Huiqiang Jiang", "date": "Nov 6, 2023", "tags": ["Large Language Models", "LLM", "Retrieval", "NLP", "ChatGPT"]},
{"title": "Becoming Proficient in Document Extraction", "content": "Introduction \n \n \n  In the domain of document handling, accurately extracting crucial information\n  from images has posed an enduring obstacle. Despite Optical Character\n  Recognition (OCR) advancements in converting images to editable text, it faces\n  numerous intricacies with diverse document formats and quality. Here enters\n  Zephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\n  addresses these hurdles, heralding a transformative era in image-based\n  document extraction.\n \n \n   \n   \n    Source:\n     Zephyr-llama-index \n   \n \n \n   The OCR Dilemma: Obstacles and Constraints Optical Character \n \n Recognition (OCR), though potent, faces impediments such as: \n \n   \n     Diverse Document Formats : Documents exhibit intricate\n    layouts, fonts, and structures, posing challenges for traditional OCR to\n    precisely interpret and extract information.\n   \n   \n     Quality and Clarity : Images with low resolution,\n    blurriness, or skewed angles hinder OCR’s accuracy in deciphering text.\n   \n   \n     Handwritten and Cursive Content : OCR often struggles with\n    handwritten text or cursive fonts, resulting in errors or incomplete\n    extraction.\n   \n   \n     Multilingual Complexity : Processing documents in multiple\n    languages poses a challenge for OCR systems lacking proficiency in\n    recognizing and extracting varied linguistic content.\n   \n \n \n   \n   Source: Created by Author using MidJourney \n \n \n   Zephyr 7b LLM: Narrowing the Divide \n \n \n  Zephyr 7b LLM revolutionizes the landscape by tackling these inherent\n  constraints of OCR technology:\n \n \n   \n     Advanced Machine Learning Algorithms: \n   \n \n \n  Employing state-of-the-art machine learning algorithms, Zephyr 7b LLM\n  undergoes extensive training with diverse document formats and languages. This\n  equips it to adapt and learn from various document structures, resulting in\n  heightened accuracy and robust extraction capabilities.\n \n \n   2. Contextual Comprehension: \n \n \n  Diverging from conventional OCR, Zephyr 7b LLM doesn’t merely identify\n  individual characters; it comprehends the context in which these characters\n  exist. This contextual understanding significantly reduces errors, ensuring\n  precise extraction even from intricate document layouts.\n \n \n   3. Adaptive Image Processing: \n \n \n  The fusion with LlamaIndex amplifies Zephyr 7b LLM’s ability to handle images\n  of varying resolutions or qualities. Leveraging adaptive image processing\n  techniques, it rectifies distortions, enhances clarity, and optimizes images\n  for meticulous OCR analysis.\n \n \n   4. Multilingual Proficiency: \n \n \n  Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency\n  facilitates seamless content extraction from documents in various languages,\n  extending global accessibility for businesses dealing with multilingual\n  documentation.\n \n \n   \n   Source: Created by Author using MidJourney \n \n \n   Implementation of Code \n \n \n  The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\n  transformation in document extraction. By merging Zephyr’s advanced OCR\n  capabilities with LlamaIndex’s image enhancement and data organization\n  features, this integration presents a comprehensive solution:\n \n \n   \n     Augmented Precision : The fusion of Zephyr’s machine\n    learning expertise and LlamaIndex’s image enhancement markedly heightens the\n    accuracy of extracted data, diminishing errors and enhancing overall\n    efficiency.\n   \n   \n     Efficient Workflow : Users experience an optimized workflow,\n    enabling swift extraction and conversion of image-based documents into\n    structured, actionable data, facilitating expedited decision-making\n    processes.\n   \n   \n     Adaptability Across Document Varieties : This integration\n    empowers users to handle diverse document formats and languages\n    effortlessly, granting access to previously challenging document types for\n    extraction and analysis.\n   \n \n \n   \n   Source: Image created by Author using MidJourney \n \n \n   Step 1: Install and Import Libraries \n \n !pip install llama-index transformers accelerate sentencepiece bitsandbytes -q \n \n   Step 2: Load the Model \n \n import torch from transformers import BitsAndBytesConfig from llama_index.prompts import PromptTemplate from llama_index.llms import HuggingFaceLLM quantization_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_compute_dtype=torch.float16,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_use_double_quant=True, ) def messages_to_prompt(messages):   prompt = \"\"   for message in messages:     if message.role == 'system':       prompt += f\"<|system|>\\n{message.content}</s>\\n\"     elif message.role == 'user':       prompt += f\"<|user|>\\n{message.content}</s>\\n\"     elif message.role == 'assistant':       prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"   # ensure we start with a system prompt, insert blank if needed   if not prompt.startswith(\"<|system|>\\n\"):     prompt = \"<|system|>\\n</s>\\n\" + prompt   # add final assistant prompt   prompt = prompt + \"<|assistant|>\\n\"   return prompt llm = HuggingFaceLLM(     model_name=\"HuggingFaceH4/zephyr-7b-alpha\",     tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",     query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),     context_window=3900,     max_new_tokens=2000,     model_kwargs={\"quantization_config\": quantization_config},     # tokenizer_kwargs={},     generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},     messages_to_prompt=messages_to_prompt,     device_map=\"auto\", ) \n from llama_index import ServiceContext, set_global_service_context service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\") \n set_global_service_context(service_context) \n \n   Step 3: Storing your index \n \n from llama_index import SimpleDirectoryReader, VectorStoreIndex from llama_index.readers.file.base import (     DEFAULT_FILE_READER_CLS,     ImageReader, ) from llama_index.response.notebook_utils import (     display_response,     display_image, ) from llama_index.indices.query.query_transform.base import (     ImageOutputQueryTransform, ) filename_fn = lambda filename: {\"file_name\": filename} llama_reader = SimpleDirectoryReader(     input_dir=\"/content/llama\",     file_metadata=filename_fn, ) llama_documents = llama_reader.load_data() llama_index = VectorStoreIndex.from_documents(llama_documents) \n \n   Step 4: Query  Transformations \n \n from llama_index.query_engine import TransformQueryEngine query_engine = llama_index.as_query_engine(similarity_top_k=2) query_engine = TransformQueryEngine(     query_engine, query_transform=ImageOutputQueryTransform(width=400) ) llama_response = query_engine.query(     \"Show an image to illustrate how tree index works and explain briefly\", ) display_response(llama_response) #Output Final Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works. tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance. when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter. for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent. the following image illustrates how a tree index works: ! Tree Index Example in this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\". i hope this helps clarify how tree index works! \n \n   Step 5: Lets read the  receipts \n \n from llama_index.readers.file.base import DEFAULT_FILE_READER_CLS from llama_index.readers.file.image_reader import ImageReader image_parser =ImageReader(     keep_image=True,     parse_text=True     ) file_extractor = DEFAULT_FILE_READER_CLS file_extractor.update({     \".jpg\": image_parser,     \".png\": image_parser,     \".jpeg\": image_parser,     }) receipt_reader = SimpleDirectoryReader(     input_dir=\"/content/data\",     file_metadata=filename_fn,     file_extractor=file_extractor, ) receipt_documents = receipt_reader.load_data() print(len(receipt_documents)) #Output 3 \n receipts_index = VectorStoreIndex.from_documents(receipt_documents) from llama_index.query_engine import TransformQueryEngine query_engine = receipts_index.as_query_engine() receipts_response = query_engine.query(     \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\", ) display_response(receipts_response) # Output  Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00. \n Conclusion \n \n  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\n  in image-based document extraction. Beyond addressing OCR’s inherent\n  challenges, it enhances the precision and efficiency of data extraction from\n  images, fostering improved productivity and decision-making in\n  document-focused workflows.\n \n “Stay connected and support my work through various platforms: \n \n   \n     GitHub : For\n    all my open-source projects and Notebooks, you can visit my GitHub profile\n    at\n     https://github.com/andysingal . If you find my content valuable, don’t hesitate to leave a star.\n   \n   \n    Patreon: If you’d like to provide additional support, you can consider\n    becoming a patron on my Patreon page at\n     https://www.patreon.com/AndyShanu .\n   \n   \n     Medium : You\n    can read my latest articles and insights on Medium at\n     https://medium.com/@andysingal .\n   \n   \n     The Kaggle :\n    Check out my Kaggle profile for data science and machine learning projects\n    at\n     https://www.kaggle.com/alphasingal .\n   \n   \n     Hugging Face :\n    For natural language processing and AI-related projects, you can explore my\n    Huggingface profile at\n     https://huggingface.co/Andyrasika .\n   \n   \n    YouTube: To watch my video content, visit my YouTube channel at\n     https://www.youtube.com/@andy111007 .\n   \n   \n    LinkedIn: To stay updated on my latest projects and posts, you can follow me\n    on LinkedIn. Here is the link to my profile:\n     https://www.linkedin.com/in/ankushsingal/.\" \n   \n \n \n  Requests and questions: If you have a project in mind that you’d like me to\n  work on or if you have any questions about the concepts I’ve explained, don’t\n  hesitate to let me know. I’m always looking for new ideas for future Notebooks\n  and I love helping to resolve any doubts you might have.\n \n \n  Remember, each “Like”, “Share”, and “Star” greatly contributes to my work and\n  motivates me to continue producing more quality content. Thank you for your\n  support!\n \n \n  If you enjoyed this story, feel free\n   to subscribe \n  to Medium, and you will get notifications when my new articles will be\n  published, as well as full access to thousands of stories from other authors.\n \n Resource: \n \n   \n     Data used for above code \n   \n   \n     llama-index", "author": "Ankush k Singal", "date": "Nov 20, 2023", "tags": ["Technology", "Software Development", "Software Engineering", "Tech", "Artificial Intelligence"]},
{"title": "Multi-Modal RAG", "content": "(co-authored by Haotian Zhang, Laurie Voss, and Jerry Liu @ LlamaIndex) Overview In this blog we’re excited to present a fundamentally new paradigm: multi-modal Retrieval-Augmented Generation (RAG). We present new abstractions in LlamaIndex that now enable the following: Multi-modal LLMs and Embeddings Multi-modal Indexing and Retrieval (integrates with vector dbs) Multi-Modal RAG One of the most exciting announcements at OpenAI Dev Day was the release of the  GPT-4V API . GPT-4V is a  multi-modal  model that takes in both text/images, and can output text responses. It’s the latest model in a recent series of advances around multi-modal models:  LLaVa , and  Fuyu-8B . This extends the capabilities of LLMs in exciting new directions. In the past year, entire application stacks have emerged around the text-in/text-out paradigm. One of the most notable examples is Retrieval Augmented Generation (RAG) — combine an LLM with an external text corpus to reason over data that the model isn’t trained on. One of the most significant impacts of RAG for end-users was how much it accelerated  time-to-insight  on unstructured text data. By processing an arbitrary document (PDF, web page), loading it into storage, and feeding it into the context window of an LLM, you could extract out any insights you wanted from it. The introduction of GPT-4V API allows us to extend RAG concepts into the hybrid image/text domain, and unlock value from an even greater corpus of data (including images). Think about all the steps in a standard RAG pipeline and how it can be extended to a multi-modal setting. Input:  The input can be text or images. Retrieval:  The retrieved context can be text or images. Synthesis:  The answer can be synthesized over both text and images. Response:  The returned result can be text and/or images. This is just a small part of the overall space too. You can have chained/sequential calls that interleave between image and text reasoning, such as  Retrieval Augmented Image Captioning  or multi-modal agent loops. Abstractions in LlamaIndex We’re excited to present new abstractions in LlamaIndex that help make multi-modal RAG possible. For each abstraction, we explicitly note what we’ve done so far and what’s still to come. Multi-modal LLM We have direct support for GPT-4V via our  OpenAIMultiModal  class and support for open-source multi-modal models via our  ReplicateMultiModal  class (currently in beta, so that name might change). Our  SimpleDirectoryReader  has long been able to ingest audio, images and video, but now you can pass them directly to GPT-4V and ask questions about them, like this: from  llama_index.multi_modal_llms  import  OpenAIMultiModal\n from  llama_index  import  SimpleDirectoryReader\n\nimage_documents = SimpleDirectoryReader(local_directory).load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model= \"gpt-4-vision-preview\" , api_key=OPENAI_API_TOKEN, max_new_tokens= 300 \n)\nresponse = openai_mm_llm.complete(\n    prompt= \"what is in the image?\" , image_documents=image_documents\n)  This is a new base model abstraction. Unlike our default  LLM  class, which has standard completion/chat endpoints, the multi-modal model ( MultiModalLLM ) can take in both image and text as input. This also unifies the interface between both GPT-4V and open-source models. Resources We have initial implementations for both GPT-4V and vision models hosted on Replicate. We also have a docs page for multi-modal models: Multi-modal docs page GPT-4V Replicate Displayed image and example output from GPT-4V given text query “Describe image as alternative text” What’s still to come: More multi-modal LLM integrations Chat endpoints Streaming Multi-Modal Embeddings We introduce a new  MultiModalEmbedding  base class that can embed both text and images. It contains all the methods as our existing embedding models (subclasses  BaseEmbedding  ) but also exposes  get_image_embedding . Our primary implementation here is  ClipEmbedding  with the CLIP model. See below for a guide on using this in action. What’s still to come More multi-modal embedding integrations Multi-Modal Indexing and Retrieval We create a new index, a  MultiModalVectorIndex  that can index both text and images into underlying storage systems — specifically a vector database and docstore. Unlike our existing (most popular) index, the  VectorStoreIndex  , this new index can store both text and image documents. Indexing text is unchanged — it is embedded using a text embedding model and stored in a vector database. Indexing images involves a separate process: Embed the image using CLIP Represent the image node as a base64 encoding or path, and store it along with its embedding in a vector db (separate collection from text). We store images and text separately since we may want to use a text-only embedding model for text as opposed to CLIP embeddings (e.g. ada or sbert). During retrieval-time, we do the following: Retrieve text via vector search on the text embeddings Retrieve images via vector search on the image embeddings Both text and images are returned as Nodes in the result list. We can then synthesize over these results. What’s still to Come More native ways to store images in a vector store (beyond base64 encoding) More flexible multi-modal retrieval abstractions (e.g. combining image retrieval with any text retrieval method) Multi-modal response synthesis abstractions. Currently the way to deal with long text context is to do “create-and-refine” or “tree-summarize” over it. It’s unclear what generic response synthesis over multiple images and text looks like. Notebook Walkthrough Let’s walk through a notebook example. Here we go over a use case of querying Tesla given screenshots of its website/vehicles, SEC fillings, and Wikipedia pages. We load the documents as a mix of text docs and images: documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data() We then define two separate vector database collections in Qdrant: a collection for text docs, and a collection for images. We then define a  MultiModalVectorStoreIndex  . # Create a local Qdrant vector store\nclient = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n\ntext_store = QdrantVectorStore(\n    client=client, collection_name=\"text_collection\"\n)\nimage_store = QdrantVectorStore(\n    client=client, collection_name=\"image_collection\"\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\n\n# Create the MultiModal index\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents, storage_context=storage_context, image_vector_store=image_store\n) We can then ask questions over our multi-modal corpus. Example 1: Retrieval Augmented Captioning Here we copy/paste an initial image caption as the input to get a retrieval-augmented output: retriever_engine = index.as_retriever(\n    similarity_top_k= 3 , image_similarity_top_k= 3 \n)\n # retrieve more information from the GPT4V response \nretrieval_results = retriever_engine.retrieve(query_str) The retrieved results contain both images and text: Retrieved Text/Image Results We can feed this to GPT-4V to ask a followup question or synthesize a coherent response: Synthesized Result Example 2: Multi-Modal RAG Querying Here we ask a question and get a response from the entire multi-modal RAG pipeline. The  SimpleMultiModalQueryEngine  first retrieves the set of relevant images/text, and feeds the input to a vision model in order to synthesize a response. from  llama_index.query_engine  import  SimpleMultiModalQueryEngine\n\nquery_engine = index.as_query_engine(\n    multi_modal_llm=openai_mm_llm,\n    text_qa_template=qa_tmpl\n)\n\nquery_str =  \"Tell me more about the Porsche\" \nresponse = query_engine.query(query_str) The generated result + sources are shown below:", "author": "Jerry Liu", "date": "Nov 10, 2023", "tags": ["Gpt 4v", "Llamaindex", "LLM", "Vision", "Rag"]},
{"title": "create-llama, a command line tool to generate LlamaIndex apps", "content": "Introducing  create-llama , the easiest way to get started with LlamaIndex! Update 2023–11–20: we now have a  guide to deploying your create-llama apps ! Want to use the power of LlamaIndex to load, index and chat with your data using LLMs like GPT-4? It just got a lot easier! We’ve created a simple to use command-line tool that will generate a full-stack app just for you — just bring your own data! To get started, run: npx create-llama The app will then ask you a series of questions about what kind of app you want. You’ll need to supply your own  OpenAI API key  (or you can customize it to use a different LLM), and make a few decisions. How does it get my data? The generated app has a  data  folder where you can put as many files as you want; the app will automatically index them at build time and after that you can quickly chat with them. If you’re using LlamaIndex.TS as the back-end (see below), you’ll be able to ingest PDF, text, CSV, Markdown, Word and HTML files. If you’re using the Python backend, you can read even more types, including audio and video files! Technical details The front-end it generates is a Next.js application, with your choice of  shadcn/ui  or vanilla HTML and CSS for styling. For the back-end, you have 3 options: Next.js : if you select this option, you’ll have a full stack Next.js application that you can deploy to a host like  Vercel  in just a few clicks. This uses  LlamaIndex.TS , our TypeScript library. Express : if you want a more traditional Node.js application you can generate an Express backend. This also uses LlamaIndex.TS. Python FastAPI : if you select this option you’ll get a backend powered by the  llama-index python package , which you can deploy to a service like  Render  or  fly.io . There are a couple of other questions you’ll be asked: Streaming or non-streaming: if you’re not sure, you’ll probably want a streaming backend. SimpleChatEngine  or  ContextChatEngine  : the ContextChatEngine is the one that uses your data. If you just want to chat with GPT, you can use the  SimpleChatEngine . Go forth and customize! Once you’ve got your app up and running, you can customize it to your heart’s content! By default, for cost reasons, the app will use GPT-3.5-Turbo. If you’d like to use GPT-4 you can configure that by modifying the file  app/api/chat/llamaindex-stream.ts  (in the Next.js backend) or you can configure it to use a different LLM entirely! LlamaIndex has integrations with dozens of LLMs, both APIs and local.", "author": "LlamaIndex", "date": "Nov 14, 2023", "tags": ["Llamaindex", "LLM", "AI", "JavaScript", "Typescript"]},
{"title": "Multimodal RAG: Building ‘AInimal Go!’, a Pokémon Go-Inspired App with ResNet, Cohere and Llamaindex", "content": "In the current landscape where GPT-4 Vision (GPT-4V) use cases are everywhere, I wanted to explore an alternative approach: pairing deep learning vision models with large language models (LLMs). My latest project, ‘AInimal Go!’, is an attempt to showcase how a specialized vision model like ResNet18 can seamlessly integrate with an LLM, using LlamaIndex as the orchestration layer and Wikipedia articles as the knowledge base. Project Overview ‘AInimal Go!’ is an interactive app that allows users to either capture or upload images of animals. Upon uploading an image, the ResNet18 model swiftly classifies the animal. Following this, the Cohere LLM API, adeptly orchestrated by LlamaIndex, takes over. It roleplays as the identified animal, enabling users to engage in unique conversations about and with the animal. The dialogue is informed and enriched by a knowledge base of nearly 200 Wikipedia articles, providing accurate and relevant responses to user queries. Why Not GPT-4V? Amidst the surge in GPT-4 Vision use cases, I wanted to explore an efficient yet powerful alternative. It is important to choose the right tool for the job — using GPT-4V for every multimodal task can be overkill, like using a sledgehammer to crack a nut. My approach was to harness the agility and precision of ResNet18 for animal identification. This method not only  curtails costs  but also underscores the adaptability of specialized models in multi-modal realms. Tools and Tech ResNet for Animal Detection:  A blazing-fast implementation to identify animals in images, utilizing the ImageNet classification scheme. Cohere LLM:  For generating engaging, informative conversations based on the identified animal. LlamaIndex:  Seamlessly orchestrates the workflow, managing the retrieval of information from pre-indexed Wikipedia articles about animals. Streamlit for UI Gif showing the demo in action Deep Dive into app.py The heart of ‘AInimal Go!’ lies in the  app.py  script, where ResNet, Cohere LLM, and LlamaIndex seamlessly come together. Now, let’s delve into the key aspects of the code: 1. Image Capture/Upload In ‘AInimal Go!’, the flow begins with the user uploading an image or capturing one using their device’s camera. This is a crucial step as it sets the stage for the subsequent interaction with the identified animal. The code snippet below illustrates how Streamlit is used to create a UI for image upload and capture. It offers two options: a file uploader for selecting an image file and a camera input for real-time capture. Once an image is provided through either method, it’s converted into a byte stream ( BytesIO ) for processing. This streamlining ensures a seamless user experience, whether the image is uploaded from a gallery or captured on the spot. # Image upload section. \n    image_file = st.file_uploader( \"Upload an image\" ,  type =[ \"jpg\" ,  \"jpeg\" ,  \"png\" ], key= \"uploaded_image\" , on_change=on_image_upload)\n    \n    col1, col2, col3 = st.columns([ 1 ,  2 ,  1 ])\n     with  col2:   # Camera input will be in the middle column \n        camera_image = st.camera_input( \"Take a picture\" , on_change=on_image_upload)\n        \n    \n     # Determine the source of the image (upload or camera) \n     if  image_file  is   not   None :\n        image_data = BytesIO(image_file.getvalue())\n     elif  camera_image  is   not   None :\n        image_data = BytesIO(camera_image.getvalue())\n     else :\n        image_data =  None \n    \n     if  image_data:\n         # Display the uploaded image at a standard width. \n        st.session_state[ 'assistant_avatar' ] = image_data\n        st.image(image_data, caption= 'Uploaded Image.' , width= 200 ) 2. Initializing ResNet for Image Classification Once the user uploads or captures an image, the next critical step is identifying the animal within it. This is where ResNet18, a robust deep learning model for image classification, comes into play. The function  load_model_and_labels  performs two key tasks: Loading Animal Labels:  It starts by loading a subset of ImageNet labels specific to animals. These labels are stored in a dictionary, mapping class IDs to their corresponding animal names. This mapping is essential for interpreting the output of the ResNet model. Initializing ResNet18:  The function then initializes the feature extractor and the ResNet18 model. The feature extractor preprocesses the images to the format required by ResNet18, while the model itself is responsible for the actual classification task. def   load_model_and_labels ():\n     # Load animal labels as a dictionary \n    animal_labels_dict = {}\n     with   open ( 'imagenet_animal_labels_subset.txt' ,  'r' )  as  file:\n         for  line  in  file:\n            parts = line.strip().split( ':' )\n            class_id =  int (parts[ 0 ].strip())\n            label_name = parts[ 1 ].strip().strip( \"'\" )\n            animal_labels_dict[class_id] = label_name\n\n     # Initialize feature extractor and model \n    feature_extractor = AutoFeatureExtractor.from_pretrained( \"microsoft/resnet-18\" )\n    model = ResNetForImageClassification.from_pretrained( \"microsoft/resnet-18\" )\n\n     return  feature_extractor, model, animal_labels_dict\n\nfeature_extractor, model, animal_labels_dict = load_model_and_labels() By integrating ResNet18 in this manner, ‘AInimal Go!’ leverages its speed and accuracy for the crucial task of identifying the animal in the user’s image. This sets the foundation for the engaging and informative conversations that follow. 3. Animal Detection with ResNet18 After initializing ResNet18, the next step is to use it for detecting the animal in the uploaded image. The function  get_image_caption  handles this task. Image Preprocessing:  The uploaded image is first opened and then preprocessed using the feature extractor initialized earlier. This preprocessing adapts the image to the format required by ResNet18. Animal Detection:  The preprocessed image is then fed into ResNet18, which predicts the class of the image. The logits (the model’s raw output) are processed to find the class with the highest probability, which corresponds to the predicted animal. Retrieving the Animal Name:  The predicted class ID is mapped to the corresponding animal name using the label dictionary created earlier. This name is then displayed to the user. def   get_image_caption ( image_data ):\n    image = Image. open (image_data)\n    inputs = feature_extractor(images=image, return_tensors= \"pt\" )\n\n     with  torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label_id = logits.argmax(- 1 ).item()\n    predicted_label_name = model.config.id2label[predicted_label_id]\n    st.write(predicted_label_name)\n     # Return the predicted animal name \n     return  predicted_label_name, predicted_label_id 4. Validating Animal Presence in Images To ensure that the conversation in ‘AInimal Go!’ is relevant and engaging, it’s crucial to verify that the uploaded image indeed depicts an animal. This verification is handled by the  is_animal  function. def   is_animal ( predicted_label_id ):\n     # Check if the predicted label ID is within the animal classes range \n     return   0  &lt;= predicted_label_id &lt;=  398 The function checks if the predicted label ID from ResNet18 falls within the range of animal classes (0 to 398 in ImageNet’s classification). This simple yet effective check is essential for maintaining the app’s focus on animal interactions. Further in the script, this function is utilized to validate the detected object: if   not  (is_animal(label_id)):\n    st.error( \"Please upload image of an animal!\" )\n    st.stop() If the uploaded image does not depict an animal, the app prompts the user to upload an appropriate image, ensuring that the conversation remains on track. 5. Initializing LLM The  init_llm  function initializes the Cohere LLM along with the necessary contexts for storage and service (specify llm and embed_model). It also loads the pre-indexed Wikipedia articles for about 200 animals. The function sets up the environment in which the LLM operates, preparing it for generating responses. def init_llm(api_key):\n    llm = Cohere(model=\"command\", api_key=st.secrets['COHERE_API_TOKEN'])\n\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\n    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n    index = load_index_from_storage(storage_context, index_id=\"index\", service_context=service_context)\n    \n    return llm, service_context, storage_context, index This function is critical for setting up the LLM, ensuring that all necessary components are in place for the chat functionality. 6. Creating the Chat Engine The  create_chat_engine  function takes the animal description and utilizes it to create a query engine. This engine is responsible for handling user queries and generating responses based on the identified animal. def   create_chat_engine ( img_desc, api_key ):\n    doc = Document(text=img_desc)\n    \n    query_engine = CitationQueryEngine.from_args(\n        index,\n        similarity_top_k= 3 ,\n        citation_chunk_size= 512 ,\n        verbose= True \n    )\n    \n     return  query_engine system_prompt=f\"\"\"\n              You are a chatbot, able to have normal interactions. Do not make up information.\n              You always answer in great detail and are polite. Your job is to roleplay as an {img_desc}. \n              Remember to make {img_desc} sounds while talking but dont overdo it.\n              \"\"\"\n                    \nresponse = chat_engine.query(f\"{system_prompt}. {user_input}\") By creating a query engine specific to the identified animal, this function ensures that the conversations in the app are relevant, informative, and engaging. I have used the CitationQueryEngine to provide the future possibility of showing the sources as well, making the conversations not only engaging but also informative with credible references. 7. Bringing It All Together With all the technical components in place, ‘AInimal Go!’ combines everything into a user-friendly chat interface. Here, users can interact directly with the AI, asking questions and receiving responses about the identified animal. This final interaction loop, skillfully managed by Streamlit, perfectly showcases the seamless integration of vision and language models in the app. Wrapping Up ‘AInimal Go!’ represents an exciting fusion of vision models, language models, and Wikipedia, with LlamaIndex serving as the orchestrator that seamlessly integrates ResNet for animal identification and Cohere’s LLM for engaging conversations. This app is a stepping stone to even more innovative visual-language applications. The possibilities are boundless, and your insights can shape its future. I encourage you to explore the demo, experiment with the code, and join me in pushing the boundaries of what AI can achieve in the realm of multimodal interactions. GitHub Repo Connect with Me on LinkedIn LinkedIn Post Live Demo", "author": "Harshad Suryawanshi", "date": "Nov 27, 2023", "tags": ["Llamaindex", "LLM", "Cohere", "Resnet", "Streamlit"]},
{"title": "LlamaIndex Newsletter 2023–11–28", "content": "Hello to Our Llama Community! 🦙 Hope your Thanksgiving was delightful! We’re thrilled to announce a major milestone: LlamaIndex has hit  1 million monthly downloads  on our Python package! A big thank you to everyone for your support, feedback, and contributions that have fueled our journey. Stay tuned for more exciting new products and features coming your way. If you have a fascinating project or video you’d like to share, we’d love to see it! Feel free to send it to us at  news@llamaindex.ai . And remember to subscribe to our newsletter on our  website  to stay in the loop. We can’t wait to connect with you there! 🤩  First, the highlights: Launched Llama Packs:  Prepackaged modules and templates streamlining LLM app development.  Blog ,  Tweet . RAGs Project:  Build your own retrieval augmented generation app just by talking.  Project ,  Tweet . Introduced FuzzyCitationEnginePack:  Precisely aligns LLM responses to source sentences via fuzzy matching, available as an easy-to-implement LlamaPack.  Docs ,  Tweet . Coming up this week: on Thursday 30th our very own Yi Ding will be giving a workshop on Building an Open Source RAG Application Using LlamaIndex.  Sign up for free here ✨ Feature Releases and Enhancements: We introduced Llama Packs 🦙📦, a series of prepackaged modules and templates designed to jumpstart your LLM app development. These packs eliminate the need for assembling and tuning custom components for each use case.  Blog ,  Tweet . We have introduced the RAGs project for programming AI agents using natural language, inspired by the interest in OpenAI’s GPTs. Our approach involves a ‘Builder Agent’ that crafts a ‘Custom Agent’ tailored to specific tasks, incorporating tools for system prompt setting, data loading, model configuration, and RAG parameter adjustments.  Project ,  Tweet . We introduced a LlamaPack that enables the setup of a fully local RAG pipeline with just one line of code. This pack includes Zephyr-7b as the LLM and bge-base as the embedding model.  Docs ,  Tweet . We introduced  FuzzyCitationEnginePack  that maps parts of an LLM-generated response from a RAG pipeline to the exact sentences in the source context using fuzzy matching. This innovation elevates citation accuracy and is now available as a LlamaPack for easy implementation with just one line of code.  Docs ,  Tweet . 👀 Demo: AI-Einblick Prompt is a JupyterLab extension that uses OpenAI’s GPT 3.5 and 4, powered by LlamaIndex, to assist in data science workflows by generating, modifying, and fixing code, creating charts, and building models, seamlessly integrated within the JupyterLab environment.  Project ,  Tweet . Ranya Khemiri  uploaded a research paper to RAGs to help with a school assignment and observed results better than file retrieval with ChatGPT.  Blog ,  Tweet . 🤝 Integrations: CogniSwitch  introduced a fusion RAG approach combining vectors, knowledge graphs, and rules for streamlined ingestion and retrieval. This allows for flexible usage, either as an independent query engine or as an integrated tool within an agent with LlamaIndex.  Docs ,  Tweet . 🗺️ Guides: Guide  on shipping your RAG application to production with create-llama. Guide  on multi-modal models: Our comparison tables detail differences in image reasoning, embeddings, and synthesis capabilities. We also provide insights into multi-modal support for vector stores, focusing on image support with future audio/video integration. Guide  on getting started with AI in your enterprise from Gradient AI. This introductory guide explains retrieval-augmented generation (RAG), its relevance for businesses, and how to balance fine-tuning, prompt engineering, and RAG for optimal results, along with strategies for RAG optimization. ✍️ Tutorials: Ankush k Singal  made a  tutorial  on Document Extraction with Zephyr 7b LLM using LlamaIndex. Wenqi Glantz  made a  tutorial  on Automating Hyperparameter Tuning with LlamaIndex. Tonic AI   analysis  on OpenAI Assistant API vs LlamaIndex RAG. Pradip Nichite  made ****a ****video  tutorial  on using RAGs which provides easy-to-follow instructions on how to build or customize a chatbot capable of advanced summarization over your data, making it accessible even for non-developers. 🎥  Webinars: Jerry Liu presented a  webinar  with Arize AI on LLM Retrieval Evaluations.", "author": "LlamaIndex", "date": "Nov 28, 2023", "tags": ["LLM", "Llamaindex", "Newsletter", "AI", "Rag"]},
{"title": "Announcing LlamaIndex 0.9", "content": "Our hard-working team is delighted to announce our latest major release, LlamaIndex 0.9! You can get it right now: pip install --upgrade llama_index In LlamaIndex v0.9, we are taking the time to refine several key aspects of the user experience, including token counting, text splitting, and more! As part of this, there are some new features and minor changes to current usage that developers should be aware of: New  IngestionPipline  concept for ingesting and transforming data Data ingestion and transforms are now automatically cached Updated interface for node parsing/text splitting/metadata extraction modules Changes to the default tokenizer, as well as customizing the tokenizer Packaging/Installation changes with PyPi (reduced bloat, new install options) More predictable and consistent import paths Plus, in beta: MultiModal RAG Modules for handling text and images! Have questions or concerns? You can  report an issue  on GitHub or  ask a question on our Discord ! Read on for more details on our new features and changes. IngestionPipeline — New abstraction for purely ingesting data Sometimes, all you want is to ingest and embed nodes from data sources, for instance if your application allows users to upload new data. New in LlamaIndex V0.9 is the concept of an  IngestionPipepline  . An  IngestionPipeline  uses a new concept of  Transformations  that are applied to input data. What is a  Transformation  though? It could be a: text splitter node parser metadata extractor embeddings model Here’s a quick example of the basic usage pattern: from  llama_index  import  Document\n from  llama_index.embeddings  import  OpenAIEmbedding\n from  llama_index.text_splitter  import  SentenceSplitter\n from  llama_index.extractors  import  TitleExtractor\n from  llama_index.ingestion  import  IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\nnodes = pipeline.run(documents=[Document.example()]) Transformation Caching Each time you run the same  IngestionPipeline  object, it caches a hash of the input nodes + transformations and the output of that transformation for each transformation in the pipeline. In subsequent runs, if there is a cache hit, that transformation will be skipped and the cached result will be used instead. The greatly speeds up duplicate runs, and can help improve iteration times when deciding which transformations to use. Here’s an example with a saving and loading a local cache: from  llama_index  import  Document\n from  llama_index.embeddings  import  OpenAIEmbedding\n from  llama_index.text_splitter  import  SentenceSplitter\n from  llama_index.extractors  import  TitleExtractor\n from  llama_index.ingestion  import  IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\n # will only execute full pipeline once \nnodes = pipeline.run(documents=[Document.example()])\nnodes = pipeline.run(documents=[Document.example()])\n # save and load \npipeline.cache.persist( \"./test_cache.json\" )\nnew_cache = IngestionCache.from_persist_path( \"./test_cache.json\" )\nnew_pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n        TitleExtractor(),\n    ],\n    cache=new_cache,\n)\n # will run instantly due to the cache \nnodes = pipeline.run(documents=[Document.example()]) And here’s another example using Redis as a cache and Qdrant as a vector store. Running this will directly insert the nodes into your vector store and cache each transformation step in Redis. from  llama_index  import  Document\n from  llama_index.embeddings  import  OpenAIEmbedding\n from  llama_index.text_splitter  import  SentenceSplitter\n from  llama_index.extractors  import  TitleExtractor\n from  llama_index.ingestion  import  IngestionPipeline, IngestionCache\n from  llama_index.ingestion.cache  import  RedisCache\n from  llama_index.vector_stores.qdrant  import  QdrantVectorStore\n\n import  qdrant_client\nclient = qdrant_client.QdrantClient(location= \":memory:\" )\nvector_store = QdrantVectorStore(client=client, collection_name= \"test_store\" )\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    cache=IngestionCache(cache=RedisCache(), collection= \"test_cache\" ),\n    vector_store=vector_store,\n)\n # Ingest directly into a vector db \npipeline.run(documents=[Document.example()])\n # Create your index \n from  llama_index  import  VectorStoreIndex\nindex = VectorStoreIndex.from_vector_store(vector_store) Custom Transformations Implementing custom transformations is easy! Let’s add a transform to remove special characters from the text before calling embeddings. The only real requirement for transformations is that they must accept a list of nodes and return a list of nodes. import  re\n from  llama_index  import  Document\n from  llama_index.embeddings  import  OpenAIEmbedding\n from  llama_index.text_splitter  import  SentenceSplitter\n from  llama_index.ingestion  import  IngestionPipeline\n from  llama_index.schema  import  TransformComponent\n\n class   TextCleaner ( TransformComponent ):\n   def   __call__ ( self, nodes, **kwargs ):\n     for  node  in  nodes:\n      node.text = re.sub( r'[^0-9A-Za-z ]' ,  \"\" , node.text)\n     return  nodes\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n        TextCleaner(),\n        OpenAIEmbedding(),\n    ],\n)\nnodes = pipeline.run(documents=[Document.example()]) Node Parsing/Text Splitting — Flattened and Simplified Interface We’ve made our interface for parsing and splitting text a lot cleaner. Before: from  llama_index.node_parser  import  SimpleNodeParser\n from  llama_index.node_parser.extractors  import  (\n\tMetadataExtractor, TitleExtractor\n) \n from  llama_index.text_splitter  import  SentenceSplitter\n\nnode_parser = SimpleNodeParser(\n  text_splitter=SentenceSplitter(chunk_size= 512 ),\n  metadata_extractor=MetadataExtractor(\n  extractors=[TitleExtractor()]\n ),\n)\nnodes = node_parser.get_nodes_from_documents(documents) After: from  llama_index.text_splitter  import  SentenceSplitter\n from  llama_index.extractors  import  TitleExtractor \n\nnode_parser = SentenceSplitter(chunk_size= 512 )\nextractor = TitleExtractor()\n\n # use transforms directly \nnodes = node_parser(documents)\nnodes = extractor(nodes) Previously, the  NodeParser  object in LlamaIndex had become extremely bloated, holding both text splitters and metadata extractors, which caused both pains for users when changing these components, and pains for us trying to maintain and develop them. In V0.9, we have  flattened  the entire interface into a single  TransformComponent  abstraction, so that these transformations are easier to setup, use, and customize. We’ve done our best to minimize the impacts on users, but the main thing to note is that  SimpleNodeParser  has been removed , and other node parsers and text splitters have been elevated to have the same features, just with different parsing and splitting techniques. Any old imports of  SimpleNodeParser  will redirect to the most equivalent module,  SentenceSplitter . Furthermore, the wrapper object  MetadataExtractor  has been removed , in favour of using extractors directly. Full documentation for all this can be found below: Node Parsers and Text Splitters Metadata Extractors Tokenization and Token Counting — Improved defaults and Customization A big pain point in LlamaIndex previously was tokenization. Many components used a non-configurable  gpt2  tokenizer for token counting, causing headaches for users using non-OpenAI models, or even some hacky fixes  like this  for OpenAI models too! In LlamaIndex V0.9, this  global tokenizer is now configurable and defaults to the CL100K tokenizer  to match our default GPT-3.5 LLM. The single requirement for a tokenizer is that it is a callable function, that takes a string, and returns a list. Some examples of configuring this are below: from  llama_index  import  set_global_tokenizer\n\n # tiktoken \n import  tiktoken\nset_global_tokenizer(\n  tiktoken.encoding_for_model( \"gpt-3.5-turbo\" ).encode\n)\n # huggingface \n from  transformers  import  AutoTokenizer\nset_global_tokenizer(\n  AutoTokenizer.from_pretrained( \"HuggingFaceH4/zephyr-7b-beta\" ).encode\n) Furthermore, the  TokenCountingHandler  has gotten an upgrade with better token counting, as well as using token counts from API responses directly when available. Packaging — Reduced Bloat In an effort to modernize the packaging of LlamaIndex, V0.9 also comes with changes to installation. The biggest change here is that  LangChain  is now an optional package, and will not be installed by default. To install  LangChain  as part of your llama-index installation you can follow the example below. There are also other installation options depending on your needs, and we are welcoming further contributions to the extras in the future. # installs langchain\npip install llama-index[langchain]\n \n# installs tools needed for running local models\npip install llama-index[local_models]\n\n# installs tools needed for postgres\npip install llama-index[postgres]\n\n# combinations!\npip isntall llama-index[local_models,postgres] If you were previously importing  langchain  modules  in your code, please update your project packaging requirements appropriately. Import Paths — More Consistent and Predictable We are making two changes to our import paths: We’ve removed uncommonly used imports from the root level to make importing  llama_index  faster We now have a consistent policy for making “user-facing” concepts import-able at level-1 modules. from  llama_index.llms  import  OpenAI, ...\n from  llama_index.embeddings  import  OpenAIEmbedding, ...\n from  llama_index.prompts  import  PromptTemplate, ...\n from  llama_index.readers  import  SimpleDirectoryReader, ...\n from  llama_index.text_splitter  import  SentenceSplitter, ...\n from  llama_index.extractors  import  TitleExtractor, ...\n from  llama_index.vector_stores  import  SimpleVectorStore, ... We still expose some of the most commonly used modules at the root level. from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex, ... MultiModal RAG Given the recent announcements of the GPT-4V API, multi-modal use cases are more accessible than ever before. To help users use these features, we’ve started to introduce a number of new modules to help support use-cases for MultiModal RAG: MultiModal LLMs (GPT-4V, Llava, Fuyu, etc.) MultiModal Embeddings (i.e clip) for join image-text embedding/retrieval MultiModal RAG, combining indexes and query engines Our documentation has a  full guide to multi-modal retrieval . Thanks for all your support! As an open-source project we couldn’t exist without our  hundreds of contributors . We are so grateful for them and the support of the hundreds of thousands of LlamaIndex users around the world. See you on the Discord!", "author": "LlamaIndex", "date": "Nov 15, 2023", "tags": ["LLM", "AI", "Llamaindex", "Genai", "Python"]},
{"title": "OpenAI Cookbook: Evaluating RAG systems", "content": "We’re excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you’ll find it useful in enhancing the effectiveness of your RAG systems, and we’re thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham’s essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system’s performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!🦙", "author": "Ravi Theja", "date": "Nov 28, 2023", "tags": ["Llamaindex", "OpenAI", "LLM", "Retrieval Augmented"]},
{"title": "Shipping your Retrieval-Augmented Generation app to production with create-llama", "content": "It’s a llama on a ship, geddit? Last week  we released create-llama , a command-line tool to generate a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). The response was fantastic, so we’ll be following up with more templates and more features. We also wanted to show you just how easy it is to get your generated app all the way to production. So here’s a step by step guide, for each of the three backends we currently support: Next.js serverless, Express, and Python. Next.js backend The serverless full-stack Next.js application is the simplest version to deploy as you only have one artifact to deploy. Because it’s a Next.js app we’ll be deploying to  Vercel , the home of Next.js. Step 1: run create-llama First run create-llama to generate your app. We strongly recommend generating a new  OpenAI API key  and supplying it at generation time (create-llama apps can be customized to use other LLMs but that’s out of scope for this tutorial). Step 2: create a GitHub repository and push your app to it The easiest way to deploy on Vercel is from a linked GitHub repository. Your generated app is already set up as a git repo, so all you have to do after creating a new empty repo is follow the instructions to push it up. This should give you a repo that looks a bit like this: Step 3: import your repo into Vercel Select the option to create a new project from a git repo: and select the repo you just created after authorizing: Step 4: configure your project Because this is a Next.js app and this is Vercel, there’s very little you need to do! The only thing you need to remember is to click “environment variables” and create a variable called  OPENAI_API_KEY  with your key. Step 5: Deploy! That’s it! Deploying a Next.js app to Vercel is pretty easy. Your deployed app should look like this: Congratulations, you’ve deployed a full-stack RAG application! Express backend If you chose to generate an Express backend with a Next.js frontend instead, let’s get you into production with those. We’ll be deploying both frontend and backend to  Render , a fantastic service for both static sites and dynamic web applications. There will be 3 big things to do here: Deploy the static frontend Deploy the backend, and give the frontend permission to call it Tell the frontend where the backend is located We promise you’ll be production in no time. Step 1: run create-llama This is just like the same step in Next.js Step 2: push the code to a new GitHub repo Like Vercel, the easiest way to push a site to production is from a linked git repo. Your generated app already has a git repo initiated with  frontend  and  backend  folders, so you can go ahead and push them both to a single GitHub repository just as in the Next.js backend. Step 3: Start a new static site We’ll be deploying your frontend first as a static site. After authorizing, select the repository where you pushed your frontend and backend; we’ll specify that we’re deploying the frontend in the next step. Step 4: configure your static site There are several changes you need to make to the default configuration to successfully publish your static frontend: Name your site something memorable, it will become the URL of your site once it’s deployed Set your root directory to  frontend Set your build command to  npm install; npm run build Set your publish directory to  out Finally, click “Advanced” and set an environment variable called  NODE_VERSION  to  20  . The default on Render is a much older version of Node so don’t skip this step! Step 5: deploy your static frontend Click the Deploy button and watch your site build! You should now have a site live at a URL something like  frontend-name-you-picked.onrender.com  . Step 6: start a new web service", "author": "LlamaIndex", "date": "Nov 20, 2023", "tags": ["LLM", "Llamaindex", "Deployment", "Rag", "AI"]},
{"title": "LlamaIndex Newsletter 2023–11–21", "content": "Hello Llama Fam 🦙 What an amazing week we’ve had! We’re excited to share that, according to the  Retool State of AI 2023 survey , 1 in 12 respondents are now using LlamaIndex. We’re grateful for all your support. If you have a fascinating project or video you’d like to share, we’d love to see it! Feel free to send it to us at  news@llamaindex.ai . And remember to subscribe to our newsletter on our  website  to stay in the loop. We can’t wait to connect with you there! 🤩  First, the highlights: LlamaIndex 0.9 Release:  we introduced LlamaIndex version 0.9 featuring streamlined data handling with a new IngestionPipeline, automated caching, improved text processing interfaces, tokenizer updates, PyPi packaging enhancements, consistent import paths, and a beta version of MultiModal RAG Modules.  Blog post ,  Tweet . Multi-Modal Evaluation Tools:  we launched multi-modal evaluation with the introduction of MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, plus a guide for their application in multi-modal settings.  Blog post ,  Tweet . create-llama  CLI Tool:  we unveiled  create-llama , a versatile CLI tool for building full-stack LLM apps with options like FastAPI, ExpressJS, and Next.js for backends and a Next.js frontend with Vercel AI SDK components.  Blog post ,  Tweet . Cohere Reranker Fine-Tuning:  we enhanced RAG pipeline retrieval performance with the fine-tuning of the Cohere reranker.  Blog post ,  Tweet . Coming up this week: we have a YouTube live event in partnership with  AI Makerspace  exploring the potential of LlamaIndex to handle complex PDFs with tables, charts and more.  Register for free! ✨ Feature Releases and Enhancements: We introduced the LlamaIndex 0.9 version with updates on streamlined data handling with new IngestionPipeline, automated caching, improved interfaces for text processing, tokenizer updates, enhanced PyPi packaging, consistent import paths, and a beta of MultiModal RAG Modules for text and image integration.  Blog post ,  Tweet . We introduced multi-modal evaluation which includes MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, and a guide on using them in multi-modal applications.  Blog post ,  Tweet . We introduced  create-llama , a CLI tool for easily building full-stack LLM apps, offering choices like FastAPI, ExpressJS, and Next.js backends with Llama Index, and a Next.js frontend with Vercel AI SDK components, enabling extensive customization for AI engineers.  Blog post ,  Tweet . We introduced fine-tuning of the cohere reranker to improve retrieval performance in the RAG pipeline.  Blog post ,  Tweet . Integrations: We integrated with Chroma’s multi-modal collections which allows for indexing both text and images in a single collection, enhancing RAG pipelines by combining text and image information for use with multi-modal models like GPT-4V, LLaVa, and Fuyu.  Docs ,  Tweet . 🗺️ Guides: Guide  on Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles. Guide  on LlamaIndex by Nanonets covering over 12 key areas such as data management, indexing/storage, querying with top-k RAG, structured outputs, chat functionalities with memory, and agent development incorporating tool use. Guide  on using Ingestion pipeline focusing on showcasing experiments on chunk overlaps and the use of metadata extractors, including title, summary, and other elements. Guide  on using Perplexity API with LlamaIndex by  Vishhvak . Guide  on using  Fleet Context  to download the embeddings for LlamaIndex’s documentation and build a hybrid dense/sparse vector retrieval engine on top of it. Guide  on building a full-stack financial analysis bot using  create-llama  and Llama Index's RAG, capable of querying text and tables across SEC filings. ✍️ Tutorials: Wenqi Glantz  made a  tutorial  on LLaVA vs. GPT-4V Amidst Snow Geese Migration. Glenn Parham’s   cookbook  on LlamaIndex, hosted in the Department of Defense’s official repository, showcases methods for applying RAG on unclassified DoD policy documents. Sudarshan Koirala  made a tutorial on Using Perplexity API with LlamaIndex. Ravi Theja   analysis  on GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques 🎥  Webinars: Check out our CEO —  Jerry Liu’s  talk on Building Production-Ready RAG Applications at  AI.engineer  Summit.", "author": "LlamaIndex", "date": "Nov 21, 2023", "tags": ["LLM", "Llamaindex", "Rag", "Newsletter", "AI"]},
{"title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data", "content": "Today we introduce  RAGs ,  a Streamlit app that allows you to create and customize your own RAG pipeline and then use it over your own data — all with natural language! This means you can now setup a “ChatGPT over your data” without needing to code. Setup and query a RAG pipeline in three simple steps: Easy Task Description : Simply describe your task (like “load this web page”) and define the parameters for your RAG systems (like retrieving a certain number of documents). Configurable Settings : Dive into the configuration view to see and alter the automatically generated parameters, such as top-k retrieval, summarization options, and more. Interactive RAG Agent:  Once set up, you can interact with your RAG agent, asking questions and getting responses based on your data. The app is designed for both  less-technical and technical users:  if you’re less-technical, you still need to clone the repo and pip install it, but you don’t need to worry about what’s going on under the hood. On the other hand, if you are technical, you can inspect and customize specific parameter settings (e.g. top-k, data). Home page for RAGs Detailed Overview The app contains the following sections, corresponding to the steps listed above. [1] 🏠 Home Page This is the section where you build a RAG pipeline by instructing the “builder agent”. Typically to setup a RAG pipeline you need the following components: Describe the dataset : Currently we support either  a single local file  or a  web page . We’re open to suggestions here! Define the Task : Your description here initializes the “system prompt” of the LLM powering the RAG pipeline. Set RAG Parameters : Configure typical RAG setup parameters, such as top-k retrieval, chunk size, and summarization options. See below for the full list of parameters. [2] ⚙️ RAG Config: Tailoring Your Experience After setting up the basics, you move to the RAG Config section. This part of the app provides an intuitive UI where you can: View Generated Parameters:  The builder agent suggests parameters based on your initial setup. Edit and Customize : You have complete freedom to tweak these settings, ensuring the RAG agent behaves exactly as you need. Update the Agent:  Any changes you make can be instantly applied by hitting the “Update Agent” button. This is the current set of parameters: System Prompt Include Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.) Top-K Chunk Size Embed Model LLM [3] 🤖 Generated RAG Agent: Interacting with Your Data The final piece of the RAGs experience is the Generated RAG Agent section. Here’s what you can expect: Interactive Chatbot Interface:  Just like ChatGPT, engage in conversations with your RAG agent. Data-Driven Responses:  The agent utilizes top-k vector search and optional summarization tools to answer your queries based on the underlying data. Seamless Integration:  The agent dynamically picks the right tools to fulfill your queries, ensuring a smooth and intelligent interaction with your dataset. Architecture We’ll cover the architecture in more detail in followups. At a high-level: We have a  builder agent  equipped with  builder tools —  tools necessary to construct a RAG pipeline. The builder agent will use these tools to set the configuration state. At the end of the initial conversational flow these parameters are then used to initialize the RAG agent. Let’s Walk through an Example! Installation and Setup Getting RAGs up and running is straightforward: Clone the RAGs project and navigate to the  rags  project folder:  https://github.com/run-llama/rags Install the required packages: pip install -r requirements.txt 3. Launch the app: streamlit run 1_🏠_Home.py Build the RAG Agent In the below diagram we show a sequence of commands to “build” a RAG pipeline. Say that you want to build a chatbot Define the dataset (here it’s a web page, can also be a local file) Define the task Define params (chunk size 512, top-k = 3) Screenshot of the home page — build a RAG agent Followup questions to set parameters and build the RAG agent View the Configuration We can see the generated configuration in the below page, and view/edit them as necessary! E.g. we can set  include_summarization  to True. Test It Out Now we can ask questions! We can ask both specific questions as well as summarization questions. This uses both the vector search and summarization tools to answer the requisite questions. Question about a specific detail (performs vector search) Summarization question over the entire document Conclusion In general RAGs is an initial take towards a world where LLM applications are built by and powered by natural language. Let us know your thoughts and feedback! Resources RAGs repo:  https://github.com/run-llama/rags Contributions and Support We’re committed to improving and expanding RAGs. If you encounter any issues or have suggestions, feel free to file a Github issue or join our  Discord community .", "author": "Jerry Liu", "date": "Nov 21, 2023", "tags": ["Llamaindex", "Streamlit", "LLM", "NLP", "AI"]},
{"title": "Introducing Llama Packs", "content": "Today we’re excited to introduce  Llama Packs 🦙📦—  a community-driven hub of prepackaged modules that you can use to kickstart your LLM application. Import them for a wide variety of use cases, from building a Streamlit app to building advanced retrieval over Weaviate to a resume parser that does structured data extraction. Just as important, inspect and customize them to your liking. They’re available on  LlamaHub : we’ve launched 16+ templates with our launch partners already, and we’re going to be adding a lot more! (To those of you in the states, Happy Thanksgiving 🦃) Context There are so many choices when building an LLM app that it can be daunting to get started building for a specific use case. Even for RAG the user needs to make the following decisions: Which LLM should I use? Embedding model? Vector database? Chunking/parsing strategy Retrieval Algorithm Wrapping in surrounding application Every use case requires different parameters, and LlamaIndex as a core LLM framework offers a comprehensive set of unopinionated modules to let users compose an application. But we needed a way for users to get started more easily for their use case. And that’s exactly where Llama Packs comes in. Overview Llama Packs can be described in two ways: On one hand, they are prepackaged  modules  that can be initialized with parameters and run out of the box to achieve a given use case (whether that’s a full RAG pipeline, application template, and more). You can also import  submodules  (e.g. LLMs, query engines) to use directly . On another hand, LlamaPacks are  templates  that you can inspect, modify, and use. They can be downloaded either through our  llama_index  Python library or the CLI in  one line of code: CLI: llamaindex-cli download-llamapack <pack_name> --download-dir <pack_directory> Python from  llama_index.llama_pack  import  download_llama_pack\n\n # download and install dependencies \nVoyageQueryEnginePack = download_llama_pack(\n   \"&lt;pack_name&gt;\" ,  \"&lt;pack_directory&gt;\" \n) Llama Packs can span abstraction levels — some are full prepackaged templates (full Streamlit / Gradio apps), and some combine a few smaller modules together (e.g. our SubQuestionQueryEngine with Weaviate). All of them are found in  LlamaHub  👇. You can filter by packs by selecting “Llama Packs” from the dropdown. Llama Packs on LlamaHub We’re excited to partner with the following companies/contributors for our launch, featuring  16+ templates.  We highlight some examples below: Streamlit / Snowflake (Caroline F.):  Streamlit Chatbot Arize (Mikyo K., Xander S.):  Arize Phoenix ActiveLoop / DeepLake (Mikayel H., Adhilkhan S.):  DeepMemory Pack ,  Multi-modal Retrieval Weaviate (Erika C.):  Sub Question Query Engine ,  Retry Query Engine Voyage AI (Hong L.):  Voyage AI Pack TruEra (Josh R.):  TruLens Eval Pack  (this is 3 packs in one) Timescale (Matvey A.):  Timescale Vector AutoRetrieval Wenqi G.:  LLaVa Completion Pack There’s not enough room in this blog post to feature every template, we’ll be running features on every pack in the next few days. Special thanks to Logan Markewich and Andrei Fajardo on the LlamaIndex team for getting Llama Packs up and running. Example Walkthrough The best way to highlight LlamaPack features is to showcase an example. We’ll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI embeddings. Voyage AI Pack. Every Pack has a detailed README on how to use / modules. First, we download and initialize the Pack over a set of documents: from  llama_index.llama_pack  import  download_llama_pack\n\n # download pack \nVoyageQueryEnginePack = download_llama_pack( \"VoyageQueryEnginePack\" ,  \"./voyage_pack\" )\n # initialize pack (assume documents is defined) \nvoyage_pack = VoyageQueryEnginePack(documents) Every Llama Pack implements a  get_modules()  function allowing you to inspect/use the modules. modules = voyage_pack.get_modules()\ndisplay(modules)\n\n# get LLM, vector index\nllm = modules[\"llm\"]\nvector_index = modules[\"index\"] The Llama Pack can be run in an  out of the box  fashion. By calling  run  , we’ll execute the RAG pipeline and get back a response. In this setting, you don’t need to worry about the internals. # this will run the full pack \nresponse = voyage_pack.run( \"What did the author do growing up?\" , similarity_top_k= 2 )\n print ( str (response))\n The author spent his time outside of school mainly writing and programming. He wrote short stories and attempted to write programs on an IBM 1401. Later, he started programming on a TRS-80, creating simple games and a word processor. He also painted still lives while studying at the Accademia. The second important thing is that you have  full access to the code of the Llama Pack . This allows you to customize the Llama Pack, rip out code, or just use it as reference to build your own app. Let’s take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM for Anthropic: from  llama_index.llms  import  Anthropic\n...\n\n class   VoyageQueryEnginePack ( BaseLlamaPack ):\n     def   __init__ ( self, documents:  List [Document] ) -&gt;  None :\n        llm = Anthropic()\n        embed_model = VoyageEmbedding(\n            model_name= \"voyage-01\" , voyage_api_key=os.environ[ \"VOYAGE_API_KEY\" ]\n        )\n        service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n        self.llm = llm\n        self.index = VectorStoreIndex.from_documents(\n            documents, service_context=service_context\n        )\n\n     def   get_modules ( self ) -&gt;  Dict [ str ,  Any ]:\n         \"\"\"Get modules.\"\"\" \n         return  { \"llm\" : self.llm,  \"index\" : self.index}\n\n     def   run ( self, query_str:  str , **kwargs:  Any ) -&gt;  Any :\n         \"\"\"Run the pipeline.\"\"\" \n        query_engine = self.index.as_query_engine(**kwargs)\n         return  query_engine.query(query_str) You can re-import the module directly and run it again: from  voyage_pack.base  import  VoyageQueryEnginePack\n\nvoyage_pack = VoyageQueryEnginePack(documents)\nresponse = voyage_pack.run( \"What did the author do during his time in RISD?\" )\n print ( str (response)) Conclusion Try it out and let us know what you think! Contributing Not on here yet? We’d  love  to feature you! If you have any templates with LlamaIndex, adding it is almost as simple as copying/pasting your existing code over into a BaseLlamaPack  subclass. Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs Resources All Llama Packs can be found on LlamaHub:  https://llamahub.ai/ The full notebook walkthrough is here:  https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_hub/llama_packs_example.ipynb", "author": "Jerry Liu", "date": "Nov 22, 2023", "tags": ["Llamaindex", "NLP", "AI", "LLM", "Llamahub"]},
{"title": "LlamaIndex + Gemini", "content": "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ LlamaIndex) Today is Google’s  public release  of its latest AI model, Gemini. We’re excited to be a day 1 launch partner for Gemini, with support immediately available in LlamaIndex today! As of 0.9.15, LlamaIndex offers full support for all currently  released and upcoming Gemini models  (Gemini Pro, Gemini Ultra). We support both a “text-only” Gemini variant with a text-in/text-out format as well as a multimodal variant that takes in both text and images as input, and outputs text. We’ve made some fundamental multi-modal abstraction changes to support the Gemini multi-modal interface, which allows users to input multiple images along with text. Our Gemini integrations are also  feature-complete:  they support (non-streaming, streaming), (sync, async), and (text completion, chat message) formats — 8 combinations in total. In addition, we also support the brand-new  Semantic Retriever API,  which bundles storage, embedding models, retrieval, and LLM in a RAG pipeline. We show you how it can be used on its own, or decomposed+bundled with LlamaIndex components to create advanced RAG pipelines. Huge shoutout to the Google Labs and Semantic Retriever teams for helping us get setup with early access. Google Labs:  Mark McDonald, Josh Gordon, Arthur Soroken Semantic Retriever:  Lawrence Tsang, Cher Hu The below sections contain a detailed walkthrough of both our brand-new Gemini and Semantic Retriever abstractions in LlamaIndex. If you don’t want to read that now, make sure you bookmark our detailed notebook guides below! Gemini (text-only) Guide Gemini (multi-modal) Guide Semantic Retriever Guide Gemini Release and Support There’s been a ton of press around Gemini, which boasts  impressive performance  at a variety of benchmarks. The Ultra variants (which are not yet publicly available) outperform GPT-4 on benchmarks from MMLU to Big-Bench Hard to math and coding tasks. Their  multimodal demos  demonstrate joint image/text understanding from domains like scientific paper understanding to literature review. Let’s walk through examples of using Gemini in LlamaIndex. We walk through both the text model ( from llama_index.llms import Gemini ) as well as the multi-modal model ( from llama_index.multi_modal_llms.gemini import GeminiMultiModal ) Text Model Full Notebook Guide Here We start with the text model. In the code snippet below, we show a bunch of different configurations, from completion to chat to streaming to async. from llama_index.llms import Gemini\n\n# completion\nresp = Gemini().complete(\"Write a poem about a magic backpack\")\n# chat\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = Gemini().chat(messages)\n# streaming (completion)\nllm = Gemini()\nresp = llm.stream_complete(\n    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n)\n# streaming (chat)\nllm = Gemini()\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = llm.stream_chat(messages)\n# async completion\nresp = await llm.acomplete(\"Llamas are famous for \")\nprint(resp)\n# async streaming (completion)\nresp = await llm.astream_complete(\"Llamas are famous for \")\nasync for chunk in resp:\n    print(chunk.text, end=\"\") The  Gemini  class of course has parameters that can be set. This includes  model_name ,  temperature ,  max_tokens , and  generate_kwargs . As an example, you can do: llm = Gemini(model=\"models/gemini-ultra\") Multi-modal Model Full Notebook Guide Here In this notebook, we test out the  gemini-pro-vision  variant that features  multi-modal inputs.  It contains the following features: supports both  complete  and  chat  capabilities supports streaming and async Supports feeding in  multiple images  in addition to text in the completion endpoint Future work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision. Let’s walk through a concrete example. Let’s say we are given a picture of the  following scene : Scene from a street in New York City We can then initialize our Gemini Vision model, and ask it a question: “Identify the city where this photo was taken”: from llama_index.multi_modal_llms.gemini import GeminiMultiModal\nfrom llama_index.multi_modal_llms.generic_utils import (\n    load_image_urls,\n)\n\nimage_urls = [\n    \" &lt; https://storage.googleapis.com/generativeai-downloads/data/scene.jpg &gt; \",\n    # Add yours here!\n]\nimage_documents = load_image_urls(image_urls)\ngemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\ncomplete_response = gemini_pro.complete(\n    prompt=\"Identify the city where this photo was taken.\",\n    image_documents=image_documents,\n) Our response is the following: New York City We can insert multiple images too. Here’s an example with an image of Messi and the Colosseum. image_urls = [\n    \" &lt; https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg &gt; \",\n    \" &lt; https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg &gt; \",\n]\nimage_documents_1 = load_image_urls(image_urls)\nresponse_multi = gemini_pro.complete(\n    prompt=\"is there any relationship between those images?\",\n    image_documents=image_documents_1,\n)\nprint(response_multi) Multi-Modal Use Cases (Structured Outputs, RAG) Full Notebook Guide Here We’ve created extensive resources about different  multi-modal use cases , from structured output extraction to RAG. Thanks to Haotian Zhang, we have examples for  both  these use cases with Gemini. Please see our extensive notebook guides for more details. In the meantime here’s the final results! Structured Data Extraction with Gemini Pro Vision Screenshot of a Google Maps Restaurant Listing Output: ('restaurant', 'La Mar by Gaston Acurio')\n('food', 'South American')\n('location', '500 Brickell Key Dr, Miami, FL 33131')\n('category', 'Restaurant')\n('hours', 'Open ⋅ Closes 11 PM')\n('price', 4.0)\n('rating', 4)\n('review', '4.4 (2,104)')\n('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n('nearby_tourist_places', 'Brickell Key Park') Multi-Modal RAG We run our structured output extractor on multiple restaurant images, index these nodes, and then ask a question “Recommend a Orlando restaurant for me and its nearby tourist places” I recommend Mythos Restaurant in Orlando. It is an American restaurant located at 6000 Universal Blvd, Orlando, FL 32819, United States. It has a rating of 4 and a review score of 4.3 based on 2,115 reviews. The restaurant offers a mythic underwater-themed dining experience with a view of Universal Studios' Inland Sea. It is located near popular tourist places such as Universal's Islands of Adventure, Skull Island: Reign of Kong, The Wizarding World of Harry Potter, Jurassic Park River Adventure, Hollywood Rip Ride Rockit, and Universal Studios Florida. Semantic Retriever The Generative Language Semantic Retriever offers specialized embedding models for high-quality retrieval, and a tuned LLM for producing grounded-output with safety settings. It can be used out of the box (with our  GoogleIndex ) or decomposed into different components ( GoogleVectorStore  and  GoogleTextSynthesizer ) and combined with LlamaIndex abstractions! Our full  semantic retriever notebook guide is here. Out of the Box Configuration You can use it out of the box with very few lines of setup. Simply define the index, insert nodes, and then get a query engine: from  llama_index.indices.managed.google.generativeai  import  GoogleIndex\n\nindex = GoogleIndex.from_corpus(corpus_id= \"&lt;corpus_id&gt;\" )\nindex.insert_documents(nodes)\nquery_engine = index.as_query_engine(...)\nresponse = query_engine.query( \"&lt;query&gt;\" ) A cool feature here is that Google’s query engine supports different  answering styles  as well as  safety settings . Answering Styles: ABSTRACTIVE (succinct but abstract) EXTRACTIVE (brief and extractive) VERBOSE (extra details) Safety Settings You can specify safety settings in the query engine, which let you define guardrails on whether the answer is explicit in different settings. See the  generative-ai-python  library for more information. Decomposing into Different Components The  GoogleIndex  is built upon two components: a vector store ( GoogleVectorStore ) and the response synthesizer ( GoogleTextSynthesizer ). You can use these as modular components in conjunction with LlamaIndex abstractions to create  advanced RAG . The notebook guide highlights three  advanced RAG use cases : Google Retriever + Reranking:  Use the Semantic Retriever to return relevant results, but then use our r eranking modules  to process/filter results before feeding it to response synthesis. Multi-Query + Google Retriever:  Use our multi-query capabilities, like our  MultiStepQueryEngine  to break a complex question into multiple steps, and execute each step against the semantic retriever. HyDE + Google Retriever:  HyDE is a popular query transformation technique that hallucinates an answer from a query, and uses the hallucinated answer for embedding lookup. Use that as a step before the retrieval step from the Semantic Retriever. Conclusion There’s a  lot  in here, and even then the blog post doesn’t even cover half of what we’ve released today. Please please make sure to check out our extensive notebook guides! Linking the resources again below: Gemini (text-only) Guide Gemini (multi-modal) Guide Semantic Retriever Guide Again, huge shoutout to the Google teams and Haotian Zhang, Logan Markewich from the LlamaIndex team for putting together everything for this release.", "author": "Jerry Liu", "date": "Dec 13, 2023", "tags": ["Llamaindex", "Gemini", "LLM", "AI", "Multimodal"]},
{"title": "LlamaIndex Newsletter 2023–11–14", "content": "Hello Llama Friends 🦙 LlamaIndex is 1 year old this week! 🎉 To celebrate, we’re taking a stroll down memory lane on our  blog  with twelve milestones from our first year. Be sure to check it out. Last week we had a blast with all the new things from OpenAI Dev day to learn and explore at LlamaIndex. There was a  special edition newsletter  with the things we released the same day as the conference, but this week’s newsletter is full of follow-up releases and explorations — don’t miss our slide deck summing up all the new features! As always, if you’ve got a cool project or a video to share we’d love to see it! Just drop us a line at  news@llamaindex.ai . 🤩  First, the highlights: Multi-Modal RAG Stack:  we unveiled Multi-Modal RAG ****for complex Q&A on documents and images, with new text/image queries and retrieval solutions.  Notebook ,  Tweet ,  Blog post . OpenAIAssistantAgent Abstractions:  we released new abstractions to connect OpenAI Assistant API with any vector database.  Docs ,  Tweet . Parallel Function Calling:  we enhanced our data extraction and tool execution using OpenAI’s parallel function calling.  Tweet . MechGPT Project:  Prof.  Markus J. Buehler ’s work merges LLM fine-tuning with knowledge graphs for scientific discovery.  Tweet ,  Paper . Feature Slide Deck:  Released a  slide deck  with 10+ new features and guides post-OpenAI updates. ✨ Feature Releases and Enhancements: We introduced a multi-modal RAG stack for complex document and image QA, featuring text/image queries, joint text/ image embeddings, and versatile storage and retrieval options.  Notebook ,  Tweet ,  Blog post . We now offer experimental GPT-4-vision support in  chat.llamaindex.ai  . Users can now upload images for enhanced chatbot interactions.  Tweet . We integrated OpenAI’s parallel function calling for efficient extraction of structured data from unstructured text and improving tool execution with agents.  Tweet . We introduced  OpenAIAssistantAgent  abstractions for seamless connection of OpenAI Assistants API with your chosen vector database.  Docs ,  Tweet . We introduced a new agent leveraging OpenAI Assistants API with features like in-house code interpretation, file retrieval, and function calling for external tools integration.  Notebook ,  Tweet . 🎥  Demos: MechGPT by Professor  Markus J. Buehler  showcases the integration of LLM fine-tuning and knowledge graph creation with LlamaIndex, leading to interesting insights in cross-disciplinary scientific research and hypothesis generation.  Tweet ,  Paper . 🗺️ Guides: We released a concise  slide deck  that aggregates over 10+ newly shipped features, guides, and analyses, complete with links to accompanying notebooks for developer use based on OpenAI’s recent updates. We also released a full  cookbook  showing how you can build advanced RAG with the Assistants API — beyond just using the in-house Retrieval tool. We produced a  guide  on evaluating the OpenAI Assistant API vs RAG with LlamaIndex. Here’s a  guide  on evaluating How well long-context LLMs (gpt-4-turbo, claude-2) recall specifics in BIG documents? (>= 250k tokens). Here’s another  guide  that highlights how function calling simplifies structured data extraction, while JSON mode ensures format correctness without schema enforcement. Finally, we released a guide to craft a GPT Builder, enabling an agent to programmatically construct another task-specific agent. This builder streamlines the creation of systems for specific functions.  Notebook ,  Tweet . ✍️ Tutorials: Bhavesh Bhat  gave us a  tutorial  on How to Chat with YouTube Videos Using LlamaIndex. David Garnitz ’s tutorial blog explores the use of VectorFlow alongside ArizePhoenix, Weaviate, and LlamaIndex to manage large data sets. Harshad Suryawanshi ’s  tutorial  covers Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex. Sudarshan Koirala ’s made a  tutorial  on Creating OpenAI Assistant Agent with LlamaIndex. Our own  Ravi Theja  released his  tutorial  on Boosting RAG with Embeddings & Rerankers. 🎥  Webinars: Check out our  webinar  with Dan Shipper, CEO of  every  to talk about the implications of OpenAI’s release updates. A second  webinar  with Victoria Lin, author of the RA-DIT paper on Fine-tuning + RAG. Last but not least,  Mayo Oshin ’s  webinar  with  Jerry Liu  on How to Analyze Tables In Large Financial Reports Using GPT-4.", "author": "Laurie Voss", "date": "Nov 14, 2023", "tags": ["LLM", "Rag", "Llamaindex", "Newsletter", "AI"]},
{"title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model", "content": "Introduction Evaluation is a critical component in enhancing your Retrieval-Augmented Generation (RAG) pipeline, traditionally reliant on GPT-4. However, the open-source  Prometheus model  has recently emerged as a notable alternative for such evaluation tasks. In this blog post, we will demonstrate how to effectively use the Prometheus model for evaluation purposes, integrating it smoothly with the LlamaIndex framework by comparing it with GPT-4 evaluation. Our primary focus will be on assessing RAG using our standard metrics: Correctness, Faithfulness, and Context Relevancy. To provide a clearer understanding, here’s what each metric entails: Correctness : Assesses whether the generated answer aligns with the reference answer, given the query (this necessitates labeled data). Faithfulness : Measures if the answer remains true to the retrieved contexts, essentially checking for the absence of hallucinations. Context Relevancy : Evaluate the relevance of both the retrieved context and the answer to the query. For an in-depth exploration, our documentation is available  here . For those who are exploring the Prometheus model for the first time, the paper summary by  Andrei  is an excellent resource to gain a better understanding. A crucial aspect to remember when using the Prometheus model is its dependence on rubric scores within the prompt for effective evaluation. An example of such Rubric scores in the context of  Correctness Evaluation is as follows: ###Score Rubrics: \nScore 1: If the generated answer is not relevant to the user query and reference answer. \nScore 2: If the generated answer is according to reference answer but not relevant to user query. \nScore 3: If the generated answer is relevant to the user query and reference answer but contains mistakes. \nScore 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise. \nScore 5: If the generated answer is relevant to the user query and fully correct according to the reference answer. You’ll find comprehensive details on this in the prompts section of this tutorial. For a detailed walkthrough of the code, feel free to follow along with our  Google Colab Notebook  accompanying this blog post. In the notebook, we conducted evaluations on both the  Paul Graham Essay Text and the  Llama2 Paper . However, for this blog post, we’ll focus exclusively on the Llama2 Paper, as it revealed some particularly interesting insights. Outline: Setup Evaluation Pipeline. Download Dataset. Define LLMs (Prometheus, GPT-4) needed for evaluation. Define Correctness, Faithfulness, and Relevancy prompt templates. Define Prometheus, GPT-4 Evaluators, and Batch Eval Runner. Run the Correctness, Faithfulness, and Relevancy Evaluation over the Llama2 dataset. 2. Results Correctness Evaluation score distribution between Prometheus and GPT-4. Feedback comparison between Prometheus and GPT-4 for correctness evaluation. Faithfulness and Relevancy Evaluation scores with Prometheus and GPT-4. Hamming Distance comparison between Prometheus and GPT-4. Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy 3. Summary with Cost Analysis. Setup Evaluation Pipeline Please be aware that certain functions mentioned here are not defined in detail within the blog post. We have showcased only the essential parts of the pipeline to provide an overview of its setup. For a comprehensive code walkthrough, we recommend visiting our  Google Colab Notebook . Download Dataset We will use the Llama2 paper dataset from Llama Datasets which contains 100 questions and their reference answers. from  llama_index.llama_dataset  import  download_llama_dataset\n\nllama2_rag_dataset, llama2_documents = download_llama_dataset(\n     \"Llama2PaperDataset\" ,  \"./data/llama2\" \n) Define Prometheus LLM hosted on HuggingFace And OpenAI for creating an Index (RAG) pipeline We need to host the model on HF Inference endpoint using Nvidia A100 GPU, 80 GB RAM. from  llama_index.llms  import  HuggingFaceInferenceAPI\n import  os\n\nHF_TOKEN =  \"YOUR HF TOKEN\" \nHF_ENDPOINT_URL =  \"HF END POINT URL\" \n\nprometheus_llm = HuggingFaceInferenceAPI(\n    model_name=HF_ENDPOINT_URL,\n    token=HF_TOKEN,\n    temperature= 0.1 ,\n    do_sample= True ,\n    top_p= 0.95 ,\n    top_k= 40 ,\n    repetition_penalty= 1.1 ,\n)\n\nos.environ[ \"OPENAI_API_KEY\" ] =  \"YOUR OPENAI API KEY\" \n\n from  llama_index.llms  import  OpenAI\n\ngpt4_llm = OpenAI( \"gpt-4\" ) Prompt templates. We will use the same prompts for the Prometheus model and GPT-4 to make consistent performance comparisons. Correctness Evaluation Prompt: prometheus_correctness_eval_prompt_template =  \"\" \"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n   1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n   2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric. \n   3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'\n   4. Please do not generate any other opening, closing, and explanations. \n   5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n\n   ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n   \n   ###Generate answer to evaluate: {generated_answer} \n\n   ###Reference Answer (Score 5): {reference_answer}\n            \n   ###Score Rubrics: \n   Score 1: If the generated answer is not relevant to the user query and reference answer.\n   Score 2: If the generated answer is according to reference answer but not relevant to user query.\n   Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n   Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n   Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n    \n   ###Feedback:\" \"\" \n Faithfulness Evaluation Prompt: prometheus_faithfulness_eval_prompt_template=  \"\" \"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n4. The output format should look as follows: \" Feedback : (write a feedback  for  criteria) [ RESULT ] ( YES  or  NO )”\n 5.   Please   do  not generate any other opening, closing, and explanations.\n\n### The  instruction to  evaluate :  Your  task is to evaluate  if  the given piece  of  information is supported by context.\n\n### Information : {query_str}\n\n### Context : {context_str}\n\n### Score   Rubrics :\n Score   YES :  If  the given piece  of  information is supported by context.\n Score   NO :  If  the given piece  of  information is not supported by context\n\n### Feedback :  \"\" \"\n\nprometheus_faithfulness_refine_prompt_template= \" \"\" ### Task   Description :  An  instruction (might include an  Input  inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n 1.   You  are provided  with  evaluation task  with  the help  of  information, context information and an existing answer.\n 2.   Write  a detailed feedback based on evaluation task and the given score rubric, not evaluating  in  general.\n 3.   After  writing a feedback, write a score that is  YES  or  NO .  You  should refer to the score rubric.\n 4.   The  output format should look  as   follows :  \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n 5.   Please   do  not generate any other opening, closing, and explanations.\n\n### The  instruction to  evaluate :  If  the information is present  in  the context and also provided  with  an existing answer.\n\n### Existing   answer : {existing_answer}\n\n### Information : {query_str}\n\n### Context : {context_msg}\n\n### Score   Rubrics :\n Score   YES :  If  the existing answer is already  YES  or  If  the  Information  is present  in  the context.\n Score   NO :  If  the existing answer is  NO  and  If  the  Information  is not present  in  the context.\n\n### Feedback :  \"\" \" Relevancy Evaluation Prompt: prometheus_relevancy_eval_prompt_template =  \"\" \"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n       1. You are provided with evaluation task with the help of a query with response and context.\n       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n       4. The output format should look as follows: \" Feedback : (write a feedback  for  criteria) [ RESULT ] ( YES  or  NO )” \n        5.   Please   do  not generate any other opening, closing, and explanations. \n\n        ### The  instruction to  evaluate :  Your  task is to evaluate  if  the response  for  the query is  in  line  with  the context information provided.\n\n        ### Query  and  Response : {query_str} \n\n        ### Context : {context_str}\n            \n        ### Score   Rubrics : \n         Score   YES :  If  the response  for  the query is  in  line  with  the context information provided.\n         Score   NO :  If  the response  for  the query is not  in  line  with  the context information provided.\n    \n        ### Feedback :  \"\" \"\n\nprometheus_relevancy_refine_prompt_template = \" \"\" ### Task   Description :  An  instruction (might include an  Input  inside it), a query  with  response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n    1.   You  are provided  with  evaluation task  with  the help  of  a query  with  response and context and an existing answer.\n    2.   Write  a detailed feedback based on evaluation task and the given score rubric, not evaluating  in  general. \n    3.   After  writing a feedback, write a score that is  YES  or  NO .  You  should refer to the score rubric. \n    4.   The  output format should look  as   follows :  \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"  \n    5.   Please   do  not generate any other opening, closing, and explanations. \n\n   ### The  instruction to  evaluate :  Your  task is to evaluate  if  the response  for  the query is  in  line  with  the context information provided.\n\n   ### Query  and  Response : {query_str} \n\n   ### Context : {context_str}\n            \n   ### Score   Rubrics : \n    Score   YES :  If  the existing answer is already  YES  or  If  the response  for  the query is  in  line  with  the context information provided.\n    Score   NO :  If  the existing answer is  NO  and  If  the response  for  the query is  in  line  with  the context information provided.\n    \n   ### Feedback :  \"\" \" Define Correctness, FaithFulness, Relevancy Evaluators from  llama_index  import  ServiceContext\n from  llama_index.evaluation  import  (\n    CorrectnessEvaluator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator,\n)\n from  llama_index.callbacks  import  CallbackManager, TokenCountingHandler\n import  tiktoken\n\n # Provide Prometheus model in service_context \nprometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n\n # CorrectnessEvaluator with Prometheus model \nprometheus_correctness_evaluator = CorrectnessEvaluator(\n    service_context=prometheus_service_context,\n    parser_function=parser_function,\n    eval_template=prometheus_correctness_eval_prompt_template,\n)\n\n # FaithfulnessEvaluator with Prometheus model \nprometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n # RelevancyEvaluator with Prometheus model \nprometheus_relevancy_evaluator = RelevancyEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n # Set the encoding model to `gpt-4` for token counting. \ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model( \"gpt-4\" ).encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n\n # Provide GPT-4 model in service_context \ngpt4_service_context = ServiceContext.from_defaults(\n    llm=gpt4_llm, callback_manager=callback_manager\n)\n\n # CorrectnessEvaluator with GPT-4 model \ngpt4_correctness_evaluator = CorrectnessEvaluator(\n    service_context=gpt4_service_context,\n     # parser_function=parser_function, \n)\n\n # FaithfulnessEvaluator with GPT-4 model \ngpt4_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n # RelevancyEvaluator with GPT-4 model \ngpt4_relevancy_evaluator = RelevancyEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n # create a dictionary of evaluators \nprometheus_evaluators = {\n     \"correctness\" : prometheus_correctness_evaluator,\n     \"faithfulness\" : prometheus_faithfulness_evaluator,\n     \"relevancy\" : prometheus_relevancy_evaluator,\n}\n\ngpt4_evaluators = {\n     \"correctness\" : gpt4_correctness_evaluator,\n     \"faithfulness\" : gpt4_faithfulness_evaluator,\n     \"relevancy\" : gpt4_relevancy_evaluator,\n} Function to run batch evaluations on defined evaluators from  llama_index.evaluation  import  BatchEvalRunner\n\n\n async   def   batch_eval_runner ( \n    evaluators, query_engine, questions, reference= None , num_workers= 8 \n ):\n    batch_runner = BatchEvalRunner(\n        evaluators, workers=num_workers, show_progress= True \n    )\n\n    eval_results =  await  batch_runner.aevaluate_queries(\n        query_engine, queries=questions, reference=reference\n    )\n\n     return  eval_results Get Query Engine, Questions, and References. query_engine, rag_dataset = create_query_engine_rag_dataset( \"./data/llama2\" )\n\nquestions = [example.query  for  example  in  rag_dataset.examples]\n\nreference = [[example.reference_answer]  for  example  in  rag_dataset.examples] Compute Correctness, Faithfulness, and Relevancy Evaluation. prometheus_eval_results = await batch_eval_runner(\n    prometheus_evaluators, query_engine, questions, reference\n)\n\ngpt4_eval_results = await batch_eval_runner(\n    gpt4_evaluators, query_engine, questions, reference\n) Results Correctness Evaluation score distribution. With the Prometheus Model: 3.0: 56.0, \n1.0: 26.0, \n5.0: 9.0, \n4.0: 8.0, \n2.0: 1.0 With GPT-4 Model: 4.5: 57.99, \n1.0: 6.0, \n4.0: 12.0, \n5.0: 10.0, \n2.0: 5.0, \n3.5: 5.0, \n2.5: 3.0, \n3.0: 1.0 Observation: Prometheus Evaluator gives more  3.0  scores compared to GPT-4 which gives  4.5 . Let’s investigate it with the help of feedback. Feedback comparison between Prometheus and GPT-4 for correctness. Query:  Based on the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models,” what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? Generated Answer:  The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion. Reference Answer:  The two primary objectives achieved in the work described in the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models” are: 1. The development and release of a collection of pretrained and fine-tuned large language models (LLMs) specifically optimized for dialogue use cases. 2. The demonstration that these fine-tuned LLMs, referred to as Llama 2-Chat, outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models, particularly in terms of helpfulness and safety based on human evaluations. The range of parameters for the large language models developed in this work is from 7 billion to 70 billion parameters. Prometheus Feedback:  The generated response is relevant to the user query and correctly identifies the two primary objectives of the work described in the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models.” However, it does not mention the demonstration of the fine-tuned LLMs outperforming open-source chat models on most benchmarks tested, which is a key point in the reference response. The range of parameters for the large language models developed is correctly identified, but the response does not mention the specific models referred to as Llama 2-Chat. So the overall score is 3. Prometheus Score:  3.0 GPT-4 Feedback:  The generated answer is relevant and almost fully correct. It correctly identifies the two primary objectives and the range of parameters for the large language models. However, it misses the detail about Llama 2-Chat outperforming other models on most benchmarks and potentially being a suitable substitute for closed-source models. GPT-4 Score:  4.5 Observation: The feedback from Prometheus is a little more precise compared to GPT-4 and it penalizes and gives a score of  3.0 but GPT-4 gives a score of  4.5 . Prometheus penalizes more even if some of the text in the reference answer is missed in the generated answer. Faithfulness and Relevancy Evaluation scores. With the Prometheus Model: faithfulness Score: 0.39 \nrelevancy Score: 0.57 With GPT-4 Model: faithfulness Score: 0.93\nrelevancy Score: 0.98 Observation: We see a higher faithfulness and relevancy score with GPT-4 compared to the Prometheus model. Let’s investigate it through feedback. Hamming Distance comparison between Prometheus and GPT-4: Faithfulness Hamming Distance: 58 \nRelevancy Hamming Distance: 41 Observation: The comparison reveals that approximately  42%  of the scores in case of  Faithfulness  and  59%  in case of  Relevancy  are common between Prometheus and GPT-4 evaluations. This indicates a decent amount of correlation in terms of faithfulness and relevance scoring between the Prometheus and GPT-4 models. Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy Query:  Based on the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models,” what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion. Context-1:  Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron∗Louis Martin†Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗ GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com †Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023 Context-2:  (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto transparency and safety, we plan to make further improvements to Llama 2-Chat in future work. 36 Prometheus Faithfulness Feedback:  The information provided in the context is not supported by the given information. The context is about the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. However, the information provided in the context does not align with the given information. The context does not mention the range of parameters for the large language models developed, which is the primary objective mentioned in the information. The context only talks about the development and release of Llama 2 and its optimization for dialogue use cases, but it does not provide any information about the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO Prometheus Faithfulness Score:  0.0 Prometheus Relevancy Feedback:  The response is not in line with the context information provided. The query asked for the two primary objectives achieved in the work and the range of parameters for the large language models developed. However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO Prometheus Relevancy Score:  0.0 GPT-4 Faithfulness Feedback:  The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YES GPT-4 Faithfulness Score:  1.0 GPT-4 Relevancy Feedback:  The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YES GPT-4 Relevancy Score:  1.0 Observation: Prometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information. GPT-4: Evaluates it correctly, unlike the Prometheus model. Summary The cost for evaluation (approx.):  $1.5  for Prometheus Model and  $15  for GPT4. The Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application. If a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4. The faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4. Note: You can check detailed analysis with code on  Google Colab Notebook . The endpoint on HF is served on AWS Nvidia A100G · 1x GPU · 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.) We used the  Prometheus model  for the analysis here. We also made a similar analysis with the  GPTQ Quantized version  of the  Prometheus model  and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open-sourcing the model and  Tom Jobbins  for the quantized version of the model. References: Prometheus paper . Prometheus model on HuggingFace.", "author": "Ravi Theja", "date": "Dec 15, 2023", "tags": ["Open Source", "Llamaindex", "Gpt 4", "LLM", "OpenAI"]},
{"title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit", "content": "In the dynamic world of AI and data analytics, the ability to bridge the gap between complex data queries and non-technical users is a game-changer. My latest project, Na2SQL, showcases this exciting advancement. Leveraging the power of LlamaIndex and OpenAI’s GPT-3.5, this app allows users, regardless of their SQL knowledge, to derive valuable insights from a database using simple natural language. Features Intuitive Natural Language Queries:  The core of this application is its ability to understand and process natural language queries. Users can ask questions in plain English and receive SQL queries and insights in return, all without any prior SQL experience. Advanced Data Processing:  The app doesn’t just stop at generating SQL queries; it executes these queries and analyzes the results to provide meaningful insights, making it a powerful tool for data analysis. User-Friendly Interface with Streamlit:  I chose Streamlit for its simplicity and effectiveness in creating interactive web applications. The app’s interface is straightforward, ensuring a smooth user experience. Database Viewer:  An interactive database viewer in the sidebar on the left allows users to explore the database structure, enhancing their understanding and interaction with the data. The Tech Stack This project harmoniously integrates several advanced technologies: OpenAI’s GPT-3.5:  At the heart of the application is GPT-3.5, enabling the app to understand natural natural language queries and transform them into valid SQL queries. Furthermore, it also generates the final analysis considering both the user’s query and the SQL output, thereby providing a comprehensive and relevant response. LlamaIndex:  A pivotal component of the app is LlamaIndex’s SQLTableQueryEngine. This powerful tool translates natural language queries into SQL, handles the execution of these queries, and plays a significant role in the subsequent analysis using GPT 3.5. Its integration ensures a smooth transition from user inputs to database insights, culminating in a meaningful final analysis that encapsulates the entire natural language-to-SQL-to-execution process. LlamaIndex’s Streamlit LlamaPack:  Using LlamaIndex’s Streamlit LlamaPack, we quickly assemble and highly functional Streamlit UI. This framework significantly simplifies the UI development process, allowing for rapid deployment and an enhanced user experience. SQLite Database:  The app interacts with an dummy SQLite ecommerce database, showcasing its ability to work with real-world data. Deep Dive into the Code In the heart of the application lies  app.py , a script that brings to life the seamless interaction between natural language processing and SQL query generation. This code is an evolution of the  Streamlit chatbot LlamaPack  available on  Llama Hub , further tailored to meet the specific needs of ecommerce data analytics. Let's dive into some key portions of the  app.py  script: 1. Initial Imports and Setup The script begins by importing necessary modules such as Streamlit, SQLAlchemy for database interaction, LlamaIndex for language model services, and other essential libraries. import  streamlit  as  st\n from  sqlalchemy  import  create_engine, inspect\n from  typing  import   Dict ,  Any \n\n from  llama_index  import  (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\n from  llama_index.llama_pack.base  import  BaseLlamaPack\n from  llama_index.llms  import  OpenAI\n import  openai\n import  os\n import  pandas  as  pd 2. StreamlitChatPack Class The  StreamlitChatPack  class extends the base LlamaPack, setting up the page and modules necessary for the app's functionality. class   StreamlitChatPack ( BaseLlamaPack ):\n\n     def   __init__ ( \n        self,\n        page:  str  =  \"Natural Language to SQL Query\" ,\n        run_from_main:  bool  =  False ,\n        **kwargs:  Any ,\n     ) -&gt;  None :\n         \"\"\"Init params.\"\"\" \n        self.page = page\n\n     # ... other methods ... 3. The  run  Method This method is where the magic happens. It sets up the Streamlit page configuration and initializes the chat functionality. def run(self, *args: Any, **kwargs: Any) - &gt;  Any:\n    \"\"\"Run the pipeline.\"\"\"\n    import streamlit as st\n\n    st.set_page_config(\n        page_title=f\"{self.page}\",\n        layout=\"centered\",\n        initial_sidebar_state=\"auto\",\n        menu_items=None,\n    )\n    \n    # ... rest of the run method ... 4. Database Schema Viewer in the Sidebar A helpful feature is the Database Schema Viewer, conveniently located in the sidebar. This viewer serves as a reference tool, allowing users to see the structure and content of the database tables, enhancing their understanding of the data they’re querying. # Sidebar for database schema viewer \nst.sidebar.markdown( \"## Database Schema Viewer\" )\n\n # Create an inspector object \ninspector = inspect(engine)\n\n # Get list of tables in the database \ntable_names = inspector.get_table_names()\n\n # Sidebar selection for tables \nselected_table = st.sidebar.selectbox( \"Select a Table\" , table_names)\n\ndb_file =  'ecommerce_platform1.db' \nconn = sqlite3.connect(db_file)\n\n # Display the selected table \n if  selected_table:\n    df = get_table_data(selected_table, conn)\n    st.sidebar.text( f\"Data for table ' {selected_table} ':\" )\n    st.sidebar.dataframe(df)\n\n # Close the connection \nconn.close() 5. Database Interaction and LLM Integration This part of the code loads the database from disk and initializes the LLM and the service context for use with Llamaindex. I’ve used GPT3.5 here, but you can easily swap it out with any other LLM of your choice. # Function to load database and LLM \n def   load_db_llm ():\n    engine = create_engine( \"sqlite:///ecommerce_platform1.db\" )\n    sql_database = SQLDatabase(engine)   # Include all tables \n    llm2 = OpenAI(temperature= 0.1 , model= \"gpt-3.5-turbo-1106\" )\n    service_context = ServiceContext.from_defaults(llm=llm2)\n     return  sql_database, service_context, engine\n\nsql_database, service_context, engine = load_db_llm() 6. Initializing the NLSQLTableQueryEngine One of the most critical aspects of the application is the initialization of the  NLSQLTableQueryEngine . This is where the app sets up the engine responsible for converting natural language queries into SQL queries, executing them and generating the final response, all with the help of GPT 3.5. # Initializing the query engine \n if   \"query_engine\"   not   in  st.session_state:\n    st.session_state[ \"query_engine\" ] = NLSQLTableQueryEngine(\n        sql_database=sql_database,\n        synthesize_response= True ,\n        service_context=service_context\n    ) 7. User Interaction and Displaying Results The script provides an interactive interface for users to input natural language queries, which are then translated into SQL queries and executed. The app concludes by displaying the SQL queries and responses, offering an informative and engaging user experience if  prompt := st.chat_input(\n     \"Enter your natural language query about the database\" \n):   # Prompt for user input and save to chat history \n     with  st.chat_message( \"user\" ):\n        st.write(prompt)\n    add_to_message_history( \"user\" , prompt)\n\n # If last message is not from assistant, generate a new response \n if  st.session_state[ \"messages\" ][- 1 ][ \"role\" ] !=  \"assistant\" :\n     with  st.spinner():\n         with  st.chat_message( \"assistant\" ):\n            response = st.session_state[ \"query_engine\" ].query( \"User Question:\" +prompt+ \". \" )\n            sql_query =  f\"```sql\\n {response.metadata[ 'sql_query' ]} \\n```\\n**Response:**\\n {response.response} \\n\" \n            response_container = st.empty()\n            response_container.write(sql_query)\n            add_to_message_history( \"assistant\" , sql_query) Wrapping Up This app is more than just a tool; it’s a step towards making data analytics accessible to a broader audience. It embodies the potential of AI in simplifying complex data interactions. I invite you to explore this application, witness its capabilities, and join me in this journey towards a more inclusive data-driven future. Link to Github Repo Connect with Me on LinkedIn Linkedin Post: Harshad S. on LinkedIn: #ai #llamaindex #streamlit #largelanguagemodels… AI Prototype 6: Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, OpenAI GPT3.5, and… www.linkedin.com", "author": "Harshad Suryawanshi", "date": "Dec 17, 2023", "tags": ["Natural Language To Sql", "Llamaindex", "LLM", "Gpt35", "OpenAI"]},
{"title": "LlamaIndex Newsletter 2023–12–19", "content": "What’s up, Llama Followers 🦙, We’re excited to bring you another week packed with the latest updates, features, exciting community demos, insightful tutorials, guides, and webinars. This week, don’t miss our special holiday  workshop  on 12/21, where we’ll dive into innovative LLM + RAG use cases with Google Gemini team. Got a groundbreaking project, compelling article, or captivating video? We’re all ears! Reach out to us at  news@llamaindex.ai . Remember to subscribe to our newsletter via our  website  to get all these exciting developments straight to your inbox. 🤩  First, the highlights: Google Gemini Partnership:  Now offering day 1 support for Gemini API on LlamaIndex, complete with comprehensive cookbooks for advanced RAG capabilities.  Tweet . MistralAI Integrations:  Introduced day-0 integrations with MistralAI LLMs and Embedding model for building RAG solutions on LlamaIndex.  Notebook ,  Tweet . Docugami Multi-Doc Llama Dataset:  Launched the Multi-Doc SEC 10Q Dataset by Taqi Jaffri, offering a range of question complexities for advanced RAG research.  Docs ,  Tweet . Proposition-Based Retrieval:  Implemented a new retrieval unit based on propositions, enhancing QA performance with LLMs.  Docs ,  Tweet . RAG Pipeline Enhancement Guide:  Introduced a guide featuring modules like Routing, Query-Rewriting, and Agent Reasoning for more complex QA over documents.  Docs . ✨ Feature Releases and Enhancements: We launched a partnership with Google Gemini, offering day 1 support for the Gemini API on LlamaIndex, including full-feature support for Gemini (text and multi-modal) and Semantic Retriever API, complemented by three comprehensive cookbooks:  Gemini LLM ,  Gemini Multi-modal , and  Semantic Retriever API , promising advanced RAG capabilities and multi-modal integrations.  Tweet . We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mistral-medium) and the MistralAI Embedding model for building RAG solutions with LlamaIndex both on Python and Typescript versions.  Notebook ,  Tweet . We launched the COVID-QA dataset on LlamaHub, a human-annotated, substantial set of 300+ QA pairs about COVID from various web articles, complete with source URLs for easy integration into RAG pipelines, offering ample scope for improvement.  Docs ,  Tweet . We launched a new multi-modal template in Create-llama, enabling image input and output generation using the latest GPT-4-vision model from OpenAI, expanding possibilities for diverse use cases.  Docs ,  Tweet . We have introduced Proposition-Based Retrieval in LlamaIndex: Implementing a new retrieval unit based on propositions, as introduced in the ‘Dense X Retrieval’ paper, enhancing QA performance with LLMs by indexing propositions and linking to the underlying text.  Docs ,  Tweet . We partnered with Docugami to launch a new Multi-Doc SEC 10Q Dataset by  Taqi Jaffri , aimed at advancing QA datasets for RAG evaluation. This dataset offers a range of question complexities: Single-Doc, Single-Chunk RAG; Single-Doc, Multi-Chunk RAG; and Multi-Doc RAG, addressing the need for more intricate datasets in RAG research.  Docs ,  Tweet . We launched a SharePoint data loader, enabling direct integration of SharePoint files into LLM/RAG pipelines.  Docs ,  Tweet . 👀 Community Demos : MemoryCache: Mozilla’s new experimental project that curates your online experience into a private, on-device RAG application using PrivateGPT_AI and LlamaIndex, enhancing personal knowledge management while maintaining privacy.  Website ,  Repo . OpenBB Finance showcases its enhanced chat widget feature in Terminal Pro, utilizing LlamaIndex’s data chunking combined with Cursor AI for improved large context management and accuracy.  Tweet AI Chatbot Starter (from the DataStax team), a web server powered by AstraDB and LlamaIndex, allows easy setup for chatting over web documentation. It can be used as a standalone service or integrated into full-stack applications, with simple credential setup and document ingestion.  Repo ,  Tweet . Na2SQL (by  Harshad )  to ****Build an End-to-End SQL Analyst App on Streamlit featuring interactive database viewing, SQL query displays, and integration with Llama Index.  Blog ,  Repo . LionAGI (by  Ocean Li ) is an agent framework for efficient data operations and support for concurrent calls and JSON mode with OpenAI. Check it to integrate it with a Llama Index RAG pipeline for automated AI assistants like an ArXiv research assistant.  Docs ,  Repo . Local RAG for Windows (from Marklysze): A comprehensive resource for integrating advanced LLMs into RAG workflows using Windows Subsystem for Linux, featuring five detailed cookbooks. 🗺️ Guides: Guide  for enhancing RAG pipelines with a Query Understanding Layer, featuring modules like Routing, Query-Rewriting, Sub-Question creation, and Agent Reasoning, all designed to enable more complex and ‘agentic’ QA over documents. Guide  to Building a Restaurant Recommendation QA System with Gemini to extract structured image data and utilize multi-modal Retrieval-Augmented Generation for enhanced query responses. Guide  to building Advanced RAG with Safety Guardrails to create constrained RAG systems with Gemini API’s semantic search, safety features, and Google Semantic Retriever integrations. Guide on  Qdrant’s Multitenancy with LlamaIndex  on setting up payload-based partitioning for user data isolation in vector services. Guide  on using Prometheus — an open-source 13B LLM for RAG Evaluations, comparing it with GPT-4 evaluation with insights on its performance in terms of cost-effectiveness, accuracy, and scoring biases. ✍️ Tutorials: Laurie’s   Advanced Querying & Retrieval Techniques comprehensive code-level tutorial  on 7 advanced querying and retrieval techniques including SubQuestionQuery Engine, Small-to-big retrieval, Metadata filtering, Hybrid search, Recursive Retrieval, Text to SQL, and Multi-document agents. Hubel Labs ’ Advanced RAG  video tutorial  with Llamaindex & OpenAI GPT: Sentence Window Retrieval vs Basic Chunking Developers Digest   video tutorial  on getting started with llamaindex.ts . Anil’s   tutorial  on How to train a custom GPT on your data with EmbedAI + LlamaIndex. 🎥  Webinars: Tony Kipkemboi  (Streamlit) and  Yi Ding  (LlamaIndex)  webinar  on Demystifying RAG apps with LlamaIndex! 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Dec 19, 2023", "tags": ["Llamaindex", "Rag", "Newsletter", "AI", "LLM"]},
{"title": "Introducing Llama Datasets 🦙📝", "content": "(Authors: Andrei Fajardo and Jerry Liu @ LlamaIndex) Today we’re excited to introduce  Llama Datasets  🦙 📝— a set of community-contributed datasets that allow users to easily benchmark their RAG pipelines for different use cases. A dataset consists of both question-answer pairs as well as source context. To use them, download them from LlamaHub; then evaluate your RAG pipeline using the dataset + a set of evaluation metrics. We’re launching with an initial set of 10 evaluation datasets and we’ll be adding more! We’ve also made it super easy to contribute your own dataset — upload your source documents + QA pairs (generated manually or synthetically). Context A big problem in building production RAG is evaluation. Unlike traditional software systems, LLM systems (and ML systems more generally) are stochastic black-boxes designed to model noisy real-world signals. This means that developers can’t easily define unit tests that assert deterministic behavior — there may always be an input that causes an error. Because developers don’t quite know what goes out given what goes in, they need to define an  evaluation dataset  that’s reflective of their production use cases, and evaluate their system over this dataset using a set of  evaluation metrics . We’ve presented  extensively on this topic  — every AI engineer should setup evaluation before trying to optimize their LLM or RAG application with advanced techniques. But we’ve increasingly found that defining the  right evaluation dataset is hard and use-case dependent . Evaluating over academic benchmarks, like BEIR and HotpotQA oftentimes fail to generalize to specific use cases. Certain parameters that work well on certain data domains (e.g. SEC filings) may fail on others (e.g. research papers). That’s what inspired us to create Llama Datasets. Instead of being prescriptive on the data you must use, we’ve decided to create a hub where you can easily pick and choose the right datasets for your use case! Llama Datasets on LlamaHub Overview Today’s launch includes the set of Llama Datasets on  LlamaHub , an accompanying  RagEvaluatorPack  to help compute metrics over a dataset, as well as accompanying dataset abstractions that you can also use on their own. To use a Llama Dataset, download it off LlamaHub and run our  RagEvaluatorPack  (or run your own evaluation modules). To generate a Llama Dataset, define a  LabelledRagDataset  with a set of  LabelledRagDataExample objects. To contribute a Llama Dataset, submit a “data card” to LlamaHub and upload your raw dataset files to our  llama_datasets  repository. Check out the below sections for a walkthrough over an example dataset. We’re launching with 10 initial datasets: Blockchain Solana Dataset Coda Help Desk Dataset (with Braintrust) FinanceBench Dataset (Patronus AI) Paul Graham Essay Dataset Llama 2 Paper Dataset Uber/Lyft 2021 10K Filings Dataset Mini Truthful QA Dataset (Arize AI) Mini Squad V2 Dataset (Arize AI) Origin of COVID-19 LLM Survey Paper Dataset Example Llama Dataset page Example Walkthrough Let’s walk through the different steps of using/contributing a Llama Dataset. 1. Downloading and Using a Llama Dataset Follow the full notebook here. Downloading a dataset is simple, do the following command (here we download Paul Graham). from  llama_index.llama_dataset  import  download_llama_dataset\n\n # download and install dependencies \nrag_dataset, documents = download_llama_dataset(\n     \"PaulGrahamEssayDataset\" ,  \"./paul_graham\" \n) This downloads a  rag_dataset  which contains the QA pairs (+ reference context), and  documents  which is the source document corpus. Let’s inspect the  rag_dataset  with  to_pandas()  : Sample rows from `rag_dataset` Generating predictions over the RAG dataset is straightforward. You can easily plug in any query engine into  amake_predictions_with  : from  llama_index  import  VectorStoreIndex\n\n # a basic RAG pipeline, uses service context defaults \nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n # generate prediction dataset \nprediction_dataset =  await  rag_dataset.amake_predictions_with(\n    query_engine=query_engine, show_progress= True \n) The  prediction_dataset  is a  RagPredictionDataset  object that looks like the following: Prediction Dataset Given the  rag_dataset  and  prediction_dataset  , you can use our evaluation modules to measure performance across a variety of metrics (e.g. faithfulness, correctness, relevancy). for  example, prediction  in  tqdm.tqdm(\n    zip(rag_dataset.examples, prediction_dataset.predictions)\n):\n    correctness_result = judges[ \"correctness\" ].evaluate(\n        query=example.query,\n        response=prediction.response,\n        reference=example.reference_answer,\n    ) To eliminate the boilerplate of writing all these evaluation modules, we’ve also provided a LlamaPack that will do all this for you! from  llama_index.llama_pack  import  download_llama_pack\n\nRagEvaluatorPack = download_llama_pack( \"RagEvaluatorPack\" ,  \"./pack\" )\nrag_evaluator = RagEvaluatorPack(\n    query_engine=query_engine, rag_dataset=rag_dataset\n)\nbenchmark_df =  await  rag_evaluator.arun() 2. Generating a Llama Dataset Follow the full notebook here. You can use our  LabelledRagDataExample  and  LabelledRagDataset  abstractions to create your own dataset. Here’s an example of adding an example manually. from llama_index.llama_dataset import (\n    LabelledRagDataExample,\n    CreatedByType,\n    CreatedBy,\n)\n\n# constructing a LabelledRagDataExample\nquery = \"This is a test query, is it not?\"\nquery_by = CreatedBy(type=CreatedByType.AI, model_name=\"gpt-4\")\nreference_answer = \"Yes it is.\"\nreference_answer_by = CreatedBy(type=CreatedByType.HUMAN)\nreference_contexts = [\"This is a sample context\"]\n\nrag_example = LabelledRagDataExample(\n    query=query,\n    query_by=query_by,\n    reference_contexts=reference_contexts,\n    reference_answer=reference_answer,\n    reference_answer_by=reference_answer_by,\n) from  llama_index.llama_dataset.rag  import  LabelledRagDataset\n\nrag_dataset = LabelledRagDataset(examples=[rag_example, rag_example_2]) You can also synthetically generate a dataset over any document corpus with GPT-4: # generate questions against chunks \n from  llama_index.llama_dataset.generator  import  RagDatasetGenerator\n from  llama_index.llms  import  OpenAI\n from  llama_index  import  ServiceContext\n\n # set context for llm provider \ngpt_4_context = ServiceContext.from_defaults(\n    llm=OpenAI(model= \"gpt-4\" , temperature= 0.3 )\n)\n\n # instantiate a DatasetGenerator \ndataset_generator = RagDatasetGenerator.from_documents(\n    documents,\n    service_context=gpt_4_context,\n    num_questions_per_chunk= 2 ,   # set the number of questions per nodes \n    show_progress= True ,\n) 3. Contributing a Llama Dataset We’ve provided a  ready-made submission notebook template here  — just fill in the blanks with your dataset! If you’re interested in contributing a dataset, we’d love to feature it! You just need to follow these steps: Create the dataset:  To create a  LabelledRagDataset  , you can create it from scratch either manually or with synthetically generated examples, or create it from an existing dataset. Generate a baseline evaluation dataset:  Benchmark a basic top-k RAG pipeline over your dataset, and report the numbers. This will serve as a point of reference for others. You can use the  RagEvaluatorPack  for this purpose. Prepare the dataset card (  card.json  ) and  README.md  :  These will be shown on the LlamaHub page for this dataset. If you want to auto-generate this given some inputs, check out our  LlamaDatasetMetadata LlamaPack. Submit a PR into  llama-hub  to register the  LlamaDataset  . Submit a PR into  llama-datasets  to upload the  LlamaDataset  and its source files. You can follow all of these steps in our  notebook template above  — simply substitute your own data. Conclusion We’d love for you to check out our datasets and let us know your feedback! We’d love your contributions as well. Resources Here are the resources mentioned in the blog post. Llama Datasets on LlamaHub  (make sure to select “Llama Datasets” from the dropdown) Downloading a Llama Dataset Notebook Creating a Llama Dataset Notebook Contributing a Llama Dataset Notebook Template README on Contributing a Llama Dataset", "author": "Jerry Liu", "date": "Dec 4, 2023", "tags": ["Llamaindex", "Dataset", "Evaluation", "LLM", "AI"]},
{"title": "LlamaIndex Newsletter 2023–12–05", "content": "Hello Llama Community 🦙, We are excited to collaborate with DeepLearningAI and TruEraAI to launch an extensive course on advanced Retrieval-Augmented Generation (RAG) and its evaluations. The course includes Sentence Window Retrieval, Auto-merging Retrieval, and Evaluations with TruLensML, providing practical tools for enhanced learning and application. To make the most of this learning opportunity, we invite you to  take the course . We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. 🤩  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  Tweet . Core  guide   for Full-Stack LLM App Development : Simplifies complex app development with tools like ‘create-llama’ for full-stack apps, ‘SEC Insights’ for multi-document processing, and ‘LlamaIndex Chat’ for chatbot customization. ✨ Feature Releases and Enhancements: We’ve launched seven advanced retrieval LlamaPacks, serving as templates to easily build advanced RAG systems. These packs simplify the process to almost a single line of code, moving away from the traditional notebook approach. The techniques include Hybrid Fusion, Query Rewriting + Fusion, Retrieval with Embedded Tables, Auto-merging Retriever, Sentence Window Retriever, Node Reference Retriever, and Multi-Document Agents for handling complex queries.  Tweet . We introduce new abstractions for structured output extraction in multi-modal settings, enabling the transformation of images into structured Pydantic objects. This enhancement is particularly useful for applications like product reviews, restaurant listings, and OCR.  Notebook ,  Tweet . We introduce the  OpenAI Cookbook , a guide focused on evaluating RAG systems using LlamaIndex. It encompasses understanding RAG systems, building them with LlamaIndex, and evaluating their performance in retrieval and response generation.  Blog ,  Notebook ,  Tweet . We launched RAGs v3 — a bot that transcends traditional limits by incorporating web search capabilities. This bot, designed to operate in natural language rather than code, offers an enhanced experience compared to the combination of ChatGPT and Bing. Leveraging our integration with Metaphor Systems — a search engine tailored for Large Language Models (LLMs) — the bot can retrieve relevant text from the internet to provide answers beyond its internal corpus. Additionally, users can now view the tools the agent uses, with the web search feature exclusively accessible to our OpenAI agent.  Repo ,  Tweet . We have significantly improved the speed of extracting structured metadata (like titles and summaries) from text to enhance RAG performance. Our new implementation offers 2x to 10x faster processing, overcoming the limitations of previous slower methods.  Docs ,  Tweet . We have made it incredibly easy to set up a RAG + Streamlit app, now possible with just a single line of code using our  StreamlitChatPack . This pack provides a ready-to-use RAG pipeline and a Streamlit chat interface, customizable in terms of data sources and retrieval algorithms.  Docs ,  Tweet . 👀 Demo: AInimal Go — an innovative multi-modal app inspired by Pokemon-Go. This interactive application, developed by  Harshad Suryawanshi , lets users capture or upload images of animals, classify them using the ResNet-18 model, and engage in conversations with the animals, augmented by a knowledge base of over 200 Wikipedia articles. Notably, the app employs a targeted ResNet model for classification, offering enhanced speed and cost efficiency, instead of using GPT-4V. Blog ,  Repo ,  HuggingFace Space ,  Tweet . 🗺️ Guides: We introduce a core  guide  within the LlamaIndex ecosystem, designed to simplify “full-stack” app development, which is notably more complex than notebook development. This includes ‘create-llama’ for building full-stack apps with advanced templates, ‘SEC Insights’ for multi-document handling of over 10,000 filings, and ‘LlamaIndex Chat’ for a customizable chatbot experience. All tools are open-source with full guides and tutorials available. Guide  on using the Table Transformer model with GPT-4V for advanced RAG applications in parsing tables from PDFs: Our method involves CLIP for page retrieval, Table Transforms for table image extraction, and GPT-4V for answer synthesis. This approach is compared with three other multi-modal table understanding techniques, including using CLIP for whole page retrieval, text extraction and indexing with GPT-4V, and OCR on table images for context. Guide  on analyzing various multi-modal models for their ability to extract structured data from complex product images on an Amazon page. The models compared include GPT-4V, Fuyu-8B, MiniGPT4, CogVLM-4, and LLaVa-13B. Key findings reveal that all models incorrectly identified the number of reviews (correct answer: 5685), only GPT-4V and Fuyu accurately determined the price, each model’s product description varied from the original, and Mini-GPT4 incorrectly assessed the product rating. ✍️ Tutorials: Jo Kristian Bergum   blog post  on Hands-On RAG guide for personal data with Vespa and LLamaIndex. Wenqi Glantz  made a  tutorial  on Llama Packs: The Low-Code Solution to Building Your LLM Apps. Liza Shulyayeva ’s in-depth  tutorial  on building and deploying a retrieval-augmented generation (RAG) app to conversationally query the contents of your video library 🎥  Webinars: Webinar  on PrivateGPT — Production RAG with Local Models. 🏆  Hackathons: Your reminder that there’s still time to join  the TruEra Challenge , an online hackathon from Dec 1st to 8th, and explore AI observability with technology from TruEra AI and Google Vertex AI. Use the LlamaIndex framework to enhance your LLM-based app. Participants receive $30 in Google Cloud credits, plus an additional $100 upon solution submission. Winners share a $9,000 cash prize pool and $14,000 in Google Cloud credits. We partnered with Zilliz Universe to participate in their  Advent of Code  event. This December, explore 25 open-source projects, with daily challenges to build something in 30 minutes or less. It’s a great opportunity to learn new skills and have winter fun. For tips, tutorials, and resources, visit the Advent of Code channel in Discord each day.", "author": "LlamaIndex", "date": "Dec 5, 2023", "tags": ["Llamaindex", "LLM", "Rag", "Newsletter", "AI"]},
{"title": "Two new llama-datasets and a Gemini vs. GPT showdown", "content": "(Authored by Andrei Fajardo at LlamaIndex) The llama-dataset collection. Each labelled llama-dataset is comprised of its associated labelled examples. With examples, we make predictions with the appropriate object depending on the task. After making predictions, we can evaluate the performance of the object by measuring some distance between predictions and the corresponding references. Intro A few weeks back, we  launched  our very first set of llama-datasets, namely the  LabelledRagDataset . The main purpose of these llama-datasets is to provide builders with the means to benchmark their LLM systems in an effective and efficient manner. In the couple of weeks since that launch date, we’ve amassed over a dozen  LabelledRagDataset s via both staff and community contributions (all of which are available for download through  LlamaHub )! The fun doesn’t stop there though: today we’re introducing two new llama-dataset types:  LabelledEvaluatorDataset  and the  LabelledPairwiseEvaluatorDataset . These new llama-dataset types are meant for evaluating or benchmarking an LLM evaluator. Indeed, the adopted standard for evaluating LLM responses is to use a strong LLM as an evaluator. This approach is certainly more scalable, faster, and cheaper than using human evaluators via crowdsourcing. However, these LLM evaluators themselves must also be continuously evaluated rather than blindly trusted. In this post, we provide a brief overview of the new llama-datasets as well as provide some very interesting results from benchmarking Google’s Gemini and OpenAI’s GPT models as LLM evaluators on the MT-Bench datasets which we’ve converted into the new llama-dataset types. A primer on the new llama-datasets Before getting into the new llama-datasets, recall that with  LabelledRagDataset  our end goal was to use it to evaluate or benchmark a Retrieval-Augmented Generation (RAG) system. The way to do that with our llama-dataset abstractions is to build a  QueryEngine  (i.e., a RAG system) and then use it to make “predictions” over the  LabelledRagDataset . With the predictions in hand, we can evaluate the quality of these predictions by comparing it to the corresponding reference attributes of the  LabelledRagDataset . Benchmarking flow with LabelledRagDataset. With a query engine, predictions are made over every labelled example. We can then compare predicted responses and contexts with the reference versions (i.e., labels). This flow is conveniently handled via the RagEvaluatorPack. In a similar vein, the new llama-datasets are meant to benchmark an LLM evaluator. Let’s go through the first kind, the  LabelledEvaluatorDataset . Here, instead of the RAG system making predictions on a  LabelledRagDataset  we have an LLM evaluator making “predictions” over a  LabelledEvaluatorDataset  — predictions in this context means that the LLM evaluator is evaluating the response produced by another LLM model to a given query. As before, with the predictions in hand, we can measure the goodness of the LLM evaluator’s evaluations by comparing it to the corresponding reference attributes of the  LabelledEvaluatorDataset . Benchmarking flow with LabelledEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of the answer to the query and optional contexts and ground truth answer. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner. The second llama-dataset we’re introducing today can be considered an extension of the first one. The  LabelledPairwiseEvaluatorDataset  is similarly used for benchmarking an LLM evaluator. However, there is a subtle difference in the evaluation task as here the LLM evaluator compares two generated answers from two separate LLMs. Outside of this difference, the flow for using this llama-dataset to benchmark an evaluator remains the same. Benchmarking flow with LabelledPairwiseEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of two answers to the query and optional contexts and ground truth answer. That is, the LLM evaluator ranks the two answers to determine the superior one. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner. Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves GPT-3.5 performance! In this section, we will put our new llama-dataset types to use in order to pit Gemini Pro against GPT models. For this, we’re going to use slightly adapted versions of the MT-Bench dataset. These adapted versions have been made available for download and use through  LlamaHub  along with today’s release! Mini MT-Bench Single Grading Dataset This llama-dataset is a  LabelledEvaluatorDataset  and is a miniature version of the MT-Bench single-grading dataset. In particular, we consider all of the 160 original questions (i.e., 80 x 2, since MT Bench is a two-turn question dataset), but only the responses produced by Llama2-70b. For the reference evaluations, we use GPT-4. As with the original  LabelledRagDataset , we’ve produced a new llama-pack  EvaluatorBenchmarkerPack  (of course, also made available in today’s release!) to make benchmarking an LLM evaluator on the new llama-datasets relatively effortless. The below snippet of code is how you can replicate the results of this benchmark from  llama_index.llama_dataset  import  download_llama_dataset\n from  llama_index.llama_pack  import  download_llama_pack\n from  llama_index.evaluation  import  CorrectnessEvaluator\n from  llama_index.llms  import  Gemini\n from  llama_index  import  ServiceContext # download dataset\nevaluator_dataset, _ = download_llama_dataset(\n    \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n) # define evaluator\ngemini_pro_context = ServiceContext.from_defaults(\n    llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n)\nevaluator = CorrectnessEvaluator(service_context=gemini_pro_context) # download EvaluatorBenchmarkerPack and define the benchmarker\nEvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\nevaluator_benchmarker = EvaluatorBenchmarkerPack(\n    evaluator=evaluators[\"gpt-3.5\"],\n    eval_dataset=evaluator_dataset,\n    show_progress=True,\n) # produce the benchmark result\nbenchmark_df = await evaluator_benchmarker.arun(\n\t\tbatch_size=5,\n\t\tsleep_time_in_seconds=0.5\n) Benchmark Results Invalid_predictions occurs whenever the LLM evaluator fails to produce the desired output structure and as well as other exceptions. Correlations represent the correlations with the scores produced by each of the evaluators with the reference scores produced by the reference evaluator GPT-4. Similarly, the remaining two metrics, MAE (i.e., mean absolute error, which is a sum of the absolute differences between each pair of evaluator and reference scores) and Hamming (i.e., which counts how many times evaluator and reference scores are equivalent), are computed with the scores produced by the evaluators and those from the reference evaluator. Observations It seems that Gemini-Pro and GPT-3.5 are quite close in terms of their closeness to the reference evaluator GPT-4! As for GPT-4 versus the reference GPT-4, this is mostly used for assessing self-consistency of the LLM, for which we see it does a fairly good job at that. MT-Bench Human Judgement Dataset For this benchmark, we’ll evaluate the LLM evaluators on the task of ranking two LLM responses, to determine which of the two is the better one. And it is for this such task that  LabelledPairwiseEvaluatorDataset  exists. The llama-dataset that we’ve curated here is a slightly adapted version of the original MT-Bench Human Judgement dataset. Specifically, in the original dataset, there are some replications with respect to the triple (query, model A, model B) examples since for some of these more than one human evaluation was provided. Since our prompt allows the LLM evaluator to deem a tie, and to our knowledge, this wasn’t made an option for the human evaluators, we have aggregated the results across the different human evaluations to get the proportion of times model A wins versus model B for each triple (query, model A, model B). We then say that human evaluators deem a tie if the proportion lies between 0.4 and 0.6. It should be emphasized here that the reference evaluations are provided by humans, and so the benchmark metrics that we produce and share here represent the LLM agreement with humans. (We skip showing the code snipped to produce the results here, because they’re essentially the same as the previously shared code snipper with the exception of requiring a  PairwiseComparisonEvaluator  instead of a  CorrectnessEvaluator .) Benchmark Results Invalid_predictions are as defined in the previous benchmark. Inconclusive’s represent the case when an LLM evaluator flips its winner after prompting it with the same evaluation task but instead flipping the order of presentation of the two answers (i.e. to mitigate against position bias). Two agreement rates, with the inclusion and exclusion of ties, are also produced — note that these are both conditional in the event that the prediction (or evaluation) is valid. Observations In terms of agreement rates, all three models seem quite close. Note again that these are conditional on the prediction/evaluation being valid. And so, one should “discount” these with the invalid and inconclusive counts. Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50–60 ties to GPT-4’s 100 ties. Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5! Go now and evaluate your evaluators (and eat your veggies)! It is, for obvious reasons, important to evaluate your LLM evaluators, as these are now being relied upon to evaluate the performance of our LLM systems — a broken compass is not really helpful! With these newly introduced llama-datasets, we hope that it is easy for you to compile your own benchmark datasets on your own data, and then even easier to produce your benchmark metrics. As mentioned before, the two llama-datasets discussed in this blog are available for download and use through  LlamaHub . Be sure to visit and make use of the datasets there to build an exhaustive benchmark suite! (We welcome contributed llama-datasets as well!) Related Links MT-Bench Human Judgement Benchmarking Notebook MT-Bench Single Grading Benchmarking Notebook", "author": "LlamaIndex", "date": "Dec 20, 2023", "tags": ["Benchmarking", "Gpt 4", "Gemini", "Llamaindex", "Evaluation"]},
{"title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama", "content": "You may have heard the fuss about the latest release from European AI powerhouse  Mistral AI : it’s called Mixtral 8x7b, a “mixture of experts” model — eight of them, each trained with 7 billion parameters, hence the name. Released originally as a  mic-drop tweet  they followed up a few days later with a  blog post  that showed it matching or exceeding GPT-3.5 as well as the much larger Llama2 70b on a number of benchmarks. Here at LlamaIndex we’re naturally fans of open source software, so open models with permissive licenses like Mixtral are right up our alley. We’ve had a few questions about how to get Mixtral working with LlamaIndex, so this post is here to get you up and running with a totally local model. Step 1: Install Ollama Previously getting a local model installed and working was a huge pain, but with the release of  Ollama , it’s suddenly a snap! Available for MacOS and Linux (and soon on Windows, though you can use it today on Windows via  Windows Subsystem For Linux ), it is itself open source and a  free download . Once downloaded, you can get Mixtral with a single command: ollama run mixtral The first time you run this command it will have to download the model, which can take a long time, so go get a snack. Also note that it requires a hefty 48GB of RAM to run smoothly! If that’s too much for your machine, consider using its smaller but still very capable cousin  Mistral 7b , which you install and run the same way: ollama run mistral We’ll assume you’re using Mixtral for the rest of this tutorial, but Mistral will also work. Once the model is running Ollama will automatically let you chat with it. That’s fun, but what’s the point of having a model if it can’t work with your data? That’s where LlamaIndex comes in. The next few steps will take you through the code line by line, but if you’d prefer to save all the copying and pasting, all of this code is available in an  open-source repo  that you can clone to follow along there. Step 2: Install your dependencies You’re going to need LlamaIndex installed, obviously! We’ll also get you going with a handful of other dependencies that are about to come in handy: pip install llama-index qdrant_client torch transformers Step 3: Smoke test If you’ve got Ollama running and LlamaIndex properly installed, the following quick script will make sure everything is in order by asking it a quick “smoke test” question in a script all by itself: # Just runs .complete to make sure the LLM is listening \n from  llama_index.llms  import  Ollama\n\nllm = Ollama(model= \"mixtral\" )\nresponse = llm.complete( \"Who is Laurie Voss?\" )\n print (response) Step 4: Load some data and index it Now you’re ready to load in some real data! You can use any data you want; in this case I’m using a  small collection of my own tweets  which you can download, or use your own! We’re going to be storing our data in the nifty, open source  Qdrant  vector database (which is why we got you to install it earlier). Create a new python file, and load in all our dependencies: from  pathlib  import  Path\n import  qdrant_client\n from  llama_index  import  (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\n from  llama_index.llms  import  Ollama\n from  llama_index.storage.storage_context  import  StorageContext\n from  llama_index.vector_stores.qdrant  import  QdrantVectorStore Then load our tweets out of our JSON file using a nifty JSONReader from  LlamaHub , our convenient collection of open source data connectors. This will give you a pile of documents ready to be embedded and indexed: JSONReader = download_loader(\"JSONReader\")\nloader = JSONReader()\ndocuments = loader.load_data(Path('./data/tinytweets.json')) Get Qdrant ready for action by initializing it and passing it into a Storage Context we’ll be using later: client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store) Now set up our Service Context. We’ll be passing it Mixtral as the LLM so we can test that things are working once we’ve finished indexing; indexing itself doesn’t need Mixtral. By passing  embed_model=\"local\" we’re specifying that LlamaIndex will embed your data locally, which is why you needed  torch  and  transformers . llm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\") Now bring it all together: build the index from the documents you loaded using the service and storage contexts you already set up, and give it a query: index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query( \"What does the author think about Star Trek? Give details.\" )\n print (response) Ollama will need to fire up Mixtral to answer the query, which can take a little while, so be patient! You should get output something like this (but with more details): Based on the provided context information, the author has a mixed opinion about Star Trek. Verify our index Now to prove it’s not all smoke and mirrors, let’s use our pre-built index. Start a new python file and load in dependencies again: import  qdrant_client\n from  llama_index  import  (\n    VectorStoreIndex,\n    ServiceContext,\n)\n from  llama_index.llms  import  Ollama\n from  llama_index.vector_stores.qdrant  import  QdrantVectorStore This time we won’t need to load the data, that’s already done! We will need the Qdrant client and of course Mixtral again: client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\") This time instead of creating our index from documents we load it directly from the vector store using  from_vector_store . We’re also passing  similarity_top_k=20  to the query engine; this will mean it will fetch 20 tweets at a time (the default is 2) to get more context and better answer the question. index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\nquery_engine = index.as_query_engine(similarity_top_k= 20 )\nresponse = query_engine.query( \"Does the author like SQL? Give details.\" )\n print (response) Build a little web service It’s no good having an index that just runs as a script! Let’s make an API out of this thing. We’ll need two new dependencies: pip install flask flask-cors Load in our dependencies as before into a new file: from  flask  import  Flask, request, jsonify\n from  flask_cors  import  CORS, cross_origin\n import  qdrant_client\n from  llama_index.llms  import  Ollama\n from  llama_index  import  (\n    VectorStoreIndex,\n    ServiceContext,\n)\n from  llama_index.vector_stores.qdrant  import  QdrantVectorStore Get the vector store, the LLM and the index loaded: # re-initialize the vector store\nclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\n# get the LLM again\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n# load the index from the vector store\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context) Set up a really basic Flask server: app = Flask(__name__)\ncors = CORS(app)\napp.config[ 'CORS_HEADERS' ] =  'Content-Type' \n\n # This is just so you can easily tell the app is running \n @app.route( '/' ) \n def   hello_world ():\n     return   'Hello, World!' And add a route that accepts a query (as form data), queries the LLM and returns the response: @app.route( '/process_form' , methods=[ 'POST' ] ) \n @cross_origin() \n def   process_form ():\n    query = request.form.get( 'query' )\n     if  query  is   not   None :\n        query_engine = index.as_query_engine(similarity_top_k= 20 )\n        response = query_engine.query(query)\n         return  jsonify({ \"response\" :  str (response)})\n     else :\n         return  jsonify({ \"error\" :  \"query field is missing\" }),  400 \n\n if  __name__ ==  '__main__' :\n    app.run() Note those last two lines, they’re important!  flask run  is incompatible with the way LlamaIndex loads dependencies, so you will need to run this API directly like so (assuming your file is called  app.py ) python app.py With your API up and running, you can use cURL to send a request and verify it: curl --location ' &lt; http://127.0.0.1:5000/process_form &gt; ' \\\\\n--form 'query=\"What does the author think about Star Trek?\"' You’re done! We covered a few things here: Getting Ollama to run Mixtral locally Using LlamaIndex to query Mixtral 8x7b Building and querying an index over your data using Qdrant vector store Wrapping your index into a very simple web API All open-source, free, and running locally! I hope this was a fun, quick introduction to running local models with LlamaIndex!", "author": "LlamaIndex", "date": "Dec 21, 2023", "tags": ["LLM", "Mistral Ai", "Ollama", "Llamaindex"]},
{"title": "LlamaIndex Newsletter 2023–12–12", "content": "Howdy, Llama Enthusiasts 🦙, We are thrilled to announce another exciting week filled with full of the latest updates, features, insightful tutorials, guides, webinars, and so much more. Have a groundbreaking project, compelling article, or captivating video? We’re all ears! Reach out to us at  news@llamaindex.ai . Don’t forget to subscribe to our newsletter via our  website  to have all these exciting developments delivered directly to your inbox. 🤩  First, the highlights: Llama Datasets:  A diverse collection of community-contributed datasets for benchmarking RAG pipelines.  Blog ,  Tweet . RAGs v5:  Enables multi-modal data handling with natural language for both text and image sources.  Tweet . Production RAG Pipeline:  New features and a guide for efficient RAG while handling updates to your data, including incremental re-indexing for Google Docs and enhanced transformation and caching processes.  Notebook ,  Tweet . Revamped LlamaHub:  A community-driven hub with universal data loaders, a new user interface, and a range of tools, templates, and datasets.  Tweet . AutoTranslateDoc:  An open-source project for translating GitHub repository documentation into over 15 languages.  Blog ,  Repo ,  Tweet ✨ Feature Releases and Enhancements: We launched  Llama Datasets  🦙📝, a collection of community-contributed datasets tailored for benchmarking RAG pipelines in various use cases. These datasets offer flexibility in selecting the most appropriate one for specific LLM applications. The initial release includes a diverse range, such as Code Help Desk, FinanceBench, Mini TruthfulQA, Mini Squad V2, Blockchain Solana, Uber 10K, Llama 2 Paper, Paul Graham Essay, Origin of COVID-19, CovidQADataset, MiniCovidQADataset and LLM Survey Paper. Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing a platform for comprehensive benchmarking across multiple metrics. All datasets are available on LlamaHub for easy download and evaluation.  Blog ,  Tweet . We launched  RAGs v5 , enabling multi-modal data handling with natural language for both text and image sources. Key features include enhanced multi-modal indexing, the capability to view sources in any RAG agent, and support for loading entire directories, not just single files.  Tweet . We have launched new features and a guide for building a  production RAG pipeline , enabling efficient question-answering with LLMs on production data even while it is continuously updated. This includes incremental re-indexing for Google Docs changes and enhanced transformation and caching processes in our updated  IngestionPipeline .  Notebook ,  Tweet . We launched a one-click, full-stack LlamaIndex template now available on  Replit ! This template features a full-stack Next.js app in TypeScript, capable of reading any files you provide, and includes a chat interface for querying those documents. It’s completely customizable and based on our popular create-llama generator.  Replit Template ,  Tweet . We have introduced  RAGEvaluatorPack  to easily benchmark your RAG pipeline on any dataset with a single line of code, offering metrics like correctness, relevancy, and context similarity.  Docs ,  Tweet . We released community templates for create-llama, offering a selection of community-contributed starter templates during setup. Current examples include  embedded-tables  for analyzing complex tables in large PDFs, and  multi-document-agent  for comparing multiple documents.  Tweet . We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds.  Tweet . We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities.  Docs ,  Tweet . We launched the revamped  LlamaHub , a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets.  Tweet . We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects.  Blog ,  Repo ,  Tweet We released support for exact match and range queries in 4 vector databases including Weaviate, Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying.  Tweet . 🗺️ Guides: Guide  on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages. Guide  on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It’s part of our Snowflake BUILD talk and PyData Global talk. ✍️ Tutorials: Blog  by  Waii.ai  on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends. Wenqi Glantz’s  tutorial  on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine. An hour comprehensive workshop  tutorial  by  AIMakerspace  on RAG strategies over complex documents through recursive retrieval. Laurie’s   video  on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses. Ravi Theja’s   video  on Understanding LlamaIndex 0.9v abstractions and features. 🤝 Integrations: We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions.  Blog . We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code.  Docs ,  Tweet . We integrated FlagEmbeddingReranker to further boost your RAG pipeline.  Notebook ,  Tweet . 🎥  Webinars: Webinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open-source multi-modal models of LLaVa, which are competitive with GPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by Haotian Zhang from the LlamaIndex team. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Dec 12, 2023", "tags": ["Llamaindex", "LLM", "Newsletter", "Rag", "AI"]},
{"title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc", "content": "Author:  Pierre-Loic Doulcet As programmers, we often find ourselves limited by language barriers. Documentation for various programming frameworks and tools is predominantly available in English, and increasingly in languages like Chinese, creating challenges for non-native speakers. I faced similar obstacles in my early programming days, and it was only through community efforts like  traduc.org ’s translation of man pages that I could surmount them. Today, we are excited to unveil a solution to this pervasive issue:  AutoTranslateDoc , a command-line tool designed to democratize access to technical documentation by breaking down language barriers. How AutoTranslateDoc Works Collect the Documentation: The tool connects to GitHub, identifying and downloading .md and .mdx files from any repository. Chunk and Prepare: The documentation is then chunked or split for translation. Translate Efficiently: Utilizing the power of LLMs like GPT-3.5 and GPT-4, each chunk of documentation is translated accurately. Verify and Enhance: The translation is automatically verified, with retranslation if needed, ensuring the highest quality. Consolidate: Finally, the chunks are amalgamated back into a cohesive document. Our initial tests on translating the llamaIndexTS documentation have been highly promising. You can now read our docs in over a dozen languages including  Chinese ,  French , and  Spanish ! Getting Started Install AutoTranslateDoc easily via npm, or clone the repo ( https://github.com/run-llama/automatic-doc-translate ) : npm install -g autotranslatedoc Try it out with run-lama/LlamaIndexTS or your favorite repo! You will need a  GitHub Personal Access Token  and an  OpenAI API Key  (the tool will prompt you to set these): # Translate\nautotranslatedoc translate run-llama LlamaIndexTS -d apps/docs -l fr #build\nautotranslatedoc build run-llama LlamaIndexTS -d apps/docs -l fr This translates the directory  apps/docs  in the GitHub repo  run-llama/LlamaIndexTS . Improving Accuracy and Consistency Our commitment to improving translation accuracy led us to innovate in both the translation process and verification methods. Strategic Document Splitting: We approach translation by dividing each page of the documentation into sections. To provide enhanced context and coherence, each section’s title hierarchy is appended to its respective chunk during translation. This technique ensures that the translated content maintains the original structure and thematic relevance. Rigorous Translation Verification: Our verification process is designed to rigorously assess the accuracy of translations. We employ several checks on the translated documentation: Translation Length Check: We compare the length of the translated text with the original to ensure consistency. Title Hierarchy Analysis: We verify that no new sections are inadvertently added in the translation. Link Count Validation: The number of hyperlinks is matched against the original to ensure none are missed or added unnecessarily. Code Block Accuracy: The presence and correctness of code blocks in the translation are checked against the original document. These checks address common issues with LLMs, such as hallucination or omission, and prompt retranslation when necessary. This rigorous process significantly enhances the accuracy of our translations. Moreover, we incorporate a unique self-critique feature, where the LLM evaluates its own translation output, further refining the quality. This dual approach of meticulous chunking and thorough verification ensures that our translations are not only accurate but also contextually relevant, maintaining the integrity and utility of the original documentation. Managing Documentation Updates: Keeping Translations Current Documentation, by its nature, is a dynamic entity that evolves over time. Recognizing this, we’ve integrated a robust system into AutoDocTranslate to manage documentation updates efficiently. Historical Tracking through JSON: When translating a repository using our tool, a .json file is generated, chronicling the history of translations. This file is crucial for tracking changes and versions in the documentation. It serves as a foundation for differential translation, a process that identifies and translates only the newly added or modified content. This feature can be accessed through the  autotranslatedoc update  command, streamlining the maintenance of up-to-date translations. Future Enhancements: We are actively working on enhancing this system with the following features: Manual Change Integration:  Recognizing that translations might undergo manual edits post-generation, we are developing functionality to account for these manual changes during updates. This will ensure that any human revisions are retained and only new or altered sections from the source documentation are translated in subsequent updates. GUI for Translation Management:  To further simplify the process of translation editing, tracking, and verification, we’re in the early stages of developing a graphical user interface (GUI). This interface will allow users to interact more intuitively with the translations. An experimental version of this feature can be accessed through the  autotranslatedoc serve  command. This GUI will enable users to visually navigate through the translations, make edits, and verify the accuracy of the content more efficiently. By continually updating and refining these features, AutoDocTranslate aims to stay at the forefront of making technical documentation universally accessible and easy to maintain in multiple languages. The Future of Technical Documentation AutoDocTranslate is more than a tool; it’s a step towards an inclusive, barrier-free tech world where language is no longer an impediment to learning and growth. We’re excited to see how it empowers programmers across the globe. Join us in this journey and contribute to a more accessible programming community!", "author": "LlamaIndex", "date": "Dec 8, 2023", "tags": ["LLM", "AI", "Translation", "I18n", "Llamaindex"]},
{"title": "Scaling LlamaIndex with AWS and Hugging Face", "content": "Over the holidays, I was running some retrieval benchmarks with LlamaIndex. I found myself rebuilding an index repeatedly with 30K documents, and finding waiting 10–20 minutes each time was too grating. So to solve this, issue, I decided to bite the bullet and figure out how to deploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my data. This brought the processing time down to around 5 minutes! Proposed system architecture If you want to skip the detailed steps, you can jump to the code at the following repository: https://github.com/run-llama/llamaindex_aws_ingestion NOTE:  I am not an AWS expert, and had zero experience with it before this project. There are likely ways to improve upon the design I came up with. This blog merely documents my first foray into getting a system working on AWS. My hope is that this helps other people get started, and opens the door for other engineers deploying more scale-able systems. Step 1: Figuring out how AWS works To use AWS effectively, there are several packages and tools that you will need: AWS account signup Install AWS CLI Used to authenticate your AWS account for CLI tools Install eksctl Used to create  EKS  clusters easily Install kubectl Used to configure and debug deployments, pods, services, etc. Install Docker As you will see, nearly all AWS deployments revolve around  yaml  files that describe what you are deploying and how they connect together, as well as some CLI commands to actually run the deployment. If at any time you aren’t sure what’s going on, I found it helpful to visit the AWS dashboard and explore the resources I had actually deployed. Usually, you will want to visit. I had the pages below favourited in AWS. Also, remember to set your region properly in the top right! My AWS console favourites Note on how deployments work For a majority of deployments, you will typically have The cluster The deployed app, scaled to X replicas A load balancer, to balance the incoming requests between X replicas In the examples below, most will have a  yaml  for the deployed app, a  yaml  for the load balancer, and a command to create the cluster you want to run on. Helpful CLI Commands A few CLI commands proved to be extremely helpful for debugging and monitoring deployments. # get the state of pods/deployments\nkubectl get pods\nkubectl get deployments\n\n# useful for seeing logs/events of pods + full yaml config\nkubectl describe pod <pod name>\nkubectl logs <pod name>\n\n# list clusters kubectl knows about\nkubectl config get-contexts\n\n# switch kubectl to another cluster\nkubectl config use-context <context name>\n\n# delete things\nkubectl delete <pod/deployment/service> <name> Step 2: Deploying Text Embeddings Interface In order to run embeddings fast, we will deploy an embeddings server using HuggingFace’s  Text Embedding Interface  (TEI). This server has production-level features and optimizations out-of-the-box, including continuous batching, flash-attention, rust implementation, and more. HuggingFace provides prebuilt docker images to simplify deployment. However, the first step to running embeddings fast is to have a GPU. If you just signed up for AWS, you will have to request a quota increase. For me, I requested a few times for G5 instances (which run an Nvidia A10G GPU), and after a few days of testing on CPU, AWS gave me access to use up to 4 G5 instances. Once you have a quota for GPU instances (like G5 nodes), you can create your cluster and deploy eksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\nsleep 5\nkubectl create -f ./tei-deployment.yaml\nsleep 5\nkubectl create -f ./tei-service.yaml\nsleep 5\necho \"Embeddings URL is:  &lt; http://$ &gt; (kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\" The code above will create a cluster, a deployment (i.e. our TEI server) and a load balancer server. You can see the yaml configs in  the repo , and you can edit them as needed. NOTE:  Make sure to write down the URL printed at the end! If you forget, you can get the URL in the  EKS  page on AWS. You’ll want the external IP for the load balancer. Step 3: Deploying RabbitMQ RabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each  Document  object is processed independently by a consumer. Deploying RabbitMQ on  EKS  was a little tricky, but using the RabbitMQ operator installed with  krew , many things are abstracted away. First, you need to create your cluster. For whatever reason, this didn’t work unless I also specified the zones eksctl create cluster \\\n  --name mqCluster \\\n  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f Since RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use  EBS  for storage. This was a frustrating step to figure out since most existing guides skip this detail! eksctl utils associate-iam-oidc-provider \\\n  --cluster=mqCluster \\\n  --region us-east -1  \\\n  --approve\nsleep  5 \neksctl create iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    -- namespace  kube-system \\\n    --cluster mqCluster \\\n    --role-name AmazonEKS_EBS_CSI_DriverRole \\\n    --role-only \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve\nsleep  5 \neksctl create addon \\\n  --name aws-ebs-csi-driver \\\n  --cluster mqCluster \\\n  --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n  --force From there, we can install the RabbitMQ operator and create our deployment kubectl apply -f  &lt; https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml &gt; \nsleep 5\nkubectl apply -f rabbitmqcluster.yaml\nsleep 5\necho \"RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\" As usual, the code for all this can be found in the  git repo . NOTE:  Make sure to write down the URL printed at the end! If you forget, you can get the URL in the  EKS  page on AWS. You’ll want the external IP for the load balancer. You can monitor your RabbitMQ queues by visiting “<rabbitmq_url>:15672” and signing in with “guest”/”guest”. Step 4: Deploying IngestionPipeline Workers This is where the real meat of work comes in. We need to create a  consumer  that will endlessly pull from our  RabbitMQ  queue, ingest data with the help of TEI, and then put that data into our vector db. To do this, we can make a FastAPI server that does two things Starts a thread to consume from our queue Starts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.) First, we write our code, as you can see in  worker.py Then, we dockerize our app by creating a simple  Dockerfile  and running: docker build -t <image_name> .\ndocker tag <image_name>:latest <image_name>:<version>\ndocker push <image_name>:<version> With our app dockerized, we can complete the  worker-deployment.yaml  file by filling in Our embeddings URL under  TEI_URL Our rabbit-mq URL under  RABBITMQ_URL Our image name under container image Our cluster details (in this case, a weaviate URL and API key) With the  yaml  file complete, now we can properly deploy the worker eksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\nsleep 5\nkubectl create -f ./worker-deployment.yaml\nsleep 5\nkubectl create -f ./worker-service.yaml Step 5: Making a User-Facing Lambda Function Our lambda function will rely on a single external dependency —  pika  — which is used to communicate with RabbitMQ. Create a python file called  lambda_function.py  with the following code: import  pika\n import  json\n\n def   lambda_handler ( event, context ):\n     try :\n        body = json.loads(event.get( 'body' ,  '{}' ))\n     except :\n        body = event.get( 'body' , {})\n        \n    user = body.get( 'user' ,  '' )\n    documents = body.get( 'documents' , [])\n     if   not  user  or   not  documents:\n         return  {\n             'statusCode' :  400 ,\n             'body' : json.dumps( 'Missing user or documents' )\n        }\n    \n    credentials = pika.PlainCredentials( \"guest\" ,  \"guest\" )\n    parameters = pika.ConnectionParameters(\n        host= \"hostname.amazonaws.com\" , \n        port= 5672 , \n        credentials=credentials\n    )\n    \n    connection = pika.BlockingConnection(parameters=parameters)\n    channel = connection.channel()\n    channel.queue_declare(queue= 'etl' )\n\n     for  document  in  documents:\n        data = {\n             'user' : user,\n             'documents' : [document]\n        }\n        channel.basic_publish(\n            exchange= \"\" , \n            routing_key= 'etl' , \n            body=json.dumps(data)\n        )\n\n     return  {\n         'statusCode' :  200 ,\n         'body' : json.dumps( 'Documents queued for ingestion' )\n    } The function above processes incoming requests, and publishes each document as a single message in our rabbitmq cluster. To deploy a lambda file with dependencies, we need to create a zip of our lambda function + all dependencies. To do this, we can create a  requirements.txt  file with our dependencies and run: pip install -r requirements.txt -t .\nzip -r9 ../ingestion_lambda.zip . -x \"*.git*\" \"*setup.sh*\" \"*requirements.txt*\" \"*.zip*\" With our code and zip file in hand, head over to the Lambda AWS page in your browser. Select  Create function Give it a name, select a python runtime (I used Python 3.11) Click  Create function  at the bottom In the code editor, you’ll see an  Upload from  button — click that, and upload your zip file Click test, give the test a name, and paste the following JSON {\n     \"body\" : { \"user\" :  \"Test\" ,  \"documents\" : [{ \"text\" :  \"test\" }]}\n} Once the test works, the  Deploy  button will not be grayed out, and you can click it. Your public URL will be listed in the upper right pane under  Function URL  — this is the URL you can use to call your lambda function from anywhere! Step 6: Reap the Scaling Benefits Now, we can run our system end-to-end! To ingest data, you can run: import  requests\n from  llama_index  import  Document, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader( \"./data\" ).load_data()\n\n # this will also be the namespace for the vector store  \n # -- for weaviate, it needs to start with a captial and only alpha-numeric \nuser =  \"Loganm\"  \n\n # upload in batches \n for  batch_idx  in   range ( 0 ,  len (documents),  30 ):\n  documents_batch = documents[batch_idx:batch_idx+ 30 ]\n  body = {\n     'user' : user,\n     'documents' : [doc.json()  for  doc  in  documents_batch]\n  }\n\n  # use the URL of our lambda function here \n response = requests.post( \"&lt;lambda_url&gt;\" , json=body)\n  print (response.text) Then, to use our data: from  llama_index  import  VectorStoreIndex\n from  llama_index.vector_stores  import  WeaviateVectorStore\n import  weaviate\n\nauth_config = weaviate.AuthApiKey(api_key= \"...\" )\nclient = weaviate.Client(url= \"...\" , auth_client_secret=auth_config)\nvector_store = WeaviateVectorStore(weaviate_client=client, class_prefix= \"&lt;user&gt;\" )\nindex = VectorStoreIndex.from_vector_store(vector_store) Step 7: Clean-up AWS doesn’t make it easy to estimate costs of all this. But after running and testing things for a few days, I had only spent ~$40CAD. This included leaving some services running overnight (whoops!). When you are done with your deployment, you’ll want to delete the resources so that you aren’t charged for things you aren’t using. To delete my clusters, I ran the following: eksctl delete cluster embeddings\neksctl delete cluster mq-worker\nkubectl rabbitmq delete production-rabbitmqcluster Then, in the AWS UI console, I deleted any remaining resources on the  EC2  and  CloudFormation  pages, as well as double-checking that everything was deleted on the  EKS  page. Conclusion Using this setup, I was able to reduce index-construction times for creating large indexes dramatically. Before, it would take about 10–20 minutes to create the index for 25K documents, and with this setup (2 rabbitmq nodes, 2 workers, 2 embeddings), it was down to 5 minutes! And with more scaling, it could be even faster. Next Steps From here, there are several improvements that I can think of better secrets management adding auto-scaling adding a retrieval lambda function (would require making a docker image for lambda + llama-index) adding queue stats to the fastapi server deploying redis for document management on the IngestionPipeline I encourage anyone to take this work and build off it. Be sure share any improvement on the github repository as well!", "author": "Logan Markewich", "date": "Jan 2, 2024", "tags": ["AWS", "Llamaindex", "Etl", "Data Processing", "Hugging Face"]},
{"title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM", "content": "Over the past year, Large Language Models (LLMs) like GPT-4 have not only transformed how we interact with machines but also have redefined the possibilities within the realm of natural language processing (NLP). A notable trend in this evolution is the increasing popularity of open-source LLMs like Llama 2, Falcon, OPT and Yi. Some may prefer them over their commercial counterparts in terms of accessibility, data security and privacy, customization potential, cost, and vendor dependency. Among the tools gaining increasing traction in the LLM space are OpenLLM and LlamaIndex — two powerful platforms that, when combined, unlock new use cases for building AI-driven applications. OpenLLM  is an open-source platform for deploying and operating any open-source LLMs in production. Its flexibility and ease of use make it an ideal choice for AI application developers seeking to harness the power of LLMs. You can easily fine-tune, serve, deploy, and monitor LLMs in a wide range of creative and practical applications. LlamaIndex  provides a comprehensive framework for managing and retrieving private and domain-specific data. It acts as a bridge between the extensive knowledge of LLMs and the unique, contextual data needs of specific applications. OpenLLM’s support for a diverse range of open-source LLMs and LlamaIndex’s ability to seamlessly integrate custom data sources provide great customization for developers in both communities. This combination allows them to create AI solutions that are both highly intelligent and properly tailored to specific data contexts, which is very important for query-response systems. In this blog post, I will explain how you can leverage the combined strengths of OpenLLM and LlamaIndex to build an intelligent query-response system. This system can understand, process, and respond to queries by tapping into a custom corpus. Setting up the environment The first step is to create a virtual environment in your machine, which helps prevent conflicts with other Python projects you might be working on. Let’s just call it  llamaindex-openllm  and activate it. python -m venv llamaindex-openllm\nsource llamaindex-openllm/bin/activate Install the required packages. This command installs OpenLLM with the optional  vllm  component (I will explain it later). pip install \"openllm[vllm]\" llama-index llama-index-llms-openllm llama-index-embeddings-huggingface For handling requests, you need to have an LLM server. Here, I use the following command to start a Llama 2 7B local server at  http://localhost:3000 . Feel free to choose any model that fits your needs. If you already have a remote LLM server, you can skip this step. openllm start meta-llama/Llama-2-7b-chat-hf --backend vllm OpenLLM automatically selects the most suitable runtime implementation for the model. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise, it falls back to PyTorch. vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. According to  this report , you can achieve 23x LLM inference throughput while reducing P50 latency using vLLM. Note : To use the vLLM backend, you need a GPU with at least the Ampere architecture (or newer) and CUDA version 11.8. This demo uses a machine with an Ampere A100–80G GPU. If your machine has a compatible GPU, you can also choose vLLM. Otherwise, simply install the standard OpenLLM package ( pip install openllm ) in the previous command. v1: Creating a simple completion service Before building a query-response system, let’s get familiar with the integration of OpenLLM and LlamaIndex and use it to create a simple completion service. The integration offers two APIs for interactions with LLMs: 1.  OpenLLM : This can be used to initiate a local LLM server directly without the need for starting a separate one using commands like  openllm start . Here’s how you can use it: from  llama_index.llms.openllm  import  OpenLLM\nllm = OpenLLM( 'meta-llama/Llama-2-7b-chat-hf' ) 2.  OpenLLMAPI : This can be used to interact with a server hosted elsewhere, like the Llama 2 7B model I started previously. Let’s try the  complete  endpoint and see if the Llama 2 7B model is able to tell what OpenLLM is by completing the sentence “OpenLLM is an open source tool for”. from  llama_index.llms.openllm  import  OpenLLMAPI\n\nremote_llm = OpenLLMAPI(address= \"http://localhost:3000\" )\n\ncompletion_response = remote_llm.complete( \"OpenLLM is an open source tool for\" , max_new_tokens= 1024 )\n print (completion_response) Run this script and here is the output: learning lifelong learning models. It is designed to be easy to use, even  for  those without extensive knowledge of machine learning. OpenLLM allows users to train, evaluate, and deploy lifelong learning models using a variety of datasets and algorithms.\n\nOpenLLM provides a number of features that make it useful  for  learning lifelong learning models. Some of these features include:\n\n 1.  Easy-to-use interface: OpenLLM provides an easy-to-use interface that makes it simple to train, evaluate, and deploy lifelong learning models.\n 2.  Support  for  a variety of datasets: OpenLLM supports a variety of datasets, including images, text, and time-series data.\n 3.  Support  for  a variety of algorithms: OpenLLM supports a variety of algorithms  for  lifelong learning, including neural networks, decision trees, and support  vector  machines.\n 4.  Evaluation tools: OpenLLM provides a number of evaluation tools that allow users to assess the performance of their lifelong learning models.\n 5.  Deployment tools: OpenLLM provides a number of deployment tools that allow users to deploy their lifelong learning models in a variety of environments.\n\nOpenLLM is written in Python and is available under an open source license. It is designed to be used in a variety of settings, including research, education, and industry.\n\nSome potential use cases  for  OpenLLM include:\n\n 1.  Training lifelong learning models  for  image classification: OpenLLM could be used to train a lifelong learning model to classify images based on their content.\n 2.  Training lifelong learning models  for  natural language processing: OpenLLM could be used to train a lifelong learning model to process and analyze natural language text.\n 3.  Training lifelong learning models  for  time-series data: OpenLLM could be used to train a lifelong learning model to predict  future  values in a time-series dataset.\n 4.  Deploying lifelong learning models in a production environment: OpenLLM could be used to deploy a lifelong learning model in a production environment, such as a recommendation system or a fraud detection system.\n\nOverall, OpenLLM is a powerful tool  for  learning lifelong learning models. Its ease of use, flexibility, and support  for  a variety of datasets and algorithms make it a valuable resource  for  researchers and practitioners in a variety of fields. Obviously, the model couldn’t correctly explain OpenLLM with some hallucinations 🤣. Nevertheless, the code works well as the server outputs a response for the request. This is a good start as we proceed with building our system. v2: Enhancing with a query-response system The initial version revealed a key limitation: the model’s lack of specific knowledge about OpenLLM. One solution is to feed the model with domain-specific information, allowing it to learn and respond according to topic-specific queries. This is where LlamaIndex comes into play, enabling you to build a local knowledge base with pertinent information. Specifically, you create a directory (for example,  data ) and build an index for all the documents in the folder. Create a folder and let’s import the GitHub README file of OpenLLM into the folder: mkdir data\ncd data\nwget https://github.com/bentoml/OpenLLM/blob/main/README.md Go back to the previous directory and create a script called  starter.py  like the following: from  llama_index.core  import  VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n from  llama_index.llms.openllm  import  OpenLLMAPI\n from  llama_index.core.node_parser  import  SentenceSplitter\n\n # Change the address to your OpenLLM server \nllm = OpenLLMAPI(address= \"http://localhost:3000\" )\n\n # Break down the document into manageable chunks (each of size 1024 characters, with a 20-character overlap) \ntext_splitter = SentenceSplitter(chunk_size= 1024 , chunk_overlap= 20 )\n\n # Create a ServiceContext with the custom model and all the configurations \nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model= \"local\" ,\n    text_splitter=text_splitter,\n    context_window= 8192 ,\n    num_output= 4096 ,\n)\n\n # Load documents from the data directory \ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\n\n # Build an index over the documents using the customized LLM in the ServiceContext \nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n # Query your data using the built index \nquery_engine = index.as_query_engine()\nresponse = query_engine.query( \"What is OpenLLM?\" )\n print (response) To improve the quality of your response, I recommend you define a  SentenceSplitter  to provide finer control over the input processing, leading to better output quality. In addition, you can set  streaming=True  to stream your response: query_engine = index.as_query_engine(streaming=True)\nresponse = query_engine.query(\"What is OpenLLM?\")\nresponse.print_response_stream() Your directory structure should look like this now: ├── starter.py\n└── data\n    └── README.md Run  starter.py  to test the query-response system. The output should be consistent with the content of the OpenLLM README. Here is the response I received: OpenLLM is an open-source platform for deploying and managing large language models (LLMs) in a variety of environments, including on-premises, cloud, and edge devices. It provides a comprehensive suite of tools and features for fine-tuning, serving, deploying, and monitoring LLMs, simplifying the end-to-end deployment workflow for LLMs. Conclusion The exploration in this article underscores the importance of customizing AI tools to fit specific needs. By using OpenLLM for flexible deployment of LLMs and LlamaIndex for data management, I have demonstrated how to create an AI-powered system. It not only understands and processes queries but also delivers responses based on a unique knowledge base. I hope this blog post has inspired you to explore more capabilities and use cases of OpenLLM and LlamaIndex. Happy coding! ⌨️", "author": "Sherlock Xu", "date": "Jan 3, 2024", "tags": ["Openllm", "Llamaindex", "AI", "Bentoml", "Open Source"]},
{"title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis", "content": "Introduction In many enterprises, data primarily resides in databases and generally, it’s very difficult to combine database data with other forms of data, such as PDFs, when trying to generate actionable insights. We envision the development of an agent that empowers anyone to leverage data from all of these data sources for informed decision-making. Imagine an agent proficient in creating documents by merging data from diverse sources, including JIRA and databases, further enriched with the latest internet-sourced information. At  waii.ai , we are committed to delivering an enterprise text-to-SQL API with the most complete and accurate translation of plain English to SQL available. Waii allows companies to build text-to-SQL right into their products as well as enable no-code analytics for their internal data/business teams. Waii works out of the box and can be self-hosted/on-prem. LlamaIndex introduces a remarkable RAG framework, facilitating the connection of various customer data sources, such as PDFs, Notion, and internal knowledge bases, to large language models (LLMs). This advancement simplifies the creation of data-augmented chatbots and analysis agents. This opens up a prime opportunity to develop an enterprise agent that can access data from multiple sources, including your preferred database. We will explore this further in the rest of the blog. Why a New Text-to-SQL LlamaIndex Plugin? To enable the Llama Index agent to utilize text-to-SQL APIs, a plugin is essential. LlamaIndex already has a built-in text-to-SQL plugin, but why did we decide to create a new LlamaHub plugin? The existing text-to-SQL plugin in LlamaIndex has been suitable for handling simple databases (less than 10 tables, 100 columns) with straightforward SQL queries. However, managing medium to large databases, which can include 100s of tables and 1000s of columns, presents a complex challenge. Limitations arise due to the restricted context windows of LLMs, and even those with large context windows, like GPT-4-turbo with its 128K tokens, can suffer from inaccuracies and regression in task retrieval when overloaded with content. This issue is discussed in a  LlamaIndex study . In contrast, Waii focuses on making query generation more efficient. We have developed a built-in compiler to deal with compilation errors from LLMs to support multiple dialects. Our internal knowledge graph, created from database metadata, constraints, and query history, aids in table/schema selection. Users can also apply semantic rules to schema/table/column, or integrate with their data catalog services, ensuring the semantic correctness of generated queries, in addition to syntactic correctness. To utilize our service, users simply need to connect their database to Waii and copy a Waii API key to create a LlamaIndex agent. LlamaIndex + Waii Agent We are thrilled to showcase the integration of Waii with LlamaIndex to create an agent capable of executing various text-to-SQL tasks and validating the data based on a PDF. We’ll be analyzing customers’ top-purchased categories during Christmas time, and compare it with  Deloitte’s holiday retail survey  report. Architecture of LlamaIndex + Waii Before diving into the code example, let’s look at the architecture first: \n\n \n   \n \n\n \n  The LlamaIndex agent operates on the client side, accompanied by a number of\n  tools: Each tool provides function specifications and allows functions to be\n  selected based on context and the user’s input to\n   chat(\"…\") . For example, if the question indicates information needs to be retrieved\n  from the “internet”, the Google search tool will be chosen. Internally it uses\n  LLM which returns selected functions with parameters for a given context.\n \n \n  When the Waii tool is chosen, whether for describing a dataset, generating a\n  query, or running a query, it sends the API request to the Waii Service.\n \n \n  The Waii Service can be deployed as a hosted SaaS or as Docker containers\n  running in your on-premises environment. The components of the Waii Service\n  include:\n \n \n   \n     The Query Generator:  coordinates the entire workflow of\n    query generation and communicates with the LLM for this purpose.\n   \n   \n     Knowledge Graph / Metadata Management : connects to\n    databases, extracting metadata and query history as a knowledge graph to\n    assist the Query Generator in choosing the right tables and schemas.\n   \n   \n     Semantic Rules : These aid the Query Generator in producing\n    semantically correct queries.\n   \n   \n     Waii Compiler : After a query is generated by the LLM, the\n    Waii Compiler patches identified issues in the query. If a compilation issue\n    is not fixable, it regenerates the query with an articulated error message.\n   \n \n Create LlamaIndex agent with Waii + PDF Loader \n \n  Let’s first create two LlamaHub tools — Waii and PDF Loader. LlamaHub tools\n  include specs to identify available functions along with their parameters, the\n  agent will select and execute which function to use based on available\n  functions and context.\n \n Let’s start with creating an agent which includes the Waii tool: \n from llama_hub.tools.google_search import GoogleSearchToolSpec from llama_hub.tools.waii import WaiiToolSpec from llama_index.agent import OpenAIAgent from llama_index.llms import OpenAI waii_tool = WaiiToolSpec(     api_key='waii_api_key',     # Connection key of WAII connected database, see      # https://github.com/waii-ai/waii-sdk-py#get-connections     database_key='database_to_use',     verbose=True ) \n And then create a PDF tool: \n from pathlib import Path from llama_index import download_loader from llama_index import VectorStoreIndex PDFReader = download_loader(\"PDFReader\") loader = PDFReader() documents = loader.load_data(file=Path('DI_2023-Deloitte-holiday-retail-survey.pdf')) index = VectorStoreIndex.from_documents(documents) engine = index.as_query_engine(similarity_top_k=5) deloitte_retail_survey_tool = QueryEngineTool(         query_engine=engine,         metadata=ToolMetadata(             name=\"deloitte_retail_survey\",             description=(                 \"Provides retail survey report for holiday sales based on Deloitte's data\"                 \"Use a detailed plain text question as input to the tool, and output using plain text based on pdf data\"             ),         ),     ) \n And at last, create an agent which combines Waii and PDF tools: \n agent = OpenAIAgent.from_tools(   [deloitte_retail_survey_tool] + waii_tool.to_tool_list(),    llm=OpenAI(model='gpt-4-1106-preview', temperature=0),    verbose=True) \n \n   [deloitte_retail_survey_tool] + waii_tool.to_tool_list() \n  indicate using all functions (such as getting answers from the database,\n  generating a query, executing a query, describing datasets, etc.) provided by\n  Waii and PDF Search.\n \n Understand your dataset \n \n  The first step in doing data analysis is to get a better understanding of your\n  dataset.\n \n You can start asking questions to your agent: \n agent.chat(\"Describe my database\") \n The output of which is: \n STARTING TURN 1 --------------- === Calling Function === Calling function: describe_dataset with args:    {\"ask\":\"Can you describe the whole database?\"} ... ======================== STARTING TURN 2 --------------- The database I have access to consists of several schemas, each with its  own set of tables and domains: 1. **CRUNCHBASE_2016**: This schema includes tables related to acquisitions,     companies, investments, and funding rounds in the Crunchbase database for    the year 2016. It's useful for analyzing acquisition trends, company     information, investment activity, and funding trends. 2. **RETAIL_DATA**: This schema contains tables related to retail operations,     such as call centers, customers, addresses, demographics, inventory,     items, promotions, stores, returns, sales, and warehouses. It can be used     to analyze call center performance, customer demographics, inventory     management, sales performance, and other retail operations. 3. ... \n \n  As you can see, the agent understood the request, called\n   describe_dataset  function provided by Waii, and generated a\n  summary of the dataset.\n \n \n  I’m interested in the\n   RETAIL_DATA  schema, so let me ask more of the schema -\n \n agent.chat(\"What can i do with the retail_data schema\") \n And I get this: \n The RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed  to support a wide range of analyses related to retail operations. Here  are some of the capabilities and types of analyses you can perform with  this schema: 1. **Call Center Analysis**: Evaluate the performance of call centers,     understand call volumes, and assess customer service efficiency. 2. **Customer Demographics**: Analyze customer profiles, including     demographics, purchasing behaviors, and preferences. This can help     in targeted marketing and customer segmentation. Specific questions that can be addressed using the RETAIL_DATA schema include: - What is the total number of call centers? - How many customers have a preferred customer flag? - What is the average price of items? \n Let me do some more data analysis. \n Generate an SQL query and run it \n \n  Let’s generate a SQL query (asking top 10 item categories sold during\n  Christmas time):\n \n agent.chat(\"Top 10 item category sold during christmas time across all years\") \n Now it calls  get_answer  function from Waii tool: \n === Calling Function === Calling function: get_answer with args:    {\"ask\":\"What are the top 10 item categories sold during            Christmas time across all years?\"} \n And got a SQL like: \n WITH christmas_sales AS (     SELECT         ss_item_sk,         SUM(ss_quantity) AS total_quantity     FROM tweakit_playground.retail_data.store_sales AS ss     INNER JOIN tweakit_playground.retail_data.date_dim AS dd         ON ss_sold_date_sk = d_date_sk     WHERE         d_holiday = 'Y' AND d_date ILIKE '%-12-25'     GROUP BY         ss_item_sk ), ranked_categories AS (     SELECT         i_category,         SUM(total_quantity) AS category_quantity     FROM christmas_sales AS cs     INNER JOIN tweakit_playground.retail_data.item AS i         ON ss_item_sk = i_item_sk     GROUP BY         i_category     ORDER BY         category_quantity DESC     LIMIT 10 ) SELECT     i_category AS category,     category_quantity FROM ranked_categories \n Let me ask it to run the query: \n agent.chat(\"Run it\") \n I got a Dataframe along with its summary: \n \n   \n \n The top 10 item categories sold during Christmas time across all years,  based on the total quantity of items, are: 1. Women: 1,487,891 2. Sports: 1,486,644 3. Children: 1,483,531 4. Electronics: 1,478,463 5. Music: 1,476,045 6. Books: 1,472,911 7. Home: 1,471,348 8. Jewelry: 1,459,025 9. Shoes: 1,456,296 10. Men: 1,451,285 \n Use with a PDF report \n \n  Let’s try to cross-check the result with the \n    holiday retailer report (in PDF form) from Deloitte \n  — Are Deloitte’s top-purchased gift categories the same as what we saw from\n  the database?\n \n \n   \n\n   Top gift categories during the holiday \n \n \n  There’s one page from the pdf we are interested in — let’s see if we can\n  retrieve this page\n \n agent.chat(\"\"\"Give me top holiday gift categories based on Deloitte's  holiday retail survey, and how it is different from the top  10 item category sold during christmas time across all years  from my database\"\"\") \n \n  The output looks like the following, which indicates it uses the\n   deloitte_retail_survey  tool to obtain insights from the PDF.\n \n STARTING TURN 1 --------------- === Calling Function === Calling function: deloitte_retail_survey with args: {\"input\": \"What are the top holiday gift categories based on Deloitte's holiday retail survey?\"} ... \n It gives the following summary: \n Based on Deloitte's holiday retail survey, the top holiday gift  categories are: 1. Clothing & Accessories 2. Gift Cards & Other 3. Food & Beverage ... From your database, the top 10 item categories sold during Christmas  time across all years are: 1. Women 2. Sports 3. Children ... Comparing the two lists, we can see some differences and similarities: - \"Clothing & Accessories\" from Deloitte's survey could correspond to    \"Women,\" \"Men,\" and possibly \"Children\" from your database. - \"Electronics & Accessories\" is a common category in both lists. - \"Gift Cards & Other\" and \"Food & Beverage\" from Deloitte's survey do    not have a direct match in the top categories from your database. ... \n \n  Bingo! Now we can compare the results from our database with PDFs. And I love\n  seeing how the agent can correlate the two lists and tell me that my store\n  doesn’t have the “Gift Cards & Other” and “Food & Beverage”\n  categories!\n \n \n  You can find the code from the Colab notebook\n   link \n \n Wrapping up \n \n  The integration of Waii’s text-to-SQL API with LlamaIndex’s RAG framework\n  marks a significant advancement in enterprise data analytics. This powerful\n  combination enables companies to effortlessly merge and analyze data from\n  various sources, including databases, PDFs, and the Internet. We demonstrated\n  the agent’s capability to generate SQL queries, understand complex datasets,\n  and correlate findings with external reports. This innovation not only\n  simplifies data analysis but also opens new avenues for informed\n  decision-making in the digital era.\n \n \n  To learn more about Waii, please contact us here:\n   https://www.waii.ai/#request-demo", "author": "Wangda Tan", "date": "Dec 6, 2023", "tags": ["Llamaindex", "Text To Sql", "OpenAI", "Data Analytics", "Sql"]},
{"title": "Introducing Query Pipelines", "content": "Today we introduce  Query Pipelines,  a new declarative API within LlamaIndex that allows you to  concisely orchestrate simple-to-advanced query workflows over your data for different use cases  (RAG, structured data extraction, and more). At the core of all this is our QueryPipeline  abstraction. It can take in many LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It can create a computational graph over these modules (e.g. a sequential chain or a DAG). It has callback support and native support with our  observability partners . The end goal is that it’s even easier to build LLM workflows over your data. Check out our comprehensive  introduction guide , as well as our  docs page  for more details. Example `QueryPipeline` setup for an advanced RAG pipeline Context Over the past year AI engineers have developed customized, complex orchestration flows with LLMs to solve a variety of different use cases. Over time some common patterns developed. At a top-level, paradigms emerged to query a user’s data — this includes RAG (in a narrow definition) to query unstructured data, and text-to-SQL to query structured data. Other paradigms emerged around use cases like structured data extraction (e.g. prompt the LLM to output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and agents that could interact with external services (combine prompt chaining There is a lot of query orchestration in RAG.  Even within RAG itself there can be a lot of work to build an advanced RAG pipeline optimized for performance. Starting from the user query, we may want to run query understanding/transformations (re-writing, routing). We also may want to run multi-stage retrieval algorithms — e.g. top-k lookup + reranking. We may also want to use prompts + LLMs to do response synthesis in different ways. Here’s a great blog on advanced RAG  components . Source: “Advanced RAG Techniques: an Illustrated Overview” by Ivan Ilin RAG has become more modular:  Instead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the  RAG Survey paper by Gao et al. This leads to creative new patterns like  DSP ,  Rewrite-Retrieve-Read , or  interleaving retrieval+generation multiple times . Previous State of LlamaIndex LlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes letting users setup  different RAG pipelines , and has been at the forefront of establishing advanced RAG patterns. We’ve also exposed low-level modules such as  LLMs ,  prompts ,  embeddings ,  postprocessors  and easy subclassability of core components like  retrievers  and  query engines  so that users can define their own workflows. But up until now, we didn’t explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively. Query Pipeline As a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity. You can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code. It has the following benefits: Express common query workflows with fewer lines of code/boilerplate:  Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module! Greater readability:  Reduced boilerplate leads to greater readability. End-to-end observability:  Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations. [In the future] Easy Serializability:  A declarative interface allows the core components to be serialized/redeployed on other systems much more easily. [In the future] Caching:  This interface also allows us to build a caching layer under the hood, allowing input re-use. Visualization of our advanced RAG QueryPipeline using `networkx` and `pyvis` Usage The QueryPipeline allows you to a DAG-based query workflow using LlamaIndex modules. There are two main ways to use it: As a sequential chain (easiest/most concise) As a full DAG (more expressive) See our  usage pattern guide  for more details. Sequential Chain Some simple pipelines are purely linear in nature — the output of the previous module directly goes into the input of the next module. Some examples: Prompt → LLM → Output parsing Retriever →Response synthesizer Here’s the most basic example, chaining a prompt with LLM. Simply initialize  QueryPipeline  with the  chain  parameter. # try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True) Setting up a DAG for an Advanced RAG Workflow Generally setting up a query workflow will require using our lower-level functions to build a DAG. For instance, to build an “advanced RAG” consisting of query rewriting/retrieval/reranking/synthesis, you’d do something like the following. from  llama_index.postprocessor  import  CohereRerank\n from  llama_index.response_synthesizers  import  TreeSummarize\n from  llama_index  import  ServiceContext\n\n # define modules \nprompt_str =  \"Please generate a question about Paul Graham's life regarding the following topic {topic}\" \nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model= \"gpt-3.5-turbo\" )\nretriever = index.as_retriever(similarity_top_k= 3 )\nreranker = CohereRerank()\nsummarizer = TreeSummarize(\n    service_context=ServiceContext.from_defaults(llm=llm)\n)\n\n # define query pipeline \np = QueryPipeline(verbose= True )\np.add_modules(\n    {\n         \"llm\" : llm,\n         \"prompt_tmpl\" : prompt_tmpl,\n         \"retriever\" : retriever,\n         \"summarizer\" : summarizer,\n         \"reranker\" : reranker,\n    }\n)\n # add edges  \np.add_link( \"prompt_tmpl\" ,  \"llm\" )\np.add_link( \"llm\" ,  \"retriever\" )\np.add_link( \"retriever\" ,  \"reranker\" , dest_key= \"nodes\" )\np.add_link( \"llm\" ,  \"reranker\" , dest_key= \"query_str\" )\np.add_link( \"reranker\" ,  \"summarizer\" , dest_key= \"nodes\" )\np.add_link( \"llm\" ,  \"summarizer\" , dest_key= \"query_str\" ) In this code block we 1) add modules, and then 2) define relationships between modules. Note that by  source_key  and  dest_key  are  optional  and are only required if first module has more than one output / the second module has more than one input respectively. Running the Pipeline If the pipeline has one “root” node and one output node, use  run  . Using the previous example, output = p.run(topic=\"YC\")\n# output type is Response\ntype(output) If the pipeline has multiple root nodes and/or multiple output nodes, use  run_multi  . output_dict = p. run_multi ({ \"llm\" : { \"topic\" :  \"YC\" }})\n print (output_dict) Defining a Custom Query Component It’s super easy to subclass  CustomQueryComponent  so you can plug it into the QueryPipeline. Check out  our walkthrough  for more details. Supported Modules Currently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own! LLMs (both completion and chat) (  LLM  ) Prompts (  PromptTemplate  ) Query Engines (  BaseQueryEngine  ) Query Transforms (  BaseQueryTransform  ) Retrievers (  BaseRetriever  ) Output Parsers (  BaseOutputParser  ) Postprocessors/Rerankers (  BaseNodePostprocessor ) Response Synthesizers (  BaseSynthesizer  ) Other  QueryPipeline objects Custom components (  CustomQueryComponent  ) Check out the  module usage guide  for more details. Walkthrough Example Make sure to check out our  Introduction to Query Pipelines guide  for full details. We go over all the steps above with concrete examples! The notebook guide also logs traces through  Arize Phoenix . You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider. Related Work The idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include  Haystack  as well as the  LangChain Expression Language . Other related works include pipelines that are setup in the no-code/low-code setting such as  Langflow  /  Flowise . Our main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There’s a lot of optimizations/guides to be done here! FAQ What’s the difference between a  QueryPipeline  and  IngestionPipeline  ? Great question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there’s potentially some shared abstractions we’ll develop for both! Conclusion + Resources That’s it! As mentioned above we’ll be adding a lot more resources and guides soon. In the meantime check out our current guides: Query Pipelines Guide Query Pipelines Walkthrough Query Pipeline Usage Pattern Query Pipelines Module Usage Guide", "author": "Jerry Liu", "date": "Jan 8, 2024", "tags": ["Llamaindex", "Retrieval Augmented", "LLM", "AI"]},
{"title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex", "content": "ChatGPT, developed by OpenAI, has changed the way we interact online. Being a general purpose chatbot, ChatGPT is limited to answering generic queries. But it becomes even more useful if you can get it to answer your questions specific to your business. To do that, you need to train ChatGPT on your data. EmbedAI  is a no-code platform for creating AI chatbots trained on your business data. This includes data sourced from web pages, PDFs, Notion documents, or YouTube videos, allowing EmbedAI to adapt to a wide range of information sources. In this blog post, we’ll show you how we used  LlamaIndex  with  EmbedAI  to enable us to train ChatGPT on your own data, helping you create a customized and effective AI chatbot tailored for your business needs. Chat with your data use-cases There’s a variety of ways that a chatbot trained on your data could be helpful, including: Customer Support Bot : Manages frequently asked questions about a product, addressing customer support inquiries efficiently. Company Search Engine : Finds internal company documents and information fast, boosting workplace efficiency. Personalized Learning Assistant : Offers tailored educational support and study guidance based on specific course content. Technical Support assistant : Provides in-depth help for complex software issues, from troubleshooting to usage tips. Healthcare Assistant : Gives general health advice and information, based on medical literature and FAQs. Finance Chatbot : Assists with financial queries, offering advice on products, market trends, and investment strategies by training on financial data Let’s delve into creating our own chat apps that integrate with various data sources like PDFs, Notion documents, videos, webpages, and more. Case 1: Custom ChatGPT for your site To train ChatGPT on your website content, we need to scrape the content from all the relevant webpages. The steps to do this are: Extract all the URLs from your website, such as from your sitemap Include only relevant URLs which you need to train on Use SimpleWebPageReader from  LlamaIndex  to download the content from these URLs Here’s some sample code to do that: Once the data is ready, an AI chatbot can be trained on these documents by using LlamaIndex’s VectorStoreIndex class. To create a ChatGPT chatbot on your website without coding you can use EmbedAI as outlined below which uses LlamaIndex internally: Case 2: Custom ChatGPT for your PDF documents If your business specific data is stored in PDF documents and you wish to create a chatbot that can surface the information in them we can do that with LlamaIndex using the PDFMiner library. This time the steps are: Upload your PDFs and store them in the cloud Install the PDFMiner library Fetch the uploaded PDFs and extract the document text using LlamaIndex loader Here’s the code for creating an AI chatbot trained on PDF documents with LlamaIndex If you want to create a ChatGPT chatbot on your PDF content without coding you can use EmbedAI as in the demo below which uses LlamaIndex internally Case 3: Custom ChatGPT for your videos Often, valuable information is embedded in videos, which isn’t as accessible for users searching for information. However, by training an AI chatbot with this content, it can become an incredibly rich resource for your users, significantly enhancing their experience. Let’s see how we can fetch the information from our youtube videos to train an AI chatbot using LlamaIndex. The steps are: Find your Channel ID Install  scrapetube  and pass it your channel ID to get your list of videos Install the Youtube transcript api and pass the video URLs from above to LlamaIndex loader to get a list of documents The code looks like this: Now you can train an AI chatbot on these documents by using SimpleVectorIndex from LlamaIndex to create a ChatGPT bot trained on your youtube videos, and as before, you can use EmbedAI to create a chatbot with no code. Case 4: Custom ChatGPT for Notion In many modern companies, a significant portion of their content is stored in Notion. As this content grows, quickly locating specific information becomes increasingly challenging. To address this, we can develop a chatbot for Notion to streamline the process of finding the necessary information. Steps to prepare the data: Fetch an access token from Notion following  their instructions Using the Notion API, parse data from Notion and generate LlamaIndex documents Train a chatbot on these using VectorStoreIndex If you prefer a No-code way to train a chatbot on your Notion documents, you can use EmbedAI as in the demo below which uses LlamaIndex internally: This doesn’t stop here. With EmbedAI, you can connect data from even more sources like Google Docs, Shopify or even use Zapier to connect with 6000+ tools and chat with their data. You can achieve this by choosing your specific data connector from  LlamaHub Challenges while building EmbedAI In EmbedAI, while connecting with a data source like Notion, the data can keep changing regularly which needs to be auto-refreshed. So the data needs a periodic refresh to add new documents or edit existing documents which needs to be handled internally. Likewise, when indexing website data it can be refreshed regularly. LlamaIndex makes it easy to handle these scenarios. LlamaIndex has a  guide to handling continuous ingestion . Querying over tabular data in EmbedAI is a major issue when dealing with PDF content containing tables. Naive chunking can give sub-optimal results and even hallucinations. LlamaIndex provides  a guide on how to deal with PDFs containing both text and tables  and achieve optimal results while querying. Shopify integration in EmbedAI needed hybrid search, as we needed to search not only on product description but also on product metadata. Thus a combination of semantic search and keyword search is needed to obtain optimal results. LlamaIndex provides a simple framework to build a hybrid search application, such as in  this example . Custom trained chatbots can help your business Training ChatGPT with your own data provides a significant advantage for your business. From enhancing customer support with bots trained on specific product knowledge to creating sophisticated company search engines, the applications are as diverse as they are impactful. LlamaIndex provides a lot of abstractions to help with building a custom chatbot trained on your data, and we use them heavily at EmbedAI. For those seeking a no-code solution to develop an AI chatbot tailored to their data, starting with EmbedAI is a straightforward option and we encourage you to  try it out .", "author": "Anil Chandra Naidu Matcha", "date": "Dec 14, 2023", "tags": ["Machine Learning", "Artificial Intelligence", "Llamaindex", "Customgpt", "Python"]},
{"title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex", "content": "Master LlamaIndex with our course developed in collaboration with Activeloop, TowardsAI, & Intel. Learn to apply advanced retrieval across industries in 40+ lessons. This is a guest post from Activeloop. LlamaIndex is proud to collaborate with Activeloop, Towards AI, and the Intel Disruptor Initiative to offer a free course on “ Advanced Retrieval Augmented Generation for Production ,” a part of the Gen AI 360: Foundational Model Certification series. This comprehensive course takes a hands-on approach to applying RAG techniques across various industries, including legal, biomedical, healthcare, e-commerce, and finance. The free course is designed for practical learning and invites participants to tackle real business challenges, such as developing a multi-modal AI shopping assistant. The course has over 40 lessons, 7 interactive projects, and 2 hours of video content, including from LlamaIndex CEO Jerry Liu. In 20+ hours of learning, the curriculum is geared towards enabling GenAI tinkerers, professionals, and executives to apply LlamaIndex and Deep Lake,  Activeloop ’s database for AI in production. Participants who complete the course will be awarded a certificate at no cost. In the rapidly evolving business landscape, leveraging Retrieval Augmented Generation (RAG) tools like Llamalndex & Deep Lake by Activeloop is essential for enterprises seeking a competitive edge in GenAI. This course is tailored to quickly upskill your team in GenAI workflows, emphasizing the integration of Activeloop’s advanced features like Deep Memory with Llamalndex for unmatched retrieval accuracy. It’s a strategic investment to enhance your team’s capabilities, ensuring your enterprise stays at the forefront of AI innovation - Jerry Liu, CEO & Co-Founder, LlamaIndex Highlighted projects across several industries: Healthcare:  Advanced RAG for Pill Searching. Combine cutting-edge NLP and computer vision techniques to build an AI app that recognizes pills from images, and lists their side effects and instructions to use. This project offers hands-on experience with the latest AI technologies like Segment Anything or GPT-4-vision. Legal:  Patent Generation and Search Engine. Gain practical knowledge in constructing a system like PatentPT, which incorporates a fine-tuned LLM to search or create patents. Learn how to build a meta-agent to smartly route user inquiries, ensuring a fluid chat experience with a database of 8 million USPTO patents for comprehensive retrieval and generation capabilities. E-commerce:  AI-Powered Shopping Assistant for Outfit Recommendations. Build a multi-modal AI assistant that curates outfit suggestions for any occasion, weather, or budget! What will I learn? Challenges with Naive RAG:  We’ll address common issues such as low precision, recall, and suboptimal response generation. Strategies for refining data processing, enhancing embedding models, refining retrieval algorithms, and optimizing prompt usage will be explored to improve RAG system performance. Advanced RAG with LlamaIndex:  Delve into basic and advanced RAG methods using LlamaIndex. The course covers the essential aspects of LlamaIndex required for RAG application development, complemented by Activeloop’s Deep Memory module, which natively integrates seamlessly with LlamaIndex to enhance retrieval accuracy by an average of 22%. Topics will range from small to large-scale retrieval, handling structured and unstructured data, and querying techniques. RAG Agents:  This module focuses on the creation and application of RAG agents with LlamaIndex, including advanced querying, summarizing databases, and designing AI assistants using various APIs. Production-grade apps: Learn to fine-tune the LlamaIndex RAG pipeline for professional deployment, evaluate RAG systems crafted with LlamaIndex, and ensure your models’ observability and effectiveness. Here’s a brief introduction to the course by Louis Bouchard from TowardsAI team. Who Should Enroll? This course is ideal for aspiring AI professionals, executives, and enthusiasts eager to apply AI in practical scenarios. Whether you want to enhance your organization’s AI capabilities or expand your knowledge, this course offers valuable hands-on experience. A basic understanding of coding and Python is recommended. Complimentary Free Trial of Deep Lake As a part of the course, all course takers can redeem a free extended trial of one month for the Activeloop Starter and Growth plans by redeeming the  GENAI360LLAMA  promo code at checkout. Check out the following video to  learn more . Join thousands of AI engineers in mastering master  Retrieval Augmented Generation with LlamaIndex . Enroll for free today!", "author": "Mikayel", "date": "Jan 11, 2024", "tags": ["Llamaindex", "Retrieval Augmented", "Large Language Models", "Course", "AI"]},
{"title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)", "content": "Introduction The C3 Voice Assistant is my latest project aimed at making Large Language Model (LLM) and Retrieval-Augmented Generation (RAG) applications more accessible. This voice-activated assistant caters to a broad audience, including those facing typing challenges or accessibility issues. Features Voice Activation:  Initiated by saying “C3.” Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word “C3” is configurable and you can choose any other word. Universal Accessibility:  Ideal for users preferring voice commands or facing typing challenges. LLM Integration:  Capable of general queries and document-specific inquiries (e.g., Nvidia’s FY 2023 10K report). User-Friendly Interface:  The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI. The Tech Stack The app is built on a robust and flexible tech stack that ensures a smooth, reliable, and efficient user experience. Here’s an overview: Frontend:  The user interface is a custom application developed using React.js. It’s designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility. Backend:  The server-side operations are powered by Python Flask. I’ve utilized the innovative ‘create-llama’ feature from LlamaIndex, which significantly streamlines the development process. Hosting:  For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks. Building the Frontend The frontend, built with React.js, focuses on user interaction and accessibility. The  App.js  script incorporates features like wake word recognition, speech-to-text conversion, state management, and dynamic UI elements like speech bubbles and spinners. 1. Component and State Initialization This section sets up the React component and initializes various states, such as  appState  to track the current state of the app (idle, listening, speaking), and  transcript  to store the text transcribed from user speech. import   React , { useState, useRef, useEffect }  from   \"react\" ;\n import   \"./App.css\" ;\n\n const   App  = () =&gt; {\n   const  [appState, setAppState] =  useState ( \"idle\" );\n   const  [transcript, setTranscript] =  useState ( \"\" );\n   // Additional state and ref declarations... \n}; 2. Speech Recognition Setup In this useEffect hook, two speech recognition instances are initialized: one for detecting the wake word “C3” and another for the main speech recognition. This setup ensures that the app starts listening for commands when “C3” is mentioned. You can easily swap “C3” with any other wake word of your choice.    useEffect (() =&gt; {\n     // Wake word listener setup \n     const   WakeWordSpeechRecognition  =\n       window . SpeechRecognition  ||  window . webkitSpeechRecognition ;\n     if  ( WakeWordSpeechRecognition  &amp;&amp; !wakeWordRecognitionRef. current ) {\n      wakeWordRecognitionRef. current  =  new   WakeWordSpeechRecognition ();\n      wakeWordRecognitionRef. current . continuous  =  true ;\n      wakeWordRecognitionRef. current . interimResults  =  false ;\n\n      wakeWordRecognitionRef. current . onresult  = (event) =&gt; {\n         const  transcript = event. results [event. results . length  -  1 ][ 0 ]. transcript \n          . trim ()\n          . toLowerCase ();\n         if  (transcript. includes ( \"c3\" )) {\n           toggleRecording ();  // Start the main speech recognition process \n        }\n      };\n\n      wakeWordRecognitionRef. current . start ();\n    }\n\n     // Main speech recognition setup \n     const   SpeechRecognition  =\n       window . SpeechRecognition  ||  window . webkitSpeechRecognition ;\n     if  ( SpeechRecognition  &amp;&amp; !recognitionRef. current ) {\n      recognitionRef. current  =  new   SpeechRecognition ();\n      recognitionRef. current . continuous  =  false ;\n      recognitionRef. current . interimResults  =  false ;\n\n      recognitionRef. current . onresult  = (event) =&gt; {\n         const  lastResultIndex = event. results . length  -  1 ;\n         const  transcriptResult = event. results [lastResultIndex][ 0 ]. transcript ;\n         setTranscript (transcriptResult);\n         setAppState ( \"playing\" );\n         setShowSpeechBubble ( true );\n         setTimeout (() =&gt;  setShowSpeechBubble ( false ), speechBubbleTimeout);\n         fetchResponseFromLLM (transcriptResult);\n      };\n\n      recognitionRef. current . onend  = () =&gt; {\n         setShowSpinner ( true );\n      };\n    }\n  }, []); 3. Handling User Speech and Response toggleRecording  controls the speech recognition process, while  fetchResponseFromLLM  sends the user's speech to the LLM backend and handles the response. This response is then spoken out via speech synthesis and also used to update the chat history displayed on the UI.   const  toggleRecording = () =&gt; {\n     try  {\n       if  (appState ===  \"idle\" ) {\n        recognitionRef. current . start ();\n         setAppState ( \"listening\" );\n      }  else   if  (appState ===  \"listening\" ) {\n        recognitionRef. current . stop ();\n      }\n    }  catch  (error) {\n    }\n  };    const  fetchResponseFromLLM =  async  (text) =&gt; {\n     try  {\n       const  response =  await   fetch (\n         `https://c3-python-nostream.onrender.com/api/chat` ,\n        {\n           method :  \"POST\" ,\n           headers : {  \"Content-Type\" :  \"application/json\"  },\n           body :  JSON . stringify ({\n             messages : [\n              {\n                 role :  \"user\" ,\n                 content :\n                   \"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\"  +\n                  text,\n              },\n            ],\n          }),\n        }\n      );\n       const  data =  await  response. json ();\n\n       setChatHistory ((prevHistory) =&gt; [\n        ...prevHistory,\n        {  query : text,  response : data. result . content  },\n      ]);\n       speak (data. result . content );\n    }  catch  (error) {\n       console . error ( \"Error communicating with LLM:\" , error);\n    }\n  }; 4. Speech Synthesis The  speak  function takes the text response from the LLM and uses the SpeechSynthesis API to read it aloud, providing an interactive experience for the user.    const  speak = (text) =&gt; {\n     if  (synthRef. current  &amp;&amp; text) {\n       const  utterance =  new   SpeechSynthesisUtterance (text);\n\n       const  voices =  window . speechSynthesis . getVoices ();\n       if  (voices. length  &gt;  0 ) {\n        utterance. voice  = voices[ 3 ];  // You can change this to select different voices \n      }\n\n      utterance. onstart  = () =&gt; {\n         console . log ( \"TTS starts speaking\" );\n         setShowSpinner ( false );\n      };\n\n      utterance. onend  = () =&gt; {\n         setAppState ( \"idle\" );\n         if  (wakeWordRecognitionRef. current ) {\n          wakeWordRecognitionRef. current . start ();  // Restart wake word listener after speaking \n        }\n      };\n      synthRef. current . speak (utterance);\n    } 5. UI Rendering The return statement of the  App  function contains the JSX code for rendering the app's UI. This includes buttons for starting/stopping the voice interaction, a display area for the transcript, and a chat sidebar showing the history of interactions. By combining voice recognition, LLM integration, and speech synthesis, this frontend component provides a comprehensive and accessible interface for interacting with the C3 Voice Assistant. Backend Server Setup Initialize Create-Llama: Run  npx create-llama@latest  in your terminal. Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend. Use  poetry install  and  poetry shell  to prepare the environment. Create a  .env  file with  OPENAI_API_KEY=<openai_api_key> . Generate Embeddings (optional): If a  ./data  directory exists, run  python app/engine/generate.py . Execute  python main.py to start the server. Test the API: Use  curl --location 'localhost:8000/api/chat' --header 'Content-Type: application/json' --data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }] }'  to test. Modify API behavior in  app/api/routers/chat.py . The server supports CORS for all origins, alterable with the  ENVIRONMENT=prod  setting. Integration Once the backend server is set up, integrating it with the frontend is straightforward. Simply update the  fetchResponseFromLLM  function in your frontend's  App.js  to call the backend server URL. This change ensures that when the frontend makes a request, it communicates with your newly configured backend, thus effectively integrating the two components. Final Thoughts Wrapping up, the C3 Voice Assistant isn’t just a tech showcase; it’s a stride towards democratizing AI. It’s about making powerful AI tools, like LLMs and RAG, accessible and user-friendly. This project is more than lines of code — it’s a push to break down tech barriers and empower everyone. Your thoughts and feedback are invaluable — let’s make AI more accessible together! Link to Github Repo:  Frontend  and  Backend Connect with Me on LinkedIn Linkedin Post", "author": "Harshad Suryawanshi", "date": "Jan 14, 2024", "tags": ["Llamaindex", "Voice Assistant", "OpenAI", "Accessibility", "Inclusive Ai"]},
{"title": "LlamaIndex Newsletter 2024–01–16", "content": "Hello LlamaIndex Enthusiasts 🦙, Get ready for an exciting week at LlamaIndex, teeming with dynamic community contributions and insightful learning resources. Dive into our range of new features, tutorials, guides, and events, all designed to enhance your LlamaIndex journey. We’re excited to announce our  very first in-person hackathon , scheduled for February 2nd-4th. Join us to connect with fellow RAG enthusiasts and compete for prizes totaling over $4,000! If you’ve been working on a fascinating project, penned an insightful article, or produced an engaging video, we’re eager to see it! Share your contributions with us at  news@llamaindex.ai . Don’t forget to subscribe to our newsletter on our  website  to receive all the latest updates directly in your inbox. 🤩  The highlights: Chain-of-Table:  Step-by-step table reasoning and operations for enhanced LLM tabular data understanding.  LlamaPack ,  Tweet . LLM Self-Consistency:  Merges textual and symbolic reasoning with majority voting for precise answers.  LlamaPack ,  Tweet . Semantic Text Splitting in RAG:  Greg Kamradt’s embedding similarity method for efficient document splitting.  LlamaPack ,  Tweet . Parallel RAG Ingestion:  Up to 15x faster document processing in LlamaIndex.  Notebook ,  Tweet . TogetherAI’s Embeddings Support:  Guide to build retrieval-augmented apps with MistralAI’s 8x7b model and TogetherAI Embeddings.  Blogpost  ,  Tweet . ✨ Feature Releases and Enhancements: We launched Chain-of-Table Framework in LlamaPack for LLM Tabular Data Understanding. This approach enables step-by-step table reasoning and operations like adding columns, row selection, grouping, and sorting, mimicking a data scientist’s method for concise data representation.  LlamaPack ,  Tweet . We launched LLM Self-Consistency Mechanism for Tabular Data in LlamaPack. This method combines textual and symbolic reasoning, utilizing a novel mix self-consistency approach with majority voting to select the best answer.  LlamaPack ,  Tweet . We have Introduced Semantic Text Splitting in RAG with LlamaPack. Check Greg Kamradt’s method of splitting documents based on embedding similarity between sentences. This auto-tuned threshold approach enhances RAG pipelines, soon to be available in LlamaPack using LlamaIndex abstractions.  LlamaPack ,  Tweet . We launched Parallel RAG Ingestion in LlamaIndex for up to 15x Faster Document Processing.  Notebook ,  Tweet . We have launched Support for TogetherAI’s Embeddings Endpoint. Check the blog for a step-by-step guide on creating a retrieval-augmented generation app with MistralAI’s 8x7b model and TogetherAI Embeddings.  Blogpost  ,  Tweet . We integrated AgentSearch-v1 as a data loader and Retriever in LlamaHub, offering a robust alternative for internet content search/retrieval without relying on Bing/Google APIs.  LlamaPack ,  Tweet . Raduaschl introduced Ensembling and Fusion in Advanced RAG with LlamaPack. Learn to build an ensembling + fusion pipeline in about 30 lines of code using QueryPipeline syntax, featuring full async support.  LlamaPack ,  Tweet . 🗺️ Guides: Guide  to Building Full-Stack RAG Applications with LlamaIndex and Azure Cosmos DB. Guide  showing to combine auto-retrieval for semi-structured retrieval with metadata with MMR to enforce diversity in results. Guide  by  MountainMicky  to understanding the Importance of Reranking in Advanced RAG Pipelines. ✍️ Tutorials: Andrej Baranovskij   tutorial  on Transforming Invoice Data into JSON with LlamaIndex and Pydantic. NVIDIA  tutorial  on Building AI apps with local LLMs running on Windows with LlamaIndex and NVIDIA Harshad Suryawanshi   tutorial  on AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5. 🎥 Events: Ravi Theja gave talk on Building Multi-Tenancy RAG System with LlamaIndex and Qdrant at FOSS United, Bangalore, India. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Jan 16, 2024", "tags": ["LLM", "Newsletter", "AI", "Rag", "Hackathon"]},
{"title": "Multimodal RAG pipeline with LlamaIndex and Neo4j", "content": "The field of AI and large language models is evolving rapidly. One year ago, nobody ever used an LLM to enhance their productivity. Today, most of us can’t imagine working without or not offloading at least some minor tasks to LLMs. Due to much research and interest, LLMs are getting better and wiser every day. Not only that, but their comprehension is starting to span across multiple modalities. With the introduction of GPT-4-Vision and other LLMs that followed it, it seems that LLMs today can tackle and comprehend images very well. Here’s one example of ChatGPT describing what’s in the image. Using ChatGPT to describe images. As you can observe, ChatGPT is quite good at comprehending and describing images. We can use its ability to understand images in an RAG application, where instead of relying only on text to generate an accurate and up-to-date answer, we can now combine information from text and pictures to generate more accurate answers than ever before. Using LlamaIndex, implementing multimodal RAG pipelines is as easy as it gets. Inspired by their  multimodal cookbook example , I decided to test if I could implement a multimodal RAG application with Neo4j as the database. To implement a multimodal RAG pipeline with LlamaIndex, you simply instantiate two vector stores, one for images and one for text, and then query both of them in order to retrieve relevant information to generate the final answer. Workflow diagram for the blog post. Image by author. Articles are first split into images and text. These elements are then converted into vector representations and indexed separately. For text we will use  ada-002  text embedding model, while for images we will be using  dual encoder model CLIP , which can embed both text and images in the same embedding space. When a question is posed by an end user, two vector similarity search are performed; one to find relevant images and the other for documents. The results are fed into a multimodal LLM, which generates an answer for the user, demonstrating an integrated approach to processing and utilizing mixed media for information retrieval and response generation. The  code is available on GitHub . Data preprocessing We will use my Medium articles from 2022 and 2023 as the  grounding dataset  for an RAG application. The articles contain vast information about Neo4j Graph Data Science library and combining Neo4j with LLM frameworks. When you download your own articles from Medium, you get them in an HTML format. Therefore, we need to employ a bit of coding to extract text and images separately. def   process_html_file ( file_path ):\n     with   open (file_path,  \"r\" , encoding= \"utf-8\" )  as  file:\n        soup = BeautifulSoup(file,  \"html.parser\" )\n\n     # Find the required section \n    content_section = soup.find( \"section\" , { \"data-field\" :  \"body\" ,  \"class\" :  \"e-content\" })\n\n     if   not  content_section:\n         return   \"Section not found.\" \n\n    sections = []\n    current_section = { \"header\" :  \"\" ,  \"content\" :  \"\" ,  \"source\" : file_path.split( \"/\" )[- 1 ]}\n    images = []\n    header_found =  False \n\n     for  element  in  content_section.find_all(recursive= True ):\n         if  element.name  in  [ \"h1\" ,  \"h2\" ,  \"h3\" ,  \"h4\" ]:\n             if  header_found  and  (current_section[ \"content\" ].strip()):\n                sections.append(current_section)\n            current_section = {\n                 \"header\" : element.get_text(),\n                 \"content\" :  \"\" ,\n                 \"source\" : file_path.split( \"/\" )[- 1 ],\n            }\n            header_found =  True \n         elif  header_found:\n             if  element.name ==  \"pre\" :\n                current_section[ \"content\" ] +=  f\"``` {element.get_text().strip()} ```\\n\" \n             elif  element.name ==  \"img\" :\n                img_src = element.get( \"src\" )\n                img_caption = element.find_next( \"figcaption\" )\n                caption_text = img_caption.get_text().strip()  if  img_caption  else   \"\" \n                images.append(ImageDocument(image_url=img_src))\n             elif  element.name  in  [ \"p\" ,  \"span\" ,  \"a\" ]:\n                current_section[ \"content\" ] += element.get_text().strip() +  \"\\n\" \n\n     if  current_section[ \"content\" ].strip():\n        sections.append(current_section)\n\n     return  images, sections I won’t go into details for the parsing code, but we split the text based on headers  h1–h4  and extract image links. Then, we simply run all the articles through this function to extract all relevant information. all_documents = []\nall_images = []\n\n # Directory to search in (current working directory) \ndirectory = os.getcwd()\n\n # Walking through the directory \n for  root, dirs, files  in  os.walk(directory):\n     for  file  in  files:\n         if  file.endswith( \".html\" ):\n             # Update the file path to be relative to the current directory \n            images, documents = process_html_file(os.path.join(root, file))\n            all_documents.extend(documents)\n            all_images.extend(images)\n\ntext_docs = [Document(text=el.pop( \"content\" ), metadata=el)  for  el  in  all_documents]\n print ( f\"Text document count:  { len (text_docs)} \" )  # Text document count: 252 \n print ( f\"Image document count:  { len (all_images)} \" )  # Image document count: 328 We get a total of 252 text chunks and 328 images. It’s a bit surprising that I created so many photos, but I know that some are only images of table results. We could use a vision model to filter out irrelevant photos, but I skipped this step here. Indexing data vectors As mentioned, we have to instantiate two vector stores, one for images and the other for text. The CLIP embedding model has a dimension of 512, while the ada-002 has 1536 dimension. text_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=\"text_collection\",\n    node_label=\"Chunk\",\n    embedding_dimension=1536\n)\nimage_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=\"image_collection\",\n    node_label=\"Image\",\n    embedding_dimension=512\n\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store) Now that the vector stores have been initiated, we use the  MultiModalVectorStoreIndex  to index both modalities of information we have. # Takes 10 min without GPU / 1 min with GPU on Google collab \nindex = MultiModalVectorStoreIndex.from_documents(\n    text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n) Under the hood,  MultiModalVectorStoreIndex  uses text and image embedding models to calculate the embeddings and store and index the results in Neo4j. Only the URLs are stored for images, not actual base64 or other representations of images. Multimodal RAG pipeline This piece of code is copied directly from the LlamaIndex multimodal cookbook. We begin by defining a multimodal LLM and the prompt template and then combine everything as a query engine. openai_mm_llm =  OpenAIMultiModal (\n    model= \"gpt-4-vision-preview\" , max_new_tokens= 1500 \n)\n\nqa_tmpl_str = (\n     \"Context information is below.\\n\" \n     \"---------------------\\n\" \n     \"{context_str}\\n\" \n     \"---------------------\\n\" \n     \"Given the context information and not prior knowledge, \" \n     \"answer the query.\\n\" \n     \"Query: {query_str}\\n\" \n     \"Answer: \" \n)\nqa_tmpl =  PromptTemplate (qa_tmpl_str)\n\nquery_engine = index. as_query_engine (\n    multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n) Now we can go ahead and test how well it performs. query_str =  \"How do vector RAG application work?\" \nresponse = query_engine.query(query_str)\n print (response) Response Generated response by an LLM. We can also visualize which images the retrieval fetched and were used to help inform the final answer. Image input to LLM. The LLM got two identical images as input, which just shows that I reuse some of my diagrams. However, I am pleasantly surprised by CLIP embeddings as they were able to retrieve he most relevant image out of the collection. In a more production setting, you might want to clean and deduplicate images, but that is beyond the scope of this article. Conclusion LLMs are evolving faster than what we are historically used to and are spanning across multiple modalities. I firmly believe that by the end of the next year, LLMs will be soon able to comprehend videos, and be therefore able to pick up non-verbal cues while talking to you. On the other hand, we can use images as input to RAG pipeline and enhance the variety of information passed to an LLM, making responses better and more accurate. The multimodal RAG pipelines implementation with LlamaIndex and Neo4j is as easy as it gets. The code is available on  GitHub .", "author": "Tomaz Bratanic", "date": "Dec 18, 2023", "tags": ["Neo4j", "LLM", "Llamaindex", "ChatGPT", "Graph Database"]},
{"title": "LlamaIndex Newsletter 2024–01–23", "content": "Hello LlamaIndex Explorers 🦙, Another exciting week at LlamaIndex, filled with vibrant community contributions and educational resources. Explore our array of new features, tutorials, guides, and demos, all tailored to enrich your experience with LlamaIndex. Before delving into the updates, we have two significant announcements: We’re thrilled to host our  first in-person hackathon , set for February 2nd-4th. This is a fantastic opportunity to meet fellow RAG enthusiasts, collaborate, and compete for prizes totaling over $8000! Don’t miss our  webinar  featuring Sehoon Kim and Amir Gholami, scheduled for Thursday at 9 am PT. They will introduce LLMCompiler, an agent compiler for parallel multi-function planning and execution. We’re always excited to see your projects, articles, or videos. If you’ve created something you’re proud of, share it with us at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  to get all the latest news straight to your inbox. 🤩  The highlights: RankGPT:  Introducing RankGPT leveraging GPT-3.5 and GPT-4 for top-tier document ranking and a novel sliding window technique for extensive context management.  Notebook ,  Tweet . Composable Retrievers:  An interface centralizing advanced retrieval and RAG techniques, enhancing RAG setups with IndexNodes for linking diverse retrievers and pipelines.  Docs ,  Tweet . Advanced QA over Tabular Data Tutorial:  A detailed guide to crafting query pipelines over tabular data, featuring Pandas, SQL, and Query Pipelines for an integrated few-shot, LLM, and custom function setup.  Text-to-SQL ,  Text-to-Pandas . Long-Context Embedding Models:  Explore models like M2-BERT-80M-32k-retrieval tackling the embedding chunking problem in RAG, with a focus on hybrid retrieval methods and hierarchical retrieval approaches.  Guide . ✨ Feature Releases and Enhancements: We have introduced RankGPT in our advanced module that utilizes GPT-3.5 and GPT-4 for efficient document ranking, featuring a unique sliding window strategy for handling large contexts.  Notebook ,  Tweet . We have launched Composable Retrievers which centralizes various advanced retrieval and RAG techniques into a versatile interface. It simplifies creating complex RAG setups by allowing you to define IndexNodes to link different retrievers or RAG pipelines.  Docs ,  Tweet . Anoop Sharma has introduced LlamaPack for Multi-Stock Ticker Analysis for analyzing various stock tickers with a single code line, enabling easy specification of tickers, time frames, and structured queries.  LlamaPack ,  Tweet . LlamaIndex.TS (LITS) supports streaming on all endpoints.  Tweet . We announced a new integration with Tonic Validate to allow simple access to LLM-powered evaluations.  Blog post 🎥  Demo: RAG-Maestro for ArXiv Research:  Developed by Aymen Kallala, this web app utilizes RAG to efficiently search scientific concepts in ArXiv papers. It extracts keywords using RAKE, queries ArXiv for relevant papers, and offers on-the-fly indexing with in-line citations — a valuable tool for ML researchers navigating through ArXiv’s extensive library.  Demo ,  GitHub Repo . 🗺️ Guides: Guide to Advanced QA over Tabular Data which provides a comprehensive tutorial on creating sophisticated query pipelines over tabular data using Pandas or SQL, constructing a query DAG using our Query Pipelines, integrating few-shot examples, linked prompts, LLMs, custom functions, retrievers, and more.  Text-to-SQL ,  Text-to-Pandas . Guide  to a Five-Part Series on Building a Full-Stack RAG Chatbot by  Marco Bertelli , extensive tutorials covering every aspect of creating an RAG chatbot — from model selection and Flask backend setup to constructing the ChatEngine and optimizing the RAG pipeline. Guide  to Long-Context Embedding Models: The models, like M2-BERT-80M-32k-retrieval, offer a solution to the embedding chunking issue in RAG by grounding retrieval in broader semantic contexts. Learn about hybrid retrieval, combining chunk and document-level similarity, and other approaches like hierarchical retrieval. ✍️ Tutorials: Wenqi   tutorial  on Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference with LlamaIndex. Andrej   tutorial  on FastAPI and LlamaIndex RAG: Creating Efficient APIs. Lulia Brezeanu   tutorial  on Advanced Query Transformations to Improve RAG. Akash Mathur  in-depth  tutorial  on Advanced RAG: Query Augmentation for Next-Level Search using LlamaIndex. Ryan Nguyen   tutorial  on Live Indexing for RAG: A Guide For Real-Time Indexing Using LlamaIndex and AWS. Nipuna  (Paragon AI) tutorial on Building a Full-Stack Complex PDF AI chatbot with LlamaIndex. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Jan 23, 2024", "tags": ["Llamaindex", "Newsletter", "AI", "Rag", "LLM"]},
{"title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render", "content": "In this post we’re going to walk you through the process of building and deploying a Slackbot that listens to your conversations, learns from them, and uses that knowledge to answer questions about what’s going on in your Slack workspace. We’ll also deploy it to production on Render! Things you’ll need to start Rudimentary understanding of LlamaIndex. If you haven’t got that, the  starter tutorial  in our documentation will give you as much as you need to understand this tutorial and takes only a few minutes. A working knowledge of Python, and Python 3.11 or higher installed A Slack workspace you can install apps to (so you’ll need to be an admin) A clone of  our Slackbot repo  on your local machine. We’ll be referring to files in this repo throughout the post. Step 1: Create a Slack app, and install it to your workspace This is the most complicated step, because Slack is very picky about permissions. The very first version of your Slackbot is going to be only about 20 lines of code. All it does is provide a “challenge” endpoint that Slack needs to verify your app is available. You can see this code as the file  1_flask.py  in the repo. Let's walk through it. First we bring in your dependencies. You’ll need to install these with pip or poetry if you don’t have them already. from  flask  import  Flask, request, jsonify Now we’ll create your flask app and set it up so it can run in development. flask_app = Flask(__name__)\n\n if  __name__ ==  \"__main__\" :\n    flask_app.run(port= 3000 ) Between those lines we’ll add our basic route: if a POST request is received that contains a JSON object with a  challenge  key, we'll return the value of that key. Otherwise we'll do nothing. @flask_app.route( \"/\" , methods=[ \"POST\" ] ) \n def   slack_challenge ():\n     if  request.json  and   \"challenge\"   in  request.json:\n         print ( \"Received challenge\" )\n         return  jsonify({ \"challenge\" : request.json[ \"challenge\" ]})\n     else :\n         print ( \"Got unknown request incoming\" )\n         print (request.json)\n     return Make your app available to Slack To configure a Slack app, it needs to be running somewhere Slack can see it. So let’s run our Slack app: python 1_flask.py And we’ll set it up so the world can see it using  ngrok . You’ll need to download and install ngrok for this step. Once you have it installed, run the following command so it can find our app running on port 3000: ngrok http 3000 ngrok will give you an HTTPS url like  https://1bf6-64-38-189-168.ngrok-free.app . Make a note of it, because we need to give that to Slack. Also keep in mind that if you stop ngrok and start it again, this URL will change and you'll need to tell Slack about that. You'll only need this during development. Register your app with Slack Go to the  Slack API site  and click “Create New App”. You’ll see a screen like this, you’ll want to pick “from scratch”: Pick a nice friendly name and the workspace you want to install it to. You’ll see a screen like this: Next you’ll want to set up what permissions your app needs. Click the “Permissions” link in the bottom right: This will bring you to the “scopes” screen where you’ll need to add all the scopes you see in this picture, namely: channels:read — the lets your app see what channels are avaialble channels:join — this lets your app join channels channels:history — this lets your app see previous messages in channels chat:write — this lets your app send messages users:read — this lets your app see people’s names Once you’ve saved those scopes, scroll up to “Install to workspace” to install your app. You now need to tell Slack where your app is so you can receive messages from it. Click the “Event Subscriptions” link in the left nav and fill it out so it looks something like this, specifically: Set your Request URL to that URL that ngrok gave you earlier Subscribe to the  message.channels  event If your app is running and ngrok is correctly tunneling, your Request URL should be Verified. Phew! That was a lot. Your Slack app is now registered and Slack will send it messages. But to get those messages, you have to tell it to join a channel. Step 2: Join a channel, and reply to messages To do this we’ll need to extend our app. You can see the final result of this step in  2_join_and_reply.py . Let's walk through what we've added: import dotenv, os\ndotenv.load_dotenv() We need some environment variables, so you’ll need to add these lines and install  python-dotenv . You'll also need to create a  .env  file in the root of your project with three values: OPENAI_API_KEY : your OpenAI API key. You don't need this quite yet but you may as well  get it now . SLACK_BOT_TOKEN : you can find this in the \"OAuth and Permissions\" section of your Slack app. SLACK_SIGNING_SECRET : you can find this in the \"Basic Information\" section of your Slack app. We’re going to use Slack’s handy Python SDK to build our app, so pip install  slack-bolt  and then update all our imports: from  slack_bolt  import  App\n from  flask  import  Flask, request, jsonify\n from  slack_bolt.adapter.flask  import  SlackRequestHandler Now initialize a Slack Bolt app using those secrets we set just now: app = App(\n    token=os.environ.get( \"SLACK_BOT_TOKEN\" ),\n    signing_secret=os.environ.get( \"SLACK_SIGNING_SECRET\" )\n)\nhandler = SlackRequestHandler(app) To listen to messages, the bot has to be in a channel. You can get it to join any and all public channels, but for the purposes of testing I’ve created a channel called  #bot-testing  and that's the one it's joining here: channel_list = app.client.conversations_list(). data \n channel   =  next((channel  for  channel in channel_list.get( 'channels' )  if  channel.get( \"name\" ) ==  \"bot-testing\" ), None)\nchannel_id = channel.get( 'id' )\napp.client.conversations_join(channel=channel_id) app.client  is the Bolt framework's Slack WebClient, so you can do anything a WebClient can do directly from within the framework. The final addition here is a very simple message listener: @app.message() \n def   reply ( message, say ):\n     print (message)\n    say( \"Yes?\" ) In the Bolt framework, the  @app.message  decorator tells the framework to trigger this method when it receives a message event. The  say  parameter is a function that will send a message back to the channel the message came from. So this code will send a message back to the channel saying \"Yes?\" every time it receives a message. Let’s try it out! Stop running  1_flask.py  and run  python 2_join_and_reply.py  instead. You don't need to restart  ngrok , it will continue to send messages to port 3000 as before. Here's me trying it out: Success! We have a very annoying bot that replies to every single thing anybody says. We can do better! Step 3: reply only to messages that mention the bot This is a pretty simple change on the surface, but Slack’s incoming message format is a little complicated so we have to add a fair bit of code. You can see the final results in  3_reply_to_mentions.py . First, to tell when our bot is being mentioned, we need our bot’s User ID. Under the hood, Slack doesn’t use user names or even @-handles, but a globally unique ID across all Slack installations. We have to get that: auth_response = app.client.auth_test()\nbot_user_id = auth_response[\"user_id\"] Now we add an annoyingly complicated chunk of code that parses through Slack’s message object to see what user is mentioned in an incoming message. If it’s the bot, the bot replies, otherwise it just ignores the message. As we go further, we’ll treat messages to the bot as “queries” and any other message as a “fact” for it to store, but we won’t be storing it just yet. @app.message() \n def   reply ( message, say ):\n     if  message.get( 'blocks' ):\n         for  block  in  message.get( 'blocks' ):\n             if  block.get( 'type' ) ==  'rich_text' :\n                 for  rich_text_section  in  block.get( 'elements' ):\n                     for  element  in  rich_text_section.get( 'elements' ):\n                         if  element.get( 'type' ) ==  'user'   and  element.get( 'user_id' ) == bot_user_id:\n                             for  element  in  rich_text_section.get( 'elements' ):\n                                 if  element.get( 'type' ) ==  'text' :\n                                    query = element.get( 'text' )\n                                     print ( f\"Somebody asked the bot:  {query} \" )\n                                    say( \"Yes?\" )\n                                     return \n     # otherwise do something else with it \n     print ( \"Saw a fact: \" , message.get( 'text' )) Oof. That took a while to get right! But now our bot only replies when it’s mentioned: Step 4: use LlamaIndex to store facts and answer questions We’re all the way at step 4 and we still haven’t done anything with LlamaIndex! But now’s the time. In  4_incremental_rag.py  you'll see a demonstration of a simple command-line Python script that uses LlamaIndex to store facts and answer questions. I won't walk you through every line (the script has helpful comments for that), but let's look at the important ones. Remember to  pip install llama-index ! First we create a new  VectorStoreIndex , an in-memory  vector store  where we'll be storing our facts. It's empty to start with. index = VectorStoreIndex([]) Next we create 3  Document  objects and insert them each into our index. Real documents can be huge blocks of text, whole PDFs, even images, but these are just some simple, Slack-message-sized facts. doc1 = Document(text=\"Molly is a cat\")\ndoc2 = Document(text=\"Doug is a dog\")\ndoc3 = Document(text=\"Carl is a rat\")\n\nindex.insert(doc1)\nindex.insert(doc2)\nindex.insert(doc3) And finally we create a  query engine  from our index and ask it a question: # run a query \nquery_engine = index.as_query_engine()\nresponse = query_engine.query( \"Who is Molly?\" )\n print (response) The result is “Molly is a cat” plus a whole lot of debugging info because we turned on noisy debugging in  4_incremental_rag.py . You can see the prompt we sent to the LLM, the context it retrieved from the index, and the response it generated and sent back to us. Step 5: use LlamaIndex to store facts and answer questions in Slack In  5_rag_in_slack.py  we are combining the two things we had before: script 3, where we reply to queries, and script 4, where we store facts and answer questions. Once again we won't walk through every line, but here are the important changes: First  pip install llama-index  if you didn't already, and bring in your deps. Initialize your index while you're at it: from  llama_index  import  VectorStoreIndex, Document\n\nindex = VectorStoreIndex([]) Where previously we were just replying with “Yes?” (line 73) let’s instead send a query to the query engine and reply with the response: query = element.get( 'text' )\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\nsay(str(response)) And where previously we were just noting that we’d seen a fact (line 82), let’s store it in the index: index.insert(Document(text=message.get( 'text' ))) The result is a Slackbot that can answer questions about what it’s been told: Amazing! You can easily imagine a bot that listens to everybody’s conversations and is able to answer questions about things people said weeks or months ago, saving everybody time and effort searching through old messages. Step 6: persist our memory Our bot has a critical flaw though: the index is stored only in memory. If we restart the bot, it forgets everything: In  6_qdrant.py  we bring in  Qdrant , an open-source, local vector database that stores these facts on disk instead. That way if we restart our bot it remembers what was said before.  pip install qdrant-client  and bring in some new deps: import  qdrant_client\n from  llama_index.vector_stores.qdrant  import  QdrantVectorStore Now we’ll initialize the Qdrant client, attach it to a storage context, and give that storage context to our index when we initialize it: client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"slack_messages\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex([],storage_context=storage_context) That’s it for this step! Your bot now survives reboots, and remembers that I typoed “Doug” as “Dough” and was too lazy to fix it for the screenshot: Step 7: make recent messages more important We now have a pretty capable bot! But it has a subtle problem: people can say conflicting things, and it doesn’t have a way to decide who was “right”, such as when I change my mind about what the dog’s name should be: In real Slack conversations, as a situation evolves people might move from saying a project is “in planning” to “underway” to “launched”. So we need a way to tell the bot that more recent messages are more important than older ones. To make this happen we have to do quite a bit of refactoring, the final results of which you can see in  7_recency.py . First we need a bunch of new deps: import  datetime, uuid\n from  llama_index.schema  import  TextNode\n from  llama_index.prompts  import  PromptTemplate\n from  llama_index.postprocessor  import  FixedRecencyPostprocessor\n from  llama_index  import  set_global_handler To make recent messages more important, we have to know when a message was sent. To do that we are going to stop inserting  Documents  into the index and instead insert  Nodes , to which we're going to attach the timestamp as metadata (under the hood, our Documents were always being converted into Nodes anyway so this doesn't change much): dt_object = datetime.datetime.fromtimestamp( float (message.get( 'ts' )))\nformatted_time = dt_object.strftime( '%Y-%m-%d %H:%M:%S' )\n\n# get the message  text \n text   =  message.get( 'text' )\n# create a node with  metadata \n node   =  TextNode(\n    text=text,\n    id_=str(uuid.uuid4()),\n    metadata={\n         \"when\" : formatted_time\n    }\n)\nindex.insert_nodes([node]) I’ve also factored out the reply logic from message handling into its own function,  answer_question , just to make things a little easier to read. The first thing we're going to change is the prompt that we give to our LLM: we have to tell it that more recent messages are important. To do this we create a prompt template: template = (\n     \"Your context is a series of chat messages. Each one is tagged with 'who:' \\n\" \n     \"indicating who was speaking and 'when:' indicating when they said it, \\n\" \n     \"followed by a line break and then what they said. There can be up to 20 chat messages.\\n\" \n     \"The messages are sorted by recency, so the most recent one is first in the list.\\n\" \n     \"The most recent messages should take precedence over older ones.\\n\" \n     \"---------------------\\n\" \n     \"{context_str}\" \n     \"\\n---------------------\\n\" \n     \"You are a helpful AI assistant who has been listening to everything everyone has been saying. \\n\" \n     \"Given the most relevant chat messages above, please answer this question: {query_str}\\n\" \n)\nqa_template =  PromptTemplate (template) The fun thing about working with LLMs is how often you end up just describing what you’re doing in English and that being what you send to the LLM. A prompt template will automatically get the  context_str  and  query_str  from the query engine. But we have to set this template on our query engine, like so: query_engine.update_prompts(\n    {\"response_synthesizer:text_qa_template\": qa_template}\n) Now there’s two more things we’re going to change. We’re going to take the results we get from the vector store and sort them by recency, something LlamaIndex has a built-in class for. It’s called the  FixedRecencyPostprocessor . We tell it the key that holds the timestamp (which we defined earlier on the nodes, above) and how many results it should return: postprocessor = FixedRecencyPostprocessor(\n    top_k=20, \n    date_key=\"when\", # the key in the metadata to find the date\n    service_context=ServiceContext.from_defaults()\n) Then we need to create our query engine with the postprocessor attached: query_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor]) While we were at it we did our final thing, which was pass  similarity_top_k=20 , which means the vector store will give us 20 Slack messages as context (the default is just 2, because usually the chunks of text in a Node are a lot bigger). Tada! Now the bot knows to take more recent statements as the truth. Step 8: draw the rest of the owl This bot is working pretty well now, but I was having such fun when building it I got carried away and added two more features: I attached metadata about  who  was speaking, not just when, so the bot can answer questions like “What did Logan say about the project?” My colleagues interacting with the bot tried to ask follow-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it’s in a thread, and treat replies in a thread as follow-up questions, even if the user doesn’t mention the bot directly: The code to make both of those happen is in  8_rest_of_the_owl.py  but I'm not going to be stepping through it line by line. We have to deploy this thing! Step 9: deploy to Render Until now we’ve been working with local scripts running through the ngrok tunnel, but even the most dedicated coder turns their laptop off sometimes. Let’s put this thing on a real server. Login to Render We’ll be deploying to  Render , a Python-friendly hosting service that’s free for small projects. Sign up for an account (I recommend logging in with GitHub). Create a new GitHub repository Render deploys things from GitHub repositories, so you’ll need to create a new one and copy 2 files from our existing repo into it: pyproject.toml 8_rest_of_the_owl.py  which we're going to rename to \"app.py\" for simplicity. Commit those and push them up to GitHub. Create a new Render web service In Render, create a new web service. Connect it to the repo on GitHub you just created: Render will probably automatically detect that this is a Python app but you should make sure the following settings are correct: Name: any name you choose Region: any region is fine Branch: main Root directory: (blank, meaning root) Runtime: Python 3 Build command:  poetry install Start command:  gunicorn app:flask_app  (this will definitely need to be set) You’ll also need to scroll down and set some environment variables: PYTHON_VERSION: 3.11.6 (or whatever version you’re using) OPENAI_API_KEY: your OpenAI API key SLACK_BOT_TOKEN: your Slack bot token SLACK_SIGNING_SECRET: your Slack signing secret from before Then click deploy and away you go! You now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations! What next? There’s a whole bunch of features you could add to this bot, roughly in increasing order of difficulty: Join every channel instead of just one, clearly! Add a way to tell the bot to forget things (delete nodes) Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar Give the bot “tags” so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way Add multi-modal abilities, so the bot can read images and even reply with generated images And tons more! This bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!", "author": "LlamaIndex", "date": "Jan 25, 2024", "tags": ["LLM", "Qdrant", "Render", "AI", "Slack"]},
{"title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines", "content": "Introduction We are seeing a huge wave of developers building Retrieval Augmented Generation (RAG) applications. The RAG tech stack generally contains a retrieval pipeline, LLM and prompt, among which LLM is accessible and developers are comfortable with prompt customization. However, developers new to search and index often need extensive help to build an effective  retrieval  pipeline. A production-ready retrieval pipeline typically consists of the following components: Document loader that parses and splits the long text Embedding model serving as core indexing component A vector database that stores the vector embeddings Advanced components to future optimize retrieval quality, such as re-ranker model to judge semantic similarity better It’s challenging to operate this complex tech stack. It involves managing software package dependencies, hosting services in Kubernetes clusters, and monitoring the performance of ML models. The high DevOps cost distracts developers from the most critical part of the user experience of RAG applications: prompt engineering, answer generation, and user interface. While experienced search infrastructure engineers may still manage a complicated tech stack for its flexibility, Zilliz believes that most RAG developers could benefit from a retrieval API service that is user-friendly and allows for lighter customization. Integrating  Zilliz Cloud Pipelines  and  LlamaIndex  brings a new approach to solving this problem. Zilliz Cloud Pipelines is a fully managed, scalable retrieval service. LlamaIndex is a flexible RAG framework that provides libraries and tools for organizing business logics such as retrieval and prompt engineering. The API service of Zilliz Cloud Pipelines is abstracted as a ManagedIndex in LlamaIndex. RAG developers using  ZillizCloudPipelineIndex  can easily scale the app from one user to millions of users without the hassle of setting up and maintaining the complex retrieval tech stack. It hides the technical complexity behind a few function calls, so that developers can focus on the core user experience of their RAG apps. In this blog, we show how to use  ZillizCloudPipelineIndex  to build a high quality RAG chatbot. The chatbot is scalable and supports multi-tenancy through metadata filtering. Set up Since Zilliz Cloud Pipelines is an API service, first you need to set up a  Zilliz Cloud  account and create a free serverless cluster. Now you can construct  ZillizCloudPipelineIndex  and get the handler to index docs and query later. from llama_index.indices import ZillizCloudPipelineIndex\n\nzcp_index = ZillizCloudPipelineIndex(\n    project_id=\" &lt; YOUR_ZILLIZ_PROJECT_ID &gt; \",\n    cluster_id=\" &lt; YOUR_ZILLIZ_CLUSTER_ID &gt; \",\n    token=\" &lt; YOUR_ZILLIZ_API_KEY &gt; \",\n)\nzcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"}) You can copy the Project ID, Cluster ID and API Key from your Zilliz account as follows: Ingest Documents Suppose your application has multiple users, and you would like to tag each user’s document to provide isolation. Your application logic can be implemented as follows. For simplicity, here we demo ingesting public documents. Currently, Zilliz Cloud Pipelines  supports  documents stored and managed in AWS S3 and Google Cloud Storage. Local document upload will also be supported soon. # user1 ingests a document, it is technical documentation  for  v2 .3  version. \nzcp_index.insert_doc_url(\n    url= \"https://publicdataset.zillizcloud.com/milvus_doc.md\" ,\n    metadata={ \"user_id\" :  \"user1\" ,  \"version\" :  \"2.3\" },\n)\n# user2 ingests a document, it is technical documentation  for  v2 .2  version. \nzcp_index.insert_doc_url(\n    url= \"https://publicdataset.zillizcloud.com/milvus_doc_22.md\" ,\n    metadata={ \"user_id\" :  \"user2\" ,  \"version\" :  \"2.2\" },\n) Query To conduct semantic search with  ZillizCloudPipelineIndex , you can use it  as_query_engine()  by specifying a few parameters: search_top_k: How many text nodes/chunks to retrieve. Optional, defaults to DEFAULT_SIMILARITY_TOP_K (2). filters: Metadata filters. Optional, defaults to None. In this example, we set the filter to only retrieve docs of a specific user, to provide user-level data isolation. output_metadata: What metadata fields to return with the retrieved text node. Optional, defaults to []. # Query the documents in ZillizCloudPipelineIndex \n from  llama_index.vector_stores.types  import  ExactMatchFilter, MetadataFilters\n\nquery_engine_for_user1 = zcp_index.as_query_engine(\n    search_top_k= 3 ,\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(key= \"user_id\" , value= \"user1\" )\n        ]   # The query would only search from documents of user1. \n    ),\n    output_metadata=[ \"user_id\" ,  \"version\" ],  # output these tags together with document text \n)\n\nquestion =  \"Can users delete entities by complex boolean expressions?\" \n # The chatbot will only answer with the retrieved information from user1's documents \nanswer = query_engine_for_user1.query(question) Thanks to the abstraction of LlamaIndex and Zilliz Cloud Pipelines, with just 30 lines of code, we can demo a RAG service that supports multi-tenancy. Most importantly, this simple RAG app could easily scale to serving millions of users without changing any code. What to do next? You can check out the official  LlamaIndex documentation  to learn about advanced customization of  ZillizCloudPipelineIndex . Please ask questions at  Zilliz user group  or  LlamaIndex discord  if you have any questions. Zilliz Cloud Pipelines will soon support local file upload and more choices of embedding and re-ranker models. Get a free  Zilliz Cloud  account and stay tuned for more updates!", "author": "Jiang Chen", "date": "Jan 25, 2024", "tags": ["Retrieval Augmented", "Llamaindex", "Milvus", "LLM", "AI"]},
{"title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool", "content": "Want to try out retrieval-augmented generation (RAG) without writing a line of code? We got you covered! Introducing the new  llamaindex-cli  tool, installed when you  pip install llama-index  ! It uses  Chroma  under the hood, so you’ll need to  pip install chromadb  as well. How to use it Set the  OPENAI_API_KEY  environment variable:  By default, this tool uses OpenAI’s API. As such, you’ll need to ensure the OpenAI API Key is set under the  OPENAI_API_KEY  environment variable whenever you use the tool. $ export OPENAI_API_KEY=&lt;api_key&gt; 2. Ingest some files:  Now, you need to point the tool at some local files that it can ingest into the local vector database. For this example, we’ll ingest the LlamaIndex  README.md  file: $ llamaindex-cli rag --files \"./README.md\" You can only specify a file glob pattern such as $ llamaindex-cli rag --files \"./docs/**/*.rst\" 3. Ask a Question : You can now start asking questions about any of the documents you’d ingested in the prior step: $ llamaindex-cli rag --question  \"What is LlamaIndex?\"  \nLlamaIndex is a data framework that helps in ingesting, structuring, and accessing  private  or domain-specific data  for  LLM-based applications. It provides tools such as data connectors to ingest data from various sources, data indexes to structure the data, and engines  for  natural language access to the data. LlamaIndex follows a Retrieval-Augmented  Generation   (RAG)  approach, where it retrieves information from data sources, adds it to the question as context, and then asks the LLM to generate an answer based on the enriched prompt. This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, up-to-date, and trustworthy solution  for  data augmentation. LlamaIndex is designed  for  both beginner and advanced users, with a high-level API  for  easy usage and lower-level APIs  for  customization and extension. 4. Open a Chat REPL : You can even open a chat interface within your terminal! Just run  llamaindex-cli rag --chat  and start asking questions about the files you’ve ingested. Customize it to your heart’s content! You can customize  llamaindex-cli  to use any LLM model, even local models like Mixtral 8x7b through  Ollama , and you can build more advanced query and retrieval techniques.  Check the documentation  for details on how to get started.", "author": "LlamaIndex", "date": "Jan 26, 2024", "tags": ["LLM", "Cli", "Llamaindex", "AI", "Chromadb"]},
{"title": "Agentic RAG With LlamaIndex", "content": "The topic of Agentic RAG explores how agents can be incorporated into existing\n  RAG pipelines for enhanced, conversational search and retrieval.\n \n\n Introduction \n \n   Considering the architecture below, it is evident how Agentic RAG creates\n      an implementation which easily scales. New documents can be added with\n      each new set being managed by a sub-agent. \n \n\n \n   \n \n\n \n  The basic structure of LlamaIndex’s approach called Agentic RAG is shown in\n  the diagram below where a large set of documents are ingested, in this case it\n  was limited to 100.\n \n \n  The large corpus of data is broken up into smaller documents. An agent is\n  created for each document, and each of the numerous document agents have the\n  power of search via embeddings and to summarise the response.\n \n \n  A top-level agent is created over the set of document agents. The meta-agent /\n  top-level agent performs tool retrieval and then uses Chain-of-Thought to\n  answer the user’s question.\n \n \n  The Rerank endpoint computes a relevance score for the query and each\n  document, and returns a sorted list from the most to the least relevant\n  document.\n \n\n \n   \n \n\n Notebook Example \n \n  Here you will find a Colab\n   notebook \n  with a fully working and executed example of this\n   implementation .\n \n To run the notebook, you will need an OpenAI and Cohere API key… \n import os import openai import cohere os.environ[\"OPENAI_API_KEY\"] = \"Your API Key goes here\" os.environ[\"COHERE_API_KEY\"] = \"Your API Key goes here\" \n For each document agent, the system prompt is: \n You are a specialized agent designed to answer queries about the  `{file_base}.html` part of the LlamaIndex docs. You must ALWAYS use at least one of the tools provided when answering a  question; do NOT rely on prior knowledge. \n The system prompt for the meta-agent /top-agent: \n  You are an agent designed to answer queries about the documentation.  Please always use the tools provided to answer a question.  Do not rely on prior knowledge \n Working Example \n I asked the agent the following question: \n \n   Tell me about LlamaIndex connectors \n \n Below is the detailed answer generated by the agent: \n\n \n   Thought: I need to use a tool to help me answer the question. \n \n \n   Action: tool_latest_search Action Input: {‘input’: ‘LlamaIndex\n      connectors’} \n \n Added user message to memory: LlamaIndex connectors \n \n  === Calling Function ===  Calling function: vector_tool_latest_search\n  with args: {  “input”: “LlamaIndex connectors”  }  Got output:\n  The LlamaIndex connectors can be found on the\n  /content/docs.llamaindex.ai/en/latest/search.html page.\n  ========================\n \n \n   Observation: The LlamaIndex connectors are not specifically mentioned in\n      the `latest_search.html` part of the LlamaIndex docs. You may need to\n      check other sections of the documentation for detailed information about\n      LlamaIndex connectors. \n \n \n   Thought: I need to search for information about LlamaIndex connectors in\n      a different tool. \n \n \n   Action: tool_latest_index Action Input: {‘input’: ‘LlamaIndex\n      connectors’} \n \n Added user message to memory: LlamaIndex connectors \n \n  === Calling Function ===  Calling function: vector_tool_latest_index with\n  args: {  “input”: “LlamaIndex connectors”  }\n \n \n  Got output: LlamaIndex provides data connectors that allow you to ingest your\n  existing data from various sources and formats. These connectors can work with\n  APIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\n  bring your data into the LlamaIndex ecosystem and make it accessible for\n  natural language access and retrieval. ========================\n \n \n   Observation: LlamaIndex offers data connectors that enable you to import\n      your existing data from a variety of sources and formats. These connectors\n      are compatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex\n      connectors allows you to seamlessly integrate your data into the\n      LlamaIndex ecosystem, making it readily available for natural language\n      access and retrieval. \n \n \n   Thought: I can answer without using any more tools. \n \n \n   Answer: LlamaIndex connectors are used to import existing data from\n      various sources and formats into the LlamaIndex ecosystem. These\n      connectors are compatible with APIs, PDFs, SQL, and more, allowing\n      seamless integration of data for natural language access and\n      retrieval. \n \n\n Below is a snipped from the Colab notebook: \n\n \n   \n\n   \n     Source \n   \n \n\n \n  This complex implementation from LlamaIndex is an example of multi-document\n  agents which can:\n \n \n   Select documents relevant to a user query \n   \n    Execute an agentic loop over the documents relevant to the query; including\n    chain-of-thought, summarisation and reranking.\n   \n \n In Conclusion \n This implementation by LlamaIndex illustrates a few key principles… \n \n   \n    Agentic RAG, where an agent approach is followed for a RAG implementation\n    adds resilience and intelligence to the RAG implementation.\n   \n   It is a good illustration of multi-agent orchestration. \n   \n    This architecture serves as a good reference framework of how scaling an\n    agent can be optimised with a second tier of smaller worker-agents.\n   \n   \n    Agentic RAG is an example of a controlled and well defined\n     autonomous \n     agent \n    implementation.\n   \n   \n    One of the most sought-after enterprise LLM implementation types are RAG,\n    Agentic RAG is a natural progression of this.\n   \n   \n    It is easy to envisage how this architecture can grow and expand over an\n    organisation with more sub bots being added.\n   \n \n \n   ⭐️ Follow me on  LinkedIn  for updates on Large Language Models ⭐️", "author": "Cobus Greyling", "date": "Jan 30, 2024", "tags": ["AI", "Artificial Intelligence", "Large Language Models", "Conversational AI", "Prompt Engineering"]},
{"title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex", "content": "What is a Retriever? Recently, retrieval augmented generation (RAG) has enabled language models to reduce hallucinations, improve response quality, and maintain up-to-date knowledge of the world  without  requiring retraining of the model itself. This is done by equipping a language model with a  retriever  and a  database.  At inference time, a RAG system uses the retriever to select relevant documents from the database, and passes them to the language model context window. Today, the most popular type of retriever is based on an embedding model. This embedding model converts all of the documents in the database to a vector representation. Then, at inference time, it converts the query to a vector representation, and retrieves the most similar documents to the query vector from the database. In this post, we are going to show you how to build a fully open source retriever using LlamaIndex and Nomic Embed, the first fully open source embedding model to exceed OpenAI Ada performance on both short and long context benchmarks. Why Open Source? As AI becomes deployed in increasingly high impact domains, such as defense, medicine, and finance, end-to-end auditability of the entire system becomes a key component of safe AI deployment. Unfortunately, the closed source embedding models used in most RAG systems today have deliberately obfuscated training protocols and cannot be audited. Further, as organizations adopting AI begin to mature, reliance on closed source embedding models will result in vendor lock-in and a limited ability to modify the embedding model to suit the needs of the business. Luckily, fully open source embedding models like Nomic Embed offer end-to-end auditability of the training process as well as a strong basis for further improvements and modifications of the model. How To To build an open source retriever with LlamaIndex and Nomic Embed, we will start by importing the relevant libraries from  llama_index.embeddings  import  NomicEmbedding\n from  llama_index  import  (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n) Next, we need to download some data for our database. For this example, we are going to use an essay by Paul Graham, which we download from  here  and place into a directory named ./data/paul_graham. Now, it’s time to get the vectors for the documents in our database. To do this, we are going to use the LlamaIndex SimpleDirectoryReader and Nomic’s hosted inference service. You’ll have to replace <NOMIC_API_KEY> with your Nomic API key, which you can get after signing up for Nomic Atlas  here . documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nnomic_api_key = \" &lt; NOMIC_API_KEY &gt; \"\nembed_model = NomicEmbedding(\n    api_key=nomic_api_key,\n    model_name=\"nomic-embed-text-v1\",\n    task_type=\"search_document\"\n)\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model, chunk_size=1024,\n)\nindex = VectorStoreIndex.from_documents(\n    documents=documents, service_context=service_context, show_progress=True\n) Notice that we set task_type to search_document in NomicEmbedding. Nomic Embed supports many different types of tasks, and search_document is optimized for building representations of documents for RAG databases. Once our database is set up, we are ready to build our retriever. Using LlamaIndex, this is as simple as a few lines of python: embed_model = NomicEmbedding(\n    api_key=nomic_api_key,\n    model_name=\"nomic-embed-text-v1\",\n    task_type=\"search_query\"\n)\n\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model\n)\n\nsearch_query_retriever = index.as_retriever(service_context=service_context, similarity_top_k=1) Again, notice that we are using a new NomicEmbedding model with task_type set to search_query. This task type is optimized for embedding queries for search over a retrieval database. Finally, we can use our retriever to surface relevant documents given user queries! As an example: retrieved_nodes_nomic = retriever_nomic.retrieve(\n    \"What software did Paul write?\"\n) returns a document that describes Paul’s first programs: Node ID: 380fbb0e-6fc1-41de-a4f6-3f22cd508df3\nSimilarity:  0.6087318771843091 \nText: What I Worked On\n\nFebruary  2021 \n\nBefore college the two main things I worked on, outside of school, were writing  and  programming. I didn 't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district' s  1401  happened to be  in  the basement of our junior high school,  and  my friend Rich Draves  and  I got permission to use it. It was like a mini Bond villain 's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\nI was puzzled by the 1401. I couldn' t figure out what to do  with  it. And  in  retrospect there 's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn' t have  any  data stored on punched cards. The only other option was to do things that didn 't rely on any input, like calculate approximations of pi, but I didn' t know enough math to do anything interesting of that  type . So I 'm not surprised I can' t remember  any  programs I wrote, because they can 't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn' t. On a machine without time-sharing, this was a social  as  well  as  a technical error,  as  the data center manage r's expression made clear.\n\nWith microcomputers, everything changed.  Conclusion & Next Steps In this post, we showed you how to build a fully open source retriever using Nomic Embed and LlamaIndex. If you want to dive deeper, you can find the source code for Nomic Embed  here . You can also use Nomic  Atlas  to visualize your retrieval database, and  LlamaIndex  to connect it to a generative model for full RAG.", "author": "bstadt", "date": "Jan 30, 2024", "tags": []},
{"title": "LlamaIndex Newsletter 2024–01–30", "content": "Hello LlamaIndex Adventurers 🦙, Welcome to another thrilling week at LlamaIndex! It’s brimming with community contributions and a wealth of educational content that will take your LlamaIndex experience to new heights. Dive into our latest features, comprehensive tutorials, insightful guides, and interactive demos, all designed to supercharge your journey with LlamaIndex. But first, let’s ignite your excitement with a reminder about our upcoming  first-ever in-person hackathon , happening February 2nd-4th. Don’t miss this incredible chance to mingle with fellow RAG aficionados, collaborate on exciting projects, and vie for a share of over $16,000 in prizes! Your creations inspire us! Whether it’s a project, article, or video that you’re proud of, we’d love to see it. Share your brilliance with us at  news@llamaindex.ai . And for those who haven’t yet, make sure to subscribe to our newsletter on our  website  — it’s your gateway to all the latest and greatest from LlamaIndex, delivered directly to your inbox. 🤩  The highlights: RAG CLI : Easy-to-use tool for local file indexing and search, with advanced integration and customization features.  Docs ,  Tweet . JSONalyze : Efficiently summarizes large JSON datasets, transforming them into SQLite for detailed SQL queries.  Docs ,  Tweet . OpenAI Embeddings : We now support the latest OpenAI  text-embedding-3-small  and  text-embedding-3-large  embeddings for improved accuracy and cost-effectiveness in data retrieval.  Docs ,  Tweet . ReAct Agent  Guide : From scratch guide for building ReAct agents, covering all key aspects from setup to memory management. Slack Bot : Step-by-step  guide  for developing a learning Slack bot, integrated with advanced data engines and deployment tools. ✨ Feature Releases and Enhancements: We have launched RAG CLI: A straightforward command-line tool for indexing and searching any local file, featuring integration with IngestionPipeline, QueryPipeline, and ChromaDB, with support for local models and customizable logic.  Docs ,  Tweet . We have introduced JSONalyze, a query engine that swiftly summarizes large JSON datasets. It transforms JSON data into an SQLite table, enabling precise SQL queries for efficient data analysis, combining LlamaIndex’s capabilities with text-to-SQL technology.  Docs ,  Tweet . We have launched day 0 support for OpenAI’s latest embedding models featuring cost-effective  text-embedding-3-small  and high-performance  text-embedding-3-large , both with customizable dimensions for enhanced retrieval accuracy in Python and TypeScript versions of LlamaIndex.  Docs ,  Tweet . We have launched Infer-Retrieve-Rerank as a LlamaPack, a technique developed by Karel Doostrlnck, as a simple yet effective LLM-based approach for tackling complex classification challenges with numerous categories, applicable in areas like medical diagnosis and job skill assessment.  LlamaPack ,  Tweet . We have launched LlamaPack with Vanna AI: An advanced text-to-SQL tool using RAG for storing, indexing, and generating SQL queries.  LlamaPack . We have integrated with Zilliz Cloud Pipeline in partnership with Zilliz Universe. This fully managed, scalable retrieval service supports multi-tenancy.  Blog ,  Tweet . We have partnered with Exa which created an advanced RAG-powered web search, designed for LLMs and now integrated with Llama Index agents, enhancing workflow automation and data source combination.  Notebook ,  Tweet . We have integrated with Neutrino, offering GPT-4 level performance at significantly reduced costs by smartly allocating queries to the most suitable model from a diverse range.  Docs ,  Twitter . 🗺️ Guides: Guide  to Building a ReAct Agent from Scratch and cookbook detailing the essential components for creating your agents, including reasoning prompts, output parsing, tool selection, and memory management. Guide  to Building Slack Bot: Create and deploy an intelligent Slack bot that learns from conversations and accurately answers organizational queries, featuring integration with Qdrant Engine and Render. ✍️ Tutorials: Marco Bertelli   tutorial  on Empowering Your Chatbot: Unveiling Dynamic Knowledge Sources with Advanced Integration. Tonic Validate  tutorial on Implementing integration tests for LlamaIndex. Chia Jeng Yang   tutorial  on Injecting Knowledge Graphs in different RAG stages. Wenqi Glantz   tutorial  on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI. 🎥  Webinar LlamaIndex  Webinar  on Efficient Parallel Function Calling Agents with LLMCompiler with Sehoon Kim and Amir Gholami. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Jan 30, 2024", "tags": ["Llamaindex", "Newsletter", "LLM", "AI", "Rags"]},
{"title": "LlamaIndex Newsletter 2024–01–02", "content": "Hello, Llama Lovers 🦙, Happy New Year! As we step into 2024, we’re thrilled to bring you a special edition of our newsletter, packed with updates from the last two weeks of 2023. This edition is brimming with the latest features, community demos, courses, insightful tutorials, guides, and webinars that we’ve curated for you. Have you been working on an interesting project, written an engaging article, or created a video? We can’t wait to hear about it! Please share your work with us at  news@llamaindex.ai . Don’t forget to subscribe to our newsletter via our  website  to receive all these exciting updates directly in your inbox. 🤩  First, the highlights: LLMCompiler Implementation:  A SOTA agent implementation for faster, efficient handling of complex queries.  Notebook ,  Tweet . MultiDocAutoRetrieverPack:  A RAG template for structured retrieval and dynamic responses to large documents and metadata.  Tweet ,  LlamaPack . Structured Hierarchical RAG:  New RAG technique for optimized retrieval over multiple documents, ensuring precise, relevant responses.  Docs ,  Tweet . Custom Agents:  A simple abstraction for custom agent reasoning loops, enabling easy integration with RAG, SQL, and other systems, and enhancing response refinement for complex queries.  Docs ,  Tweet . New lower-level agent API:  For enhanced transparency, debuggability, and control, supporting step-wise execution and task modification.  Docs ,  Tweet . ✨ Feature Releases and Enhancements: We have introduced a simple abstraction for building custom agent reasoning loops, surpassing prepackaged frameworks like ReAct. This tool allows for easy integration with RAG, SQL, or other systems, and we demonstrated how to build an agent with retry logic for routers, enhancing its ability to manage complex, multi-part questions and refine query responses.  Docs ,  Tweet . We have implemented the LLMCompiler project, a SOTA agent framework enabling DAG-based planning and parallel function execution. This surpasses traditional sequential methods in speed, allowing for quicker and more efficient handling of complex queries in any LLM and data pipeline.  Notebook ,  Tweet . We have introduced MultiDocAutoRetrieverPack, a RAG template for efficiently handling large documents and metadata, offering structured retrieval and dynamic responses tailored to specific queries.  Tweet ,  LlamaPack . We have introduced a Structured Hierarchical RAG technique, optimizing RAG over multiple documents. It involves modeling documents as structured metadata for auto-retrieval, indexed in a vector database. This method dynamically selects documents based on inferred properties and performs recursive retrieval within each document for precise, relevant responses in your RAG pipeline.  Docs ,  Tweet . We have launched a new feature for advanced RAG that allows step-wise feedback for complex query executions, improving interpretability and control. This is particularly beneficial for weaker models that struggle with multi-part tasks. We also introduced a step-by-step chat interface for enhanced user interaction and control.  Notebook ,  Tweet . We have integrated with OpenRouterAI, offering a unified API for easy LLM access, cost efficiency, and reliable fallback options. OpenRouterAI allows users to compare costs, latency, and throughput for various models, like mixtral-8x7b, directly on their platform.  Notebook ,  Tweet . We have introduced a new lower-level agent API that enhances transparency, debuggability, and control. This API allows for granular control over agents, decouples task creation from execution, and supports step-wise execution. It also enables viewing each step, upcoming steps, and soon, modifying intermediate steps with human feedback.  Docs ,  Tweet . 👀 Community Demos : Automated LeetCode Crash Course:  The Project integrates advanced ML with traditional algorithms to streamline LeetCode study for technical interviews. It involves extracting and summarizing LeetCode problems using an LLM, organizing these summaries in a vector store, and employing scikit-learn for clustering.  Blog ,  Code . RAG Assisted Auto Developer : A project by  Ocean Li  for building a devbot that understands and writes code. It integrates various tools: LlamaIndex for indexing codebases, Autogen / OpenAI Code Interpreter for code writing and testing, and  lionagi.ai  for orchestration.  Notebook . 📚 Courses: We’ve partnered with  ActiveLoop AI  to provide a  free course  on retrieval-augmented generation for production, featuring 33 lessons, 7 hands-on assignments, and a certification upon completion. Beginner-friendly  course  from  IBM Skills Network  on using LlamaIndex with IBM Watsonx to create effective product recommendations. 🗺️ Guides: Guide  to Semi-Structured Image QA with Gemini: Learn to extract data from unlabeled images and query it, using multi-modal models and advanced retrieval techniques, as demonstrated with the SROIE v2 dataset which contains images of receipts/invoices. Guide  to Advanced RAG Concepts: A comprehensive survey by  Ivan Ilin , covering twelve core concepts including chunking, hierarchical indexing, query rewriting, and more. Each section provides resources and guides from our system for deeper understanding and practical application. Guide  to Building Hybrid Search: Learn to create a hybrid search for RAG from scratch. The process involves generating sparse vectors, fusing sparse and dense queries, and implementing this in a Qdrant engine database for effective RAG integration. Guide  to Building Structured Retrieval with LLMs: Set up auto-retrieval in Pinecone vector database, monitor prompts with Arize AI Phoenix, and tailor prompts for specific queries to enhance your document handling and structured data analysis. Guide  on Evaluating LLM Evaluators: our new evaluation method and dataset bundle, are designed to benchmark LLMs as evaluators against human annotations. This involves comparing LLM judge predictions (1–5 score) with ground-truth judgments, using metrics like Correlation, Hamming Distance, and Agreement Rate. ✍️ Tutorials: Ryan Nguyen   tutorial  on Processing Tables in RAG Pipelines with LlamaIndex and UnstructuredIO. Wenqi Glantz   tutorial  on Safeguarding RAG Pipelines: A Step-by-Step Guide to Implementing Llama Guard with LlamaIndex. Wenqi Glantz   tutorial  on 10+ Ways to Run Open-Source Models with LlamaIndex. Jina AI  tutorial  on enhancing RAG applications by integrating Jina v2 embeddings with LlamaIndex and Mixtral LLM via Hugging Face. Ankush Singal   tutorial  on Benchmarking RAG Pipelines With A Evaluation Pack in Forward-Looking Active Retrieval Augmented Generation (FLARE). Laurie’s   tutorial  on Effortlessly Running Mistral AI’s Mixtral 8x7b: Learn to use OLLAMA with LlamaIndex for a one-line setup of a local, open-source retrieval-augmented generation app with API, featuring Qdrant engine integration for vector storage. Tomaz Bratanic   tutorial  on Multimodal RAG pipeline with LlamaIndex and Neo4j. Sudarshan Koirala  video  tutorial  on using Mistral API with LlamaIndex. Chia Jeng Yang   tutorial  on Technical Considerations for Complex RAG. 🎥  Webinars: Webinar  with Google Developers on advanced RAG applications and multi-modal settings with Google Gemini. Webinar  of Jerry Liu with Louis-François on the Future of AI: LlamaIndex, LLMs, RAG, Prompting, and more. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Jan 2, 2024", "tags": ["Llamaindex", "LLM", "AI", "Newsletter", "Rag"]},
{"title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex", "content": "In this technical walkthrough, we’ll highlight the functionality of Tonic Validate and its integration with LlamaIndex. Sign up for a free account  here  before you start. Introduction As enterprise adoption of generative AI technologies continues, companies are turning to Retrieval Augmented Generation (RAG) systems to extend the application of large-language models (LLMs) to their private data (e.g., a chatbot that can answer questions based on internal technical documentation). Traditionally in software engineering, companies have placed a high emphasis on implementing continuous integration tests to ensure systems remain performant when updates are made. More recently, these same principles have been applied to machine learning models in production. However, as a young technology, RAG currently lacks best practices for integration tests to ensure breaking changes aren’t introduced to the production system. In this article, we will demonstrate how you can use Tonic Validate’s RAG performance monitoring capabilities, LlamaIndex, and GitHub Actions to create novel integration tests that alert you to changes in RAG system performance. To make things easy, Tonic Validate is available natively within LlamaIndex’s core library — you can read more about that  here . What is Tonic Validate? Tonic Validate is a RAG benchmarking and evaluation platform that monitors performance of RAG systems in production. It provides comprehensive metrics for measuring the performance of each component in your RAG system, visualizations for comparing performance across time as the system changes, and workflows for creating benchmark question-answer sets and reviewing LLM responses. Tonic Validate shines a light on how your RAG system is truly performing, enabling continuous performance monitoring of your production RAG systems. You can  learn more  and  sign up for a free account . Setting up LlamaIndex To get started, let’s create an example RAG system for us to test. In this case, LlamaIndex provides a tool called  create-llama  which can generate a full-stack RAG application for us. To install it, we need to make sure we have Node.JS installed and run the following command: npx create-llama@latest This command will take you through a series of prompts. Here are the options to select for each prompt: What is your project named? » llama-validate-demo\nWhich template would you like to use? » Chat without streaming\nWhich framework would you like to use? » FastAPI (Python)\nWould you like to install dependencies automatically? » No\nWhich model would you like to use? » gpt-4–1106-preview\nWhich data source would you like to use? » Use an example PDF\nWould you like to use a vector database? » No, just store the data in the file system Once these options are selected, your project should be created in a folder called  llama-validate-demo . For this demo, we are going to replace the example PDF  create-llama  provides with our own larger dataset. The dataset consists of a collection of essays from Paul Graham’s blog. This should more closely replicate a real world scenario where a company runs RAG on a larger internal dataset. To add the essays, download them from  our Github  and unzip them inside the root folder of your created project. Make sure the unzipped folder is named  data . Be sure to delete any existing files in the folder before copying the new dataset. After you have the essays in the right directory, you can set up your OpenAI API key by setting it as an environment variable called  OPENAI_API_KEY . You can do this either via setting the environment variable system wide or by creating a  .env  file in the root folder of your  create-llama  project. Then you can run the following commands in the root folder for your  create-llama  project: poetry install\npoetry shell\npython app/engine/generate.py This will install the dependencies and generate the RAG embeddings for the Paul Graham essays. After this, you can run the chatbot with: python main.py To test the chatbot, you can send a request via curl: curl - location  'localhost:8000/api/chat'  \\\n - header  'Content-Type: application/json'  \\\n - data  '{ \"messages\": [{ \"role\": \"user\", \"content\": \"In the early days, how were the Airbnb founders financing their startup?\" }] }' LlamaIndex will then return a response: {\n     \"result\" : {\n         \"role\" :  \"assistant\" ,\n         \"content\" :  \"In the early days, the Airbnb founders financed their startup by creating and selling themed breakfast cereals. They created limited-edition cereal boxes, such as \\\"Obama O's\\\" and \\\"Cap'n McCain's,\\\" during the 2008 U.S. presidential election, which became a collectible and helped them raise funds for their company. This creative approach to funding allowed them to sustain the business in its initial phase before securing more traditional forms of investment.\" \n    }\n} Finally, in  llama-validate-demo/app/api/routers/chat.py  we want to replace the  return _Result  line at the end of the chat function with the following. return  _Result(\n    result=_Message(\n        role= MessageRole . ASSISTANT ,\n        content=response.response,\n        context=[x.text  for  x  in  response.source_nodes]\n    )\n) This allows the LlamaIndex API to return the RAG context that was used to answer the question asked. Now, we can move on to setting up Tonic Validate! Setting up Tonic Validate To set up Tonic Validate, first install it via poetry: poetry add tonic-validate Now, we can create our tests for Tonic Validate. To get started, create a file inside  llama-validate-demo/tests  called  validate_test.py . We will also need to create a list of test questions and answers which you can find  here . Alternatively, you can also use the Tonic Validate UI to create the test set and call it via the SDK — we’ll be adding a feature to help generate these benchmarks using synthetic data to make this process even easier. Download the  qa_pairs.json  file from the link and paste it into  llama-validate-demo/tests . Once we have both of these files, we can add the following code into  validate_test.py . import  json\n import  os\n from  tonic_validate  import  ValidateApi\n from  tonic_validate.metrics  import  AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric\n from  llama_index.evaluation  import  TonicValidateEvaluator\n import  requests\n\n from  dotenv  import  load_dotenv\n\nload_dotenv()\n\n def   get_llm_response ( prompt ):\n    url =  \"http://localhost:8000/api/chat\" \n\n    payload = json.dumps({\n         \"messages\" : [\n            {\n                 \"role\" :  \"user\" ,\n                 \"content\" : prompt\n            }\n        ]\n    })\n    headers = {  'Content-Type' :  'application/json'  }\n    response = requests.request( \"POST\" , url, headers=headers, data=payload).json()\n    result = response[ 'result' ]\n     return  result[ 'content' ], result[ 'context' ] This code sets up the dependency imports and also specifies a  get_llm_response  function which sends a request to the LlamaIndex API server we set up earlier to get a response. Now, let’s create a function that gets the list of questions to ask LlamaIndex for our testing. def   get_q_and_a ():\n     # Load qa_pairs.json \n    qa_pairs = json.load( open ( './tests/qa_pairs.json' ))\n     return  ([x[ 'question' ]  for  x  in  qa_pairs], [x[ 'answer' ]  for  x  in  qa_pairs]) This function gets the question-answer pairs from our json file. The questions are what we will ask the RAG system and the answers are the correct answers for those questions. For instance, if the question was “What is the capital of France?” then the answer would be “Paris”. Next, we can add the code that queries LlamaIndex: def   get_responses ( questions ):\n    llm_answers = []\n    context_lists = []\n     for  item  in  questions:\n        llm_answer, llm_context_list = get_llm_response(item)\n        llm_answers.append(llm_answer)\n        context_lists.append(llm_context_list)\n     return  (llm_answers, context_lists) This code iterates over the questions, queries LlamaIndex, and logs each response into an array. We have two arrays. One is the actual answer from LlamaIndex. The other is a list of the snippets of text (called the context list) that LlamaIndex provided to help the LLM answer the question. Now we have a list of LLM responses generated from a list of test questions. Let’s score them: def   score_run ( questions, context_lists, reference_answers, llm_answers ):\n    metrics = [\n        AnswerSimilarityMetric(),\n        RetrievalPrecisionMetric(),\n        AugmentationPrecisionMetric(),\n        AnswerConsistencyMetric()\n    ]\n    scorer = TonicValidateEvaluator(metrics, model_evaluator= \"gpt-4-1106-preview\" )\n    run = scorer.evaluate_run(\n        questions, llm_answers, context_lists, reference_answers\n    )\n     return  run, metrics We first need to define the metrics in Tonic Validate that we want to use. You can find a list of available metrics and their definitions  here . After we create the metrics, we can take advantage of Tonic Validate’s integration with LlamaIndex. Since Tonic Validate is built into LlamaIndex’s evaluation framework as an evaluator, all we need to do is create a  TonicValidateEvaluator , which scores the LlamaIndex responses across the chosen metrics. Then we return the results along with the metrics. Finally, we can create our test function for pytest which evaluates LlamaIndex. def   test_llama_index ():\n    questions, reference_answers = get_q_and_a()\n    llm_answers, context_lists = get_responses(questions)\n    run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)\n     # Upload results to web ui \n    validate_api = ValidateApi()\n     # Get project id from env \n    project_id = os.getenv( \"PROJECT_ID\" )\n    validate_api.upload_run(project_id, run) This runs all the code we’ve written to get the scores and then sends them to Tonic Validate’s API to visualize in the UI. In order to send the metrics for each run to the UI, you need to sign up for a free account, which you can do  here . I highly recommend utilizing the UI to make visualizing and monitoring performance changes a breeze. Once you sign up, you will be taken through a short onboarding process where you create an API key and a project. The API key should be stored in an environment variable called  TONIC_VALIDATE_API_KEY  and the project ID in an environment variable called  PROJECT_ID . Once you have set up your account and configured your environment variables, you can run the test via the following commands: poetry shell\npytest You can also make the test fail if the metrics score too low. This would be a pertinent step to add in if you want to avoid introducing breaking changes to a production RAG system; for example, if you update the model version and the answer similarity score suddenly drop below a certain threshold, you could have the test fail and issue a warning to debug the issue. # Check none of the metrics scored too low     \n for  metric  in  metrics:\n     if  metric.name == AnswerSimilarityMetric.name:\n         assert  run.overall_scores[metric.name] &gt;=  3.5 \n     else :\n         assert  run.overall_scores[metric.name] &gt;=  0.7 Setting up GitHub Actions With LlamaIndex and Tonic Validate configured, we have the ability to connect data to an LLM and measure the accuracy of LLM responses. You can push this setup into production and have a functional chatbot. As is common in modern software development practices, you will likely continue to fix bugs, make improvements, and add new data or features to your RAG system. Before pushing to production, QA testing is in place to catch any changes to your code that may introduce unintended effects. For example, adding a new dataset or updating an LLM to a new version could lead to changes in the quality of responses. One approach, the one that we recommend, for adding QA testing for your RAG system is to use GitHub Actions to establish an integration test using Tonic Validate that checks the LLM response quality of your RAG system, allowing you to catch and rectify any performance degradation before it is pushed into production. To set up Tonic Validate to run in GitHub Actions, we can create a folder  llama-validate-demo/.github/workflows  with a file called  python-app.yml . In this file, we will include the following code configuration that defines the integration test workflow: # This workflow will install Python dependencies and run tests with LlamaIndex\n\nname: Python application\n\non:\n  push:\n    branches: [  \"main\"  ]\n  pull_request:\n    branches: [  \"main\"  ]\n\npermissions:\n  contents: read\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    environment: Actions\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python  3.11 \n      uses: actions/setup-python@v3\n      with:\n        python-version:  \"3.11\" \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install poetry\n        poetry config virtualenvs.create  false \n        poetry install --no-root --no-dev --no-directory\n    - name: Set PYTHONPATH\n      run: echo  \"PYTHONPATH=$GITHUB_WORKSPACE\"  &gt;&gt; $GITHUB_ENV\n    - name: Set up vector index\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n      run: |\n        python app/engine/generate.py\n    - name: Start up test server\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        MODEL: gpt -4 -1106 -preview\n      run: |\n        python main.py &amp;\n        sleep  10 \n    - name: Test with pytest\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        TONIC_VALIDATE_API_KEY: ${{ secrets.TONIC_VALIDATE_API_KEY }}\n        PROJECT_ID: ${{ secrets.PROJECT_ID }}\n      run: |\n        pytest This configures GitHub to run the tests defined with Tonic Validate upon every commit. The GitHub Actions configuration downloads the repo, sets up the dependencies, generates the embeddings, and then starts up the test server and runs the test. After this file is set up, we just need to set our secrets in GitHub. In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret called  OPENAI_API_KEY ,  TONIC_VALIDATE_API_KEY , and  PROJECT_ID . These values will all be the same as the values you set earlier. Now your GitHub actions set up is complete and you can proactively monitor changes to your RAG system during development and before going into production. Try pushing some commits to it and watch it run! To view the results, go to  Tonic Validate’s web app  and navigate to your project. You should see a view like this that shows recent metrics and their evolution over time: Now you and your team can track your RAG system’s performance over time to make sure there aren’t any dips in performance! Thank you for reading and make sure to check out Tonic Validate! For more information on Tonic Validate, visit our  website  and sign up for a  free account  today. You can also visit our GitHub  page  to view all of the code used in this post and the rest of our SDK. Our LlamaIndex integration is available  here .", "author": "Ethan P", "date": "Jan 26, 2024", "tags": ["LLM", "AI", "Evaluation", "Github", "Ci Cd Pipeline"]},
{"title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG", "content": "Introduction Retrieving the appropriate chunks, nodes, or context is a critical aspect of building an efficient Retrieval-Augmented Generation (RAG) application. However, a vector or embedding-based search may not be effective for all types of user queries. To address this,  Hybrid search  combines both keyword-based methods (BM25) and vector (embedding) search techniques. Hybrid search has a specific parameter,  Alpha  to balance the weightage between keyword (BM25) and vector search in retrieving the right context for your RAG application. (alpha=0.0 - keyword search (BM25) and alpha=1.0 - vector search) But here’s where it gets interesting: fine-tuning Alpha isn’t just a task; it’s an art form. Achieving the ideal balance is crucial for unlocking the full potential of hybrid search. This involves adjusting different Alpha values for various types of user queries in your RAG system. In this blog post, we will look into tuning Alpha within the Weaviate vector database using the  Retrieval Evaluation  module of LlamaIndex with and without rerankers with the help of Hit Rate and MRR metrics. Before diving into the implementation, let’s first understand the different query types and metrics we will be using in this article. Different User Query Types: User queries in an RAG application vary based on individual intent. For these diverse query types, it’s essential to fine-tune the  Alpha  parameter. This process involves routing each user query to a specific  Alpha  value for effective retrieval and response synthesis.  Microsoft  has identified various user query categories, and we have selected a few for tuning our hybrid search. Below are the different user query types we considered: Web Search Queries:  Brief queries similar to those typically inputted into search engines. Concept Seeking Queries:  Abstract questions that necessitate detailed, multi-sentence answers. Fact Seeking Queries:  Queries that have a single, definitive answer. Keyword Queries:  Concise queries composed solely of crucial identifier words. Queries With Misspellings:  Queries containing typos, transpositions, and common misspellings. Exact Sub-string Searches:  Queries that exactly match sub-strings from the original context. Let’s look at sample examples in each of these different user query types: Web Search Queries Transfer capabilities of LLaMA language model to non-English languages 2. Concept Seeking Queries What is the dual-encoder architecture used in recent works on dense retrievers? 3. Fact Seeking Queries What is the total number of propositions the English Wikipedia dump is segmented into in FACTOID WIKI? 4. Keyword Queries GTR retriever recall rate 5. Queries With Misspellings What is the advntage of prposition retrieval over sentnce or passage retrieval? 6. Exact Sub-string Searches first kwords for the GTR retriever. Finer-grained Retrieval Evaluation Metrics: We will utilize Hit Rate and MRR metrics for retrieval evaluation. Let’s get into understanding these metrics. Hit Rate: Hit Rate measures the proportion of queries for which the correct chunk/ context appears within the top-k results chunks/ contexts. Put simply, it evaluates how frequently our system correctly identifies the chunk within its top-k chunks. Mean Reciprocal Rank (MRR): MRR assesses a system’s accuracy by taking into account the position of the highest-ranking relevant chunk/ context for each query. It calculates the average of the inverse of these positions across all queries. For instance, if the first relevant chunk/ context is at the top of the list, its reciprocal rank is 1. If it’s the second item, the reciprocal rank becomes 1/2, and this pattern continues accordingly. The remainder of this blog post is divided into two main sections: Implementing  Alpha  Tuning in Hybrid Search for Various Query Types. Analyzing the results of two different document datasets: Indexing a Single Document:  The  LLM Compiler Paper . Indexing Three Documents:  The  LLM Compiler ,  Llama Beyond English , and  Dense X Retrieval  Papers. You can also continue following along in the  Google Colab Notebook  from this point forward. Implementation We will adopt a systematic approach to implement the experimental workflow, which involves the following steps: Data Download. Data Loading. Weaviate Client Setup. Index Creation and Node Insertion. Define LLM (GPT-4) Define CohereAI Reranker. Generation of Synthetic Queries for Various Query Types. Define CustomRetriever. Functions for Retrieval Evaluation and Metrics Calculation. Conducting Retrieval Evaluation for Different Query Types and Alpha Values. Let’s begin by defining some essential functions for our implementation. get_weaviate_client  - sets up weaviate client. load_documents  - load the documents from the file path. create_nodes  - create nodes by chunking the documents using a text splitter. connect_index  - connect to weaviate index. insert_nodes_index  - insert nodes into the index. def   get_weaviate_client ( api_key, url ):\n  auth_config = weaviate.AuthApiKey(api_key=api_key)\n\n  client = weaviate.Client(\n    url=url,\n    auth_client_secret=auth_config\n  )\n   return  client\n\n def   load_documents ( file_path, num_pages= None ):\n   if  num_pages:\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()[:num_pages]\n   else :\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n   return  documents\n\n def   create_nodes ( documents, chunk_size= 512 , chunk_overlap= 0 ):\n  node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n  nodes = node_parser.get_nodes_from_documents(documents)\n   return  nodes\n\n def   connect_index ( weaviate_client ):\n  vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)\n  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n  index = VectorStoreIndex([], storage_context=storage_context)\n   return  index\n\n def   insert_nodes_index ( index, nodes ):\n  index.insert_nodes(nodes) Download Data !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2401.01055.pdf\" -O \"llama_beyond_english.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\" 2.  Load Data # load documents, we will skip references and appendices from the papers.\ndocuments1 = load_documents(\"llm_compiler.pdf\", 12)\ndocuments2 = load_documents(\"dense_x_retrieval.pdf\", 9)\ndocuments3 = load_documents(\"llama_beyond_english.pdf\", 7)\n\n# create nodes\nnodes1 = create_nodes(documents1)\nnodes2 = create_nodes(documents2)\nnodes3 = create_nodes(documents3) 3.  Setup Weaviate Client url = 'cluster URL'\napi_key = 'your api key'\n\nclient = get_weaviate_client(api_key, url) 4.  Create an Index and Insert Nodes. index = connect_index(client)\n\ninsert_nodes_index(index, nodes1) 5.  Define LLM # Deing LLM for query generation\nllm = OpenAI(model='gpt-4', temperature=0.1) 6.  Create Synthetic Queries We will create queries as discussed earlier, check prompts for each of the query types in the notebook, and code for each type of query. Showing code snippet for reference. queries = generate_question_context_pairs(\n    nodes, \n  llm=llm, \n  num_questions_per_chunk= 2 , \n  qa_generate_prompt_tmpl = qa_template\n) 7.  Define reranker reranker = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=4) 8.  Define CustomRetriever We will define  CustomRetriever  class to perform retrieval operations with and without a reranker. class   CustomRetriever ( BaseRetriever ):\n     \"\"\"Custom retriever that performs hybrid search with and without reranker\"\"\" \n\n     def   __init__ ( \n        self,\n        vector_retriever: VectorIndexRetriever,\n        reranker: CohereRerank\n     ) -&gt;  None :\n         \"\"\"Init params.\"\"\" \n\n        self._vector_retriever = vector_retriever\n        self._reranker = reranker\n\n     def   _retrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n         \"\"\"Retrieve nodes given query.\"\"\" \n\n        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n         if  self._reranker !=  None :\n            retrieved_nodes = self._reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n         else :\n            retrieved_nodes = retrieved_nodes[: 4 ]\n\n         return  retrieved_nodes\n\n     async   def   _aretrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n         \"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\" \n         return  self._retrieve(query_bundle)\n\n     async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n         if   isinstance (str_or_query_bundle,  str ):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n         return   await  self._aretrieve(str_or_query_bundle) 9.  Define functions for retriever evaluation and metrics computation We will look into retriever performance for different  alpha  values with and without reranker. # Alpha values and datasets to test \nalpha_values = [ 0.0 ,  0.2 ,  0.4 ,  0.6 ,  0.8 ,  1.0 ]\n\n # Function to evaluate retriever and return results \n async   def   evaluate_retriever ( alpha, dataset, reranker= None ):\n    retriever = VectorIndexRetriever(index,\n                                     vector_store_query_mode= \"hybrid\" ,\n                                     similarity_top_k= 10 ,\n                                     alpha=alpha)\n    custom_retriever = CustomRetriever(retriever,\n                                       reranker)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names([ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever)\n    eval_results =  await  retriever_evaluator.aevaluate_dataset(dataset)\n     return  eval_results\n\n # Function to calculate and store metrics \n def   calculate_metrics ( eval_results ):\n    metric_dicts = []\n     for  eval_result  in  eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[ \"hit_rate\" ].mean()\n    mrr = full_df[ \"mrr\" ].mean()\n     return  hit_rate, mrr 10. Retrieval Evaluation Here we do retrieval evaluation on different query types (datasets) and alpha values to understand which alpha will be suitable for which query type. You need to plug in the reranker accordingly to compute the retrieval evaluation with and without the reranker. # Asynchronous function to loop over datasets and alpha values and evaluate \n async   def   main ():\n    results_df = pd.DataFrame(columns=[ 'Dataset' ,  'Alpha' ,  'Hit Rate' ,  'MRR' ])\n\n     for  dataset  in  datasets_single_document.keys():\n         for  alpha  in  alpha_values:\n            eval_results =  await  evaluate_retriever(alpha, datasets_single_document[dataset])\n            hit_rate, mrr = calculate_metrics(eval_results)\n            new_row = pd.DataFrame({ 'Dataset' : [dataset],  'Alpha' : [alpha],  'Hit Rate' : [hit_rate],  'MRR' : [mrr]})\n            results_df = pd.concat([results_df, new_row], ignore_index= True )\n\n     # Determine the grid size for subplots \n    num_rows =  len (datasets_single_document) //  2  +  len (datasets_single_document) %  2 \n    num_cols =  2 \n\n     # Plotting the results in a grid \n    fig, axes = plt.subplots(num_rows, num_cols, figsize=( 12 , num_rows *  4 ), squeeze= False )   # Ensure axes is always 2D \n\n     for  i, dataset  in   enumerate (datasets_single_document):\n        ax = axes[i // num_cols, i % num_cols]\n        dataset_df = results_df[results_df[ 'Dataset' ] == dataset]\n        ax.plot(dataset_df[ 'Alpha' ], dataset_df[ 'Hit Rate' ], marker= 'o' , label= 'Hit Rate' )\n        ax.plot(dataset_df[ 'Alpha' ], dataset_df[ 'MRR' ], marker= 'o' , linestyle= '--' , label= 'MRR' )\n        ax.set_xlabel( 'Alpha' )\n        ax.set_ylabel( 'Metric Value' )\n        ax.set_title( f' {dataset} ' )\n        ax.legend()\n        ax.grid( True )\n\n     # If the number of datasets is odd, remove the last (empty) subplot \n     if   len (datasets_single_document) % num_cols !=  0 :\n        fig.delaxes(axes[- 1 , - 1 ])   # Remove the last subplot if not needed \n\n     # Adjust layout to prevent overlap \n    plt.tight_layout()\n    plt.show()\n\n # Run the main function \nasyncio.run(main()) Analyze the results: Having completed the implementation phase, we now turn our attention to analyzing the outcomes. We conducted two sets of experiments: one on a single document and another on multiple documents. These experiments varied in alpha values, types of user queries, and the inclusion or exclusion of a reranker. The accompanying graphs display the results, focusing on the Hit Rate and MRR (Mean Reciprocal Rank) as retrieval evaluation metrics. P lease keep in mind that following observations are specific to the datasets used in our study. We encourage you to conduct the experiment with your own documents and draw your relevant observations and conclusions. With Single Document: Without Reranker: With Reranker: With Multiple Documents: Without Reranker: With Reranker: Observations: There is a boost in Hit Rate and MRR in single and multiple documents indexing with the help of a reranker. Time and again it proves using reranker is pretty useful in your RAG application. Though most of the time hybrid search wins over keyword/ vector search, it should be carefully evaluated for different query types based on user queries in the RAG application. The behavior is different when you index a single document and multiple documents, which suggests it’s always better to tune alpha as you add documents into the index. Let’s look at a deeper analysis of different query types: Web Search Queries: — MRR is higher with hybrid search with alpha=0.2/0.6 based on with/ without rerankers irrespective of single/ multiple documents indexing. — The Hit rate is higher with alpha=1.0 for both single/ multiple documents indexing and with/ without rerankers. Concept Seeking Queries: — MRR and Hit Rate are higher with hybrid search (with different alpha values) in Multiple documents indexing. — MRR and Hit Rate are higher at Alpha=0.0 indicating keyword search works better in Single document indexing. Should be noted that MRR has different behavior with and without reranking. Fact Seeking Queries — MRR and Hit Rate are higher with Hybrid search with/ without reranker in Multiple documents indexing. — MRR and Hit Rate are higher with hybrid search with reranker and keyword search (alpha=0.0) is better without reranker in single documents indexing. Keyword Queries — MRR and Hit Rate are higher with Hybrid search with/ without reranker in Multiple documents indexing. — MRR and Hit Rate are higher with hybrid search with reranker and keyword search is better without reranker in single documents indexing. (though MRR is slightly higher with alpha=0.2) Queries With Misspellings — MRR and Hit Rate are higher with Hybrid search with/ without reranker in single and multiple documents indexing. (Though in some cases hybrid search with alpha=1.0 wins). — This also demonstrates that vector search performs better with misspelled queries, as keyword searches lose effectiveness in such cases. Exact Sub-string Searches — MRR and Hit Rate are higher with Keyword search with/ without reranker in Single documents indexing and without reranker in multiple documents indexing. — MRR and Hit Rate are higher with Hybrid search (alpha=0.4) with reranker in multiple documents indexing. What’s Next? In this blog post, we looked into the tuning of Alpha in a hybrid search system for a range of query types. It was interesting to see how the results varied when indexing either a single document or multiple documents. Going forward, you might consider experimenting with documents from diverse domains, employing different query lengths for various query types. Should you come across any noteworthy observations, we encourage you to share them with us in the comments. It would certainly be interesting to discuss these findings with the wider community. References: Hybrid Search Explained Azure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities", "author": "Ravi Theja", "date": "Jan 31, 2024", "tags": ["Llamaindex", "LLM", "OpenAI", "Retrieval Augmented", "AI"]},
{"title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex", "content": "Unlocking the power of AI should be as intuitive as using your favorite apps. That’s the philosophy behind RAGArch, my latest creation designed to demystify and streamline the process of setting up Retrieval-Augmented Generation (RAG) pipelines. This tool is born from a simple vision: to provide a straightforward, no-code platform that empowers both seasoned developers and curious explorers in the world of AI to craft, test, and implement RAG pipelines with confidence and ease. Features RAGArch leverages LlamaIndex’s powerful LLM orchestration capabilities, to provide a seamless experience and granular control over your RAG pipeline. Intuitive Interface:  RAGArch’s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively. Custom Configuration : The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project’s needs. Live Testing:  Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome. One-Click Code Generation : Once you’re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application. Tools and Technologies The creation of RAGArch was made possible by integrating a variety of powerful tools and technologies: UI:  Streamlit Hosting:  Hugging Face Spaces LLMs:  OpenAI GPT 3.5 and 4, Cohere API, Gemini Pro LLM Orchestration:  Llamaindex Embedding Models:  “BAAI/bge-small-en-v1.5”, “WhereIsAI/UAE-Large-V1”, “BAAI/bge-large-en-v1.5”, “khoa-klaytn/bge-small-en-v1.5-angle”, “BAAI/bge-base-en-v1.5”, “llmrails/ember-v1”, “jamesgpt1/sf_model_e5”, “thenlper/gte-large”, “infgrad/stella-base-en-v2” and “thenlper/gte-base” Vector Stores:  Simple (Llamaindex default), Pinecone and Qdrant Deep Dive into the Code The  app.py  script is the backbone of RAGArch, integrating various components to provide a cohesive experience. The following are the key functions of app.py upload_file This function manages file uploads and uses Llamaindex's  SimpleDirectoryReader  to load documents into the system. It supports a wide array of document types, including PDFs, text files, HTML, JSON files, and more, making it versatile for processing diverse data sources. def   upload_file ():\n    file = st.file_uploader( \"Upload a file\" , on_change=reset_pipeline_generated)\n     if  file  is   not   None :\n        file_path = save_uploaded_file(file)\n        \n         if  file_path:\n            loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n             print ( f\"Total documents:  { len (loaded_file)} \" )\n\n            st.success( f\"File uploaded successfully. Total documents loaded:  { len (loaded_file)} \" )\n         return  loaded_file\n     return   None save_uploaded_file This utility function saves the uploaded file to a temporary location on the server, making it accessible for further processing. It’s a crucial part of the file handling process, ensuring data integrity and availability. def  save_uploaded_file (uploaded_file):\n     try :\n         with  tempfile. NamedTemporaryFile ( delete = False , suffix=os. path . splitext (uploaded_file. name )[ 1 ])  as   tmp_file :\n            tmp_file. write (uploaded_file. getvalue ())\n             return  tmp_file. name \n    except  Exception   as   e :\n        st. error (f \"Error saving file: {e}\" )\n         return   None select_llm Allows users to select a Large Language Model and initializes it for use. You can choose from Google’s Gemini Pro, Cohere, OpenAI’s GPT 3.5 and GPT 4. def   select_llm ():\n    st.header( \"Choose LLM\" )\n    llm_choice = st.selectbox( \"Select LLM\" , [ \"Gemini\" ,  \"Cohere\" ,  \"GPT-3.5\" ,  \"GPT-4\" ], on_change=reset_pipeline_generated)\n    \n     if  llm_choice ==  \"GPT-3.5\" :\n        llm = OpenAI(temperature= 0.1 , model= \"gpt-3.5-turbo-1106\" )\n        st.write( f\" {llm_choice}  selected\" )\n     elif  llm_choice ==  \"GPT-4\" :\n        llm = OpenAI(temperature= 0.1 , model= \"gpt-4-1106-preview\" )\n        st.write( f\" {llm_choice}  selected\" )\n     elif  llm_choice ==  \"Gemini\" :\n        llm = Gemini(model= \"models/gemini-pro\" )\n        st.write( f\" {llm_choice}  selected\" )\n     elif  llm_choice ==  \"Cohere\" :\n        llm = Cohere(model= \"command\" , api_key=os.environ[ 'COHERE_API_TOKEN' ])\n        st.write( f\" {llm_choice}  selected\" )\n     return  llm, llm_choice select_embedding_model Offers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face’s MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models. def  select_embedding_model ():\n    st. header ( \"Choose Embedding Model\" )\n    col1, col2 = st. columns ([ 2 , 1 ])\n     with   col2 :\n        st. markdown ( \"\" \"\n                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                    \" \"\" )\n    model_names = [\n         \"BAAI/bge-small-en-v1.5\" ,\n         \"WhereIsAI/UAE-Large-V1\" ,\n         \"BAAI/bge-large-en-v1.5\" ,\n         \"khoa-klaytn/bge-small-en-v1.5-angle\" ,\n         \"BAAI/bge-base-en-v1.5\" ,\n         \"llmrails/ember-v1\" ,\n         \"jamesgpt1/sf_model_e5\" ,\n         \"thenlper/gte-large\" ,\n         \"infgrad/stella-base-en-v2\" ,\n         \"thenlper/gte-base\" \n    ]\n    selected_model = st. selectbox ( \"Select Embedding Model\" , model_names,  on_change=reset_pipeline_generated)\n     with  st. spinner ( \"Please wait\" )  as   status :\n        embed_model =  HuggingFaceEmbedding (model_name=selected_model)\n        st. session_state [ 'embed_model' ] = embed_model\n        st. markdown (F \"Embedding Model: {embed_model.model_name}\" )\n        st. markdown (F \"Embed Batch Size: {embed_model.embed_batch_size}\" )\n        st. markdown (F \"Embed Batch Size: {embed_model.max_length}\" )\n\n\n     return  embed_model, selected_model select_node_parser Function This function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. I have included some of the most commonly used node parsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser and MarkdownNodeParser. def   select_node_parser ():\n    st.header( \"Choose Node Parser\" )\n    col1, col2 = st.columns([ 4 , 1 ])\n     with  col2:\n        st.markdown( \"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                    \"\"\" )\n    parser_types = [ \"SentenceSplitter\" ,  \"CodeSplitter\" ,  \"SemanticSplitterNodeParser\" ,\n                     \"TokenTextSplitter\" ,  \"HTMLNodeParser\" ,  \"JSONNodeParser\" ,  \"MarkdownNodeParser\" ]\n    parser_type = st.selectbox( \"Select Node Parser\" , parser_types, on_change=reset_pipeline_generated)\n    \n    parser_params = {}\n     if  parser_type ==  \"HTMLNodeParser\" :\n        tags = st.text_input( \"Enter tags separated by commas\" ,  \"p, h1\" )\n        tag_list = tags.split( ',' )\n        parser = HTMLNodeParser(tags=tag_list)\n        parser_params = { 'tags' : tag_list}\n        \n     elif  parser_type ==  \"JSONNodeParser\" :\n        parser = JSONNodeParser()\n        \n     elif  parser_type ==  \"MarkdownNodeParser\" :\n        parser = MarkdownNodeParser()\n        \n     elif  parser_type ==  \"CodeSplitter\" :\n        language = st.text_input( \"Language\" ,  \"python\" )\n        chunk_lines = st.number_input( \"Chunk Lines\" , min_value= 1 , value= 40 )\n        chunk_lines_overlap = st.number_input( \"Chunk Lines Overlap\" , min_value= 0 , value= 15 )\n        max_chars = st.number_input( \"Max Chars\" , min_value= 1 , value= 1500 )\n        parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n        parser_params = { 'language' : language,  'chunk_lines' : chunk_lines,  'chunk_lines_overlap' : chunk_lines_overlap,  'max_chars' : max_chars}\n        \n     elif  parser_type ==  \"SentenceSplitter\" :\n        chunk_size = st.number_input( \"Chunk Size\" , min_value= 1 , value= 1024 )\n        chunk_overlap = st.number_input( \"Chunk Overlap\" , min_value= 0 , value= 20 )\n        parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = { 'chunk_size' : chunk_size,  'chunk_overlap' : chunk_overlap}\n        \n     elif  parser_type ==  \"SemanticSplitterNodeParser\" :\n         if   'embed_model'   not   in  st.session_state:\n            st.warning( \"Please select an embedding model first.\" )\n             return   None ,  None \n        \n        embed_model = st.session_state[ 'embed_model' ]\n        buffer_size = st.number_input( \"Buffer Size\" , min_value= 1 , value= 1 )\n        breakpoint_percentile_threshold = st.number_input( \"Breakpoint Percentile Threshold\" , min_value= 0 , max_value= 100 , value= 95 )\n        parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n        parser_params = { 'buffer_size' : buffer_size,  'breakpoint_percentile_threshold' : breakpoint_percentile_threshold}\n        \n     elif  parser_type ==  \"TokenTextSplitter\" :\n        chunk_size = st.number_input( \"Chunk Size\" , min_value= 1 , value= 1024 )\n        chunk_overlap = st.number_input( \"Chunk Overlap\" , min_value= 0 , value= 20 )\n        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = { 'chunk_size' : chunk_size,  'chunk_overlap' : chunk_overlap}\n\n     # Save the parser type and parameters to the session state \n    st.session_state[ 'node_parser_type' ] = parser_type\n    st.session_state[ 'node_parser_params' ] = parser_params\n    \n     return  parser, parser_type Below the node parser selection, I have also included a preview of the first node of the text after splitting/parsing, just to give the users an idea of how the chunking is actually happening based the selected node parser and the relevant parameters. select_response_synthesis_method This function allows users to choose how the RAG pipeline synthesizes responses. I have included varioud response synthesis methods supported by Llamaindex including  refine ,  tree_summarize ,  compact ,  simple_summarize ,  accumulate  and  compact_accumulate. Users can click on the more information link to get more details about response synthesis and the different types. def   select_response_synthesis_method ():\n    st.header( \"Choose Response Synthesis Method\" )\n    col1, col2 = st.columns([ 4 , 1 ])\n     with  col2:\n        st.markdown( \"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                    \"\"\" )\n    response_modes = [\n         \"refine\" ,\n         \"tree_summarize\" ,  \n         \"compact\" , \n         \"simple_summarize\" , \n         \"accumulate\" , \n         \"compact_accumulate\" \n    ]\n    selected_mode = st.selectbox( \"Select Response Mode\" , response_modes, on_change=reset_pipeline_generated)\n    response_mode = selected_mode\n     return  response_mode, selected_mode select_vector_store Enables users to choose a vector store, which is a critical component for storing and retrieving embeddings in the RAG pipeline. This function supports the selection from multiple vector store options including Simple (Llamaindex default), Pinecone and Qdrant. def   select_vector_store ():\n    st.header( \"Choose Vector Store\" )\n    vector_stores = [ \"Simple\" ,  \"Pinecone\" ,  \"Qdrant\" ]\n    selected_store = st.selectbox( \"Select Vector Store\" , vector_stores, on_change=reset_pipeline_generated)\n\n    vector_store =  None \n     if  selected_store ==  \"Pinecone\" :\n        pc = Pinecone(api_key=os.environ[ 'PINECONE_API_KEY' ])\n        index = pc.Index( \"test\" )\n        vector_store = PineconeVectorStore(pinecone_index=index)\n     elif  selected_store ==  \"Qdrant\" :\n        client = qdrant_client.QdrantClient(location= \":memory:\" )\n        vector_store = QdrantVectorStore(client=client, collection_name= \"sampledata\" )\n    st.write(selected_store)\n     return  vector_store, selected_store generate_rag_pipeline Function This core function ties together the selected components to generate a RAG pipeline. It initializes the pipeline with the chosen LLM, embedding model, node parser, response synthesis method, and vector store. It is triggered by pressing the ‘Generate RAG Pipeline’ button. def   generate_rag_pipeline ( file, llm, embed_model, node_parser, response_mode, vector_store ):\n     if  vector_store  is   not   None :\n         # Set storage context if vector_store is not None \n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n     else :\n        storage_context =  None \n\n     # Create the service context \n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n\n     # Create the vector index \n    vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress= True )\n     if  storage_context:\n        vector_index.storage_context.persist(persist_dir= \"persist_dir\" )\n\n     # Create the query engine \n    query_engine = vector_index.as_query_engine(\n        response_mode=response_mode,\n        verbose= True ,\n    )\n\n     return  query_engine generate_code_snippet Function This function is the culmination of the user’s selections, generating the Python code necessary to implement the configured RAG pipeline. It dynamically constructs the code snippet based on the chosen LLM, embedding model, node parser, response synthesis method, and vector store, including the parameters set for the node parser. def   generate_code_snippet ( llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice ):\n    node_parser_params = st.session_state.get( 'node_parser_params' , {})\n     print (node_parser_params)\n    code_snippet =  \"from llama_index.llms import OpenAI, Gemini, Cohere\\n\" \n    code_snippet +=  \"from llama_index.embeddings import HuggingFaceEmbedding\\n\" \n    code_snippet +=  \"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n\" \n    code_snippet +=  \"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n\" \n    code_snippet +=  \"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n\" \n    code_snippet +=  \"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\\n\" \n    code_snippet +=  \"import qdrant_client\\n\\n\" \n\n     # LLM initialization \n     if  llm_choice ==  \"GPT-3.5\" :\n        code_snippet +=  \"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\\n\" \n     elif  llm_choice ==  \"GPT-4\" :\n        code_snippet +=  \"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\\n\" \n     elif  llm_choice ==  \"Gemini\" :\n        code_snippet +=  \"llm = Gemini(model='models/gemini-pro')\\n\" \n     elif  llm_choice ==  \"Cohere\" :\n        code_snippet +=  \"llm = Cohere(model='command', api_key='&lt;YOUR_API_KEY&gt;')  # Replace &lt;YOUR_API_KEY&gt; with your actual API key\\n\" \n\n     # Embedding model initialization \n    code_snippet +=  f\"embed_model = HuggingFaceEmbedding(model_name=' {embed_model_choice} ')\\n\\n\" \n\n     # Node parser initialization \n    node_parsers = {\n         \"SentenceSplitter\" :  f\"SentenceSplitter(chunk_size= {node_parser_params.get( 'chunk_size' ,  1024 )} , chunk_overlap= {node_parser_params.get( 'chunk_overlap' ,  20 )} )\" ,\n         \"CodeSplitter\" :  f\"CodeSplitter(language= {node_parser_params.get( 'language' ,  'python' )} , chunk_lines= {node_parser_params.get( 'chunk_lines' ,  40 )} , chunk_lines_overlap= {node_parser_params.get( 'chunk_lines_overlap' ,  15 )} , max_chars= {node_parser_params.get( 'max_chars' ,  1500 )} )\" ,\n         \"SemanticSplitterNodeParser\" :  f\"SemanticSplitterNodeParser(buffer_size= {node_parser_params.get( 'buffer_size' ,  1 )} , breakpoint_percentile_threshold= {node_parser_params.get( 'breakpoint_percentile_threshold' ,  95 )} , embed_model=embed_model)\" ,\n         \"TokenTextSplitter\" :  f\"TokenTextSplitter(chunk_size= {node_parser_params.get( 'chunk_size' ,  1024 )} , chunk_overlap= {node_parser_params.get( 'chunk_overlap' ,  20 )} )\" ,\n         \"HTMLNodeParser\" :  f\"HTMLNodeParser(tags= {node_parser_params.get( 'tags' , [ 'p' ,  'h1' ])} )\" ,  \n         \"JSONNodeParser\" :  \"JSONNodeParser()\" ,\n         \"MarkdownNodeParser\" :  \"MarkdownNodeParser()\" \n    }\n    code_snippet +=  f\"node_parser =  {node_parsers[node_parser_choice]} \\n\\n\" \n\n     # Response mode \n    code_snippet +=  f\"response_mode = ' {response_mode} '\\n\\n\" \n\n     # Vector store initialization \n     if  vector_store_choice ==  \"Pinecone\" :\n        code_snippet +=  \"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\" \n        code_snippet +=  \"index = pc.Index('test')\\n\" \n        code_snippet +=  \"vector_store = PineconeVectorStore(pinecone_index=index)\\n\" \n     elif  vector_store_choice ==  \"Qdrant\" :\n        code_snippet +=  \"client = qdrant_client.QdrantClient(location=':memory:')\\n\" \n        code_snippet +=  \"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\\n\" \n     elif  vector_store_choice ==  \"Simple\" :\n        code_snippet +=  \"vector_store = None  # Simple in-memory vector store selected\\n\" \n\n    code_snippet +=  \"\\n# Finalizing the RAG pipeline setup\\n\" \n    code_snippet +=  \"if vector_store is not None:\\n\" \n    code_snippet +=  \"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\" \n    code_snippet +=  \"else:\\n\" \n    code_snippet +=  \"    storage_context = None\\n\\n\" \n\n    code_snippet +=  \"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n\" \n\n    code_snippet +=  \"_file = 'path_to_your_file'  # Replace with the path to your file\\n\" \n    code_snippet +=  \"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\" \n    code_snippet +=  \"if storage_context:\\n\" \n    code_snippet +=  \"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\" \n\n    code_snippet +=  \"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\" \n\n     return  code_snippet Conclusion RAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It’s designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation. Your insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let’s start a conversation on Linkedin. I’m always eager to collaborate and share knowledge with fellow tech adventurers. GitHub Repo Connect with Me on LinkedIn Live Demo", "author": "Harshad Suryawanshi", "date": "Feb 2, 2024", "tags": ["Rag", "No Code", "Llamaindex", "OpenAI", "Code Generation"]},
{"title": "LlamaIndex Newsletter 2024–02–06", "content": "Hello, LlamaIndex Explorers 🦙, Step into a week full of exciting updates at LlamaIndex! Our community’s vibrant contributions and extensive educational resources are here to amplify your LlamaIndex exploration. Before diving into the updates, we have an exciting announcement: We’ve launched a  $2,000 bounty program with Replit . This initiative invites open-source contributors to create projects and templates focused on advanced RAG with LlamaIndex, from building RAG across thousands of documents to implementing cutting-edge RAG research and crafting advanced templates. We’re inspired by your creativity! If you have a project, article, or video you’re excited about, we’re eager to see it. Send your amazing work to  news@llamaindex.ai . If you haven’t subscribed to our newsletter yet, don’t miss out. Visit our  website  and subscribe today to get all the newest updates from LlamaIndex straight to your inbox. 🤩  The highlights: Ollama Multimodal Integration Launch:  Introduced day-1 integration with Ollama Multi-Modal for developing local multimodal applications, including image extraction, multimodal RAG, and captioning.  Notebook,   Tweet create-llama   Enhanced RAG : Updated create-llama for improved website content crawling and the creation of comprehensive RAG applications.  Tweet . Nomic Embedding :  Guide  to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex. ✨ Feature Releases and Enhancements: We introduced day-1 integration with the Ollama Multi-Modal release enabling the creation of local multimodal applications on MacBook, including structured image extraction, multimodal RAG, and image captioning.  Notebook,   Tweet We have updated create-llama on crawling a website’s content, and create a full-stack RAG application based on the data.  Tweet . 🗺️ Guides: Guide  to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex. 🎥 Demo: LlamaBot:  Rohan  developed an open-source Discord bot that listens to, remembers, and answers questions across servers, was inspired by a similar bot for Slack and developed using LlamaIndex, Gemini Pro, and Qdrant Engine.  GitHub Repository ,  Tweet . ✍️ Tutorials: Wenqi Glantz   tutorial  on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI. Ravi Theja   tutorial  on Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG. Wenqi Glantz   tutorial  on 12 RAG Pain Points and Proposed Solutions. Cobus Reyling   tutorial  on Agentic RAG With LlamaIndex. ChristopherGS   tutorial  on Retrieval Augmented Generation (RAG) with Llama Index and Open-Source Models. Andrei  workshop tutorial on Evaluation of Multimodal RAG Systems using the LlamaIndex. Tutorial  on Building RAG application with Pinecone and LlamaIndex. Sudalai Rajkumar   tutorial  on RAG — Encoder and Reranker evaluation. Harshad Suryawanshi   tutorial  on RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex. Otmane Boughaba’s   tutorial  on Building a Local RAG API with LlamaIndex, Qdrant, Ollama, and FastAPI. Iulia Brezeanu   tutorial  on How to Find the Best Multilingual Embedding Model for Your RAG. 🎥 Events Jerry Liu   keynote  on Beyond Naive Rag: Adding Agentic Layers. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Feb 6, 2024", "tags": ["Llamaindex", "Newsletter", "LLM", "Rag", "Replit"]},
{"title": "How to build LLM Agents in TypeScript with LlamaIndex.TS", "content": "Agents are autonomous systems that can execute end-to-end tasks without much or fewer instructions. These agents are capable of solving tasks related to questions and answering, using tools to achieve a desired behavior, or even planning tasks. In this article, we will explore a few capabilities of LlamaIndex.TS’s built-in agents to achieve a set of goals. We have two parts: Building a simple agent for calculation. Using agents with your personal/private data to answer questions. All the full code examples will be at the end of the post. Setup To start you need to have  llamaindex  package installed on your node environment setup and an OpenAI key. To install the package: npm install llamaindex  To set up the OpenAI key you can set your environment variable: export OPENAI_API_KEY=sk-*************** Agent for calculations The first agent will be responsible for getting a user input of numbers and based on this selecting the right tools to achieve the task Import the classes: import { FunctionTool, OpenAIAgent } from \"llamaindex\"; Function Tool The first step will be creating tools that the agent will have access to, we will be creating an  add  function and a  multiply  function. OpenAI provides us a function calling API which we can send our function arguments and get back a response We start by creating two functions: // Define  a  function  to  sum two numbers\nfunction sum({  a ,  b  }: {  a : number;  b : number }): number {\n  return  a  +  b ;\n}\n\n// Define  a  function  to  multiply two numbers\nfunction multiply({  a ,  b  }: {  a : number;  b : number }): number {\n  return  a  *  b ;\n} Now we can set up the  FunctionTool  class which will be given to the agent, this class requires the properties of the function and metadata for the tool, helping the Large Language Models (LLMs) to identify which tool the LLM should use and the parameters. // Sum properties to give to the LLM \n const  sumJSON = {\n   type :  \"object\" ,\n   properties : {\n     a : {\n       type :  \"number\" ,\n       description :  \"The first number\" ,\n    },\n     b : {\n       type :  \"number\" ,\n       description :  \"The second number\" ,\n    },\n  },\n   required : [ \"a\" ,  \"b\" ],\n};\n\n // Multiply properties to give to the LLM \n const  multiplyJSON = {\n   type :  \"object\" ,\n   properties : {\n     a : {\n       type :  \"number\" ,\n       description :  \"The number to multiply\" ,\n    },\n     b : {\n       type :  \"number\" ,\n       description :  \"The multiplier\" ,\n    },\n  },\n   required : [ \"a\" ,  \"b\" ],\n};\n\n // Create sum function tool \n const  sumFunctionTool =  new   FunctionTool (sum, {\n   name :  \"sum\" ,\n   description :  \"Use this function to sum two numbers\" ,\n   parameters : sumJSON,\n});\n\n // Creat multiply function tool \n const  multiplyFunctionTool =  new   FunctionTool (multiply, {\n   name :  \"multiply\" ,\n   description :  \"Use this function to multiply two numbers\" ,\n   parameters : multiplyJSON,\n}); Chat with Agent Now we have the tools to give to the agent, we can set up the agent: // Setup the agent with the respective tools \n const  agent =  new   OpenAIAgent ({\n   tools : [sumFunctionTool, multiplyFunctionTool],\n   verbose :  true ,\n}); And then ask a question: // Chat with LLM \n const  response =  await  agent. chat ({\n   message :  \"How much is 5 + 5? then multiply by 2\" ,\n});\n\n // Agent output \n console . log ( String (response)); Now the agent will choose the right tools to achieve the desired task using the functions provided by you. Then you should see an output as: ===  Calling   Function  ===\n Calling   function : sum  with   args : {\n   \"a\" :  5 ,\n   \"b\" :  5 \n}\n Got  output  10 \n==========================\n===  Calling   Function  ===\n Calling   function : multiply  with   args : {\n   \"a\" :  10 ,\n   \"b\" :  2 \n}\n Got  output  20 \n==========================\n The  result  of  adding  5  and  5  is  10.   When  you multiply  10  by  2 , the result is  20. Using Agents with your documents The second agent will be responsible for going through a set of Dan Abramov essays and answering questions based on the available data. Firstly, we will import the necessary classes and functions Import the classes and functions import {\n  OpenAIAgent,\n  SimpleDirectoryReader,\n  VectorStoreIndex,\n  SummaryIndex,\n  QueryEngineTool,\n} from \"llamaindex\"; Loading Documents Now we will load the documents and insert them into a local vector store index which will be responsible for storing the documents and allowing the agent to query the most relevant data for the task and a summarize vector index which can better help on tasks summary related. // Load the documents \n const  documents =  await   new   SimpleDirectoryReader (). loadData ({\n   directoryPath :  \"node_modules/llamaindex/examples\" ,\n});\n\n // Create a vector index from the documents \n const  vectorIndex =  await   VectorStoreIndex . fromDocuments (documents);\n const  summaryIndex =  await   SummaryIndex . fromDocuments (documents) Creating the Query Engine Tool Now we will create the tooling that allows the Agents to access the Vector Index: // Create a query engine from the vector index \n const  abramovQueryEngine = vectorIndex. asQueryEngine ();\n const  abramovSummaryEngine = summaryIndex. asQueryEngine ();\n\n // Create a QueryEngineTool with the vector engine \n const  vectorEngineTool =  new   QueryEngineTool ({\n   queryEngine : abramovQueryEngine,\n   metadata : {\n     name :  \"abramov_query_engine\" ,\n     description :  \"Use this engine to answer specific questions about Abramov\" ,\n  },\n});\n\n // Create a QueryEngineTool with the summary engine \n const  summaryEngineTool =  new   QueryEngineTool ({\n   queryEngine : abramovSummaryEngine,\n   metadata : {\n     name :  \"abramov_summary_engine\" ,\n     description :  \"Use this engine to generate summaries about Abramov\" ,\n  },\n}); Creating the OpenAI Agent Now we can create and provide the necessary tools for the agent // Setup the agent  \n const  agent =  new   OpenAIAgent ({\n   tools : [vectorEngineTool, summaryEngineTool],\n   verbose :  true ,\n}); Chat with Agent Now you can chat with the agent about Dan Abramov and it will select the right tools to achieve the goal. \n // Chat with Agent \n const  response =  await  agent. chat ({\n   message :  \"Where he worked in his 20s?\" ,\n});\n\n // Log output \n console . log ( String (response)); The tool output === Calling Function ===\nCalling  function :  abramov_query_engine   with   args :  {\n   \"query\" :  \"work experience in 20s\" \n}\nGot output The individual had their first job  as  a software developer in their  20 s. They worked  for  a Russian-American outsourcing company  and  their salary was $ 18 k/year.\n==========================\nIn his  20 s, Abramov worked  as  a software developer  for  a Russian-American outsourcing company. His salary during that time was $ 18 , 000  per year. Conclusion Autonomous agents are powerful when talking about creating workflows and automation that can take your business to the next level or make your life easier. There is still a long way to go before they are fully autonomous but the arrival of LLMs is allowing the first steps of these reasoning and decision-making engines Do you already use agents in your day-to-day? and wanna discuss agents or help with agents? Reach me on  Twitter References Code example:  https://github.com/EmanuelCampos/agents-typescript-example LLamaIndexTS Documentation:  https://ts.llamaindex.ai/modules/agent/openai https://www.llamaindex.ai/", "author": "Emanuel Ferreira", "date": "Feb 8, 2024", "tags": ["AI", "Retrieval Augmented", "Agents", "Machine Learning"]},
{"title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning", "content": "ADU Planner in action In the midst of a pressing housing shortage, the American dream of homeownership is being reimagined through the innovative concept of Accessory Dwelling Units (ADUs). These compact, efficient homes, nestled in the backyards of existing properties, are more than just a trend — they’re a revolution in modular and prefabricated living solutions. In 2022, the ADU market, combined with modular and prefabricated houses, soared to an impressive $150 billion globally, with projections indicating a leap to $300 billion by 2032 ( link ). Yet, the journey to erecting an ADU has been far from simple. Traditionally, it involves extensive land surveys and numerous consultations with vendors to sift through an array of floor plans. Recognizing the need for a more efficient pathway, our team has crafted an innovative solution: a GenAI-powered ADU planning system. This cutting-edge tool deftly navigates through the maze of city building codes, effortlessly connecting users with local vendors and presenting viable floor plan options — all within our user-friendly app. Our mission is to streamline your ADU creation process, making it less daunting and more accessible. Imagine the potential when the complexities of planning are reduced to a few clicks — this is the power of GenAI at your fingertips. Award-Winning Innovation: ADU Planner Takes the Lead We’re thrilled to announce a milestone achievement: Our “ADU Planner” project clinched the  top honor  at the  LlamaIndex RAG Hackathon , held in the first weekend of February 2024. This victory underscores our commitment to transforming the ADU space with our pioneering technology. Discover the features that set our project apart and explore the full scope of our innovative planner through the following resources: ·  Dive into the Details : Visit our  Devpost project page  for an in-depth look. ·  Explore Our Code : For the tech enthusiasts, our codebase is available on  GitHub . We’re just getting started, and this recognition only fuels our drive to innovate and deliver solutions that matter. Watch Our Demo \n\n \n\n Technologies At the heart of our “ADU Planner” lies a synergy of cutting-edge technology and user-friendly design. The seamless interface is crafted with React, providing an intuitive frontend experience, while our robust Flask backend is the powerhouse of functionality, energized by the latest AI from GPT-3.5/4V and augmented by the precision of the LlamaIndex PDF parser and the versatility of Google Maps. Here’s how our workflow unfolds: 1.  Start with Simplicity:  Users begin by entering their address into our frontend. This action triggers our Google geocoding-powered backend to spring into action, capturing a detailed satellite image of the property in question. 2.  Deciphering the Details:  Next, the journey bifurcates into a dual analysis mode. One path involves parsing through local building codes, a task adeptly handled by GPT-3.5, to unearth specific ADU regulations such as minimum and maximum floor area constraints. Concurrently, GPT-4V meticulously scans the satellite imagery to pinpoint any potential obstructions and delineate the areas ripe for ADU development. 3.  Bringing Plans to Life:  With the viable regions marked, our system embarks on a virtual quest, scouring local builders’ websites for ADU floor plans that not only fit the legal criteria but also the physical realities of your property. These plans are then vividly rendered within the identified buildable zones. 4.  A Click to the Future:  Engaging with a chosen floor plan is as simple as a click, transporting users to the builder’s domain where immersive 3D renderings, pricing details, and more await. To cap it off, our tool thoughtfully prepares data exports to facilitate those all-important initial discussions with ADU builders. This is more than a tool — it’s a gateway to turning your backyard into a space of possibility. Flowchart Flowchart Code Samples Querying PDFs In the following example, we’ll query local ADU building codes for Saratoga, CA. Once you’ve setup your LlamaIndex and OpenAI API keys, you’ll be able to parse the PDFs as follows: from  llama_parse  import  LlamaParse\nparser = LlamaParse(result_type= 'markdown' )\ndocs =  sum ([\n  parser.load_data(file)  for  file  in  [\n     'Documents/ADU_FAQ.pdf' ,\n     'Documents/ADU_Handbook.pdf' ,\n     'Documents/SCC-ADU-Guidebook-FINAL-9.8.23.pdf' ]], []) Then, we can index the documents into an ephemeral, or in-memory,  Chroma DB  embeddings store. chroma_client = chromadb.EphemeralClient()\nchroma_collection = chroma_client.get_or_create_collection('embeddings')\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(docs, storage_context=storage_context) Next, we can integrate the documents into our agentic flow by creating a tool for our agent to query them. tools = [\n  QueryEngineTool(\n       query_engine=index.as_query_engine(),\n       metadata=ToolMetadata(\n           name='saratoga_adu_codes',\n           description=('Provides information about ADU building codes for the city of Saratoga, CA.'),\n       ),\n   )\n] All together, our agent will be able to answer questions about the documents as follows: from  llama_index.agent  import  OpenAIAgent\n\nagent = OpenAIAgent.from_tools(tools, verbose= True )\nagent.chat( \"\"\"\\\nAnswer the following questions regarding Accessory Dwelling Unit (ADU) construction planning in Saratoga, California (CA):\n- What are the typical side and rear setbacks for a detached ADU?\n\"\"\" )\n\n\n&gt;&gt;&gt;  'For detached ADUs in Saratoga, California, the typical side and rear setbacks are a minimum of four feet from the lot lines.' Image analysis Firstly, we’ll prepare an image and plot it for visual recognization. import  os\n from  pathlib  import  Path\n from  PIL  import  Image\n import  matplotlib.pyplot  as  plt\n\ninput_image_path = Path( \"input_images\" )\n if   not  input_image_path.exists():\n    Path.mkdir(input_image_path)\n\nimage_extensions = [ '.jpg' ,  '.jpeg' ,  '.png' ,  '.gif' ,  '.bmp' ]\n\n # Filter out non-image files \nimage_paths = [ str (input_image_path / img_path)  for  img_path  in  os.listdir(input_image_path) \n                if   any (img_path.lower().endswith(ext)  for  ext  in  image_extensions)]\n\n def   plot_images ( image_paths ):\n    images_shown =  0 \n    plt.figure(figsize=( 16 ,  9 ))\n     for  img_path  in  image_paths:\n         if  os.path.isfile(img_path):\n            image = Image. open (img_path)\n\n            plt.subplot( 2 ,  3 , images_shown +  1 )\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown +=  1 \n             if  images_shown &gt;=  6 :   # Adjusted to match the subplot dimensions (2,3) \n                 break \n\nplot_images(image_paths) Secondly, we can do a image analysis and recognize the property layout based on GPT-4V, using the following prompt sample as an example. instruction = '''\nYou are an intelligent Accessory Dwelling Units (ADU) architect designer and contractor.\nPlease analyze the image, describle the layout, and then describe the pool, property, and driveways in the format of:\n1. property: xxx\n2. pool: xxx\n...\n''' from  llama_index.multi_modal_llms.openai  import  OpenAIMultiModal\n from  llama_index  import  SimpleDirectoryReader\n\n # put your local directore here \nimage_documents = SimpleDirectoryReader( \"./input_images\" ).load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model= \"gpt-4-vision-preview\" , api_key=OPENAI_API_TOKEN, max_new_tokens= 1500 , temperature =  0.0 \n)\n\nresponse = openai_mm_llm.complete(\n    prompt = instruction,\n    image_documents=image_documents,\n)\n\n print (response) Based on the aerial image provided, here is an analysis of the layout:\n\n 1.  Property: The property appears to be a residential lot with a single-family home. The house has a hipped roof with multiple sections, indicating a complex floor plan with possibly several rooms  or  wings. There is a landscaped area surrounding the house with various trees  and  shrubs,  and  the terrain seems to be sloped, as indicated by the terracing on the land.\n\n 2.  Pool: There is a kidney-shaped pool located to the northwest of the main house. It is surrounded by a paved area, likely  for  lounging  and  poolside activities,  and  is accessible via a curved pathway that leads from the house to the pool area.\n\n 3.  Driveways: It is  not  entirely clear from the image where the driveway is located, as the specific access point to the property is  not  visible. However, there seems to be a paved path leading from the bottom right of the image towards the house, which could be the driveway  or  a walkway. If it is the driveway, it would be located on the southeast side of the property, leading up to the house.\n\nPlease note that without a broader view  or  additional context, some details about the property, such as the exact location of the driveway  or  additional structures, may  not  be accurately determined. Conclusions Our application markedly simplifies the ADU construction process for homeowners. It adeptly navigates complex tasks such as land surveying and identifying viable floor plans. While ADU vendors need to validate the chosen floor plan and cities must grant construction approval, our tool propels this progression by facilitating comprehensive at-home analysis. Looking ahead, we are poised to officially launch this application and forge partnerships with reputable ADU vendors, enhancing oversight in modular and prefabricated home construction. We are committed to personalizing our service further by adapting our recommendations to align with individual budget constraints and specific layout preferences, ensuring that our users’ visions for their homes are realized with precision and care.", "author": "Rujun Gao", "date": "Feb 12, 2024", "tags": []},
{"title": "LlamaIndex v0.10", "content": "Today we’re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date ( see this gargantuan PR ), and it takes a massive step towards making LlamaIndex a next-generation,  production-ready data framework  for your LLM applications. LlamaIndex v0.10 contains some major updates: We have created a  llama-index-core  package, and split all integrations and templates into separate packages:  Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still use from llama_index.llms.openai import OpenAI  for a LLM. LlamaHub  will be the central hub for all integrations:  the former  llama-hub  repo itself is consolidated into the main  llama_index  repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned! ServiceContext is deprecated:  Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default. Upgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we’ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10. Check out the below sections for more details, and go to the very last section for resource links to everything. Splitting into `llama-index-core` and integration packages The first and biggest change we’ve made is a massive packaging refactor. LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loaders 35+ agent tools 50+ LlamaPack templates 50+ LLMs 25+ embeddings 40+ vector stores and more across the  llama_index  and  llama-hub  repos. The rapid growth of our ecosystem has been awesome to see, but it’s also come with growing pains: Many of the integrations lack proper tests Users are responsible for figuring out dependencies If an integration updates, users will have to update their entire  llama-index  Python package. In response to this, we’ve done the following. Created  llama-index-core  :  This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations. Created separate packages for all integrations/templates:  Every integration is now available as a separate package. This includes  all  integrations, including those on LlamaHub! See  our Notion registry  page for a full list of all packages. The  llama-index  package still exists, and it imports  llama-index-core  and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages ( llama-index-llms-openai ,  llama-index-embeddings-openai , and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in  llama-index-readers-file ). NOTE:  if you don’t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining  llama-index-legacy  (pinned to the latest release 0.9.48) for the foreseeable future. Revamped Folder Structure We’ve completely revamped the folder structure in the  llama_index  repo. The most important folders you should care about are: llama-index-core  : This folder contains all core LlamaIndex abstractions. llama-index-integrations : This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details. llama-index-packs  : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user’s application. Other folders: llama-index-legacy  : contains the legacy LlamaIndex code. llama-index-experimental  : contains experimental features. Largely unused right now (outside parameter tuning). llama-index-finetuning  : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental. The sub-directories in  integrations  and  packs  represent individual packages. The name of the folder corresponds to the package name. For instance,  llama-index-integrations/llms/llama-index-llms-gemini  corresponds to the  llama-index-llms-gemini  PyPI package. Within each package folder, the source files are arranged in the same paths that you use to import them. For example, in the Gemini LLM package, you’ll see a folder called  llama_index/llms/gemini  containing the source files. This folder structure is what allows you to  preserve the top-level  llama_index  namespace during importing.  In the case of Gemini LLM, you would pip install  llama-index-llms-gemini  and then import using  from llama_index.llms.gemini import Gemini . Every one of these subfolders also has the resources needed to packagify it:  pyproject.toml ,  poetry.lock , and a  Makefile  , along with a script to automatically create a package. If you’re looking to contribute an integration or pack, don’t worry! We have a  full contributing guide  designed to make this as seamless as possible, make sure to check it out. Integrations All  third-party integrations are now under  llama-index-integrations . There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_llms readers tools vector_stores For completeness here are all the other categories:  agent ,  callbacks ,  evaluation ,  extractors ,  graph_stores ,  indices ,  output_parsers ,  postprocessor ,  program ,  question_gen ,  response_synthesizers ,  retrievers ,  storage ,  tools . The integrations in the most common categories can be found in our temporary  Notion package registry page . All integrations can be found in  our Github repo . The folder name of each integration package corresponds to the name of the package — so if you find an integration you like, you now know how to pip install it! We are actively working to make all integrations viewable on LlamaHub. Our vision for LlamaHub is to be  the  hub for all third-party integrations. If you’re interested in contributing a package, see our  contributing  section below! Usage Example Here is a simple example of installing and using an Anthropic LLM. pip install llama-index-llms-anthropic from  llama_index.llms.anthropic  import  Anthropic\nllm = Anthropic(api_key= \"&lt;api_key&gt;\" ) Here is an example of using a data loader. pip install llama-index-readers-notion from  llama_index.readers.notion  import  NotionPageReader\nintegration_token = os.getenv( \"NOTION_INTEGRATION_TOKEN\" )\npage_ids = [ \"&lt;page_id&gt;\" ]\nreader = NotionPageReader(integration_token=integration_token)\ndocuments = reader.load_data(page_ids=page_ids) Here is an example of using a LlamaPack: pip install llama-index-packs-sentence-window-retriever from  llama_index.packs.sentence_window_retriever  import  SentenceWindowRetrieverPack\n # create the pack \n # get documents from any data loader \nsentence_window_retriever_pack = SentenceWindowRetrieverPack(\n  documents\n)\nresponse = sentence_window_retriever_pack.run( \"Tell me a bout a Music celebritiy.\" ) Dealing with Breaking Changes This update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these: # no more using `llama_index.llms` as a top-level package \n from  llama_index.llms  import  OpenAI\n # no more using `llama_index.vector_stores` as a top-level package \n from  llama_index.vector_stores  import  PineconeVectorStore\n # llama_hub imports are now no longer supported. \n from  llama_hub.slack.base  import  SlackReader Instead you can do these: from  llama_index.llms.openai  import  OpenAI\n from  llama_index.vector_stores.pinecone  import  PineconeVectorStore\n #  NOTE:  no longer import a separate llama_hub package \n from  llama_index.readers.slack  import  SlackReader See our  migration guide  (also described below) for more details. LlamaHub as a Central Hub for Integrations With these packaging updates, we’re expanding the concept of LlamaHub to become a central hub of  all  LlamaIndex integrations to fulfill its vision of becoming an integration site at the center of the LLM ecosystem. This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, embeddings, vector stores, callbacks, and more. This effort is still a WIP.  If you go to  llamahub.ai  today, you’ll see that the site has not been updated yet, and it still contains the current set of integrations (data loaders, tools, LlamaPacks, datasets). Rest assured we’ll be updating the site in a few weeks; in the meantime check out our  Notion package registry  /  repo  for a list of all integrations/packages. Sunsetting llama-hub repo Since all integrations have been moved to the llama_index repo, we are sunsetting the llama-hub repo (but  LlamaHub  itself lives on!). We did the painstaking work of migrating and packaging all existing llama-hub integrations. For all future contributions please submit directly to the llama_index repo! `download` syntax A popular UX for fetching integrations through LlamaHub has been the  download  syntax:  download_loader  ,  download_llama_pack  , and more. This will still work, but have different behavior. Check out the details below: download_llama_pack  : Will download a pack under  llama-index-packs  to a local file on your disk. This allows you to directly use and modify the source code from the template. Every other download function  download_loader  ,  download_tool  : This will directly run pip install on the relevant integration package. Deprecating ServiceContext Last but not least, we are deprecating our  ServiceContext  construct and as a result improving the developer experience of LlamaIndex. Our  ServiceContext  object existed as a general configuration container containing an LLM, embedding model, callback, and more; it was created before we had proper LLM, embedding, prompt abstractions and was meant to be an intermediate user-facing layer to let users define these parameters. Over time however, this object became increasingly difficult to use. Passing in an entire  service_context  container to any module (index, retriever, post-processor, etc.) made it hard to reason about which component was actually getting used. Since all modules use OpenAI by default, users were getting asked to unnecessarily specify their OpenAI key even in cases where they’d want to use a local model (because the embedding model default was still OpenAI). It was also laborious to import and type out. Another related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the  service_context  to  all  modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage. Therefore we’ve made the following changes: ServiceContext is now deprecated:  You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis. You can now define global settings:  Define this once, and don’t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks. All references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well. Usage Example To build a  VectorStoreIndex  and then query it, you can now pass in the embedding model and LLM directly from  llama_index.embeddings.openai  import  OpenAIEmbedding\n from  llama_index.llms.openai  import  OpenAI\n from  llama_index.core.callbacks  import  CallbackManager\n\nembed_model = OpenAIEmbedding()\nllm = OpenAI()\ncallback_manager = CallbackManager()\nindex = VectorStoreIndex.from_documents(\n documents, embed_model=embed_model, callback_manager=callback_manager\n)\nquery_engine = index.as_query_engine(llm=llm) Or you can define a global settings object from  llama_index.core.settings  import  Settings\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.callback_manager = callback_manager\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine() Contributing to LlamaIndex v0.10 v0.10 makes the  llama_index  repo the central place for all community contributions, whether you are interested in contributing core refactors, or integrations/packs! If you’re contributing an integration/pack, v0.10 makes it way easier for you to contribute something that can be independently versioned, tested, and packaged. We have utility scripts to make the package creation process for an integration or pack effortless: # create a new pack \ncd ./llama-index-packs\nllamaindex-cli  new -package --kind  \"packs\"  --name  \"my new pack\" \n\n # create a new integration \ncd ./llama-index-integrations/readers\nllamaindex-cli  new -pacakge --kind  \"readers\"  --name  \"new reader\" Take a look at our updated  contributing guide here  for more details. Migration to v0.10 If you want to use LlamaIndex v0.10, you will need to do two main things: Adjust imports to fit the new package structure for core modules/integrations Deprecate ServiceContext Luckily, we’ve created a comprehensive migration guide that also contains a CLI tool to  automatically  upgrade your existing code and notebooks to v0.10! Just do llamaindex-cli upgrade <source-dir> Check out the full migration guide here. Next Steps We’ve painstakingly revamped all of our README, documentation and notebooks to reflect these v0.10 changes. Check out the below section for a compiled list of all resources. Documentation v0.10 Documentation v0.10 Installation Guide v0.10 Quickstart Updated Contribution Guide Temporary  v0.10 Package Registry v0.10 Migration Guide Repo Repo README llama-index-integrations llama-index-packs Example Notebooks These are mostly to show our updated import syntax. Sub-Question Query Engine  (primarily uses core) Weaviate Vector Store Demo OpenAI Agent over RAG Pipelines Bug reports We’ll be actively monitoring our  Github Issues  and  Discord . If you run into any issues don’t hesitate to hop into either of these channels!", "author": "LlamaIndex", "date": "Feb 12, 2024", "tags": ["LLM", "Llamaindex", "Upgrade", "AI"]},
{"title": "LlamaIndex Newsletter 2023–02–13", "content": "Greetings, LlamaIndex Adventurers 🦙, Welcome to an exhilarating week of discoveries at LlamaIndex! Our community’s lively input and the abundance of learning tools await to supercharge your journey through LlamaIndex. Before we dive into the updates, we have two major announcements: LlamaIndex v0.10 : Our latest open-source release marks a monumental step towards production readiness. With a new core package and hundreds of integrations and LlamaPacks now available as separate PyPi packages, we’ve massively improved organization and version tracking. Major updates include the refactoring of LlamaHub into a central hub for all integrations and the deprecation of ServiceContext for an enhanced development experience.  Blog ,  Migration Guide . Introducing Short Courses on Advanced RAG Development: Master complex RAG systems with our series, covering everything from unstructured data to agent integration. Learn through LlamaIndex Query Pipelines, from basic text-to-SQL to advanced query techniques, and build scalable RAG applications with hands-on guidance.  Video1 ,  Video2 . Your creativity fuels our inspiration! We’re excited to see any projects, articles, or videos you’re passionate about. Share your incredible creations with us at  news@llamaindex.ai . Haven’t joined our newsletter yet? Make sure to subscribe on our  website  for the latest LlamaIndex updates delivered directly to your inbox. 🤩  The highlights: Self-RAG : Introducing Self-RAG, now part of LlamaIndex as a LlamaPack. Boosts LLM training and RAG workflows with dynamic capabilities.  Notebook ,  Tweet . LlamaIndex + FlowiseAI Integration : Seamlessly merge LlamaIndex with FlowiseAI for effortless, no-code RAG app development.  Docs ,  Tweet . RAG Guide with MistralAI : MistralAI’s new doc includes a RAG guide with LlamaIndex. Utilize Mistral-medium for enhanced RAG functions.  Docs . ✨ Feature Releases and Enhancements: We have introduced a seamless integration between LlamaIndex and FlowiseAI, enabling easy, no-code development of advanced RAG applications with a drag-and-drop interface for quick chatbot or agent integration.  Docs ,  Tweet . We have introduced Self-RAG, a dynamic retrieval tool by  Akari Asai ’s team, now available as a LlamaPack for easy integration, enhancing LLM training and RAG workflows with dynamic, iterative capabilities.  Notebook ,  Tweet . We have introduced the RAG CLI tool that allows you to search any file on your filesystem using on-device language model embeddings, featuring the power of Mistral-7B and bge-m3 for an advanced, customizable experience.  Docs ,  Tweet . We have launched full-stack agent servers with a single CLI command using  create-llama  from LlamaIndex, offering instant access to 50+ tools for any agent project.  Tweet . We have introduced agents in LlamaIndex.TS, enabling advanced AI software development in TypeScript with features like function calling and multi-document handling.  Blog ,  Docs ,  Tweet . DeepEval is integrated with LlamaIndex, significantly enhancing RAG evaluation capabilities and introducing unit testing for LlamaIndex apps in CI/CD environments.  Docs . 🗺️ Guides: Guide  to RAG with LlamaIndex in MistralAI’s new documentation with Mistral-medium and Mistral embedding models. Guide  to Building Agentic RAG to incorporate user feedback in real-time enhancing complex searches with a human-in-the-loop approach. Guide  to Integrating Huggingface’s New Messages API with OpenAI compatibility, simplifying the integration process for Inference Endpoints and Text Generation Inference. ✍️ Tutorials: Plaban Nayak   tutorial  on Setting up Query Pipeline For Advanced RAG Workflow using LlamaIndex. Krish Naik   tutorial  on Step-by-Step Guide to Building a RAG LLM App with Llama2 and LlamaIndex. HelixML  tutorial  to Knowledge Memorization by fine-tuning Mistral-7B for enhanced knowledge memorization, offering a new way to reason across contexts without RAG’s limitations. Wenqi Glantz   tutorial  on NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit. 🎥  Webinar: Webinar  of Laurie with Ankit Khare(Rockset) delves into the essentials of RAG — its purpose, methodology, how LlamaIndex facilitates it, and exciting developments for 2024. Webinar  with Zilong Wang, and Tianyang Liu on Advanced Tabular Data Understanding with LLMs. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Feb 13, 2024", "tags": ["Newsletter", "Llamaindex", "AI", "Rag", "LLM"]},
{"title": "A Cheat Sheet and Some Recipes For Building Advanced RAG", "content": "It’s the start of a new year and perhaps you’re looking to break into the RAG scene by building your very first RAG system. Or, maybe you’ve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user’s queries and data structures. In either case, knowing where or how to begin may be a challenge in and of itself! If that’s true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems. The RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper ( “Retrieval-Augmented Generation for Large Language Models: A Survey” Gao, Yunfan, et al. 2023 ). Basic RAG Mainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user’s query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component. LlamaIndex Basic RAG Recipe: from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n\n # load data \ndocuments = SimpleDirectoryReader(input_dir= \"...\" ).load_data()\n\n # build VectorStoreIndex that takes care of chunking documents \n # and encoding chunks to embeddings for future retrieval \nindex = VectorStoreIndex.from_documents(documents=documents)\n\n # The QueryEngine class is equipped with the generator \n # and facilitates the retrieval and generation steps \nquery_engine = index.as_query_engine()\n\n # Use your Default RAG \nresponse = query_engine.query( \"A user's query\" ) Success Requirements for RAG In order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements: Retrieval must be able to find the most relevant documents to a user query. Generation must be able to make good use of the retrieved documents to sufficiently answer the user query. Advanced RAG With the success requirements defined, we can then say that building advanced RAG is really about the application of more sophisticated techniques and strategies (to the Retrieval or Generation components) to ensure that they are ultimately met. Furthermore, we can categorize a sophisticated technique as either one that addresses one of the two high-level success requirements independent (more or less) of the other, or one that addresses both of these requirements simultaneously. Advanced techniques for Retrieval must be able to find the most relevant documents to a user query Below we briefly describe a couple of the more sophisticated techniques to help achieve the first success requirement. Chunk-Size Optimization:  Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses. LlamaIndex Chunk Size Optimization Recipe  ( notebook guide ) : from  llama_index  import  ServiceContext\n from  llama_index.param_tuner.base  import  ParamTuner, RunResult\n from  llama_index.evaluation  import  SemanticSimilarityEvaluator, BatchEvalRunner\n\n ### Recipe \n ### Perform hyperparameter tuning as in traditional ML via grid-search \n ### 1. Define an objective function that ranks different parameter combos \n ### 2. Build ParamTuner object \n ### 3. Execute hyperparameter tuning with ParamTuner.tune() \n\n # 1. Define objective function \n def   objective_function ( params_dict ):\n    chunk_size = params_dict[ \"chunk_size\" ]\n    docs = params_dict[ \"docs\" ]\n    top_k = params_dict[ \"top_k\" ]\n    eval_qs = params_dict[ \"eval_qs\" ]\n    ref_response_strs = params_dict[ \"ref_response_strs\" ]\n\n     # build RAG pipeline \n    index = _build_index(chunk_size, docs)   # helper function not shown here \n    query_engine = index.as_query_engine(similarity_top_k=top_k)\n  \n     # perform inference with RAG pipeline on a provided questions `eval_qs` \n    pred_response_objs = get_responses(\n        eval_qs, query_engine, show_progress= True \n    )\n\n     # perform evaluations of predictions by comparing them to reference \n     # responses `ref_response_strs` \n    evaluator = SemanticSimilarityEvaluator(...)\n    eval_batch_runner = BatchEvalRunner(\n        { \"semantic_similarity\" : evaluator}, workers= 2 , show_progress= True \n    )\n    eval_results = eval_batch_runner.evaluate_responses(\n        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n    )\n\n     # get semantic similarity metric \n    mean_score = np.array(\n        [r.score  for  r  in  eval_results[ \"semantic_similarity\" ]]\n    ).mean()\n\n     return  RunResult(score=mean_score, params=params_dict)\n\n # 2. Build ParamTuner object \nparam_dict = { \"chunk_size\" : [ 256 ,  512 ,  1024 ]}  # params/values to search over \nfixed_param_dict = {  # fixed hyperparams \n   \"top_k\" :  2 ,\n     \"docs\" : docs,\n     \"eval_qs\" : eval_qs[: 10 ],\n     \"ref_response_strs\" : ref_response_strs[: 10 ],\n}\nparam_tuner = ParamTuner(\n    param_fn=objective_function,\n    param_dict=param_dict,\n    fixed_param_dict=fixed_param_dict,\n    show_progress= True ,\n)\n\n # 3. Execute hyperparameter search \nresults = param_tuner.tune()\nbest_result = results.best_run_result\nbest_chunk_size = results.best_run_result.params[ \"chunk_size\" ] 2. Structured External Knowledge:  In complex scenarios, it may be necessary to build your external knowledge with a bit more structure than the basic vector index so as to permit recursive retrievals or routed retrieval when dealing with sensibly separated external knowledge sources. LlamaIndex Recursive Retrieval Recipe  ( notebook guide ) : from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n from  llama_index.node_parser  import  SentenceSplitter\n from  llama_index.schema  import  IndexNode\n\n ### Recipe \n ### Build a recursive retriever that retrieves using small chunks \n ### but passes associated larger chunks to the generation stage \n\n # load data \ndocuments = SimpleDirectoryReader(\n  input_file= \"some_data_path/llama2.pdf\" \n).load_data()\n\n # build parent chunks via NodeParser \nnode_parser = SentenceSplitter(chunk_size= 1024 )\nbase_nodes = node_parser.get_nodes_from_documents(documents)\n\n # define smaller child chunks \nsub_chunk_sizes = [ 256 ,  512 ]\nsub_node_parsers = [\n    SentenceSplitter(chunk_size=c, chunk_overlap= 20 )  for  c  in  sub_chunk_sizes\n]\nall_nodes = []\n for  base_node  in  base_nodes:\n     for  n  in  sub_node_parsers:\n        sub_nodes = n.get_nodes_from_documents([base_node])\n        sub_inodes = [\n            IndexNode.from_text_node(sn, base_node.node_id)  for  sn  in  sub_nodes\n        ]\n        all_nodes.extend(sub_inodes)\n     # also add original node to node \n    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n    all_nodes.append(original_node)\n\n # define a VectorStoreIndex with all of the nodes \nvector_index_chunk = VectorStoreIndex(\n    all_nodes, service_context=service_context\n)\nvector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k= 2 )\n\n # build RecursiveRetriever \nall_nodes_dict = {n.node_id: n  for  n  in  all_nodes}\nretriever_chunk = RecursiveRetriever(\n     \"vector\" ,\n    retriever_dict={ \"vector\" : vector_retriever_chunk},\n    node_dict=all_nodes_dict,\n    verbose= True ,\n)\n\n # build RetrieverQueryEngine using recursive_retriever \nquery_engine_chunk = RetrieverQueryEngine.from_args(\n    retriever_chunk, service_context=service_context\n)\n\n # perform inference with advanced RAG (i.e. query engine) \nresponse = query_engine_chunk.query(\n     \"Can you tell me about the key concepts for safety finetuning\" \n) Other useful links We have several of guides demonstrating the application of other advanced techniques to help ensure accurate retrieval in complex cases. Here are links to a select few of them: Building External Knowledge using Knowledge Graphs Performing Mixed Retrieval with Auto Retrievers Building Fusion Retrievers Fine-tuning Embedding Models used in Retrieval Transforming Query Embeddings (HyDE) Advanced techniques for Generation must be able to make good use of the retrieved documents Similar to previous section, we provide a couple of examples of the sophisticated techniques under this category, which can be described as ensuring that the retrieved documents are well aligned to the LLM of the Generator. Information Compression:  Not only are LLMs are restricted by context length, but there can be response degradation if the retrieved documents carry too much noise (i.e. irrelevant information). LlamaIndex Information Compression Recipe  ( notebook guide ) : from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n from  llama_index.query_engine  import  RetrieverQueryEngine\n from  llama_index.postprocessor  import  LongLLMLinguaPostprocessor\n\n ### Recipe \n ### Define a Postprocessor object, here LongLLMLinguaPostprocessor \n ### Build QueryEngine that uses this Postprocessor on retrieved docs \n\n # Define Postprocessor \nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str= \"Given the context, please answer the final question\" ,\n    target_token= 300 ,\n    rank_method= \"longllmlingua\" ,\n    additional_compress_kwargs={\n         \"condition_compare\" :  True ,\n         \"condition_in_question\" :  \"after\" ,\n         \"context_budget\" :  \"+100\" ,\n         \"reorder_context\" :  \"sort\" ,   # enable document reorder \n    },\n)\n\n # Define VectorStoreIndex \ndocuments = SimpleDirectoryReader(input_dir= \"...\" ).load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n # Define QueryEngine \nretriever = index.as_retriever(similarity_top_k= 2 )\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=[node_postprocessor]\n)\n\n # Used your advanced RAG \nresponse = retriever_query_engine.query( \"A user query\" ) 2. Result Re-Rank:  LLMs suffer from the so-called “Lost in the Middle” phenomena which stipulates that LLMs focus on the extreme ends of the prompts. In light of this, it is beneficial to re-rank retrieved documents before passing them off to the Generation component. LlamaIndex Re-Ranking For Better Generation Recipe  ( notebook guide ) : import  os\n from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\n from  llama_index.postprocessor  import  LongLLMLinguaPostprocessor\n\n ### Recipe \n ### Define a Postprocessor object, here CohereRerank \n ### Build QueryEngine that uses this Postprocessor on retrieved docs \n\n # Build CohereRerank post retrieval processor \napi_key = os.environ[ \"COHERE_API_KEY\" ]\ncohere_rerank = CohereRerank(api_key=api_key, top_n= 2 )\n\n # Build QueryEngine (RAG) using the post processor \ndocuments = SimpleDirectoryReader( \"./data/paul_graham/\" ).load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine(\n    similarity_top_k= 10 ,\n    node_postprocessors=[cohere_rerank],\n)\n\n # Use your advanced RAG \nresponse = query_engine.query(\n     \"What did Sam Altman do in this essay?\" \n) Advanced techniques for simultaneously addressing Retrieval and Generation success requirements In this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries). Generator-Enhanced Retrieval:  These techniques make use of the LLM’s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response. LlamaIndex Generator-Enhanced Retrieval Recipe  ( notebook guide ) : from  llama_index.llms  import  OpenAI\n from  llama_index.query_engine  import  FLAREInstructQueryEngine\n from  llama_index  import  (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)\n ### Recipe \n ### Build a FLAREInstructQueryEngine which has the generator LLM play \n ### a more active role in retrieval by prompting it to elicit retrieval \n ### instructions on what it needs to answer the user query. \n\n # Build FLAREInstructQueryEngine \ndocuments = SimpleDirectoryReader( \"./data/paul_graham\" ).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nindex_query_engine = index.as_query_engine(similarity_top_k= 2 )\nservice_context = ServiceContext.from_defaults(llm=OpenAI(model= \"gpt-4\" ))\nflare_query_engine = FLAREInstructQueryEngine(\n    query_engine=index_query_engine,\n    service_context=service_context,\n    max_iterations= 7 ,\n    verbose= True ,\n)\n\n # Use your advanced RAG \nresponse = flare_query_engine.query(\n     \"Can you tell me about the author's trajectory in the startup world?\" \n) 2. Iterative Retrieval-Generator RAG:  For some complex cases, multi-step reasoning may be required to provide a useful and relevant answer to the user query. LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engine  import  RetryQueryEngine\n from  llama_index.evaluation  import  RelevancyEvaluator\n\n ### Recipe \n ### Build a RetryQueryEngine which performs retrieval-generation cycles \n ### until it either achieves a passing evaluation or a max number of \n ### cycles has been reached \n\n # Build RetryQueryEngine \ndocuments = SimpleDirectoryReader( \"./data/paul_graham\" ).load_data()\nindex = VectorStoreIndex.from_documents(documents)\nbase_query_engine = index.as_query_engine()\nquery_response_evaluator = RelevancyEvaluator()  # evaluator to critique  \n                                                 # retrieval-generation cycles \nretry_query_engine = RetryQueryEngine(\n    base_query_engine, query_response_evaluator\n)\n\n # Use your advanced rag \nretry_response = retry_query_engine.query( \"A user query\" ) Measurement Aspects of RAG Evaluating RAG systems are, of course, of utmost importance. In their survey paper, Gao, Yunfan et al. indicate 7 measurement aspects as seen in the top-right portion of the attached RAG cheat sheet. The llama-index library consists of several evaluation abstractions as well as integrations to RAGAs in order to help builders gain an understanding of the level to which their RAG system achieves the success requirements through the lens of these measurement aspects. Below, we list a select few of the evaluation notebook guides. Answer Relevancy and Context Relevancy Faithfulness Retrieval Evaluation Batch Evaluations with BatchEvalRunner You’re Now Equipped To Do Advanced RAG After reading this blog post, we hope that you feel more equipped and confident to apply some of these sophisticated techniques for building Advanced RAG systems!", "author": "Andrei", "date": "Jan 5, 2024", "tags": ["Retrieval Augmented", "LLM", "Retrieval Generation", "AI"]},
{"title": "LlamaIndex Newsletter 2024–01–09", "content": "Hola, LlamaIndex Lovers 🦙, Welcome to another thrilling week at LlamaIndex, filled with vibrant community contributions and enriching educational content. Immerse yourself in our engaging tutorials, guides, community demos, and webinars, all crafted to amplify your LlamaIndex experience. Before we jump into our latest updates, we’re thrilled to share two major announcements: 🧑‍🏫  Join Our LlamaIndex Community Office Hours : Struggling with complex LLM/RAG queries or have feedback that our documentation doesn’t cover?  Register  for our community office hours for a chance to have an enlightening conversation and get your questions answered! 🗺️  Explore Our Open-Source Roadmap for 2024 :  We’re excited to unveil our ambitious roadmap for the LlamaIndex ecosystem. Over the next 3–6 months, we aim to enhance LlamaIndex’s production readiness, accessibility, and its advanced features, including RAG, agents, and more. This  living document  is available on our GitHub discussions page — a must-visit to be part of our exciting journey! Additionally, if you’ve been working on an interesting project, written an insightful article, or created a captivating video, we’d love to hear about it! Please share your work with us at  news@llamaindex.ai . And remember to subscribe to our newsletter through our  website  to get all these exciting updates straight to your inbox 🤩  The highlights: Query Pipelines : Introducing a new declarative API for effortless orchestration of simple to complex RAG query workflows.  Docs ,  Blogpost ,  Tweet . ETL Pipeline Launch : New repository for setting up production ETL pipelines in RAG/LLM apps, boasting a 4x speed boost and integrating Hugging Face, RabbitMQ, and AWS EKS.  Github Repo ,  Blogpost ,  Tweet . Multimodal ReAct Agent : Launch of an agent capable of processing text and images, enhancing RAG pipeline and web search functionalities using GPT-4V.  Notebook ,  Tweet . RAGatouille LlamaPack : Introduction of an easy-to-use pack for ColBERT retrieval, enabling one-line code integration in LlamaIndex RAG pipelines.  Docs ,  Tweet . Advanced RAG Cheat Sheet : A comprehensive cheat sheet with techniques for RAG enhancement, perfect for both new and experienced LLM users. ✨ Feature Releases and Enhancements: We have introduced Query Pipelines, a declarative API designed to simplify the creation and customization of advanced RAG workflows. This tool enables the orchestration of query workflows, ranging from basic sequential chains to complex DAGs, tailored to specific use cases.  Docs ,  Blogpost ,  Tweet . We have launched a repository for easily setting up a production ETL pipeline for RAG/LLM apps, offering a 4x speed increase over laptop-based operations. This solution integrates Hugging Face, RabbitMQ, Llama Index, and AWS EKS, providing fast document indexing and efficient data handling, complete with an AWS Lambda API endpoint. Ideal for RAG apps transitioning to production, especially on AWS.  Github Repo ,  Blogpost ,  Tweet . We have launched the Multimodal ReAct Agent, combining GPT-4V with the ability to process both text and images. This agent can perform tasks like querying a RAG pipeline or conducting web searches based on visual and textual inputs.  Notebook ,  Tweet . RAGatouille LlamaPack: RAGatouille simplifies the use of ColBERT, a more advanced retrieval model compared to dense embedding-based retrieval techniques. This pack allows you to build an end-to-end LlamaIndex RAG pipeline with just one line of code by ingesting documents using any of our 150+ data loaders, combined with your preferred LLM for response synthesis.  Docs ,  Tweet . We have integrated with Pathway’s open data processing framework which enables us to handle dynamic data sources in production, automatically updating indexes based on real-time changes, ensuring up-to-date and accurate query responses.  Docs ,  Tweet . Ian McCrystal  has added the StripeDocsLoader to LlamaHub, enabling a quick setup of RAG over Stripe’s documentation using Llama Index.  Docs . Jeremy Dyer  has integrated NVIDIA’s Triton Inference Server which allows you to run optimized inference on any AI framework. It supports the TensorRT-LLM backend, enhancing LLM performance on Nvidia GPUs.  Notebook ,  Tweet . 👀 Community Demos : Context-Augmented Agent for Food Delivery: A full-stack application guide by lucastonon for creating an RAG agent. This tool performs in-browser tasks like opening restaurant pages and adding food to carts, purely via voice commands, integrating with Llama Index, Pinecone, OpenAI’s Whisper, LLMs, Function Calling, vue.js, and FastAPI.  Github Repo ,  Tweet . GRDN.AI : A fascinating side project from Danielle Heymann, using a genetic algorithm and LLM to optimize plant placement based on compatibility. This project harnesses local models from HuggingFace, accessed through LlamaIndex for the LLM part, combining traditional mathematical strategies with LLMs.  Blogpost ,  Tweet . Build an AI Shopping Assistant with RAG and Agents : This assistant can analyze a picture of an item and suggest weather-appropriate accessories. The work by D. Kiedanski and Lucas Micol from Tryolabs explains how to transform APIs into problem-solving tools for a LlamaIndex agent. 🗺️ Guides: Guide  to Advanced RAG: Our comprehensive cheat sheet offers insights into improving RAG with techniques like optimized retrieval, effective document use in generation, and interleaving generation with retrieval. Ideal for both new and seasoned LLM users, it’s a must-have resource, complete with LlamaIndex links. Guide  to building advanced RAG CHATBOT with NVIDIA’S TensorRT-LLM: This chatbot is designed to maintain contiguous document or code blocks, avoiding awkward chunking. It features a stack combining Llama Index’s auto-merging retriever with NVIDIA’s TensorRT-LLM and a custom postprocessor, optimized for RAG using open-source models. ✍️ Tutorials: BentoML  tutorial  on Building An Intelligent Query-Response System with LlamaIndex and OpenLLM. Akash Mathur   tutorial  on Advanced RAG: Optimizing Retrieval with Additional Context & MetaData using LlamaIndex. 🎥  Webinars: Weights & Biases  podcast  with Jerry Liu on Revolutionizing AI Data Management. 🏢 Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.", "author": "LlamaIndex", "date": "Jan 9, 2024", "tags": ["Llamaindex", "Rag", "AI", "Newsletter", "LLM"]},
{"title": "Introducing LlamaCloud and LlamaParse", "content": "Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring  production-grade   context-augmentation  to your LLM and RAG applications. Using LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components: LlamaParse:  Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You’ll be able to answer complex questions that simply weren’t possible previously. Managed Ingestion and Retrieval API:  An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in  LlamaHub , including LlamaParse, and our data storage integrations. LlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use;  contact us  for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you’re interested,  get in touch . (We’ve also launched a  new version of our website  🦙!) RAG is Only as Good as your Data A core promise of LLMs is the ability to automate knowledge search, synthesis, extraction, and planning over any source of unstructured data. Over the past year a new data stack has emerged to power these context-augmented LLM applications, popularly referred to as Retrieval-Augmented Generation (RAG). This stack includes loading data, processing it, embedding it, and loading into a vector database. This enables downstream orchestration of retrieval and prompting to provide context within an LLM app. This stack is different from any ETL stack before it, because unlike traditional software, every decision in the data stack directly  affects the accuracy  of the full LLM-powered system. Every decision like chunk size and embedding model affects LLM outputs, and since LLMs are black boxes, you can’t unit test your way to correct behavior. We’ve spent the past year hard at work at the forefront of providing tooling and educating users on how to build high-performing, advanced RAG for various use cases. We crossed the 2M monthly download mark, and are used by large enterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and Biases, DataStax, and many more. But while getting started with our famous 5-line starter example is easy, building production-grade RAG remains a complex and subtle problem. In our hundreds of user conversations, we learned the biggest pain points: Results aren’t accurate enough:  The application was not able to produce satisfactory results for a long-tail of input tasks/queries. The number of parameters to tune is overwhelming:  It’s not clear which parameters across the data parsing, ingestion, retrieval. PDFs are specifically a problem:  I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it? Data syncing is a challenge:  Production data often updates regularly, and continuously syncing new data brings a new set of challenges. These are the problems we set out to solve with LlamaCloud. Data Pipelines to Bring you to Production We built LlamaCloud and LlamaParse as the data pipelines to get your RAG application to production more quickly. LlamaParse LlamaParse is a state-of-the-art parser designed to specifically unlock RAG over complex PDFs with embedded tables and charts. This simply wasn’t possible before with other approaches, and we’re incredibly excited about this technology. LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document. For the past few months we’ve been obsessed with this problem. This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXiv papers to 10K filings to medical reports. Naive chunking and retrieval algorithms do terribly. We were the first to propose a  novel recursive retrieval RAG technique  for being able to hierarchically index and query over tables and text in a document. The only challenge that remained was how to properly parse out tables and text in the first place. Comparison of LlamaParse vs. PyPDF over the Apple 10K filing.  Full comparisons are here . A green highlight in a cell means that the RAG pipeline correctly returned the cell value as the answer to a question over that cell. A red highlight means that the question was answered incorrectly. This is where LlamaParse comes in. We’ve developed a proprietary parsing service that is incredibly good at parsing PDFs with complex tables into a well-structured markdown format. This representation directly plugs into the advanced Markdown parsing and recursive retrieval algorithms available in the open-source library. The end result is that you are able to build RAG over complex documents that can answer questions over both tabular and unstructured data. Check out the results below for a comparison: Comparison of baseline PDF approach (top) vs. LlamaParse + advanced retrieval (bottom) Results over the  Uber 10K Dataset . For more information on our evaluation metrics check out our  evaluation page  here. This service is available in a  public preview mode:  available to everyone, but with a usage limit (1k pages per day). It operates as a standalone service that also plugs into our managed ingestion and retrieval API (see below). Check out our  LlamaParse onboarding here  for more details. from  llama_parse  import  LlamaParse\n\nparser = LlamaParse(\n    api_key= \"llx-...\" ,   # can also be set in your env as LLAMA_CLOUD_API_KEY \n    result_type= \"markdown\" ,   # \"markdown\" and \"text\" are available \n    verbose= True \n) For unlimited commercial use of LlamaParse,  get in touch  with us. Next Steps Our early users have already given us important feedback on what they’d like to see next. Currently we primarily support PDFs with tables, but we are also building out better support for figures, and and an expanded set of the most popular document types: .docx, .pptx, .html. Managed Ingestion and Retrieval Our other main offering in LlamaCloud is a managed ingestion and retrieval API which allows you to easily declare performant data pipelines for any context-augmented LLM application. Get clean data for your LLM application, so you can spend less time wrangling data and more time writing core application logic. LlamaCloud empowers enterprise developers with the following benefits: Engineering Time Savings:  Instead of having to write custom connectors and parsing logic in Python, our APIs allow you to directly connect to different data sources. Performance:  we provide good out-of-the-box performance for different data types, while offering an intuitive path for experimentation, evaluation, and improvement. Ease Systems Complexity:  Handle a large number of data sources with incremental updates. Let’s do a brief tour through the core components! Ingestion:  Declare a managed pipeline to process and transform/chunk/embed data backed by our 150+ data sources in LlamaHub and our 40+ storage integrations as destinations. Automatically handle syncing and load balancing. Define through the UI or our open-source library. Retrieval:  Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language. Playground:  Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop. LlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment. LlamaCloud Retrieval: Access advanced retrieval over your storage system via an API. We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you’re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come  talk to us . Launch Partners and Collaborators We opened up access to LlamaParse at  our hackathon  we co-hosted with  Futureproof Labs  and  Datastax  at the beginning of February. We saw some incredible applications of LlamaParse in action,  including parsing building codes for Accessory Dwelling Unit (ADU) planning ,  parsing real-estate disclosures for home buying , and dozens more. Eric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into Mendable’s data stack: “We integrated LlamaParse into our  open source data connector repo  which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.” We’re also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem, from storage to compute. DataStax Datastax has incorporated LlamaParse into their RAGStack to bring a privacy-preserving out-of-the-box RAG solution for enterprises: \"Last week one of our customers Imprompt has launched a pioneering 'Chat-to-Everything' platform leveraging RAGStack powered by LlamaIndex to enhance their enterprise offerings while prioritizing privacy.\" said Davor Bonaci, CTO and executive vice president, DataStax. \"We're thrilled to partner with LlamaIndex to bring a streamlined solution to market. By incorporating LlamaIndex into RAGStack, we are providing enterprise developers with a comprehensive Gen AI stack that simplifies the complexities of RAG implementation, while offering long-term support and compatibility assurance.” MongoDB “MongoDB’s partnership with LlamaIndex allows for the ingestion of data into the MongoDB Atlas Vector database, and the retrieval of the index from Atlas via LlamaParse and LlamaCloud, enabling the development of RAG systems and other AI applications,” said Greg Maxson, Global Lead, AI Ecosystems at MongoDB. “Now, developers can abstract complexities associated with data ingestion, simplify RAG pipeline implementations, and more cost effectively develop large language model applications, ultimately accelerating generative AI app development and more quickly bringing apps to market.” Qdrant André Zayarni, CEO of Qdrant, says “The Qdrant team is excited to partner with LlamaIndex to combine the power of optimal data preprocessing, vectorization, and ingestion with Qdrant for a powerful fullstack RAG solution.” NVIDIA We are also excited to collaborate with NVIDIA to integrate LlamaIndex with the  NVIDIA AI Enterprise  software platform for production AI: “LlamaCloud will help enterprises get generative AI applications from development into production with connectors that link proprietary data to the power of large language models,” said Justin Boitano, vice president, Enterprise and Edge Computing, NVIDIA. “Pairing LlamaCloud with NVIDIA AI Enterprise can accelerate the end-to-end LLM pipeline — including data processing, embedding creation, indexing, and model inference on accelerated computing across clouds, data centers and out to the edge.” FAQ Is this competitive with vector databases? No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are working hard to do the following: Integrate LlamaCloud with storage providers of existing design partners Make LlamaCloud available in a more “self-serve” manner. Next Steps As mentioned in the above sections, LlamaParse is available for public preview starting today with a usage cap. LlamaCloud is in a private preview mode; we are offering access to a limited set of enterprise design partners. If you’re interested come talk to us! LlamaParse:  Repo ,  Cookbook ,  Contact Us LlamaCloud:  Contact Us", "author": "Jerry Liu", "date": "Feb 20, 2024", "tags": ["Llamaindex", "LLM", "AI", "Retrieval Augmented", "Data"]},
{"title": "Building Multi-Tenancy RAG System with LlamaIndex", "content": "Introduction: The concept of Multi-Tenancy in RAG (Retriever-Augmented Generation) systems has become increasingly vital, especially when it comes to data security and privacy. Multi-Tenancy, in simple terms, refers to a system’s ability to serve multiple users (‘tenants’) independently and securely. Consider this scenario: In a RAG system, there are two users, User-1 and User-2. Both have their own set of documents which they have indexed into the system. The critical aspect of Multi-Tenancy here is that when User-1 queries the system, they should only receive answers from the documents they have indexed, and not from the documents indexed by User-2, and vice versa. This separation is crucial for maintaining data confidentiality and security, as it prevents the accidental or unauthorized cross-referencing of private information between different users. In the context of Multi-Tenancy in RAG systems, this means designing a system that not only understands and retrieves information effectively but also strictly adheres to user-specific data boundaries. Each user’s interaction with the system is isolated, ensuring that the retriever component of the RAG pipeline accesses only the information relevant and permitted for that particular user. This approach is important in scenarios where sensitive or proprietary data is involved, as it safeguards against data leaks and privacy breaches. In this blog post, we will look into Building a Multi-Tenancy RAG System with LlamaIndex. Solving Multi-Tenancy Challenges The key to managing Multi-Tenancy lies within the metadata. When indexing documents, we incorporate user-specific information into the metadata before adding it to the index. This ensures that each document is uniquely tied to an individual user. During the query phase, the retriever uses this metadata to filter and only access documents associated with the querying user. Subsequently, it performs a semantic search to retrieve the most relevant information segments, or ‘top_k chunks’, for that user. By implementing this approach, we effectively prevent the unauthorized cross-referencing of private information between different users, upholding the integrity and confidentiality of each user’s data. Now that we’ve discussed the concept, let’s dive into the construction of a Multi-Tenancy RAG system. For an in-depth step-by-step guide, feel free to follow along with the subsequent instructions in our  Google Colab Notebook . Download Data: We will use  An LLM Compiler for Parallel Function Calling  and  Dense X Retrieval: What Retrieval Granularity Should We Use?  papers for the demonstrations. !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\" Load Data: We will load the data of  LLMCompiler  paper for user  Jerry  and  Dense X Retrieval  paper for user  Ravi reader = SimpleDirectoryReader(input_files=['dense_x_retrieval.pdf'])\ndocuments_jerry = reader.load_data()\n\nreader = SimpleDirectoryReader(input_files=['llm_compiler.pdf'])\ndocuments_ravi = reader.load_data() Create An Empty Index: We will initially create an empty index to which we can insert documents, each tagged with metadata containing the user information. index = VectorStoreIndex.from_documents(documents=[]) Ingestion Pipeline: The IngestionPipeline is useful for data ingestion and performing transformations, including chunking, metadata extraction, and more. Here we utilize it to create nodes, which are then inserted into the index. pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n    ]\n) Update Metadata and Insert Documents: We will update the metadata of the documents with each user and insert the documents into the index. # For user Jerry \n for  document  in  documents_jerry:\n    document.metadata[ 'user' ] =  'Jerry' \n\nnodes = pipeline.run(documents=documents_jerry)\n # Insert nodes into the index \nindex.insert_nodes(nodes)\n\n # For user Ravi \n for  document  in  documents_ravi:\n    document.metadata[ 'user' ] =  'Ravi' \n\nnodes = pipeline.run(documents=documents_ravi)\n # Insert nodes into the index \nindex.insert_nodes(nodes) Define Query Engines: We will define query engines for both the users with necessary filters. # For Jerry\njerry_query_engine = index.as_query_engine(\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(\n                key=\"user\",\n                value=\"Jerry\",\n            )\n        ]\n    ),\n    similarity_top_k=3\n)\n\n# For Ravi\nravi_query_engine = index.as_query_engine(\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(\n                key=\"user\",\n                value=\"Ravi\",\n            )\n        ]\n    ),\n    similarity_top_k=3\n) Querying: # Jerry has Dense X Rerieval paper and should be able to answer following question.\nresponse = jerry_query_engine.query(\n    \"what are propositions mentioned in the paper?\"\n) The paper mentions propositions as an alternative retrieval unit choice. Propositions are defined as atomic expressions of meanings in text that correspond to distinct pieces of meaning in the text. They are minimal and cannot be further split into separate propositions. Each proposition is contextualized and self-contained, including all the necessary context from the text to interpret its meaning. The paper demonstrates the concept of propositions using an example about the Leaning Tower of Pisa, where the passage is split into three propositions, each corresponding to a distinct factoid about the tower. # Ravi has LLMCompiler paper\nresponse = ravi_query_engine.query(\"what are steps involved in LLMCompiler?\") LLMCompiler consists of three key components: an LLM Planner, a Task Fetching Unit, and an Executor. The LLM Planner identifies the execution flow by defining different function calls and their dependencies based on user inputs. The Task Fetching Unit dispatches the function calls that can be executed in parallel after substituting variables with the actual outputs of preceding tasks. Finally, the Executor executes the dispatched function calling tasks using the associated tools. These components work together to optimize the parallel function calling performance of LLMs. # This should not be answered as Jerry does not have information about LLMCompiler\nresponse = jerry_query_engine.query(\"what are steps involved in LLMCompiler?\") I’m sorry, but I couldn’t find any information about the steps involved in LLMCompiler in the given context. As demonstrated, if Jerry queries the system regarding a document indexed by Ravi, the system does not retrieve any answers from that document. What’s Next? We have included a  MultiTenancyRAGPack  within the LlamaPacks and  Replit template  which offers a Streamlit interface for hands-on experience. Be sure to explore it. References: Multi-Tenancy RAG with Qdrant and LlamaIndex.", "author": "Ravi Theja", "date": "Jan 15, 2024", "tags": ["Llamaindex", "Multitenancy", "LLM", "OpenAI", "AI"]},
{"title": "Querying a network of knowledge with llama-index-networks", "content": "The main premise behind RAG is the injection of context (or knowledge) to the LLM in order to yield more accurate responses from it. As such, a crucial component to a RAG system is the data source from which it gets its knowledge. It’s intuitive then to reason that the more knowledge that the RAG system can tap into, the better it would ultimately become (in terms of answering queries of potentially both depth and breadth). The spirit of this concept is not so different from that found in basically every other data-driven discipline — access to more (good) data, that is subsequently and effectively used, usually leads to better outcomes. It is with that backdrop, that we’re thrilled to announce the release of our newest latest  llama-index  library extension, called  llama-index-networks . This library extension makes it possible to build a network of RAGs built on top of external data sources and supplied by external actors. This new network paradigm allows for a new way for data suppliers to provide their data to those who want it in order to build more knowledgeable systems! In this blog post, we’ll introduce the main classes of the new extension library and show you how in just a couple lines of code, you can make your QueryEngine ready to contribute as part of a network of RAGs. We’ll also share ideas of what this can mean for how data suppliers actually supply their data to consumers within this new era of LLMs. A note on terminology: in this post, we use  llama-index-networks  to refer to the actual extension, whereas  llama-index[networks]  refers to an installation of  llama-index  that comes with the  llama-index-networks  extension. The story of Alex, Beth, and Bob An illustrative example of actors in a network and their problem statements. To illustrate how the llama-index-networks package can be used, we consider three made-up characters, Alex, Bob, and Beth, and the following scenario: Both Bob and Beth each have their own set of documents and both have already built quite the outstanding RAG systems over them (using llama-index of course!) Alex has heard about these insightful documents that both Bob and Beth have and would like to be able to query the individual RAGs built over the both of them. Bob and Beth, being as kind as they are (or, perhaps they were paid some undisclosed dollar amount), agree to give access to Alex to their RAG systems. To facilitate this new fashion of knowledge exchange, they agree to setup a network of RAGs that Alex can query. Bob and Beth build a web service over their RAGs ContributorService is built around a QueryEngine. With the  llama-index-networks  package, Bob and Beth can make their respective  QueryEngine ’s ready to participate in the network with only a few lines of code. \"\"\"Beth's contributor service file.\n\nBeth builds her QueryEngine and exposes it behind the standard\nLlamaIndex Network Contributor Service. \n\nNOTE: Bob would probably make use of Docker and cloud \ncompute services to make this production grade.\n\"\"\" \n\n from  llama_index.networks.contributor  import  ContributorService\n import  uvicorn\n\nbeth_query_engine = ...\nbeth_contributor_service = ContributorService.from_config_file(\n     \".env.contributor\" ,   # settings/secrets for the service \n    beth_query_engine\n)\n\n\n if  __name__ ==  \"__main__:\n    uvicorn.run(beth_contributor_service.app, port=8000) Bob would use similar lines of code to make his  QueryEngine  ready to contribute to any LlamaIndex network. Note, that the dotenv file  .env.contributor  contains settings for the service as well as any necessary api keys (e.g.,  OPENAI_API_KEY  or  ANTHROPIC_API_KEY ), which under the hood is implemented as REST service using FastAPI. Alex builds a NetworkQueryEngine Alex builds a NetworkQueryEngine that connects to Beth and Bob’s individual ContributorService’s. For Alex’s part, he uses the  NetworkQueryEngine  class of the  llama-index-networks  extension to be able to connect to both Beth and Bob’s  ContributorService ’s. \"\"\"Alex's network query engine.\n\nAlex builds a NetworkQueryEngine to connect to a \nlist of ContributorService’s.\n\"\"\" \n\n from  llama_index.networks.contributor  import  ContributorClient\n from  llama_index.networks.query_engine  import  NetworkQueryEngine\n from  llama_index.llms.groq  import  Groq\n\n # Use ContributorClient to connect to a ContributorService \nbeth_client = ContributorClient.from_config_file(\n    env_file= \".env.beth_contributor.client\" \n)\nbob_client = ContributorClient.from_config_file(\n    env_file= \".env.bob_contributor.client\" \n)\ncontributors = [beth_client, bob_client]\n\n # NetworkQueryEngine \nllm = Groq()\nnetwork_query_engine = NetworkQueryEngine.from_args(\n    contributors=contributors,\n    llm=llm\n)\n\n # Can query it like any other query engine \nnetwork_query_engine.query( \"Why is the sky blue?\" ) Here the dotenv files store the service parameters such as an  api_url  required to connect to the  ContributorService . Before moving on to the next section of this blog post, we’ll take the next few lines to explain a bit of what’s involved under the hood when Alex query’s his  NetworkQueryEngine . When the  query()  method is invoked (Async is also supported!), the query is sent to all contributors. Their responses are stored as new  Nodes  and a response is synthesized (with the associated  ResponseSynthesizer ). After reading that, some may notice that this is the usual pattern when working with our  QueryEngine  abstractions; and, that indeed was the point. Using a  NetworkQueryEngine  should be very similar to how you would use any other  QueryEngine  in our library! This marks the end of our little story about Alex, Bob and Beth. Before wrapping up this blog post, we first provide a few potential opportunities that may arise through the usage of  llama-index-networks . A new world of data supply and consumption RAG marketplaces is a use case that can be made possible with llama-index[networks]. One possible world that could easily be powered by  llama-index[networks]  are marketplaces for RAG. A place where data suppliers package their data in the form of RAGs to data consumers look to expand their own query system’s knowledge. Potential RAG (data) suppliers could be your local newspaper, book publishing companies, etc. In this new data supply and consumption model, there is more onus on the data supplier to prepare the data in a fashion that makes it easier to consume — specifically, the suppliers own the building of the query engine. This should benefit the data consumer greatly since, with a standard interface that is provided by the likes of a  ContributorService  (that encapsulates a  QueryEngine ), they can get access to the knowledge they seek from the data easier than ever before (i.e., in relation to traditional data marketplaces that exchange raw data). It is with this kind of vision that we’ve built  llama-index[networks]  to make it: (i) easier for data suppliers to supply the knowledge contained in their data in new and arguably more effective ways, and (ii) easier for data consumers to connect to these new forms of external knowledge. Internal networks: another potential use case In addition to powering RAG marketplaces, we foresee the need of connecting RAGs that an overarching company may own, but not necessarily manage. More concretely, a franchise may have the rights to the data across all of its operations. And, while they could build a “central”, monolithic RAG over the entire database, it may still be more efficient and effective to build RAGs over the individual operators and query these instead. The idea of exchanging information to build better, more knowledgeable systems is not new. However, the idea of using RAGs to facilitate that exchange may be (to our knowledge, it is), and we believe that both existing and new use cases requiring such collaboration can benefit from concept. A quick note on privacy An important consideration in the collaboration of data is privacy and security. It bears mentioning that the examples above assume that the data that is being shared across the network is in compliance with data privacy and security laws and regulations. It is our belief that as this technology grows, that the necessary features and capabilities will be developed and incorporated to facilitate in-compliance RAG networks. Check out the demo to learn more! To see an actual demo of a network connecting to a set of contributors, check out the Github repository  code  for  llama-index-networks  and navigate to the  examples/demo  subfolder.", "author": "Andrei", "date": "Feb 27, 2024", "tags": ["LLM", "Retrieval Augmented", "AI", "Distributed", "Machine Learning"]},
{"title": "Unlocking the 3rd Dimension for Generative AI (Part 1)", "content": "It would be an understatement to say that generative AI has been taking the world by storm over the past couple of years. While text (1D) and image (2D) models are reaching a level of quality that is truly transforming the way we create digital content, the same cannot be said for 3D models. \n 3D generative AI has made significant strides in creating 3D digital assets, with techniques such as Neural Radiance Fields (NeRFs) showing promising results for applications such as video game development. However, the limitations of current generative AI technologies become apparent when it comes to applications beyond the digital realm, especially in the context of engineering and manufacturing. After spending more than 2 years actively engaged with the state-of-the-art tools for 3D generative AI, I personally have not been able to generate a single model that I would actually want to have manufactured into a physical object. \n To put it simply: the current state of 3D generative AI is not very useful for engineers. \n\n The Motivation Behind neThing.xyz \n Our mission at  polySpectra  is to help engineers make their ideas real. The key insight that led to the invention of  neThing.xyz  (pronounced “anything dot x,y,z”) was that AI is actually quite good at writing code, and by training a “codegen” AI on domain specific languages for “code CAD”, our AI can produce code that can be rapidly converted into 3D CAD models. \n For some quick context, AI code generation tools are now achieving ~95% evaluation benchmarks against human programmers. At the current pace, I wouldn’t be surprised if an AI hits 100% in the next three weeks. ( See this leaderboard  for more details.) \n\n In 3D modeling, there is a growing buzz around “code CAD” — a term that signifies a paradigm shift in how we approach computer-aided design. Unlike traditional graphical CAD interfaces, which rely heavily on visual tools and manual user input, code CAD leverages programming to create and manipulate 3D models. This approach offers a more direct and potentially more powerful method for generating complex designs, as it allows for precision and automation that can be difficult to achieve with mouse-driven interfaces. \n  A testament to the rising prominence of code CAD was its debut this month in the Too Tall Toby speed CAD competition. In the second match of the video below,  Jern  competes using the code CAD package  Build123d , against “Mr. Alex” who is using the traditional CAD tool SolidWorks. \n \n (Jump to 1:18 for competition-grade Code CAD!) \n So my idea was simple: if AI can code, and code can CAD, why can’t AI CAD? \n\n Why RAG? \n Trying to create a 3D generative AI with this code generation approach led us to confront a significant challenge: the need for incredibly long prompts to provide the AI with enough context about our code CAD domain-specific languages. This was essential for it to stand any chance of producing working code, let alone something useful. \n I first studied Retrieval-Augmented Generation (RAG) under the tutelage of Mayo Oshin. ( Check out his RAG course! ). I knew that RAG was going to be a necessary part of the strategy for making neThing.xyz “smarter”, and through Mayo’s course I had the opportunity to meet Jerry Liu and ask him some questions about the more nuanced elements of retrieval. \n The initial version of neThing.xyz was ok, but I really had to wrangle the LLM: each query involved about 10,000 tokens. So if a user asks for “a box”, it is not 2 input tokens I’m paying for, it is 10,002 tokens. I knew I needed a more scalable approach… \n Enter the RAG-a-thon \n The announcement of the “ RAG-a-thon ” presented the perfect opportunity to quickly integrate RAG into neThing.xyz. For those familiar with the whirlwind nature of hackathons, you’ll understand when I say that time always seems to be in shorter supply than anticipated, often leading one to overestimate what can be accomplished. (For me, I usually overestimate what I can achieve by 3–10x!) With this in mind, I tried to set a modest goal for the weekend: add LlamaIndex to neThing.xyz. \n My ultimate aim was to leverage LlamaIndex to dramatically expand the corpus of documentation available to neThing.xyz. But in the theme of setting a low bar, I started by just breaking down my very large system prompt into a set of documents that LlamaIndex could retrieve from AstraDB, bringing back only the most relevant example code for a given user’s query. \n With significant assistance from Logan Markewich from the LlamaIndex team, I managed to implement RAG via LlamaIndex and AstraDB in a single day. This immediately reduced the average number of tokens per user query from about 10,000 to roughly 2,000. The impact of this was huge, resulting in an 80% cost reduction in our OpenAI bill in just one day — a change that was incredibly meaningful for us as a small business. \n While I am personally passionate about the myriad ways in which RAG can make LLMs smarter, I want to emphasize the immediate ROI that RAG provided. By simply reorganizing the same set of information for retrieval through LlamaIndex, we achieved significant cost savings with minimal effort. As an entrepreneur, an 80% reduction in costs with just eight hours of work is a deal I’d take any day of the week. \n On the final Sunday of the RAG-a-thon, I dedicated most of my time to ensuring that my demo would function correctly. The “demo gods” blessed me that day: I won first place in the “continuous innovation” track! \n\n Examples of neThing.xyz in Action: \n Text: \n \n Curves: \n \n Threads: \n \n Pipes: \n \n Lattices: \n \n What’s Next? \n My goal with neThing.xyz is to make the best 3D generative AI for engineers, with a focus on “text-to-CAD”. This is a really hard problem, and I shared some of these challenges with LlamaIndex in our recent webinar, and in more detail on  Wevolver . \n Our key objectives are to make neThing.xyz faster, smarter, and cheaper. We are leveraging LlamaIndex to orchestrate the entire RAG pipeline, and we are really excited about the amazing pace of developments in this open source community. \n What would it take to get a part like this from a natural language prompt? \n \n\n Honestly, I don’t know how to do it. \n We are just getting things off the ground and I would be tremendously excited to have you join our community. Our AI will only ever be as smart as the sum of the community that trained it, and we are excited to see what you will create! \n Please give  neThing.xyz  a try today and share your honest feedback with us via our  new community forum . Your input will play a crucial role in our ongoing development efforts, helping us to refine and improve the tool. \n Make it real. \n Raymond \n P.S. —  This article is titled Part 1 for a reason. What do you want to see in Part 2? Tell me below!", "author": "Raymond Weitekamp", "date": "Feb 29, 2024", "tags": ["LLM", "Hackathon"]},
{"title": "LlamaIndex Newsletter 2024-03-12", "content": "Salutations, LlamaIndex fans! 🦙 It's been another thrilling week in LlamaLand! With the release of Anthropic’s new models Claude-3 Opus, Sonnet, and Haiku, we have numerous tutorials, cookbooks, and updates to share with you. A quick reminder: we are running our  first ever user survey . It takes only 3 minutes and it helps us out a lot! Now let’s dive in. 🤩  The highlights: LlamaParse JSON Mode : A new feature that transforms PDF content into structured data, simplifying RAG pipeline development for complex documents containing images, text and tables.  Notebook ,  Tweet . Hierarchical Code Splitting : Enhance code understanding with a novel technique that organizes large codebases into a hierarchical structure for improved navigation and task-solving.  LlamaPack ,  Tweet . Anthropic Cookbook Series : Learn to build various LLM applications with Claude 3, ranging from simple to complex, through detailed guides and tutorials.  Cookbooks ,  Tweet . ✨ Feature Releases and Enhancements: We launched LlamaParse JSON Mode, a new feature that structures text and images from PDFs into a dict format. With the integration of multimodal models like claude-3 opus, it's now simpler to develop RAG pipelines for complex PDFs containing text, images, and tables.  Notebook ,  Tweet . We launched a novel hierarchical code splitting technique to enhance RAG/agents for code comprehension, featuring  CodeHierarchyNodeParser  by ryanpeach. This method breaks down large code files into a hierarchical structure, enabling a knowledge graph-like approach for efficient code navigation and task-solving.  LlamaPack ,  Tweet . We integrated with Videodb to run RAG over video streams using LlamaIndex. This tool allows you to upload, search, and stream videos based on spoken words or visual scenes, now available as a built-in retriever in LlamaIndex.  Tweet . 🎥 Demos: Build an AI Browser Copilot : a project by  Daniel Huynh  that demonstrates how to create a browser agent using RAG, local embeddings, and Mixtral to execute browser tasks from a Colab notebook, showcased with a video on navigating HuggingFace datasets. RAG over your code : a project by  Akshay  on creating a local code assistant using LlamaIndex, MistralAI, and Streamlit to index and query GitHub repositories, offering a foundational guide for advanced code QA. https://nething.xyz/:  a project by Raymond Weitekamp on generating production-ready 3D CAD models from text prompts. It uses LLM code generation to create commands to generate printable 3D objects from text prompts using LlamaIndex.  Blog ,  Tweet . 🗺️ Guides: Guide  to the Anthropic Cookbook Series: Create context-augmented LLM apps using Claude 3, from basic RAG to advanced agents, through six notebooks and four videos. Video  guide  exploring diverse applications of Claude-3 with LlamaIndex. tooling, covering Vanilla RAG, routing, sub-question query planning, structured data extraction, text-to-SQL, and agents—a perfect starter kit for Claude enthusiasts. ✍️ Tutorials: Tutorial  by  Ankush k Singal  on building local LLM agents with Llama.cpp for step-wise execution and incorporating human feedback during execution. “RAG over Complex PDFs V2\" : a comprehensive tutorial by  AI Makerspace  on crafting advanced RAG pipelines for handling messy PDFs with LlamaParse and LlamaIndex, where naive RAG falls short. Tutorial  on building a real-time RAG chatbot using Google Drive and Sharepoint by  Anup Surendran  and  Berke Can Rizai . Step-by-step  tutorial  to improve the quality of your RAG application using JinaAI reranker, LlamaIndex, and MistralAI. 🎥  Webinars: Webinar  with  Parth Sarthi , lead author of RAPTOR - Tree-Structured Indexing and Retrieval. 📅 Events: We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.", "author": "LlamaIndex", "date": "Mar 12, 2024", "tags": ["LLM", "Newsletter", "Rag"]},
{"title": "Launching the first GenAI-native document parsing platform", "content": "Our mission at LlamaIndex is to connect the world’s data to the power of LLMs, and today we’re pleased to announce our latest big step towards that goal with the world’s first GenAI-native document parsing platform, LlamaParse. We launched the first public version of LlamaParse  3 weeks ago  and the response has been huge with well over 2,000 users parsing over 1 million pages! We’ve been hard at work releasing hundreds of bug fixes and new features since then, and today we’re releasing a game-changing new feature,  GenAI-powered parsing instructions . Using LLMs for world-class parsing The key insight behind parsing instructions is that  you know what kind of documents you have , so you already know what kind of output you want. Why make the parser guess when an LLM-enabled parser can take simple, natural-language instructions from you and provide radically better parsing results? Example 1: rich table support Since we first released LlamaParse it has featured  industry-leading table extraction  capabilities. Under the hood, this has been using LLM intelligence since the start. It seamlessly integrates with the advanced indexing/retrieval capabilities that the open-source framework offers, enabling users to build state-of-the-art document RAG. Now with JSON mode (see below) and parsing instructions, you can take this even further. Example 2: parsing comic books Parsing translated manga presents a particular challenge for a parser since a regular parser interprets the panels as cells in a table, and the reading order is right-to-left even though the book is in English, as shown in this extract from \"The manga guide to calculus\", by Hiroyuki Kojima: Using LlamaParse, you can give the parser plain, English-language instructions on what to do: The provided document is a manga comic book. \nMost pages do NOT have title. It does not contain tables. \nTry to reconstruct the dialogue happening in a cohesive way. (You can see the full code in our  demonstration notebook , including what it looks like to parse this without the instructions) The result is a perfect parse! # The Asagake Times\n\nSanda-Cho Distributor\n\nA newspaper distributor?\n\nDo I have the wrong map? Example 3: mathematical equations Another challenging format for parsing is complex mathematical equations (by coincidence, the manga we picked as an example is all about how to do mathematics): To parse this, we take the same instructions as before and add one sentence:  Output any math equation in LATEX markdown (between $$)  . The result of parsing is clear LaTeX instructions, which render the equations perfectly: Anything an LLM can do, our parser can do You can use this kind of natural-language instruction to do all sorts of advanced pre-processing on your documents — simplify language, include sentiment analysis, translate them to another language! We can’t wait to see what you do with the power of LlamaParse. JSON mode Parsing instructions are definitely the headline feature, but we have dozens of other features new to LlamaParse since launch. A standout is JSON mode, a rich programmatic format perfect for when you want more precision about exactly what you want to parse out. JSON mode’s output includes the full structure of the document that was parsed tables, text and headings marked tables are available as CSV and JSON images are marked and available for extraction (see below) a wealth of metadata about each node If you are building a custom RAG strategy JSON mode gives you everything you need to build it. Check out our  JSON mode examples! Image extraction One of the best features of JSON mode is image extraction: every page that contains images comes with a list of images, marked up with metadata including their size and position on the page, and you can  retrieve these images directly  and  include them in your indexing  to extract even more information from your complex, image-heavy documents. Expanded document types We launched LlamaParse with exceptional support for PDFs, and we have continued to expand its capability every day. We’ve also added support for a large array of document types: Microsoft Word (.doc, .docx) Microsoft PowerPoint (.pptx) Rich Text Format (.rtf) Apple Pages (.pages) Apple Keynote (.key) ePub books (.epub) And dozens more! All of these document types “just work” without any additional work on your part, and we are constantly expanding the list of supported file types. Check out this  demo notebook  where we demonstrate parsing a PowerPoint file. And one more thing… unlimited parsing! The huge demand for LlamaParse has included many people asking to go beyond our free daily limits via paid plans, and we’re happy to answer those requests. Our pricing is simple: 7000 pages/week are free Additional pages are $0.003/page, or $3 per 1000 pages Maximum size for one document is 750 pages And of course we retain our generous free tier of 1000 pages/day. The public version of LlamaParse is a hosted service. If you want to extend LlamaParse capabilities to build advanced document RAG, or wish to deploy LlamaParse in a private cloud,  get in touch.", "author": "LlamaIndex", "date": "Mar 13, 2024", "tags": ["LLM", "Parsing", "LlamaParse"]},
{"title": "PII Detector: hacking privacy in RAG", "content": "A couple of days ago at the DataStax HQ, I had the chance to participate at the LlamaIndex RAG-A-THON. Over the span of the weekend, we had to implement a solution that leverages Retrieval Augmented Generation (RAG) technique. Because of my background in cybersecurity, I was leaning towards the security pitfalls and obstacles of the RAG technique. One of the first things that came to mind was the fact that a lot of the unstructured data used is unsanitized and can contain sensitive data. PII: What? Why? PII stands for Personally Identifiable Information. It refers to any information that can be used to identify a specific individual. This can be names, addresses, phone numbers, email addresses, social security numbers, and financial information. There are a couple of reasons why handling PIIs is important: Privacy:  PII often includes sensitive and private details (like addresses), so protecting it preserves customers’ privacy. Identity Theft:  PII can also lead to identity theft (e.g. one’s social security number gets compromised). Legal Compliance:  Protecting PII is also the law. Many countries and regions have enacted laws and regulations that require organizations to protect PII. GDPR (General Data Protection Regulation) in the EU or HIPAA (Health Insurance Portability and Accountability Act) in the United States govern the way we handle PII. Trust and Reputation:  A data breach or mishandling of PII will severely damage one’s reputation and trust. Financial Security:  PII may include financial information, such as credit card numbers and banking details. Compromised PII can lead to fraudulent transactions. National Security Concerns:  All of the above are crucial in sovereign environments. PII in RAG Everything listed is applicable to almost all applications leveraging RAG. Remember that the RAG technique contains two components — the model and the vector database. For this reason, each of these components need to address PII. Model Language models, are trained on large datasets that may contain real-world data, potentially including PII and customer data. When the models generate text, there is a risk that they’ll produce content that includes PII. This is even more crucial if you’re creating a multi-tenant application, and you want to prevent data leak. This risk can be mitigated by either filtering or anonymizing the response. Training the models on anonymized data that is stripped of any sensitive information is the better approach to prevent leaks of PII. Vector Database Vector databases, just like regular databases should not persist sensitive information plainly. This kind of information should only be persisted using encryption, hashing, salt and access controls. Having said that, one should also make sure that the similarity search returned by the Database won’t retrieve personal data. On top of that, various regulations such as GDPR and HIPAA still apply here. So, if the original data contain PII, you might need to add another instance in Europe or any additional region in accordance with regulations. Persisted data should be encrypted or hashed (and additionally salted). Introducing: Presidio Presidio  is an open-source library maintained by Microsoft (see our  GitHub  repo). It’s derived from the Latin word praesidium which means “protection” or garrison. It enables organizations to preserve privacy using a unified SDK. It provides fast  identification  and  anonymization  modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more. Disclaimer: Nothing is bulletproof. It’s your responsibility to make sure sensitive data is anonymized. How Does Presidio Work? Predefined  or  custom PII recognizers  leverage  Named Entity Recognition (NER) ,  regular expressions ,  rule-based logic  and  checksum  (e.g. bitcoin address validation). It’s extensible, so you can add your own entities and your own detection mechanisms. It’s customizable, so you can create your own anonymizers, and exclude/include certain entities (e.g. exclude anonymization of geographical locations). LlamaIndex Post Processors There was already some PII integration using NER models and LLMs! These were implemented as post processors that run in the end of the pipeline: from  llama_index.postprocessor  import  NERPIINodePostprocessor\n from  llama_index  import  ServiceContext\n from  llama_index.schema  import  TextNode\n\ntext =  \"\"\"\nMy name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in Amsterdam.\nHave you been to a Pálmi Einarsson concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? Bob, it's Bob.\nMy great great grandfather was called Yulan Peres, \nand my great great grandmother was called Jennifer Holst\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.FilmFranchise.dk/\n\"\"\" \n\nnode = TextNode(text=text)\n\nservice_context = ServiceContext.from_defaults()\nprocessor = NERPIINodePostprocessor(service_context=service_context)\n\n from  llama_index.schema  import  NodeWithScore\n\nnew_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\n print (new_nodes[ 0 ].node.get_text()) Running the above code resulted in the following: My name is [PER_12] and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in [LOC_123].\nHave you been to a [PER_153] concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? [PER_286], it's [PER_286].\nMy great great grandfather was called [PER_339], \nand my great great grandmother was called [PER_395]\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.[ORG_521].dk/ As can be seen in this example, while NER models do a decent job in detecting PII, they might miss some entities such as IBAN code, credit card numbers, emails, medical license and more. Presidio detects more out of the box entities than traditional models. This is possible because Presidio leverages a couple of methods in detecting PII — from NER models to regular expressions and rule-based logic. Integrating Presidio with LlamaIndex I ended up integrating  PresidioPIINodePostprocessor  that got the text as an input and masked it. Doing this was possible using Presidio’s analyzer and anonymizer: from  presidio_analyzer  import  AnalyzerEngine\n from  presidio_anonymizer  import  AnonymizerEngine\n\nanalyzer = AnalyzerEngine(supported_languages=[ \"en\" ])\nresults = analyzer.analyze(text=text, language= 'en' )\nengine = AnonymizerEngine()\nnew_text = engine.anonymize(text=text, analyzer_results=results) This was pretty fun and simple. However, given the input text “Alice and Bob are friends”, the output would be: “<PERSON> and <PERSON> are friends”. I could not have that. So, I added a counter and mapped the original values with the masked values, making sure that whenever an entity was seen again, the previously asked value was used: def   anonymize_function ( origin, entity_type ):\n     nonlocal  pii_counter\n     nonlocal  inverted_mapping\n     nonlocal  mapping\n     if  entity_type  not   in  inverted_mapping:\n        inverted_mapping[entity_type] = {}\n    typed_mapping = inverted_mapping[entity_type]\n     if  origin  in  typed_mapping:\n         return  typed_mapping[origin]\n    new_value =  f\"< {entity_type} _ {pii_counter} >\" \n    typed_mapping[origin] = new_value\n    mapping[new_value]=origin\n    pii_counter+= 1 \n     return  typed_mapping[origin]\n\n from  presidio_analyzer  import  AnalyzerEngine\n from  presidio_anonymizer  import  AnonymizerEngine\n from  presidio_anonymizer.entities  import  OperatorConfig\n\nanalyzer = AnalyzerEngine(supported_languages=[ \"en\" ])\nresults = analyzer.analyze(text=text, language= 'en' )\nengine = AnonymizerEngine()\nnew_text = engine.anonymize(text=text, analyzer_results=results, \n                            operators={ \"DEFAULT\" : OperatorConfig( \"custom\" , \n                            params={ \"lambda\" : anonymize_function})}) Note: Currently presidio doesn’t contain the entity type as an input parameter in the lambda function, so I had to add this functionality. Test and Benchmark Once this was all up and running, I was able to call the newly added presidio post processor with the text from the previous run: from  llama_index.postprocessor  import  PresidioPIINodePostprocessor\n from  llama_index  import  ServiceContext\n from  llama_index.schema  import  TextNode\n\ntext =  \"\"\"\nMy name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in Amsterdam.\nHave you been to a Pálmi Einarsson concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? Bob, it's Bob.\nMy great great grandfather was called Yulan Peres, \nand my great great grandmother was called Jennifer Holst\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.FilmFranchise.dk/\n\"\"\" \n\nnode = TextNode(text=text)\n\nservice_context = ServiceContext.from_defaults()\nprocessor = PresidioPIINodePostprocessor(service_context=service_context)\n\n from  llama_index.schema  import  NodeWithScore\n\nnew_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\n print (new_nodes[ 0 ].node.get_text()) Running the above code resulted in the following: My name is <PERSON_12> and my credit card number is <CREDIT_CARD_11>. \nMy email is <EMAIL_ADDRESS_10> and I live in <LOCATION_9>.\nHave you been to a <PERSON_8> concert before?\nWhat is the limit for card <CREDIT_CARD_7>? My IBAN is <IBAN_CODE_6>. \nWhat's your last name? <PERSON_5>, it's <PERSON_5>.\nMy great great grandfather was called <PERSON_4>, \nand my great great grandmother was called <PERSON_3>\nI can't browse to your site, keep getting address <IP_ADDRESS_2> blocked error\nJust posted a photo <URL_1> Overall Presidio detected 12 entities while the other NER solution detected 8. Notice that credit card numbers, email address, IBAN, IP address and the URL (at least some of it) weren’t detected. I was curious to see how the parsing of these strings would work on the LLM, so I populated the index and queried the following: from  llama_index  import  VectorStoreIndex\n\nindex = VectorStoreIndex([n.node  for  n  in  new_nodes])\nresponse = index.as_query_engine().query(\n     \"What is my name?\" \n)\n print (response) Which resulted in: Your name is <PERSON_12>. How It Ended Anyway, this project won the 3rd place (in the continuous track) in the RAG-A-THON. Note: this picture doesn’t contain PII Update Presidio is now fully integrated into LlamaIndex as a post processor, follow this  notebook  to learn how to use Presidio for PII masking. The next steps would be to add more customization and anonymization options.", "author": "Roey Ben Chaim", "date": "Mar 13, 2024", "tags": ["Hackathon", "LLM", "Privacy"]},
{"title": "One-click Open Source RAG Observability with Langfuse", "content": "This is a guest post from the team at Langfuse There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once. Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your  production  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point? Thus, we started working on  Langfuse.com  ( GitHub ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends’ problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either  self-host  Langfuse or use the  cloud instance  maintained by us. We are thrilled to announce our new integration with LlamaIndex today. This feature was  highly requested  by our community and aligns with our project's focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase! The challenge We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let’s take this simple example of a VectorStoreIndex and a ChatEngine. from  llama_index.core  import  SimpleDirectoryReader\n from  llama_index.core  import  VectorStoreIndex\n\ndocuments = SimpleDirectoryReader( \"./data\" ).load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nchat_engine = index.as_chat_engine()\n\n print (chat_engine.chat( \"What problems can I solve with RAG?\" ))\n print (chat_engine.chat( \"How do I optimize my RAG application?\" )) In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine. This is awesome to get started, but we quickly run into questions like: “What context is actually retrieved from the index to answer the questions?” “How is chat memory managed?” “Which steps add the most latency to the overall execution? How to optimize it?” One-click OSS observability to the rescue We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager. Preparation Install the community package (pip install llama-index-callbacks-langfuse) Copy/paste the environment variables from the Langfuse project settings to your Python project: 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' and 'LANGFUSE_HOST' Now, you only need to set the global langfuse handler: from  llama_index.core  import  set_global_handler\n\nset_global_handler( \"langfuse\" ) And voilá, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns. Group multiple chat threads into a session Working with lots of teams building GenAI/LLM/RAG applications, we’ve continuously added more features that are useful to debug and improve these applications. One example is  session tracking  for conversational applications to see the traces in context of a full message thread. To activate it, just add an id that identifies the session as a trace param before calling the chat_engine. from  llama_index.core  import  global_handler\n\nglobal_handler.set_trace_params(\n  session_id= \"your-session-id\" \n)\n\nchat_engine.chat( \"What did he do growing up?\" )\nchat_engine.chat( \"What did he do at USC?\" )\nchat_engine.chat( \"How old is he?\" ) Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing: Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces. Trace more complex applications and use other Langfuse features for prompt management and evaluation This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable. We have also built additional features to version control and collaborate on prompts (langfuse  prompt management ), track  experiments , and  evaluate  production traces. For RAG specifically, we collaborated with the RAGAS team and it’s easy to run their popular eval suite on traces captured with Langfuse (see  cookbook ). Get started The easiest way to get started is to follow the  cookbook  and check out the  docs . Feedback? Ping us We’d love to hear any feedback. Come join us on our  community discord  or add your thoughts to this  GitHub thread .", "author": "Langfuse", "date": "Mar 18, 2024", "tags": ["LLM", "Observability"]},
{"title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM", "content": "Generative AI is rapidly transforming the global economy. Enterprises are increasingly looking to adopt generative AI to drive business transformation, but face challenges around protecting IP, ensuring security and compliance, and moving smoothly from proof of concept to production. That’s why LlamaIndex is excited to announce that it is integrated with  NVIDIA NIM inference microservices  to help enterprises seamlessly deploy generative AI at scale. NVIDIA NIM, part of the  NVIDIA AI Enterprise  software platform, optimizes inference on more than two dozen popular AI models from NVIDIA and its partner ecosystem. LlamaIndex is an open-source tool for connecting your data to LLMs and extracting valuable insights. By integrating NVIDIA NIM runtimes with LlamaIndex’s data connection capabilities, enterprises will be able to: Connect generative AI models hosted using NVIDIA NIM to their own proprietary data sources, allowing them to generate accurate model outputs while keeping sensitive data secure Search across and extract insights from both structured and unstructured enterprise data to enhance the knowledge and accuracy of AI models Build data-enriched generative AI applications for use cases like enterprise search, question answering, analytics, and more For developers, using NVIDIA NIM with LlamaIndex provides a seamless path from experimentation to production. They can access the NIM microservices from the newly launched NVIDIA API catalog to quickly build AI applications using industry-standard protocols, and then easily transition those applications to work on their self-hosted NVIDIA NIM instance for enhanced security, customization, and cost effectiveness at scale. The combination of NVIDIA NIM for optimized model inference and LlamaIndex’s data connection helps unlock the full potential of enterprise-level generative AI. We look forward to seeing the innovative applications that emerge from this integration as more organizations embrace AI to transform their business. You can expect an in-depth technical blog post about how to use NVIDIA NIM and LlamaIndex very soon!", "author": "LlamaIndex", "date": "Mar 18, 2024", "tags": ["Nvidia"]},
{"title": "LlamaIndex Newsletter 2024-03-19", "content": "Greetings, LlamaIndex enthusiasts! 🦙 Welcome to another exciting weekly update from the world of LlamaVerse! We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our  launch post  for all the details! 🤩  The highlights: New observability with Instrumentation:  Enhanced developer workflow with a new Instrumentation module for improved observability.  Docs ,  Tweet . LlamaParse accepts natural language parsing instructions : Easily extract math snippets from PDFs into LaTeX with LlamaParse.  Blogpost ,  Tweet . Financial Data Parsing:  Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis.  Notebook ,  Tweet . ✨ Feature Releases and Enhancements: We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application.  Docs ,  Tweet . We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline.  Blogpost ,  Tweet . We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data.  Notebook ,  Tweet . We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code.  Blogpost ,  Docs ,  Tweet . Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts.  LlamaPack ,  Tweet 🎥 Demos: Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency.  Blogpost ,  Code ,  Tweet . 🗺️ Guides: Guide  to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique. ✍️ Tutorials: Thomas Reid ’s  tutorial  on using LlamaParse can help properly extract text from a Tesla quarterly filings. Sudarshan Koirala   video tutorial  on RAG with LlamaParse, Qdrant, and Groq. Kyosuke Morita  tutorial  showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex. Cobus Greyling   tutorial  on Agentic RAG: Context-Augmented OpenAI Agents. Roey Ben Chaim ’s  tutorial  on PII Detector: hacking privacy in RAG. 🎥  Webinars: Webinar  with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT 📅 Events: We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.", "author": "LlamaIndex", "date": "Mar 19, 2024", "tags": ["LlamaParse", "AI", "LLM", "Newsletter"]},
{"title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations", "content": "This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‘vibes’. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what’s affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: “With increased complexity comes more points of failure.”. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let’s look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website", "author": "Uptrain", "date": "Mar 19, 2024", "tags": ["AI", "Evaluation", "Rag"]},
{"title": "LlamaIndex Newsletter 2024–02–20: introducing LlamaCloud", "content": "Hi there, LlamaIndex Enthusiasts 🦙, Today marks a milestone for the LlamaIndex ecosystem with the  introduction of LlamaCloud , a next-generation suite of managed parsing, ingestion, and retrieval services tailored for  production-grade   context augmentation  in your LLM and RAG applications. As an enterprise AI engineer using LlamaCloud, you can concentrate on crafting the business logic, leaving the heavy lifting of data management to us. Process vast amounts of production data effortlessly, enhancing response quality instantly. LlamaCloud debuts with: LlamaParse:  A specialized parsing service for complex documents, including tables and figures, seamlessly integrated with LlamaIndex for handling semi-structured documents. This enables answering intricate queries previously out of reach. Managed Ingestion and Retrieval API:  Simplify data loading, processing, and storage for your RAG applications, supported by over 150 data sources via  LlamaHub , including LlamaParse, and more than 40 data storage solutions. LlamaParse is now in public preview, with a current focus on PDFs and a usage cap for public users;  contact us  for commercial terms. The managed API is in private preview, and available to a select group of enterprise partners. Interested?  Reach out  for more details. Your inventive spirit is our driving force! We look forward to discovering the projects, articles, or videos that excite you. Send your outstanding contributions to  news@llamaindex.ai . If you haven’t yet, join our newsletter via our  website  to get all the newest LlamaIndex news directly in your inbox. 🤩  The highlights: Corrective RAG LlamaPack:  We have launched LlamaPack with CRAG insights, refining information retrieval for enhanced accuracy and relevance.  Tweet ,  LlamaPack . SELF-DISCOVER LlamaPack : We have launched SELF-DISCOVER paper implementation as LlamaPack, leveraging meta-reasoning in LLMs for adaptive, complex problem-solving.  Tweet ,  LlamaPack . RAG Production  Guide :  A comprehensive guide to production-ready RAG, featuring insights and strategies from Sisil Mehta at JasperAI. ✨ Feature Releases and Enhancements: We have integrated insights from the Corrective Retrieval Augmented Generation (CRAG) paper as LlamaPack, enhancing our system’s ability to evaluate and refine retrieved information for more accurate and relevant responses.  Tweet ,  LlamaPack . We have integrated SELF-DISCOVER a novel approach as LlamaPack to empower LLMs with meta-reasoning, allowing them to self-discover and adapt the most suitable reasoning modules from a selection, enabling more versatile and complex problem-solving capabilities.  Tweet ,  LlamaPack . 🎥 Demos: DanswerAI , an out-of-the-box ChatGPT integration for enterprise knowledge, enhances efficiency across sales, IT, engineering, and customer support teams by connecting to common workplace tools like GDrive, Slack, and Jira, powered by Llama Index. GenAI for ADU Planning:  A comprehensive  app  by  Rujun Gao  that navigates local ADU regulations, analyzes buildable space via satellite imagery, offers floor plan suggestions, and connects users to local contractors, showcasing the power of multi-modal AI automation in enhancing productivity 🗺️ Guides: Guide  to building production-ready RAG covering practical tips and tricks inspired by  Sisil Mehta  from JasperAI. Guide  to Nomic-embed-text-v1.5 with LlamaIndex: Get embeddings of any dimension from 64 to 768, inspired by Matryoshka Representation Learning. Guide  to creating a RAG-powered research agent, notebook on building a workflow for scientific investigation, leveraging ArXiv, Wikipedia, textbooks, and more, with capabilities for fetching abstracts, generating ideas, and comprehensive information lookup, powered by Llama Index and LionAGI. ✍️ Tutorials: Jerry Liu   tutorial  on Introduction to LlamaIndex v0.10. Jerry Li   tutorial  on Building Agents from scratch using Query Pipelines. Ravi Theja   tutorial  video on Building Multi-Modal applications with Ollama and LlamaIndex. Brett Young  tutorial  on Building a RAG-Based Digital Restaurant Menu with LlamaIndex and W&B Weave. Raghav   tutorial  on MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB. Tech With Tim   tutorial  on querying unstructured data, analyzing tabular data with Pandas, and actioning results in a concise, step-by-step approach. Florian June ’s  tutorial  on using RAGAs and LlamaIndex for RAG evaluation. Davide Gallitelli ’s  tutorial  on Deploying a HuggingFace embedding model to Amazon SageMaker and consuming it with Llama-Index. 🎥  Webinar: Webinar  with Henry Heng, co-founder of Flowise on building advanced no-code RAG apps over your data.", "author": "LlamaIndex", "date": "Feb 20, 2024", "tags": ["Llamaindex", "Llamacloud", "Newsletter", "LLM", "AI"]},
{"title": "LlamaIndex Newsletter 2024–02–27", "content": "Yo, LlamaIndex Fans 🦙, Dive into a week brimming with thrilling developments at LlamaIndex! The dynamic input from our community and our rich selection of learning materials are all set to enhance your journey with LlamaIndex. Last week, the LlamaIndex ecosystem took a significant leap forward with the launch of LlamaCloud, a suite of advanced services designed for  production-level   context enhancement  in LLM and RAG applications: LlamaParse:  Offers sophisticated parsing for complex documents, making it possible to answer detailed queries. Managed Ingestion and Retrieval API:  Facilitates easier data management, connecting with over 150 sources and 40+ storage solutions. LlamaParse is now available for a public preview, primarily focusing on PDFs with a user cap, while the API is in a private preview for select enterprise partners. If you haven’t explored these new features yet, we invite you to  check them out  for more details or to discuss commercial terms. Your innovation inspires us! We’re eager to see the projects, articles, or videos that inspire you. Share your remarkable works with us at  news@llamaindex.ai . And if you haven’t already, subscribe to our newsletter on our website to receive the latest LlamaIndex updates straight to your inbox. 🤩  The highlights: Enhanced RAG Retrieval with Sub-Document Summaries:  Introducing a novel chunking method that improves RAG performance by incorporating hierarchical metadata into chunks, ensuring precise and context-aware information retrieval.  Notebook ,  Tweet . MistralAI Cookbook:  A comprehensive guide to leveraging the Mistral-Large model from MistralAI, featuring near-GPT-4 reasoning, function calling, and JSON output for cutting-edge applications.  Docs ,  Tweet . Gemma Cookbook:  A comprehensive guide to using Gemma, GoogleDeepMind’s latest LLM offering, with options for 2B and 7B parameters, facilitating the development of local RAG systems on your laptop.  Notebook ,  Tweet . ColBERT Integration:  Document reranking with ColBERT via LlamaIndex, delivering a solution that is about 100x faster than BERT-based models for more efficient data processing.  Notebook ,  Tweet . Counselor Copilot — Social Impact Through RAG:  Spotlight on Counselor Copilot, an innovative RAG project supporting the Trevor Project’s crisis counselors, providing real-time assistance with context, suggestions, and actions to aid LGBTQ+ youth effectively.  BlogPost ,  Tweet . ✨ Feature Releases and Enhancements: We have launched a new chunking strategy to enhance RAG retrieval: Sub-Document Summaries. This approach overcomes the limitations of naive chunking by injecting hierarchical metadata, offering a nuanced balance of global context awareness and precision through subdocument summaries for improved performance.  Notebook ,  Tweet . We have launched a cookbook for the latest  mistral-large  model from MistralAI offering advanced features like near GPT-4 level reasoning, Function calling, JSON Output, and more.  Docs ,  Tweet . We have launched a cookbook on Gemma, a new family of state-of-the-art LLMs by GoogleDeepMind, with 2B and 7B parameter options using Ollama to build local RAG on your laptop.  Notebook ,  Tweet . We have introduced ColBERT through LlamaIndex, offering a one-line integration for a reranking model that’s ~100x faster than traditional BERT-based models, ensuring efficient document handling with superior performance.  Notebook ,  Tweet . We have introduced a way to integrate advanced RAG into full-stack web apps with create-llama, using LlamaPacks, in just two lines of code.  create-llama ,  Tweet . 🎥 Demos: Counselor Copilot : An interesting RAG project by  Riya Jagetia  and team, designed to assist crisis counselors at the Trevor Project in supporting LGBTQ+ youth. This tool acts as a real-time copilot, offering context, suggested replies, and various actions to enhance counselor effectiveness, showcasing a unique and socially impactful application of advanced RAG techniques.  BlogPost ,  Tweet . 🗺️ Guides: Guide  to simplifying advanced RAG development: Our latest insights pinpoint solutions for key challenges, including our innovative LlamaParse for complex PDF QA, shared in our AI in Production presentation. ✍️ Tutorials: Marco Bertelli   tutorial  on Building an Interactive Chatbot with React. Wenqi Glantz   tutorial  on The Journey of RAG Development: From Notebook to Microservices. Wenqi Glantz   tutorial video  on 12 RAG Pain Points and Solutions in the RAG pipeline. 🎥  Webinar: Webinar  with Sisil from JasperAI on Practical Tips and Tricks for Productionizing RAG.", "author": "LlamaIndex", "date": "Feb 27, 2024", "tags": ["LLM", "AI", "Rag", "Newsletter", "Llamaindex"]},
{"title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI", "content": "This is a guest post by Protect AI. We believe that RAG will be one of the preferred approaches for enterprises when developing LLM applications to generate prompt responses that are more relevant, and accurate, tailored to and based on company-specific content. However, while analyzing web pages with ChatGPT may leave the LLM vulnerable to injections embedded within the webpage, it is crucial to recognize that injections may also be concealed within the vector database or knowledge graph where data is retrieved and injected into the LLM. That is why we’re thrilled to describe how LLM Guard by Protect AI can secure your data sources accessed for context in your LLM application, built with LlamaIndex. LLM Guard  is an open source solution by  Protect AI  designed to fortify the security of Large Language Models ( LLMs ). It is designed for easy integration and deployment in production environments. It provides extensive security scanners for both prompts and responses of LLMs to detect, redact, and sanitize against adversarial prompt attacks, data leakage, and integrity breaches (e.g. offensive content, hallucination). LLM Guard was built for a straightforward purpose: despite the potential of LLMs, corporate adoption has been hesitant. This reluctance stems from the significant security risks and a lack of control and observability of implementing these technologies. With over 2.5M downloads of its models, and a Google Patch Reward, LLM Guard is the open source standard and market leader in LLM security at inference. Secure RAG with LlamaIndex In the following  example , we showcase a practical approach to improve the security of your RAG application. Specifically, we will explore a RAG application designed to facilitate the automated screening of candidate CVs by HR teams. Within the batch of CVs, there exists a diverse pool of candidates, including one who lacks experience and consequently is not the most suitable candidate. The nature of the attack manifests as an embedded prompt injection within the CV of this particular candidate, concealed in white text, rendering it challenging to detect with the naked eye. In the notebook example , we conducted the attack initially and then repeated the process subsequent to fortifying the application with LLM Guard. With this example, we show how you can use LLM Guard and LlamaIndex for both input and output scanning of documents to detect any malicious content. Although ideally, we should scan documents before the  ingestion , for simplicity in the example, we chose to do scanning during  retrieval . In real-use cases, it's critical to do the scanning both during retrieval of real-time data from APIs (not vector stores) which we still need to verify as it can contain poisoned sources of information. For output scanning, it can simply be done by taking the results generated by LlamaIndex and running them through LLM Guard. llm = OpenAI(model= \"gpt-3.5-turbo\" , temperature= 0.1 , output_parser=output_parser)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    transformations=transformations,\n    callback_manager=callback_manager,\n)\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\ninput_scanners = [\n    Anonymize(vault, entity_types=[ \"PERSON\" ,  \"EMAIL_ADDRESS\" ,  \"EMAIL_ADDRESS_RE\" ,  \"PHONE_NUMBER\" ]), \n    Toxicity(), \n    PromptInjection(),\n    Secrets()\n]\n\nllm_guard_postprocessor = LLMGuardNodePostProcessor(\n    scanners=input_scanners,\n    fail_fast= False ,\n    skip_scanners=[ \"Anonymize\" ],\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k= 3 ,\n    node_postprocessors=[llm_guard_postprocessor]\n)\nresponse = query_engine.query( \"I am screening candidates for adult caregiving opportunity. Please recommend me an experienced person. Return just a name\" )\n print ( str (response))\n LLM Guard protects your LLM applications As demonstrated in the practical example of securing an HR screening application with  LLM Guard , the significance of mitigating potential attacks, cannot be overstated. Besides that, as LLMs evolve rapidly and embed advanced capabilities like agency and multi-modality, the complexity and impact of potential breaches escalate significantly. Thus, prioritizing RAG security becomes not just a necessity but rather fundamental in safeguarding against increasingly sophisticated threats and ensuring the integrity of critical enterprise LLM applications. Try out LLM Guard by going to our  library  or  documentation . Also,  join our Slack  channel for any questions!", "author": "Protect AI", "date": "Mar 20, 2024", "tags": ["Security", "Privacy"]},
{"title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot", "content": "Co-authored by:  Riya Jagetia ,   Tarun Malik ,   Divija N ,   Sharon Tan ,   Zehra Rizvi ,  Amanda Piyapanee At the recent LlamaIndex RAG-a-thon [1], our team’s  “Counselor Copilot”  won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our  DevPost  [2] writeup. Introduction Against the backdrop of growing strain on mental health services [3, 4], non-profit organizations like The Trevor Project [5] are a critical part of the care ecosystem. Focusing on helping LGBTQ+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors. Problem: The Dual Challenge Faced by Crisis Counselors However, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care. In light of these challenges, there’s a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services. While our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well. The Winning Solution: An AI Copilot for Crisis Counselors Counselor Copilot is a real-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care.  There is no prompting that is needed from counselors; the copilot works seamlessly in the background. Further, the copilot never directly replies to contacts; instead, replies are suggested and can be edited. Counselor copilot takes into account contact context and chat history to provide real-time reply suggestions to the counselors Specifically, the copilot automates counselor tasks that include but are not limited to: Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated. Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor. Using existing resources and guidelines from the organization to suggest appropriate replies. 4. Searching for location-specific resources for contacts, and quickly sharing those resources via email. 5. Completing case forms in a CRM for contacts, including summarizing the interaction. While these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time. With Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher-quality conversations with patients. Below is a demo of our solution: How we built it When the chat is initiated, the Counselor Copilot gets the contact’s data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real-time and then provide a summary of that data to counselors as context at the beginning of each conversation. Further, we used a LlamaIndex ReAct Agent to monitor the conversation and — based on the chat history and contact context — deploy the right tool. Tools at the ReAct Agent’s disposal include: Escalating the conversation to a supervisor Suggesting a response and related resources based on The Trevor Project’s guidelines Searching the web for location-specific resources and sending the resources to the contact For tool #2, we created a vector database that contains The Trevor Project’s documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation. Lastly, we used the conversation content to fill out a form with key Salesforce fields (e.g. name, age, city, state), as well as to summarize the conversation. Possible Extensions We’re excited by the potential for others to build on our work [6] and extend Counselor Copilot further. Some ideas include: Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselors More targeted conversation management: Add a tool for the agent to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment) Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent training Conclusion: A Step Toward Efficient and Effective Crisis Care Our AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need​​. References https://rag-a-thon.devpost.com/ https://devpost.com/software/counselor-copilot https://www.mhanational.org/issues/state-mental-health-america https://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-services https://www.thetrevorproject.org/ https://github.com/zrizvi93/trevorhack", "author": "Sharon Tan", "date": "Feb 24, 2024", "tags": ["Llamaindex", "Mental Health", "Retrieval Augmented", "Generative Ai Use Cases", "LLM"]},
{"title": "LlamaIndex Newsletter 2024-03-26", "content": "Hi there, LlamaIndex followers! 🦙 Welcome to another thrilling weekly update from the LlamaUniverse. We're excited to bring you a fantastic array of updates, including Privacy-Preserving In-Context Learning with LlamaPacks and RAG Networks. Dive into our guides on MistralAI, explore Gemma LLMs, and enjoy a plethora of engaging tutorials using LlamaIndex, alongside upcoming webinars and events. 🤩  The highlights: Privacy-Preserving In-Context Learning:  Leveraging  Xinyu Tang ’s   paper , we've introduced LlamaPack for LLM/RAG apps, enabling the creation of few-shot demonstrations that maintain privacy and data integrity.  LlamaPack ,  Tweet . Privacy-Preserving RAG Network:  We present Privacy-Preserving RAG Network which facilitates the use of confidential datasets in healthcare and online platforms while safeguarding user privacy.  Blogpost ,  Tweet . Advanced RAG and Agents with MistralAI:   Guide  on using MistralAI with LlamaIndex and LlamaParse, advancing RAG capabilities and agent development through custom pipelines and sophisticated parsing. ✨ Feature Releases and Enhancements: We launched a LlamaPack based on  Xinyu Tang ’s   paper  ****for secure in-context learning in LLM/RAG apps, focusing on generating few-shot demonstrations from private datasets with differential privacy, ensuring the synthetic examples reflect the data distribution without exposing sensitive details.  LlamaPack ,  Tweet . We introduced a privacy-preserving RAG network by  Andrei  in LlamaIndex, enabling the use of sensitive datasets like healthcare and online user data without compromising individual privacy. This approach allows data providers to synthetically generate and share data for RAG queries securely.  Blogpost ,  Tweet . We introduce a template by  Sasha  for agent-human interaction in RAG implementations, focusing on minimal human input. It triggers human intervention only for vague or malformed queries, enhancing clarity and precision in the response process.  LlamaPack ,  Tweet . BAM Elevate  integrated Databricks Vector Search into LlamaIndex, enabling vector search capabilities within the Databricks ecosystem.  Blogpost ,  Tweet . We launched LlamaParse integration with LlamaIndex TypeScript, an industry-leading parser for PDFs and various document types accessible directly from JS/TS. Utilize the create-llama command-line tool or integrate LlamaParse directly into your app for enhanced document processing.  Example ,  Tweet . 🗺️ Guides: Guide  to Advanced RAG and Agents with MistralAI using LlamaIndex and LlamaParse to construct sophisticated RAG and agents, including custom query pipelines, document parsing, and reference applications. Guide  to Integrating Custom Models with LlamaIndex:  Leonie Monigatti  demonstrates the process of incorporating your custom model, like Gemma, into LlamaIndex Guide  to combat prompt injection attacks, like the \"white text\" attack, by rigorously screening data during ingestion and retrieval, ensuring the integrity of LLM-powered systems against deceptive manipulations by  Oleksandr Yaremchuk  from  Protect AI . ✍️ Tutorials: Akriti Upadhyay ’s  tutorial  to prototype on patient data safely, featuring synthetic dataset generation, storage in Qdrant Vector DB, and querying with llama.cpp LLM using LlamaIndex. Frank Baele ’s  tutorial  on developing a production-grade RAG pipeline with LlamaParse, detailing document parsing, advanced ingestion techniques, Vector DB selection, and insights on evaluation, deployment, and budget management. Video tutorial  by Ashish on creating an advanced PDF RAG agent, utilizing LlamaParse for text and tables extraction, defining retrievers and routers, and adding a sub-question layer, all integrated with LlamaIndex and MistralAI. UpTrain   tutorial  on Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations. Ravi Theja   tutorial  on showcasing RAG with LlamaIndex on 15 Indian languages using Navarasa-2.0 - a Gemma finetuned model on 15 Indian languages. 🎥  Webinars: Register for a webinar  with  Daniel Huynh  featuring LaVague, an agent that can navigate the web in your Jupyter/Colab notebook. 📅 Events: Join us  for a Panel discussion on 'Why RAG Will Never Die - The Context Window Myth’ with panelists from LlamaIndex, Vectara, Nvidia, and TogetherAI. We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.", "author": "LlamaIndex", "date": "Mar 26, 2024", "tags": ["LLM", "Rag"]},
{"title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB", "content": "The widespread consumption of videos on platforms like YouTube, Instagram, and others highlights the importance of efficiently processing and analyzing video content. This capability unlocks vast opportunities across various sectors, including media and entertainment, security, and education. However, the main challenge is effectively extracting meaningful information from videos, which are inherently complex and multimodal data streams. This blog post introduces a solution that leverages the LlamaIndex  Python API  for using the advanced capabilities of OpenAI’s  GPT4V , combined with the efficient data management by  LanceDB  across all data formats, to process videos. …But what does ‘RAG’ even mean? Retrieval-augmented generation (RAG) is a technique that merges information retrieval with generative AI to produce systems capable of generating precise and contextually relevant responses by tapping into large data repositories. Core Concept of RAG RAG operates in two stages: Retrieval : Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords. Generation : Integrates retrieved information to produce coherent responses, allowing the AI to “learn” from a wide range of content dynamically. RAG Architecture The architecture typically involves a dense vector search engine for retrieval and a transformer model for generation. The process: Performs a semantic search to fetch relevant documents. Processes these documents with the query to create a comprehensive context. The generative model then crafts a detailed response based on this enriched context. Extending to Multimodality Multimodal RAG integrates various data types (text, images, audio, video) in both retrieval and generation phases, enabling richer information sourcing. For example, responding to queries about “climate change impacts on polar bears” might involve retrieving scientific texts, images, and videos to produce an enriched, multi-format response. Let’s return to our use case and dive into how it’s all done. Moving forward, you can access the full code on  Google Colab . The solution is divided into the following sections. Click on the topic to skip to a specific part: Video Downloading Video Processing Building the Multi-Modal Index and Vector Store Retrieving Relevant Images and Context Reasoning and Response Generation 1. Video Downloading To begin, we need to locally download multimodal content from a publicly available source, I used pytube to download a YouTube video by 3Blue1Brown on the Gaussian function. # SET CONFIG\nvideo_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\noutput_video_path = \"./video_data/\"\noutput_folder = \"./mixed_data/\"\noutput_audio_path = \"./mixed_data/output_audio.wav\"\n\nfilepath = output_video_path + \"input_vid.mp4\"\nPath(output_folder).mkdir(parents=True, exist_ok=True) def   download_video ( url, output_path ):\n     \"\"\"\n    Download a video from a given url and save it to the output path.\n\n    Parameters:\n    url (str): The url of the video to download.\n    output_path (str): The path to save the video to.\n\n    Returns:\n    dict: A dictionary containing the metadata of the video.\n    \"\"\" \n   from  pytube  import  YouTube\n\n    yt = YouTube(url)\n    metadata = { \"Author\" : yt.author,  \"Title\" : yt.title,  \"Views\" : yt.views}\n    yt.streams.get_highest_resolution().download(\n        output_path=output_path, filename= \"input_vid.mp4\" \n    )\n     return  metadata\n Run  metadata_vid = download_video(video_url, output_video_path)  to invoke the function and store the video locally. 2. Video Processing We need to now extract multimodal content — Images, Text(via Audio). I extracted 1 frame every 5 seconds of the video (~160 frames) using  moviepy  . def   video_to_images ( video_path, output_folder ):\n     \"\"\"\n    Convert a video to a sequence of images and save them to the output folder.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_folder (str): The path to the folder to save the images to.\n\n    \"\"\" \n    clip = VideoFileClip(video_path)\n    clip.write_images_sequence(\n        os.path.join(output_folder,  \"frame%04d.png\" ), fps= 0.2   #configure this for controlling frame rate. \n    ) Following this, we extract the audio component: def   video_to_audio ( video_path, output_audio_path ):\n     \"\"\"\n    Convert a video to audio and save it to the output path.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_audio_path (str): The path to save the audio to.\n\n    \"\"\" \n    clip = VideoFileClip(video_path)\n    audio = clip.audio\n    audio.write_audiofile(output_audio_path) Next, let’s extract text from the audio using the SpeechRecognition library: def   audio_to_text ( audio_path ):\n     \"\"\"\n    Convert an audio file to text.\n\n    Parameters:\n    audio_path (str): The path to the audio file.\n\n    Returns:\n    test (str): The text recognized from the audio.\n\n    \"\"\" \n    recognizer = sr.Recognizer()\n    audio = sr.AudioFile(audio_path)\n\n     with  audio  as  source:\n         # Record the audio data \n        audio_data = recognizer.record(source)\n\n         try :\n             # Recognize the speech \n            text = recognizer.recognize_whisper(audio_data)\n         except  sr.UnknownValueError:\n             print ( \"Speech recognition could not understand the audio.\" )\n         except  sr.RequestError  as  e:\n             print ( f\"Could not request results from service;  {e} \" )\n\n     return  text Run the below chunk to complete the extraction and storage process: video_to_images(filepath, output_folder)\nvideo_to_audio(filepath, output_audio_path)\ntext_data = audio_to_text(output_audio_path)\n\n with   open (output_folder +  \"output_text.txt\" ,  \"w\" )  as  file:\n    file.write(text_data)\n print ( \"Text data saved to file\" )\nfile.close()\nos.remove(output_audio_path)\n print ( \"Audio file removed\" ) 3. Building the Multi-Modal Index and Vector Store After processing the video, we proceed to construct a multi-modal index and vector store. This entails generating embeddings for both textual and visual data using OpenAI’s CLIP model, subsequently storing and managing these embeddings in LanceDB VectorStore via the  LanceDBVectorStore  class. from  llama_index.indices.multi_modal.base  import  MultiModalVectorStoreIndex\n from  llama_index  import  SimpleDirectoryReader, StorageContext\n\n from  llama_index  import  SimpleDirectoryReader, StorageContext\n from  llama_index.vector_stores  import  LanceDBVectorStore\n\n\n from  llama_index  import  (\n    SimpleDirectoryReader,\n)\n\ntext_store = LanceDBVectorStore(uri= \"lancedb\" , table_name= \"text_collection\" )\nimage_store = LanceDBVectorStore(uri= \"lancedb\" , table_name= \"image_collection\" )\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n # Create the MultiModal index \ndocuments = SimpleDirectoryReader(output_folder).load_data()\n\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n) 4. Retrieving Relevant Images and Context With the index in place, the system can then retrieve pertinent images and contextual information based on input queries. This enhances the prompt with precise and relevant multimodal data, anchoring the analysis in the video’s content. Lets set up the engine for retrieving, I am fetching top 5 most relevant  Nodes  from the vectordb based on the similarity score: retriever_engine = index.as_retriever(\n    similarity_top_k= 5 , image_similarity_top_k= 5 \n) A  Node  object is a “chunk” of any source Document, whether it’s text, an image, or other. It contains embeddings as well as meta information of the chunk of data. By default, LanceDB uses  l2  as metric type for evaluating similarity. You can specify the metric type as  cosine  or  dot  if required. Next, we create a helper function for executing the retrieval logic: from  llama_index.response.notebook_utils  import  display_source_node\n from  llama_index.schema  import  ImageNode\n\n\n def   retrieve ( retriever_engine, query_str ):\n    retrieval_results = retriever_engine.retrieve(query_str)\n\n    retrieved_image = []\n    retrieved_text = []\n     for  res_node  in  retrieval_results:\n         if   isinstance (res_node.node, ImageNode):\n            retrieved_image.append(res_node.node.metadata[ \"file_path\" ])\n         else :\n            display_source_node(res_node, source_length= 200 )\n            retrieved_text.append(res_node.text)\n\n     return  retrieved_image, retrieved_textdef retrieve(retriever_engine, query_str):\n    retrieval_results = retriever_engine.retrieve(query_str) Lets input the query now and then move on to complete the process by retrieving and visualizing the data : query_str = \"\"\"\nUsing examples from the video, explain all things covered regarding\nthe Gaussian function\n\"\"\" \nimg, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\nimage_documents = SimpleDirectoryReader(\n    input_dir=output_folder, input_files=img\n).load_data()\ncontext_str =  \"\" .join(txt)\nplot_images(img) You should see something similar to the example below (note that the output will vary depending on your query): Displaying the similar Text objects (nodes) Retrieved Images Observe that the  node  object displayed shows the  Id  of the data chunk , its similarity score and the source text of the chunk that was matched (for images we get the filepath instead of text). 5. Reasoning and Response Generation The final step leverages GPT4V to reason about the correlations between the input query and the augmented data. Below is the prompt template : qa_tmpl_str = (\n     \"\" \"\n Given the provided information, including relevant images and retrieved context from the video, \\\n accurately and precisely answer the query without any additional prior knowledge.\\n\" \n     \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\" \n     \"---------------------\\n\" \n     \"Context: {context_str}\\n\" \n     \"Metadata for video: {metadata_str} \\n\" \n     \"---------------------\\n\" \n     \"Query: {query_str}\\n\" \n     \"Answer: \" \n \"\" \"\n) The  OpenAIMultiModal  class from LlamaIndex enables us to incorporate image data directly into our prompt object. Thus, in the final step, we enhance the query and contextual elements within the template to produce the response as follows: from  llama_index.multi_modal_llms.openai  import  OpenAIMultiModal\n\nopenai_mm_llm = OpenAIMultiModal(\n    model= \"gpt-4-vision-preview\" , api_key=OPENAI_API_TOKEN, max_new_tokens= 1500 \n)\n\n\nresponse_1 = openai_mm_llm.complete(\n    prompt=qa_tmpl_str. format (\n        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n    ),\n    image_documents=image_documents,\n)\n\npprint(response_1.text) The generated response captures the context pretty well and structures the answer correctly : The video “A pretty reason why Gaussian + Gaussian = Gaussian” by 3Blue1Brown delves into the Gaussian function or normal distribution, highlighting several critical aspects: Central Limit Theorem:  It starts with the central limit theorem, illustrating how the sum of multiple random variable copies tends toward a normal distribution, improving with more variables. Convolution of Random Variables:  Explains the addition of two random variables as their distributions’ convolution, focusing on visualizing this through diagonal slices. Gaussian Function:  Details the Gaussian function, emphasizing the normalization factor for a valid probability distribution, and describes the distribution’s spread and center with standard deviation (σ) and mean (μ). Convolution of Two Gaussians:  Discusses adding two normally distributed variables, equivalent to convolving two Gaussian functions, and visualizes this using the graph’s rotational symmetry. Rotational Symmetry and Slices:  Shows the rotational symmetry of e^(-x²) * e^(-y²) around the origin, a unique Gaussian function property. It explains computing the area under diagonal slices, equating to the functions’ convolution. Resulting Distribution:  Demonstrates the convolution of two Gaussian functions yielding another Gaussian, a notable exception in convolutions usually resulting in a different function type. Standard Deviation of the Result:  Concludes that convolving two normal distributions with mean 0 and standard deviation (σ) produces a normal distribution with a standard deviation of sqrt(2) * σ. Implications for the Central Limit Theorem:  Highlights the convolution of two Gaussians’ role in the central limit theorem, positioning the Gaussian distribution as a distribution space fixed point. The author uses visual examples and explanations throughout to clarify the mathematical concepts related to the Gaussian function and its significance in probability and statistics. Conclusion The Multimodal RAG architecture offers a powerful and efficient solution for processing and analyzing video content. By leveraging the capabilities of OpenAI’s GPT4V and LanceDB, this approach not only simplifies the video analysis process but also enhances its accuracy and relevance. Whether for content creation, security surveillance, or educational purposes, the potential applications of this technology are vast and varied. As we continue to explore and refine these tools, the future of video analysis looks promising, with AI-driven solutions leading the way towards more insightful and actionable interpretations of video data. Stay tuned for upcoming projects !", "author": "Raghav Dixit", "date": "Feb 17, 2024", "tags": ["AI", "Generative Ai Tools", "Vector Database", "Large Language Models", "Open Source"]},
{"title": "LlamaIndex Newsletter 2024-03-05", "content": "Greetings, LlamaIndex devotees! 🦙 It was another fun week to be at the center of the LLM universe, and we have tons to share! 🤩  The highlights: We shared our thoughts on the future of  long-context RAG.  As LLMs with context windows over 1M tokens begin to appear, what changes about RAG, and how will LlamaIndex evolve?  Tweet ,  Blog post llama-index-networks  lets you build a super-RAG application by combining answers from independent RAG apps over the network.  Tweet ,  Blog post ,  repo People loved our release of LlamaParse, a world-beating PDF parsing service, so we made it even better!  Tweet ,  blog post ✨ Feature Releases and Enhancements: We released a new llama-index-networks feature that lets you combine multiple independent RAG applications over the network, allowing you to run a single query across all the applications and get a single, combined answer.  Tweet ,  Blog post ,  repo Inference engine  Groq  wowed us and the world with their incredibly fast query times and we were delighted to introduce first-class support for their LLM APIs.  Tweet ,  notebook Users love LlamaParse, the world-beating PDF parsing service we released last week. We pushed improved parsing and OCR support for 81+ languages! We also increased the usage cap from 1k to 10k pages per day.  Tweet ,  blog post We migrated our  blog off of Medium , we hope you like the new look and the absence of nag screens! RAPTOR is a new tree-structured technique for advanced RAG; we turned the paper into a LlamaPack, allowing you to use the new technique in one line of code.  Tweet ,  package ,  notebook ,  original paper 🎥 Demos: The Koda Retriever is a new retrieval concept: hybrid search where the alpha parameter controlling the importance of vector search vs. keyword search is tuned on a per-query basis by the LLM itself, based on a few-shot examples.  Tweet ,  notebook ,  package ,  blog post Mixedbread.ai  released some state-of-the-art rerankers that perform better than anything seen before; we whipped up a quick cookbook to show you how to use them directly in LlamaIndex.  Tweet ,  Notebook ,  blog post 🗺️ Guides: Function-calling cookbook with open source models  shows you how to use Fireworks AI’s OpenAI-compatible API to use all native LlamaIndex support for function calling.  Notebook ,  Tweet . We released a best practices cookbook showing how to use LlamaParse, our amazing PDF parser.  Tweet ,  notebook A  comprehensive guide to semantic chunking for RAG  by Florian June covers embedding-based chunking, BERT-based chunking techniques, and LLM-based chunking for everything you need to know about this highly effective technique to improve retrieval quality.  Tweet ,  Blog post ✍️ Tutorials: Our own  Andrei  presented a notebook on building Basic RAG with LlamaIndex at  Vector Institute ’s RAG bootcamp.  Tweet ,  Notebook ClickHouse presented an in-depth tutorial using LlamaIndex to query both structured and unstructured data, and built a bot that queries Hacker News to find what people are saying about the most popular technologies.  Tweet ,  blog post POLM (Python, OpenAI, LlamaIndex, MongoDB) is a new reference architecture for building RAG applications and MongoDB has a beautiful, step-by-step tutorial for building it out.  Tweet ,  blog post 🎥  Webinar: Our CEO Jerry Liu will do a joint webinar with Adam Kamor of  Tonic.ai  about building fully-local RAG applications with Ollama and Tonic. People love local models!  Tweet ,  Registration page Jerry also did a webinar with  Traceloop  on leveling up your LLM application with observability.  Tweet ,  YouTube Our hackathon at the beginning of February was a huge success! Check out this webinar in which we invited the winners to come and talk about their projects.  Tweet ,  YouTube .", "author": "LlamaIndex", "date": "Mar 5, 2024", "tags": ["LLM", "Newsletter", "AI"]},
{"title": "LlamaIndex Newsletter 2024-04-02", "content": "Greetings, LlamaIndex community! 🦙 Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more. 🤩  The highlights: DeepLearningAI Course:  JavaScript RAG Web Apps with LlamaIndex collaborative course with DeepLearningAI.  Course ,  Tweet . RAFTDatasetPack LlamaPack : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents.  LlamaPack ,  Tweet . Memory Efficiency with Cohere Embeddings:  Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations.  Notebook ,  Tweet . Python Docs Makeover:  Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details.  API Ref ,  Tweet ,  Docs ✨ Feature Releases and Enhancements: We introduced RAFT - Retrieval Augmented Fine Tuning, a method from  Tianjun Zhang  and  Shishir Patil  to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation.  LlamaPack ,  Tweet . We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.  Course ,  Tweet . We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG.  Notebook ,  Tweet We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation.  API Ref ,  Tweet ,  Docs 🎥 Demos: RestAI , a project by  Pedro Dias  is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI. Ragdoll  and  Ragdoll Studio  by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI. 🗺️ Guides: Guide  to Designing RAG Systems by  Michał Oleszak  for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation. ✍️ Tutorials: Sujit Patil   tutorial  on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment. Florian June's  tutorial  on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch. Laurie's  video tutorial  on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries. Akriti’s   tutorial  on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant. Marco Bertelli's  tutorial  on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience. Sudarshan Koirala’s   tutorial  on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking. 🎥  Webinars: Register for a webinar  with  Tianjun Zhang  and  Shishir Patil  on how to do retrieval-augmented fine-tuning (RAFT). Webinar  with  Daniel  on  CodeGPT  - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components. Vectara’s   Panel Discussion  on 'Why RAG will Never Die?’.", "author": "LlamaIndex", "date": "Apr 2, 2024", "tags": ["LLM"]},
{"title": "Towards Long Context RAG ", "content": "Google recently released  Gemini 1.5 Pro with a 1M context window , available to a limited set of developers and enterprise customers. Its performance has caught the imagination of  AI Twitter . It  achieves 99.7% recall in the “Needle in a Haystack”  experiment popularized by  Greg Kamradt . Early users have shared results feeding dozens of research papers, financial reports at once and report impressive results in terms of its ability to synthesize across vast troves of information. Naturally, this begs the question - is RAG dead? Some  folks think so , while others  disagree . Those in the first camp make valid points. Most small data use cases can fit within a 1-10M context window. Tokens will get cheaper and faster to process over time. Having an LLM natively interleave retrieval/generation via attention layers leads to a higher response quality compared to the one-shot retrieval present in naive RAG. We were fortunate to have a preview of Gemini 1.5 Pro’s capabilities, and through playing around with it developed a thesis for how context-augmented LLM applications will evolve. This blog post clarifies  our mission as a data framework  along with  our view of what long-context LLM architectures will look like.  Our view is that while long-context LLMs will simplify certain parts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG architectures to handle the new use cases that long-context LLMs bring along. No matter what new paradigms emerge, our mission at LlamaIndex is to build tooling towards that future. Our Mission Goes Beyond RAG The goal of LlamaIndex is very simple:  enable developers to build LLM applications over their data.  This mission goes beyond just RAG. To date we have invested a considerable amount of effort in advancing RAG techniques for existing LLMs, and we’ve done so because it’s enabled developers to unlock dozens of new use cases such as QA over semi-structured data, over complex documents, and agentic reasoning in a multi-doc setting. But we’re also excited about Gemini Pro, and we will continue to advance LlamaIndex as a production data framework in a long-context LLM future. An LLM framework is intrinsically valuable.  As an open-source data framework, LlamaIndex paves the cowpaths towards building any LLM use case from prototype to production. A framework makes it easier to build these use cases versus building from scratch. We enable  all  developers to build for these use cases, whether it’s setting up the proper architecture using our core abstractions or leveraging the hundreds of integrations in our ecosystem. No matter what the underlying LLM advancements are and whether RAG continues to exist in its current form, we continue to make the framework production-ready, including watertight abstractions, first-class documentation, and consistency. We also launched LlamaCloud last week . Our mission for LlamaCloud remains building the data infra enabling any enterprise to make their vast unstructured, semi-structured, and structured data sources production-ready for use with LLMs. Initial Gemini 1.5 Pro Observations During our initial testing we played around with some PDFs: SEC 10K Filings, ArXiv papers, this monster  Schematic Design Binder , and more. We will do a lot more deeper analyses once the APIs are available, but in the meantime we share observations below. Gemini results are impressive and consistent with what we’ve seen in the technical report and on socials: Gemini has impressive recall of specific details:  We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing. Gemini has impressive summarization capabilities.  The model can analyze large swaths of information across multiple documents and synthesize answers. This figure shows a question-response pair from Gemini over the 2019 Uber 10K filing. The question and answer is shown at the top and the source table is shown at the bottom. Gemini is able to return the correct answer. There are some parts where we noticed Gemini struggles a bit. Gemini doesn’t read all tables and charts correctly.  Gemini Pro still has a hard time being able to read figures and complex tables. Gemini can take a long time.  Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds. Gemini can hallucinate page numbers.  When asked to give a summary but also with page number citations, Gemini hallucinated the sources. An example where Gemini 1.5 Pro still hallucinates. The model hallucinates a number when asked about the total number of gross bookings across all segments - the number is visible in the chart and can also be pieced together from the table.  Directionally though it’s an exciting glimpse of the future and warrants a bigger discussion on which RAG paradigms will fade and new architectures that will emerge. See below! Long Contexts Resolve Some Pain Points, but some Challenges Remain Gemini 1.5 Pro is just the first of many long-context LLMs to emerge, which will inevitably change how users are building RAG. Here are some existing RAG pain points that we believe long-context LLMs will solve: Developers will worry less about how to precisely tune chunking algorithms.  We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages. Developers will need to spend less time tuning retrieval and chain-of-thought over single documents . An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer. Summarization will be easier.  This is related to the above statement. A lot of summarization strategies over big documents involve “hacks” such as sequential refinement or hierarchical summarization (see our  response synthesis modules  as a reference guide). This can now be alleviated with a single LLM call. Personalized memory will be better and easier to build:  A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction). There are, however, some lingering challenges: 10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge.  1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many “small” document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context. Embedding models are lagging behind in context length.  So far the largest context window we’ve seen for embeddings are  32k from together.ai . This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller. Cost and Latency.  Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that  Yao Fu  brought up is that a  KV Cache  can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below. A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies . We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document. Towards New RAG Architectures Proper usage of long-context LLMs will necessitate new architectures to best take advantage of their capabilities, while working around their remaining constraints. We outline some proposals below. 1. Small to Big Retrieval over Documents To the extent that long-context LLMs need retrieval augmentation over big knowledge bases (e.g. in the gigabytes), we will need  small-to-big retrieval:  index and retrieve small chunks, but have each chunk link to big chunks that will ultimately be fed to LLMs during synthesis. This architecture already exists in LlamaIndex in different forms ( sentence window retriever  and  recursive retrieval over chunk sizes ), but can be scaled up even more for long-context LLMs - embed document summaries, but link to entire documents. One reason we want to embed and index smaller chunks is due to the fact that current embedding models are not keeping up with LLMs in terms of context length. Another reason is that there can actually be retrieval benefits in having multiple granular embedding representations compared to a single document-level embedding for a document. If there is a single embedding for a document, then that embedding has the burden of encoding all information throughout the entire document. On the other hand, we’ve found that embedding many smaller chunks and having each small chunk link to a bigger chunk, will lead to better retrieval of the relevant information. Check out the diagram above for an illustration of two flavors of small-to-big retrieval. One is indexing document summaries and linking them to documents, and the other is indexing smaller chunks within a document and linking them to the document. Of course, you could also do both - a general best practice for improving retrieval is to just try out multiple techniques at once and fuse the results later. 2. Intelligent Routing for Latency/Cost Tradeoffs The arrival of long-context LLMs will inevitably raise questions on the amount of context that is suitable for each use case. Injecting LLMs with long context comes with real cost and latency tradeoffs and isn’t suitable for every use case or even every question. Although cost and latency will decrease in the future, we anticipate users will need to think carefully about this tradeoff for the next year or two. Certain questions that are asking about specific details are well suited for existing RAG techniques of top-k retrieval and synthesis. More complex questions require more context from disparate pieces of different documents, and in those settings it is less clear how to correctly answer these questions while optimizing for latency and cost: Summarization questions require going over entire documents. Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt. We imagine an intelligent routing layer that operates on top of multiple RAG and LLM synthesis pipelines over a knowledge base. Given a question, the router can ideally choose an optimal strategy in terms of cost and latency in terms of retrieving context to answer the question. This ensures that a single interface can handle different types of questions while not becoming prohibitively expensive. 3. Retrieval Augmented KV Caching An optimization that Google and other companies are certainly working on is resolving latency and cost concerns through a  KV Cache.  At a high-level, a KV cache stores activations from pre-existing key and query vectors in an attention layer, preventing the need to recompute activations across the entire text sequence during LLM generation (we found  this  to be a nice intro reference to how a KV Cache works). Using a KV Cache to cache all document tokens within the context window prevents the need to recompute activations for these tokens on subsequent conversations, bringing down latency and cost significantly. But this leads to interesting retrieval strategies on how to best use the cache, particularly for knowledge corpuses that exceed the context length. We imagine a “ retrieval augmented caching ” paradigm emerging, where we want to retrieve the most relevant documents that the user would want to answer, with the expectation that they will continue to use the documents that are in the cache. This could involve interleaving retrieval strategies with  traditional caching algorithms  such as LRU caching. But a difference with existing KV cache architectures is that the position matters, since the cached vector is a function of all tokens leading up to that position, not just the tokens in the document itself. This means that you can’t just swap out a chunk from the KV cache without affecting all cached tokens that occur after it positionally. In general the API interface for using a KV Cache is up in the air. It’s also up in the air as to whether the nature of the cache itself will evolve or algorithms will evolve to best leverage the cache. What’s Next We believe the future of LLM applications is bright, and we are excited to be at the forefront of this rapidly evolving field. We invite developers and researchers to join us in exploring the possibilities of long-context LLMs and building the next generation of intelligent applications.", "author": "Jerry Liu", "date": "Mar 1, 2024", "tags": ["LLM"]},
{"title": "LlamaIndex Newsletter 2024-04-09", "content": "Hello, LlamaIndex members! 🦙 Welcome to another thrilling weekly update from LlamaUniverse! We're excited to present a variety of outstanding updates, including Anthropic's Function Calling, Cookbooks, RankLLM, Guides, Tutorials, and much more. 🤩  The highlights: Anthropic's Claude Function Calling Agent:  Enhance QA/RAG and workflow automation with advanced tool calling in an agent framework.  Notebook ,  Tweet . RankLLM Integration:  RankLLM is an open-source LLM collection for reranking, surpassing GPT-4 based alternatives is now integrated with LlamaIndex.  Notebook ,  Tweet . LlamaIndex + MistralAI Cookbook Series:  Launched a cookbook series with MistralAI for building diverse RAG applications, from basic to advanced, with distinctive methods and abstractions.  Cookbooks ,  Tweet ✨ Feature Releases and Enhancements: We have introduced the Anthropic’s Claude Function Calling Agent, leveraging advanced tool calling capabilities within an agent framework for enhanced QA/RAG and workflow automation.  Notebook ,  Tweet . RankLLM (by  Ronak Pradeep ) integration with LlamaIndex - an open-source LLM collection fine-tuned for reranking, offering top-notch results and outperforming GPT-4 based rerankers.  Notebook ,  Tweet . We have launched the LlamaIndex + MistralAI Cookbook Series for creating a range of RAG applications, from simple setups to advanced agents, featuring unique abstractions and techniques.  Cookbooks ,  Tweet We launched create-llama for building full-stack RAG/agent applications with a single CLI command, akin to create-react-app, for a comprehensive chatbot setup including tool use.  Tweet . 🎥 Demos: AutoRAG  by Marker-Inc-Korea: Streamline RAG pipeline optimization with an automated three-step process, from data preparation to evaluation and optimal pipeline adoption, enhancing the efficiency of the RAG pipeline using LlamaIndex. 🗺️ Guides: Guide  to Building Advanced RAG with Temporal Filters: Learn how to enhance your RAG pipeline with time-based metadata for more effective financial report analysis using LlamaIndex and  KDB.AI  vector store. Guide  to Adaptive RAG for dynamically selecting RAG strategies based on query complexity, enhancing efficiency across varying question types. ✍️ Tutorials: (λx.x)eranga ’s   tutorial  on the step-by-step process for building RAG with local models (LlamaIndex, Ollama, HuggingFace Embeddings, ChromaDB) and wrapping it all in a Flask server. Ivan Ilin ’s  video tutorial  on  iki.ai  - an LLM-powered digital library, for organizing, and sharing information within teams or organizations. Tutorial  on scaling LLM Applications with Koyeb on deploying a full-stack RAG application globally without infrastructure setup, using Koyeb, LlamaIndex.TS, and TogetherAI. Ankush Singal 's  tutorial  on Building Multi-Document Agents with LlamaIndex covers advanced multi-document agent concepts, where documents serve as sub-agents enabling complex QA, semantic search, and summarization. Rohan ’s  tutorial  on building a Full-Stack RAG application that streams intermediate results to visual UI components with event queues and server-side events. Hanane Dupouy 's  tutorial  on building a Finance Agent using an LLM with Yahoo Finance and LlamaIndex abstractions to analyze financial data for publicly traded companies, covering everything from balance sheets to stock recommendations. 🎥  Webinars: Webinar  with  Daniel Huynh  ****featuring LaVague - an agent that can navigate the web in your Jupyter/Colab notebook. Webinar  with  Logan Kelly  featuring  CallSine  that utilizes LlamaIndex abstractions and LLMs for personalized sales outreach.", "author": "LlamaIndex", "date": "Apr 9, 2024", "tags": []},
{"title": "LlamaIndex Newsletter 2024-04-16", "content": "Hello, LlamaIndex Family! 🦙 Welcome to another thrilling weekly update from LlamaGalaxy! We're excited to bring you a variety of outstanding updates, including the Chain of Abstraction LlamaPack, create-tsi, demos, guides, tutorials, and much more. Before we delve into these updates, we have an exciting tutorial series on Agents and Tools for you to check out. Perfect for beginners, this series covers everything from advanced QA/RAG implementations to step-wise execution. By the end, you’ll have gained a deeper understanding of how to use agent reasoning with tool use to build simple applications. Check them out: Overview ReAct agents Function Calling agents Retrieval-Augmented agent Controlling tool outputs Agents with step-by-step execution 🤩  The highlights: Chain of Abstraction LlamaPack:  Chain of Abstraction technique as llamapack a method enabling multi-step reasoning for enhanced tool use introduced by Silin Gao's team.  LlamaPack ,  Tweet . Create-tsi Toolkit:  Launched a toolkit for building full-stack RAG applications with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers.  Code ,  Tweet . Improved Agent Control :  return_direct  feature in tools allows direct output returns, reducing costs and enhancing response efficiency.  Docs ,  Tweet . ✨ Feature Releases and Enhancements: We have introduced the Chain of Abstraction Technique Developed by Silin Gao, and team as LlamaPack, this new method enables LLMs to generate multi-step reasoning chains for efficient sequence planning, enhancing tool use beyond single-shot functions.  LlamaPack ,  Tweet . We have launched create-tsi: A toolkit in collaboration with T-Systems and Marcus Schiesser to generate GDPR-compliant, full-stack AI applications via a CLI interface. Build enterprise-grade RAG bots with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers.  Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controllability by allowing direct output returns as final responses. This optimizes for reduced latency and costs, and effectively halts the agent after crucial actions like booking confirmations or answering queries.  Docs ,  Tweet . 🎥 Demos: RAG-enhanced MetaGPT : A robust multi-agent framework that features structured team dynamics for problem-solving, now supercharged with domain-specific knowledge from LlamaIndex modules. This framework supports diverse data inputs, sophisticated retrieval options, and efficient data management for enhanced agent performance. 🗺️ Guides: Guide  to Building and Evaluating Advanced RAG by Hamza Gharbi for setting up a basic RAG pipeline, defining custom evaluation functions, and optimizing retrieval techniques. Paper  by Prof.  Markus J. Buehler : Using LLM-Generated Knowledge Graphs to Accelerate Biomaterials Discovery - This study showcases how a comprehensive knowledge graph from over 1000 scientific papers reveals novel insights and connections, driving innovation in biomaterials through art as inspiration. The KG construction was done with the help of LlamaIndex modules. Guide  to Full-Stack RAG Application with AWS Bedrock: Set up Bedrock embeddings, use LlamaIndex for PDF retrieval, and build an interactive Streamlit interface, an ideal resource for enterprises starting with AWS services. Guide  to Building a Lightweight ColBERT Retrieval Agent: Learn how to create an agent capable of advanced document retrieval and maintaining conversation memory, without the complexity of heavyweight agent frameworks. Guide  to the Best RAG Techniques: 'ARAGOG'  paper  by Matous Eibich is a comprehensive evaluation survey exploring various RAG methods from classic vector databases to LlamaIndex's advanced techniques. Key findings highlight the effectiveness of HyDE, LLM reranking, and sentence window retrieval for improving precision and answer similarity. ✍️ Tutorials: Akash Mathur ’s  tutorial  on Data Management in LlamaIndex: Featuring LlamaCloud and its open-source counterpart, this tutorial showcases efficient live data handling to minimize costs and latency in LLM applications. Leonie ’s interactive  tutorial  to create an app that lets you converse with code from a GitHub repository. kingzzm’s   tutorial  on enhancing RAG Performance to overcome the issue of 'broken' context in RAG construction by dynamically creating contiguous chunks with auto-merging retrieval. Activeloop ’s  tutorial  on Multimodal RAG for Pill Search teaches how to identify pills using images and text. This helps in identifying unknown pills, checking drug interactions and side effects, and confirming proper dosage amounts. Fanghua Yu's  tutorial  on using LlamaParse for Knowledge Graph Creation from Documents. 🎥  Webinars: Webinar  with  Tianjun Zhang  and  Shishir Patil , the two lead co-authors of RAFT, where they presented RAFT and discussed fine-tuning and RAG.", "author": null, "date": "Apr 16, 2024", "tags": []},
{"title": "LlamaIndex Newsletter 2024-04-23", "content": "Hello LlamaIndex Community! 🦙 Welcome to another thrilling weekly update from LlamaWorld! We're excited to bring you a variety of outstanding updates, including Cookbooks, demos, guides, and tutorials. 🤩  The highlights: MistralAI's 8x22b Model Cookbook:  Released cookbook for MistralAI's 8x22b model with detailed guidance on RAG, query routing, and tool applications.  Docs ,  Tweet . Llama 3 Model Cookbook:  A comprehensive cookbook for Meta's Llama 3 model from simple prompt runs to complex RAG pipeline, agents and tools, accessible directly from Hugging Face.  Docs ,  Tweet . create-llama Llama 3 template : create-llama template for Meta's Llama 3 to quickly start building full-stack LLM applications using the  nextjs-llama3  template with a single CLI command.  Tweet . ✨ Feature Releases and Enhancements: We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases.  Docs ,  Tweet . We have released a cookbook for latest Meta's new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools.  Docs ,  Tweet . We have introduced a template for integrating Meta's Llama 3 in create-llama. Simply run  npx create-llama  and select the  nextjs-llama3  template to build full-stack LLM application with Llama 3 in one CLI command.  Tweet . 🎥 Demos: Open Source AI Diagram Generator  by  Rohan  using LlamaIndex's Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience. DREAM : A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD. Firecrawl  from  Mendable  is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing. 🗺️ Guides: Guide  to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI's Mixtral 8x7b, and our LlamaParse document parser. Guide  to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software. Guide  to Building a Code-Writing Agent:  TechWithTimm  demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code. Guide  to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex's finetuning techniques, including steps from quantization to fine-tuning with QLoRA. ✍️ Tutorials: Khye Wei's  tutorial  from Microsoft demonstrates how to use LlamaIndex with Azure's AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine. Hanane Dupouy 's  tutorial  on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex's ReAct agent and API abstractions. Andy Singal 's  tutorial  on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions. Mariboo’s  tutorial  on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetuning abstractions.", "author": "LlamaIndex", "date": "Apr 23, 2024", "tags": []},
{"title": "LlamaIndex Newsletter 2024-05-14", "content": "Hello LlamaIndex Family! 🦙 Welcome to another thrilling weekly update from LlamaIndex! We're excited to share a variety of outstanding updates, guides, and tutorials with you. But first, we have an exciting announcement to make. We are thrilled to announce a new  course  in collaboration with DeepLearningAI—Building Agentic RAG. In this course, you’ll learn how to build a research assistant that can reason over multiple documents and answer complex questions. You’ll also learn how to step through the execution of the agent and steer it with human feedback.  Check it out  and take your RAG skills to the next level! 🤩  The highlights: Day 0 Support for GPT-4o  -   Tweet Llama3 Cookbook  -   Tweet TypeScript Agent  Guide . ✨ Feature Releases and Enhancements: We have introduced day 0 support for GPT-4o in both Python and TypeScript. Additionally, we've created demo notebooks ( demo1  and  demo2 ) to help you easily experiment with GPT-4o using LlamaIndex.  Tweet . We have launched Llama3 cookbooks showcasing interesting use cases for Llama 3, from basic chat functionalities to advanced agent development. Ideal for anyone building with local models, either on your laptop or through an API.  Notebook,   Tweet . 🗺️ Guides: Guide  to building agents in TypeScript: Dive into our comprehensive, open-source guide developed by Laurie that walks you through every step of agent development, from setting up with basic functions to integrating advanced features like local and remote LLMs, and data querying with vectorDB. Guide  to using RAG for content moderation:  CloudRaft  shows how to set up a RAG pipeline to moderate user-generated images effectively, ensuring compliance with predefined rules through techniques like semantic search and efficient inferencing with small LLMs. ✍️ Tutorials: Kxsystems  advanced workshop on \"Building Advanced RAG over Complex PDFs with LlamaParse\" to demonstrate how LlamaParse can tackle the challenge of extracting diverse elements like text, tables, images, and graphs from complex research papers.  Video Tutorial ,  BlogPost ,  Notebook . Arslan Shahid   tutorial  on Generating PowerPoints with Llama 3, using LlamaIndex to create a Llama3 RAG pipeline. The approach not only answers questions but also generates PowerPoint slide decks by utilizing the python-pptx library to write code programmatically for slide creation. Hanane Dupouy   demonstrates  Building a Financial Agent that can Perform Reflection. The approach helps in analyzing stock prices by implementing two types of reflection: CRITIC (tool use) and self-reflection (no tools). zhaozhiming’s   tutorial  on evaluating RAG systems, utilizing evaluation libraries like TruLens, Ragas, UpTrain, and DeepEval to assess RAG systems using metrics such as faithfulness, relevance, and answer correctness.", "author": "LlamaIndex", "date": "May 14, 2024", "tags": []},
{"title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex", "content": "What is Lyzr? Lyzr  is a full-stack agent framework that specializes in building fully autonomous AI agents for enterprises. Their focus is on achieving Organizational General Intelligence (OGI) by harnessing agent data. Lyzr offers pre-built agents like  Jazon , an AI sales development representative, and  Skott , an AI content marketing agent, as well as a no-code builder for custom agent creation. Their platform enables organizations to build, deploy, and manage AI agents that can handle complex tasks and workflows autonomously. How does LlamaIndex help? LlamaIndex plays a crucial role in Lyzr's technology stack: Context augmentation : LlamaIndex components supply essential context to Lyzr's agents, creating Retrieval-Augmented Generation (RAG) systems that enable them to perform focused and effective work. Custom data access : LlamaIndex’s data connectors are the preferred way for Lyzr agents to access customer-specific information. LlamaIndex’s huge library of connectors means they can connect no matter where the customer stores their data. Flexible retrieval : Lyzr uses LlamaIndex's customizable retrieval methods to optimize performance for different use cases. Lyzr’s AutoRAG system determines the optimal retrieval model, chunk size, and other parameters based on input data and use case and passes those to LlamaIndex. What have the results been like? The integration of LlamaIndex into Lyzr's framework has contributed to significant growth and improved performance: 1. Rapid revenue growth : Lyzr's annual recurring revenue jumped from around $100,000 to about $1.5 million in less than 60 days. 2. Enhanced agent accuracy : LlamaIndex's advanced capabilities and accurate RAG have led to highly accurate agents with very low error rates, providing a great alternative to OpenAI’s Assistant API. 3. Scalability : The flexibility provided by LlamaIndex has allowed Lyzr to sustain its growth and expand its agent offerings. What do customers think? Customer reception of Lyzr's LlamaIndex-powered agents has been overwhelmingly positive: 1. High adoption rate : 75% of Lyzr's customers use two or more AI agents including custom workflow agents, indicating strong adoption of Lyzr Agent Framework. 2. Customer Persona : Lyzr’s ‘fully autonomous’ AI agents seem to have captured customer’s imagination with SaaS CTOs being the primary adopter of Lyzr AI Agents to automate their backend workflows in a more reliable, secure and predictable manner. 3. Positive testimonials : Customers like SurePeople love Lyzr: \"SurePeople is delighted to announce our partnership with Lyzr.ai, a key player in fortifying the scalability, security, and future-readiness of our AI infrastructure. Thanks to their versatile Agents, we're empowered to operate at the forefront of innovation, underpinned by a robust framework that bolsters our AI applications. In an ever-evolving landscape of artificial intelligence, Lyzr.ai's Agents ensure we remain at the cutting edge. Additionally, our collaboration has been enriched by their exceptionally skilled and cooperative team.\" – Niko Drakoulis, CEO of SurePeople What's next for Lyzr? Lyzr has ambitious plans for the future, building on their success with LlamaIndex, including new agents such as  Kathy , an AI competitor analyst, and Diane, an AI HR agent, with several others in the pipeline. They’re also developing a framework called Lyzr AgentMesh to enable interaction between these different AI agents, creating a cohesive AI-driven workforce. “We are extremely thankful to Jerry and team for bringing LlamaIndex to the AI community. You guys have saved countless hours of tackling data retrieval challenges for us and many other builders in this space.” - Siva Surendira, Founder, Lyzr AI. By continuing to use LlamaIndex as their RAG partner, Lyzr is well-positioned to expand its offerings and further establish itself as a leader in autonomous AI agent technology for enterprises. Want to see what LlamaCloud can do for you? Sign up for LlamaCloud  and  get on the waitlist  for full access!", "author": "LlamaIndex", "date": "Jul 10, 2024", "tags": ["Case Study"]},
{"title": "Building a multi-agent concierge system", "content": "Why build this? Interactive chat bots are by this point a familiar solution to customer service, and agents are a frequent component of chat bot implementations. They provide memory, introspection, tool use and other features necessary for a competent bot. We have become interested in larger-scale chatbots: ones that can complete dozens of tasks, some of which have dependencies on each other, using hundreds of tools. What would that agent look like? It would have an enormous system prompt and a huge number of tools to choose from, which can be confusing for an agent. Imagine a bank implementing a system that can: Look up the price of a specific stock Authenticate a user Check your account balance Which requires the user be authenticated Transfer money between accounts Which requires the user be authenticated And also that the user checks their account balance first Each of these top-level tasks has sub-tasks, for instance: The stock price lookup might need to look up the stock symbol first The user authentication would need to gather a username and a password The account balance would need to know which of the user's accounts to check Coming up with a single primary prompt for all of these tasks and sub-tasks would be very complex. So instead, we designed a multi-agent system with agents responsible for each top-level task, plus a \"concierge\" agent that can direct the user to the correct agent. What we built We built a system of agents to complete the above tasks.  It's open-source ! There are four basic \"task\" agents: A stock lookup agent (which takes care of sub-tasks like looking up symbols) An authentication agent (which asks for username and password) An account balance agent (which takes care of sub-tasks like selecting an account) A money transfer agent (which takes care of tasks like asking what account to transfer to, and how much) There are also three \"meta\" agents: A  concierge agent : this agent is responsible for interacting with the user when they first arrive, letting them know what sort of tasks are available, and providing feedback when tasks are complete. An  orchestration agent : this agent never provides output directly to the user. Instead, it looks at what the user is currently trying to accomplish, and responds with the plain-text name of the agent that should handle the task. The code then routes to that agent. A  continuation agent : it's sometimes necessary to chain agents together to complete a task. For instance, to check your account balance, you need to be authenticated first. The authentication agent doesn't know if you were simply trying to authenticate yourself or if it's part of a chain, and it doesn't need to. When the authentication agent completes, the continuation agent checks chat history to see what the original task was, and if there's more to do, it formulates a new request to the orchestration agent to get you there without further user input. A  global state  keeps track of the user and their current state, shared between all the agents. The flow of the the system looks something like this: The system in action To get a sense of how this works in practice, here's sample output including helpful debug statements. Output that would be ordinarily shown to the user has two  >> , while user input has one  > . At the beginning of the conversation nothing's happened yet, so you get routed to the concierge: No current speaker, asking orchestration agent to decide\n\nConcierge agent selected\n\n>> Hi there! How can I assist you today? Here are some things I can help you with:\n\n>> Looking up a stock price\n\n>> Authenticating you\n\n>> Checking an account balance (requires authentication first)\n\n>> Transferring money between accounts (requires authentication and checking an account balance first)\n\n>> What would you like to do?\n\n> Transfer money The \"transfer money\" task requires authentication. The orchestration agent checks if you're authenticated while deciding how to route you (it does this twice for some reason, it's a demo!): No current speaker, asking orchestration agent to decide\n\nOrchestrator is checking if authenticated\n\nOrchestrator is checking if authenticated\n\nAuth agent selected It correctly determines you're not authenticated, so it routes you to the authentication agent: >> To transfer money, I need to authenticate you first. Could you please provide your username and password?\n\n> seldo This is a fun part: you've provided input, but it's not sufficient to complete the task (you didn't give a password). So when the flow goes back to the orchestration agent, the global state indicates that the \"authenticate\" agent is already running and hasn't completed yet, so it routes back to the authentication agent, and does that again for the password: There's already a speaker: authenticate\n\nAuth agent selected\n\nRecording username\n\n>> Thank you! Now, could you please provide your password?\n\n> monkey\n\nThere's already a speaker: authenticate\n\nAuth agent selected\n\nLogging in seldo\n\nChecking if authenticated\n\nAuthentication is complete Now the auth agent has called a done() function that indicates to the global state that it has completed its task. So the flow now goes to the continuation agent, which looks at the chat history and sees that the user was trying to transfer money. So it generates a prompt, as if spoken by the user, and sends that to the orchestration agent: >> You have been successfully authenticated. Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nOrchestrator checking if account has a balance\n\nOrchestrator checking if account has a balance\n\nAccount balance agent selected Now you're authenticated, but you haven't checked your balance yet, which the orchestration agent knows is necessary for transferring money. So it routes you to the account balance agent (after checking twice for some reason): >> Before you can transfer money, you need to check your account balance. Let's start by looking up your account balance. Could you please provide the name of the account you're interested in?\n\n> Checking\n\nThere's already a speaker: account_balance\n\nAccount balance agent selected\n\nLooking up account ID for Checking\n\nLooking up account balance for 1234567890\n\nAccount balance lookup is complete\n\n>> Your Checking account has a balance of $1000. Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nTransfer money agent selected The account balance agent asks you which account, uses a tool to get the ID for that account, and then marks itself as done. The continuation agent kicks in again and sees that you still haven't completed your original task of transferring money, so it prompts the orchestrator agent again. Unfortunately the orchestrator gets a little confused, and loops twice before finally routing you to the transfer money agent: Money transfer is complete\n\n>> Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nTransfer money agent selected\n\nMoney transfer is complete\n\n>> Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nOrchestrator checking if account has a balance\n\nTransfer money agent selected\n\n>> You have already checked your account balance. Please provide the following details to proceed with the money transfer:\n\n>> 1. The account ID to which you want to transfer the money.\n\n>> 2. The amount you want to transfer.\n\n> To account ID 1234324\n\nThere's already a speaker: transfer_money\n\nTransfer money agent selected\n\n>> How much would you like to transfer to account ID 1234324?\n\n> 500\n\nThere's already a speaker: transfer_money\n\nTransfer money agent selected\n\nChecking if balance is sufficient\n\nTransferring 500 from 1234567890 account 1234324\n\nMoney transfer is complete\n\n>> The transfer of $500 to account ID 1234324 has been successfully completed. If you need any further assistance, feel free to ask!\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said no_further_tasks We've reached the end of the task! The continuation agent sees that there are no further tasks, and routes you back to the concierge. The code Now let's look at some highlights of the code that gets all of this done. The core of the system is a central loop that runs forever. At the core of that is a very simple block that simply asks the orchestration agent who should speak next, and sets the  next_speaker  value which is contained in the state object that is passed between all the agents. Note that if there's already a sub-agent speaking, that agent gets to keep speaking. current_history = root_memory.get()\n\n # who should speak next? \n if  (state[ \"current_speaker\" ]):\n   print ( f\"There's already a speaker:  {state[ 'current_speaker' ]} \" )\n  next_speaker = state[ \"current_speaker\" ]\n else :\n   print ( \"No current speaker, asking orchestration agent to decide\" )\n  orchestration_response = orchestration_agent_factory(state).chat(\n    user_msg_str, \n    chat_history=current_history\n  )\n  next_speaker =  str (orchestration_response).strip() The orchestration agent has a very strict prompt; its output only goes to other machines. It includes a natural-language summary of the dependencies between agents:     system_prompt = ( f\"\"\"\n        You are on orchestration agent.\n        Your job is to decide which agent to run based on the current state of the user and what they've asked to do. Agents are identified by short strings.\n        What you do is return the name of the agent to run next. You do not do anything else.\n        \n        The current state of the user is:\n         {pprint.pformat(state, indent= 4 )} \n\n        If a current_speaker is already selected in the state, simply output that value.\n\n        If there is no current_speaker value, look at the chat history and the current state and you MUST return one of these strings identifying an agent to run:\n        * \" {Speaker.STOCK_LOOKUP.value} \" - if they user wants to look up a stock price (does not require authentication)\n        * \" {Speaker.AUTHENTICATE.value} \" - if the user needs to authenticate\n        * \" {Speaker.ACCOUNT_BALANCE.value} \" - if the user wants to look up an account balance\n            * If they want to look up an account balance, but they haven't authenticated yet, return \" {Speaker.AUTHENTICATE.value} \" instead\n        * \" {Speaker.TRANSFER_MONEY.value} \" - if the user wants to transfer money between accounts (requires authentication and checking an account balance first)\n            * If they want to transfer money, but is_authenticated returns false, return \" {Speaker.AUTHENTICATE.value} \" instead\n            * If they want to transfer money, but has_balance returns false, return \" {Speaker.ACCOUNT_BALANCE.value} \" instead\n        * \" {Speaker.CONCIERGE.value} \" - if the user wants to do something else, or hasn't said what they want to do, or you can't figure out what they want to do. Choose this by default.\n\n        Output one of these strings and ONLY these strings, without quotes.\n        NEVER respond with anything other than one of the above five strings. DO NOT be helpful or conversational.\n    \"\"\" ) A simple if-else block takes the output of the orchestration agent and uses it to instantiate the sub-agent to run next. This is when the state object gets passed to each sub-agent:          if  next_speaker == Speaker.STOCK_LOOKUP:\n             print ( \"Stock lookup agent selected\" )\n            current_speaker = stock_lookup_agent_factory(state)\n            state[ \"current_speaker\" ] = next_speaker\n         elif  next_speaker == Speaker.AUTHENTICATE:\n             print ( \"Auth agent selected\" )\n            current_speaker = auth_agent_factory(state)\n            state[ \"current_speaker\" ] = next_speaker\n         elif  next_speaker == Speaker.ACCOUNT_BALANCE:\n             print ( \"Account balance agent selected\" )\n            current_speaker = account_balance_agent_factory(state)\n            state[ \"current_speaker\" ] = next_speaker\n         elif  next_speaker == Speaker.TRANSFER_MONEY:\n             print ( \"Transfer money agent selected\" )\n            current_speaker = transfer_money_agent_factory(state)\n            state[ \"current_speaker\" ] = next_speaker\n         elif  next_speaker == Speaker.CONCIERGE:\n             print ( \"Concierge agent selected\" )\n            current_speaker = concierge_agent_factory(state)\n         else :\n             print ( \"Orchestration agent failed to return a valid speaker; ask it to try again\" )\n            is_retry =  True \n             continue And then the full chat history is passed as part of a regular chat message to the newly-instantiated agent: response = current_speaker.chat(user_msg_str, chat_history=current_history) The agent reads its prompt and the user input and decides what to say. As we saw in our very first block of code, if the speaker is already selected, then the loop will keep talking to the current sub-agent. This continues until the sub-agent has completed its task, at which point its prompt instructs it to call the  done()  function:      def   done () ->  None :\n         \"\"\"When you complete your task, call this tool.\"\"\" \n         print ( \"Money transfer is complete\" )\n        state[ \"current_speaker\" ] =  None \n        state[ \"just_finished\" ] =  True This modifies the state, setting the current speaker to none. This triggers the outer loop to run the continuation agent, to see if there's anything else to do:          elif  state[ \"just_finished\" ] ==  True :\n             print ( \"Asking the continuation agent to decide what to do next\" )\n            user_msg_str =  str (continuation_agent_factory(state).chat( \"\"\"\n                Look at the chat history to date and figure out what the user was originally trying to do.\n                They might have had to do some sub-tasks to complete that task, but what we want is the original thing they started out trying to do.                                                                      \n                Formulate a sentence as if written by the user that asks to continue that task.\n                If it seems like the user really completed their task, output \"no_further_task\" only.\n            \"\"\" , chat_history=current_history))\n             print ( f\"Continuation agent said  {user_msg_str} \" )\n             if  user_msg_str ==  \"no_further_task\" :\n                user_msg_str =  input ( \">> \" ).strip()\n            state[ \"just_finished\" ] =  False The continuation agent's prompt instructs it to reply as if it were the user asking to perform a task, or to output  no_further_task  if there's no more to do. If there's a new task, the output of the continuation agent becomes the input to the orchestrator, which selects a new speaker. If there's no further task, the loop pauses for more user input. And that's the full system! The sub-agents can be arbitrarily complicated, multi-turn systems in themselves, and the outer loop doesn't need to know how they work, just how they depend on each other. What's next We think there's some novel stuff in here: coordinating multiple agents \"speaking\" simultaneously, creating implicit \"chains\" of agents through natural language instructions, using a \"continuation\" agent to manage those chains, and using a global state this way. We're excited to see what you do with the patterns we've laid out here. Don't forget to  check out the open-source repo !", "author": "Laurie Voss", "date": "Jul 17, 2024", "tags": ["llama-agents", "Agents"]},
{"title": "Retrieving Privacy-Safe Documents Over A Network", "content": "In a  recent blog post , we introduced our  llama-index-networks  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores—that one may not otherwise have access to—means more accurate responses to an even wider breadth of queries. A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we’ll be relying on some recent developments in the area of Privacy-Enhancing Techniques. The story of Alex, Bob and Beth continues To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply. We showed then how such data a collaboration could be permitted through  llama-index-networks  by taking the following steps: Bob and Beth both build their respective QueryEngine’s (RAG in llama-index lingo) Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth’s ContributorService’s In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets. Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can’t unless protective measures are applied before sharing across the network. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can’t unless protective measures are applied before sharing across the network. Sidebar: differential privacy primer In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual’s inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm. By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion). A light introduction to differential privacy. A light introduction to differential privacy. Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth’s data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled “Privacy-preserving in-context learning with differentially private few-shot generation” by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network! There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration. Bob and Beth create privacy-safe synthetic copies of their private datasets Bob and Beth both build their respective QueryEngine’s over their synthetic datasets Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth’s ContributorService’s Creating differentially private synthetic copies of a private dataset Fortunately, for step 0., we can make use of the  DiffPrivateSimpleDataset  pack. from  llama_index.core.llama_datasets.simple  import  LabelledSimpleDataset\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\n from  llama_index.llms.openai  import  OpenAI\n import  tiktoken\n\n # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies \n\nllm = OpenAI(\n    model= \"gpt-3.5-turbo-instruct\" ,\n    max_tokens= 1 ,\n    logprobs= True ,\n    top_logprobs= 5 ,   # OpenAI only allows for top 5 next token \n)                     # as opposed to entire vocabulary \ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\n\nbeth_private_dataset: LabelledSimpleDataset = ...  # a dataset that contains \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # examples with two attributes \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # `text` and `reference_label` \n\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\n    llm=llm,\n    tokenizer=tokenizer,\n    prompt_bundle=prompt_bundle,     # params for preparing required prompts \n    simple_dataset=simple_dataset,   # to generate the synthetic examples  \n)\n\nbeth_synthetic_dataset =  await  beth_synthetic_generator.arun(\n\t\tsize= 3 ,   # number of synthetic observations to create \n\t\tsigma= 0.5    # param that determines the level of privacy \n) With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It’s worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.) Example: Symptom2Disease In this section of the blog post, we go over an actual example application of the privacy-safe networks over the  Symptom2Disease  dataset. This dataset consists of 1,200 examples each containing a “symptoms” description as well as the associated “disease” label — the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network. Generate privacy-safe synthetic observations of Symptom2Disease We use the training subset and apply the  DiffPrivateSimpleDatasetPack  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a  LabelledSimpleDataset  object. import  pandas  as  pd\n from  sklearn.model_selection  import  train_test_split\n from  llama_index.core.llama_dataset.simple  import  (\n    LabelledSimpleDataExample,\n    LabelledSimpleDataset,\n)\n from  llama_index.core.llama_dataset.base  import  CreatedBy, CreatedByType\n\n # load the Symptom2Disease.csv file \ndf = pd.read_csv( \"Symptom2Disease.csv\" )\ntrain, test = train_test_split(df, test_size= 0.2 )\n\n # create a LabelledSimpleDataset (which is what the pack works with) \nexamples = []\n for  index, row  in  df.iterrows():\n    example = LabelledSimpleDataExample(\n        reference_label=row[ \"label\" ],\n        text=row[ \"text\" ],\n        text_by=CreatedBy( type =CreatedByType.HUMAN),\n    )\n    examples.append(example)\n\nsimple_dataset = LabelledSimpleDataset(examples=examples) Now we can use the llama-pack to create our synthetic observations. import  llama_index.core.instrumentation  as  instrument\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\n from  llama_index.llms.openai  import  OpenAI\n import  tiktoken\n from  .event_handler  import  DiffPrivacyEventHandler\n import  asyncio\n import  os\n\nNUM_SPLITS =  3 \nT_MAX =  150 \n\nllm = OpenAI(\n    model= \"gpt-3.5-turbo-instruct\" ,\n    max_tokens= 1 ,\n    logprobs= True ,\n    top_logprobs= 5 ,\n)\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\n\nprompt_bundle = PromptBundle(\n    instruction=(\n         \"You are a patient experiencing symptoms of a specific disease. \" \n         \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\n\" \n         \"Start your answer directly after 'Symptoms: '. Begin your answer with [RESULT].\\n\" \n    ),\n    label_heading= \"Disease\" ,\n    text_heading= \"Symptoms\" ,\n)\n\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\n    llm=llm,\n    tokenizer=tokenizer,\n    prompt_bundle=prompt_bundle,\n    simple_dataset=simple_dataset,\n)\n\nsynthetic_dataset =  await  dp_simple_dataset_pack.arun(\n    sizes= 3 ,\n    t_max=T_MAX,\n    sigma= 1.5 ,\n    num_splits=NUM_SPLITS,\n    num_samples_per_split= 8 ,   # number of private observations to create a \n)                              # synthetic obsevation \nsynthetic_dataset.save_json( \"synthetic_dataset.json\" ) Create a network with two contributors Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well. Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we’re using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user). Step 1:  Contributor’s build their Retriever over their synthetic datasets. import  os\n from  llama_index.core  import  VectorStoreIndex\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\n from  llama_index.core.schema  import  TextNode\n\n\n # load the synthetic dataset \nsynthetic_dataset = LabelledSimpleDataset.from_json(\n     \"./data/contributor1_synthetic_dataset.json\" \n)\n\n\nnodes = [\n    TextNode(text=el.text, metadata={ \"reference_label\" : el.reference_label})\n     for  el  in  synthetic_dataset[:]\n]\n\nindex = VectorStoreIndex(nodes=nodes)\nsimilarity_top_k =  int (os.environ.get( \"SIMILARITY_TOP_K\" ))\nretriever = index.as_retriever(similarity_top_k=similarity_top_k) Step 2:  Contributor’s expose their Retrievers behind a ContributorRetrieverService from  llama_index.networks.contributor.retriever.service  import  (\n    ContributorRetrieverService,\n    ContributorRetrieverServiceSettings,\n)\n\nsettings = ContributorRetrieverServiceSettings()  # loads from .env file \nservice = ContributorRetrieverService(config=settings, retriever=retriever)\napp = service.app Step 3:  Define the NetworkRetriever that connects to the ContributorRetrieverServices from  llama_index.networks.network.retriever  import  NetworkRetriever\n from  llama_index.networks.contributor.retriever  import  ContributorRetrieverClient\n from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\n\n # ContributorRetrieverClient's connect to the ContributorRetrieverService \ncontributors = [\n    ContributorRetrieverClient.from_config_file(\n        env_file= f\"./client-env-files/.env.contributor_ {ix} .client\" \n    )\n     for  ix  in   range ( 1 ,  3 )\n]\nreranker = CohereRerank(top_n= 5 )\nnetwork_retriever = NetworkRetriever(\n    contributors=contributors, node_postprocessors=[reranker]\n) With the  NetworkRetriever  established, we can retrieve synthetic observations from the two contributors data against a query. related_records = network_retriever.aretrieve( \"Vomitting and nausea\" )\n print (related_records)  # contain symptoms/disease records that are similar to \n\t\t\t\t\t\t\t\t\t\t\t  # to the queried symptoms. Evaluating the  NetworkRetriever To evaluate the efficacy of the  NetworkRetriever  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. hit rate:  a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. mean reciprocal rank:  similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. In addition to evaluating the  NetworkRetriever  we consider the two baselines that represent Retrieving only over the individual Contributor’s synthetic datasets. Retriever evaluations, with sigma equal to 1.5. In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever’s in the test set. This shouldn’t be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor’s synthetic observations—this is the point after all of a network! Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated). Finally, this level of privacy can be controlled via the noise parameter  sigma . In the example above we used a  sigma  of 1.5, which for this dataset amounts to an  epsilon  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are  generally considered to be quite private .) Below, we share the evaluations that result from using a  sigma  of 0.5, which amounts to an  epsilon  of 15.9—higher values of  epsilon  or privacy-loss means less privacy. # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon \nepsilon = dp_simple_dataset_pack.sigma_to_eps(\n\t\tsigma= 0.5 ,\n\t\tmechanism= \"gaussian\" ,\n\t\tsize= 3 * 24 ,\n\t\tmax_token_cnt= 150    # number of max tokens to generate per synthetic example \n) Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5. So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening. # synthetic example epsilon = 1.3 \n{\n     \"reference_label\" :  \"Psoriasis\" ,\n     \"text\" :  \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\" ,\n     \"text_by\" : {\n         \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\n         \"type\" :  \"ai\" \n    }\n},\n\n # synthetic example epsilon = 15.9 \n{\n   \"reference_label\" :  \"Migraine\" ,\n   \"text\" :  \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\" ,\n   \"text_by\" : {\n     \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\n     \"type\" :  \"ai\" \n  }\n}, We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM. In summary We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. Learn more! To delve deeper into the materials of this blog post, we share a few links below: Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( link ) Demo notebooks for the  DiffPrivateSimpleDataset  ( link ) The source code for creating the synthetic Symptom2Disease observations using the  DiffPrivateSimpleDataset  ( link )", "author": "Andrei", "date": "Mar 20, 2024", "tags": ["Privacy", "llama-index-networks"]},
{"title": "LlamaIndex Newsletter 2024-04-30", "content": "Greetings, LlamaIndex fans! 🦙 It’s delightful springtime weather out here in San Francisco and we hope you’re having a good day! Check out this week’s summary of news, guides and tutorials. 🤩  The highlights: Day 0 support for Microsoft’s Phi-3 Mini!  Tweet create-llama now supports Llama 3 and Phi-3 and has lots of new features!  Tweet Simon was on a security podcast!  Tweet ✨ Feature Releases and Enhancements: Jina AI released powerful new open-source rerankers and we have day 0 support as usual!  Tweet Phi-3 mini was released by Microsoft, a powerful new small model, and we  put it through its paces  (spoiler: it’s good!) and released day-0 support via Ollama!  Tweet Our create-llama application generator was updated with many features including being able to show the sources it retrieved from, as well as Llama3 and Phi-3 support. Build an app from scratch in 30 seconds!  Tweet Language Agent Tree Search (LATS) is a powerful new technique that iteratively plans out an array of potential futures, interleaving tool use and reflection to solve problems. We released a Llama Pack implementation.  Tweet 🎥 Demos: Memary  is a reference implementation of using long-term memory in knowledge graph form for building agents.  Tweet Our hackathon winners wrote a blog post about their winning project, a knowledge-retrieval bot trained on documentation, including how they built it.  Tweet 🗺️ Guides: Co-founder Jerry shared his latest deck, a guide to building a context-augmented research assistant that enables multi-hop Q&A, reflection and more.  Slides ,  tweet Corrective RAG or CRAG adds a retrieval evaluation module that determines whether the retrieved context is “correct” and improves retrieval. Check out this guide on how to build it step-by-step!  Tweet Jerry also went in-depth on the ingredients necessary for building a complex agent.  Tweet Michael of KX Systems demonstrated making retrieval a multi-hop process for better results.  Tweet A reference architecture for advanced RAG with LlamaIndex and AWS Bedrock.  Tweet ✍️ Tutorials: Build a best-in-class RAG application using Qdrant as a vector store, Jina AI embeddings, and Mixtral 8x7b as the LLM.  Tweet Learn 3+ patterns for building LLM apps on AWS with LlamaIndex.  Tweet A 9-part series on taking RAG from prototype to production.  Tweet 🎥  Webinars: KX Systems are hosting a webinar on May 1 about getting the most out of LlamaParse!  Tweet Co-founder Simon appeared on the MLSecOps podcast talking about security in LLM applications.  Tweet 👯‍♀️  Community: We launched a  LlamaIndex user group in Korea !", "author": "LlamaIndex", "date": "Apr 30, 2024", "tags": ["Newsletter"]},
{"title": "The latest updates to LlamaCloud", "content": "To build a production-quality LLM agent over your data, you need a production-quality data processing layer. LlamaCloud is that data processing and management layer for your AI knowledge assistants. Since  launching a LlamaCloud waitlist last week , we’ve gotten hundreds of signups and published case studies showing how it cuts  production development hours by 50% . On top of that, our team has shipped a slew of new features at a breakneck pace in the past week. We’re excited to highlight these new features that collectively help you  set up a chat interface in minutes,   increase developer collaboration within your team, and access more data and metadata. Set up a Chat Interface in Minutes We are releasing  LlamaCloud Chat,  which gives you an easy-to-use chat interface over your data. This chat interface is a conversational RAG pipeline built over the advanced retrieval interface that a given pipeline provides, and has out-of-the-box support for streaming and citations - it’s powered by the same DNA as  create-llama , our fully open-source set up tool  for LLM applications. The LlamaCloud UI already lets you set up a data pipeline over any data in minutes, and now you get a full-blown ChatGPT over your data in minutes. Besides the chat UI, you also have additional flexibility: You can customize metadata filters in the retrieval parameters You can view retrieved nodes and their source files Besides chunk-level retrieval, you can now do  file-level retrieval  (more on this soon!) LlamaCloud is fundamentally a developer tool: with these updates, we enable developers to spend less time on data pipeline setup and iteration, and more time on writing the orchestration logic on top of this interface. Increased Developer Collaboration The team selection interface Organization settings We’ve added  organizational features  into LlamaCloud, enabling any individual user to create an organization and add other users to the organization. Any user within an organization will have a view of all the organization’s projects and indexes within each project. This allows your team to have a single-source of truth for your data pipelines. In the past each developer would spend time re-indexing/experimenting with the same sources of data. This feature enables transparency, re-use, and generally more rapid development velocity. Improved Data and Metadata Access We’ve made several updates here - we’ve added more data connectors and added features to let you more easily access and customize metadata. We added a Notion, Slack, and Jira Connector Our Sharepoint connector now natively integrates with user IDs that you can filter for, enabling you to build LLM applications with access control. You can now attach metadata to any uploaded file as a CSV - do this through the UI or our API! Want to see what LlamaCloud can do for you? Come sign up on our  waitlist  for access. If you’re interested in chatting about enterprise plans,  get in touch. If you’ve gotten access to LlamaCloud, check out our  rich repository of demonstrations and examples  on how to build different LLM application use cases.", "author": "LlamaIndex", "date": "Jul 19, 2024", "tags": ["Llamacloud"]},
{"title": "LlamaIndex Newsletter 2024-05-07", "content": "Hello LlamaIndex fam! 🦙 If you’re in SF, join us for the first-ever  Llama 3 Hackathon  ( invitation here )! Shack15 is an amazing venue and it’s sure to be a fun time. If you can’t make it, stay tuned for the rundown on the cool projects that come out of the event. Now, on to the highlights: 🤩  The highlights: LlamaIndex.TS hits v0.3!  Loads of new features inside! LlamaIndex Python hits v0.10.34 ! A bumper release! That’s a lot! ✨ Feature Releases and Enhancements: Two big releases this week! LlamaIndex.TS hit version 0.3!  Tweet ,  Blog post Features:  Agent support including ReAct, Anthropic and OpenAI agents, as well as a generic AgentRunner class Standardized Web Streams compatible with React 19, Deno, and Node 22 More comprehensive type system Enhanced support for deployment on Next.js, Deno, Cloudflare Workers and Waku LlamaIndex Python hit version 0.10.34!  Tweet Features:    Introspective agents that work through reflection Support for huggingface text-generation-inference API Structured planning agent A chat summary memory buffer Rust-based LLM support with Mistral.rs Milvus sparse hybrid search Google Firestore vector store support A new LlamaPack for the Reflection Agentic Pattern.  Tweet 🎥 Demos: Filter AirBnB listings using natural language with this open-source demo! It uses Mistral AI’s Mixtral 8x7b and Qdrant engine, plus Streamlit to build UI.  Tweet ,  Blog post Fully local RAG with Llama 3, Ollama and LlamaIndex! A short, sweet guide.  Tweet ,  Blog post Fine-tune your embedding model using labels from a reranker.  Tweet ,  Blog post 🗺️ Guides: Hanane Dupouy walks us through building an agent that can perform complex financial calculations.  Tweet ,  Slides Plaban Nayak sets up a local, open-source RAG pipeline that uses Llama 3 and Qdrant to demonstrate how to improve the accuracy of your RAG with reranking.  Tweet ,  Blog post Jason Zhou talks about the components needed for agentic RAG.  Tweet Divyanshu Dixit walks us through agents dedicated to workflow automation.  Tweet ,  Blog post ✍️ Tutorials: Tyler Hutcherson of Redis and our own Laurie Voss walk you through building agentic RAG with semantic caching and other production-ready techniques.  Video ,  Notebook Cleanlab has a tutorial on getting trustworthiness scores from your RAG pipeline to allow you to avoid hallucinations and course-correct.  Tweet ,  Notebook 🎥  Webinars: On May 8 we’ll be co-hosting a webinar with Pulumi on deploying AI applications to AWS.  Tweet Our own Andrei and friends walk you from basic RAG through handling long-context RAG all the way to evaluating your RAG pipeline.  Tweet ,  Video ,  Notebook", "author": "LlamaIndex", "date": "May 7, 2024", "tags": ["Newsletter"]},
{"title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications", "content": "Strategic alliance and joint product promises to broaden the adoption of generative AI across industries Arize AI, a pioneer and leader in AI observability and LLM evaluation, and LlamaIndex, a leading data framework for LLM applications, debuted a new joint offering today called LlamaTrace, a hosted version of Arize OSS Phoenix. According to a soon-to-release survey, 47.7% of AI engineers and developers building generative AI applications are leveraging retrieval today in their LLM Applications. By connecting data to generative AI, orchestration frameworks like LlamaIndex can be game-changers in accelerating generative AI development. However, for many teams and enterprises technical challenges remain in getting modern LLM systems – with layers of abstraction – ready for the real world. To help, Arize and LlamaIndex are debuting an LLM tracing and observability platform that works natively with the LlamaIndex and Arize ecosystem. With a foundation based on  Arize Phoenix OSS , the hosted version of Phoenix offers the ability to persist application telemetry data generated during AI development in order to better experiment, iterate, and collaborate in development or production. The solution has a foundation in open source and features a fully hosted, online, persistent deployment option for teams that do not want to self host. AI engineers can instantly log traces, persist datasets, run experiments, run evaluations – and share those insights with colleagues. The new offering is available today, and can be accessed through either a LlamaIndex or Arize account. “We share a vision with LlamaIndex in enabling builders to reduce the time it takes to deploy generative AI into production but in a way that is super battle hardened for business-critical use cases,” said Jason Lopatecki, CEO and Co-Founder of Arize. “As leaders in our respective spaces with a common philosophy in empowering AI engineers and developers, we’re uniquely positioned here to do something that can move modern LLMOps forward and broaden adoption.” “Prototyping a RAG pipeline or agent is easy, but every AI engineer needs the right data processing layer, orchestration framework, and experimentation/monitoring tool in order to take these applications to production. LlamaTrace by Arize offers the richest toolkit we’ve seen in enabling developers to observe, debug, and evaluate every granular step of a very complex LLM workflow, and it nicely complements the production-ready data platform and orchestration framework that LlamaCloud and LlamaIndex offer.” - Jerry Liu, CEO of LlamaIndex About Arize AI Arize AI is an AI observability and LLM evaluation platform that helps teams deliver and maintain more successful AI in production. Arize’s automated monitoring and observability platform allows teams to quickly detect issues when they emerge, troubleshoot why they happened, and improve overall performance across both traditional ML and generative use cases. Arize is headquartered in Berkeley, CA.", "author": "LlamaIndex", "date": "Jul 11, 2024", "tags": ["Evaluation", "Observability"]},
{"title": "Using LlamaIndex and llamafile to build a local, private research assistant", "content": "This is a guest post from our friends at Mozilla about  Llamafile llamafile , an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from  HuggingFace  then run the file. That's it.  On most computers, you won't need to install anything. There are a few reasons why you might want to run an LLM on your laptop, including: 1. Privacy: Running locally means you won't have to share your data with third parties. 2. High availability: Run your LLM-based app without an internet connection. 3. Bring your own model: You can easily test many different open-source LLMs (anything available on HuggingFace) and see which one works best for your task. 4. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-based system without paying for API calls. In this blog post, we'll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we'll show how to use LlamaIndex with your llamafile as the LLM & embedding backend for a local RAG-based research assistant. You won't have to sign up for any cloud service or send your data to any third party--everything will just run on your laptop. Note: You can also get all of the example code below as a Jupyter notebook from our  GitHub repo . Download and run a llamafile First, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There's nothing to install or configure (with a few caveats, discussed  here ). Each llamafile bundles 1) model weights & metadata in gguf format + 2) a copy of  `llama.cpp`  specially compiled using [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT-like browser interface, a CLI, and an OpenAI-compatible REST API for chat models. There are only 2 steps to setting up a llamafile: 1. Download a llamafile 2. Make the llamafile executable We'll go through each step in detail below. Step 1: Download a llamafile There are many llamafiles available on the  HuggingFace model hub  (just search for 'llamafile') but for the purpose of this walkthrough, we'll use  TinyLlama-1.1B  (0.67 GB,  model info ). To download the model, you can either click this download link:  TinyLlama-1.1B  or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection. wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile  This model is small and won't be very good at actually answering questions but, since it's a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it's good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like  Mistral-7B-Instruct  (5.15 GB,  model info ). Step 2: Make the llamafile executable If you didn't download the llamafile from the command line, figure out where your browser stored your downloaded llamafile. Now, open your computer's terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile` If you're using macOS, Linux, or BSD , you'll need to grant permission for your computer to execute this new file. (You only need to do this once.): If you're on Windows, instead just rename the file by adding \".exe\" on the end  e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe` chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile Kick the tires Now, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded: ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n\nllamafile v0.7.0 This post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don't work as expected, please  post an issue on the llamafile issue tracker . The easiest way to use your llamafile is via its built-in chat interface. In a terminal, run ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile Your browser should open automatically and display a chat interface. (If it doesn't, just open your browser and point it at http://localhost:8080). When you're done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you're running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile. In the rest of this walkthrough, we'll be using the llamafile's built-in inference server instead of the browser interface. The llamafile's server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available  here . To start the llamafile in server mode, run: ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding Summary: Download and run a llamafile # 1. Download the llamafile-ized model \nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n\n # 2. Make it executable (you only need to do this once) \n chmod  +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n # 3. Run in server mode \n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding Build a research assistant using LlamaIndex and llamafile Now, we'll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose  homing pigeons . We'll show how to prepare your data, index into a vector store, then query it. One of the nice things about running an LLM locally is privacy. You can mix both \"public data\" like Wikipedia pages and \"private data\" without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won't have to worry about leaking data. Below, we'll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic. To get started, download our example data: mkdir  data\n\n # Download 'The Homing Pigeon' manual from Project Gutenberg \nwget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n\n # Download some notes on homing pigeons \nwget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md Next, we'll need to install LlamaIndex and a few of its integrations: # Install llama-index \npip install llama-index-core\n # Install llamafile integrations and SimpleWebPageReader \npip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web Start your llamafile server and configure LlamaIndex In this example, we'll use the same llamafile to both produce the embeddings that will get indexed in our vector store and as the LLM that will answer queries later on. (However, there is no reason you can't use one llamafile for the embeddings and separate llamafile for the LLM functionality--you would just need to start the llamafile servers on different ports.) To start the llamafile server, open a terminal and run: ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080 Now, we'll configure LlamaIndex to use this llamafile: # Configure LlamaIndex \n from  llama_index.core  import  Settings\n from  llama_index.embeddings.llamafile  import  LlamafileEmbedding\n from  llama_index.llms.llamafile  import  Llamafile\n from  llama_index.core.node_parser  import  SentenceSplitter\n\nSettings.embed_model = LlamafileEmbedding(base_url= \"http://localhost:8080\" )\n\nSettings.llm = Llamafile(\n\tbase_url= \"http://localhost:8080\" ,\n\ttemperature= 0 ,\n\tseed= 0 \n)\n\n # Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don't take up the model's entire \n # context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both \n # the system prompt and the user's actual question. \nSettings.transformations = [\n\tSentenceSplitter(\n    \tchunk_size= 256 ,\n    \tchunk_overlap= 5 \n\t)\n] Prepare your data and build a vector store Now, we'll load our data and index it. # Load local data \n from  llama_index.core  import  SimpleDirectoryReader\nlocal_doc_reader = SimpleDirectoryReader(input_dir= './data' )\ndocs = local_doc_reader.load_data(show_progress= True )\n\n # We'll load some Wikipedia pages as well \n from  llama_index.readers.web  import  SimpleWebPageReader\nurls = [\n\t 'https://en.wikipedia.org/wiki/Homing_pigeon' ,\n\t 'https://en.wikipedia.org/wiki/Magnetoreception' ,\n]\nweb_reader = SimpleWebPageReader(html_to_text= True )\ndocs.extend(web_reader.load_data(urls))\n\n # Build the index \n from  llama_index.core  import  VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(\n\tdocs,\n\tshow_progress= True ,\n)\n\n # Save the index \nindex.storage_context.persist(persist_dir= \"./storage\" ) Query your research assistant Finally, we're ready to ask some questions about homing pigeons. query_engine = index.as_query_engine()\n print (query_engine.query( \"What were homing pigeons used for?\" )) \tHoming pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection. print (query_engine.query( \"When were homing pigeons first used?\" )) The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries. Conclusion In this post, we've shown how to download and set up an LLM running locally via llamafile. Then, we showed how to use this LLM with LlamaIndex to build a simple RAG-based research assistant for learning about homing pigeons. Your assistant ran 100% locally: you didn't have to pay for API calls or send data to a third party. As a next step, you could try running the examples above with a better model like  Mistral-7B-Instruct . You could also try building a research assistant for different topic like \"semiconductors\" or \"how to bake bread\". To find out more about llamafile, check out the project on  GitHub , read this  blog post  on bash one-liners using LLMs, or say hi to the community on  Discord .", "author": "Kate Silverstein", "date": "May 14, 2024", "tags": ["llamafile"]},
{"title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB", "content": "This is a guest post from Team CLAB, the winners of \"Best Use of LlamaIndex\" at our recent hackathon with MongoDB. Imagine this: you’re deep in a coding project, and a critical question pops up about a specific tool or library. You start the dreaded documentation shuffle — searching through wikis, FAQs, maybe even firing up a separate chatbot for specific tools (like those from LlamaIndex, FireworksAI or anyone else). It’s frustrating! 🤯 We wanted to change that. That’s why Team CLAB built  LlamaWorksDB (try it out!) , your friendly AI-powered doc wizard ✨. No more scattered searches! It taps into the knowledge of multiple sponsors of our hackathon including LlamaIndex, Fireworks.ai, and MongoDB, all through a single chatbot interface. Need something explained from MongoDB’s docs? Got it! Want a code example from Fireworks.ai? Easy! The foundation: LlamaIndex and data ingestion LlamaIndex was the heart and soul of LlamaWorksDB. It’s like a super versatile toolbox for handling all kinds of documentation! We primarily used their open-source readers to grab info straight from websites. A cool hack we did was customize the  SimpleWebPageReader . We taught it to ignore website navigation bars, saving us a ton of precious tokens. 💪 While this worked great for the documentation sites, we used LlamaIndex’s  GithubRepositoryReader  to easily read through each repo. from  llama_index.readers.web  import  SimpleWebPageReader\n import  re\n\n class   LlamaDocsPageReader ( SimpleWebPageReader ):\n    def   load_data ( self, urls ):\n       documents =  super ().load_data(urls)\n       processed_documents = []\n        for  doc  in  documents:\n           processed_doc = self.process_document(doc)\n           processed_documents.append(processed_doc)\n        return  processed_documents\n\n    def   process_document ( self, document ):\n        # Split the document text by \"Table of Contents\" \n       pattern =  r'(?i)\\n\\n*table\\s*of\\s*contents\\n\\n*' \n       parts = re.split(pattern, document.text, maxsplit= 1 )\n        # If there is a part after \"Table of Contents\", use it as the document text \n        if   len (parts) >  1 :\n           document.text =  \"Table of contents\" .join(parts[ 1 :])\n        return  document Choosing how to split up the docs was interesting. LlamaIndex has options ranging from the basic  SentenceSplitter  to their  SemanticNodeParser , which uses AI to group similar ideas. We went with the latter for those perfectly sized, meaningful chunks. Finally, we embedded each ‘node’ and sent each as a document to MongoDB. Talk about streamlined! MongoDB stored the text, metadata,  and  our embeddings — ideal for the kind of search we wanted to build. We used Nomic’s flexible embedding model via Fireworks, which let us fine-tune the dimensions for maximum efficiency. # FireworksEmbedding defaults to using model \nembed_model = FireworksEmbedding(api_key=os.getenv( 'FIREWORKS_API_KEY' ),\n                                model= \"nomic-ai/nomic-embed-text-v1.5\" ,\n                                embed_batch_size= 10 ,\n                                dimensions= 768   # can range from 64 to 768 \n                                )\n\n # the tried and true sentence splitter \ntext_splitter = SentenceSplitter(chunk_size= 1000 , chunk_overlap= 200 )\n # the semantic splitter uses our embedding model to group semantically related sentences together \nsemantic_parser = SemanticSplitterNodeParser(embed_model=embed_model)\n\n # we set up MongoDB as our document and vector database \nvector_store = MongoDBAtlasVectorSearch(\n   pymongo.MongoClient(os.getenv( 'MONGO_URI' )),\n   db_name= \"fireParse\" ,\n   collection_name= \"llamaIndexDocs\" ,\n   index_name= \"llama_docs_index\" \n)\n\n #finally we use LlamaIndex's pipeline to string this all together \npipeline = IngestionPipeline(\n   transformations=[\n       semantic_parser,  #can replace with text_splitter \n       embed_model,\n   ],\n   vector_store=vector_store,\n) Once we have everything set up, we can create documents from URLs in MongoDB! Below is an example of using three URLs but we used hundreds. example_urls = [\n    \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook\" ,\n    \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/anthropic_haiku/\" ,\n   \"https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearch/\" \n]\n\n # read in the documents and pass them through our pipeline \ndocuments = LlamaDocsPageReader(html_to_text= True ).load_data(example_urls)\npipeline.run(documents=documents, show_progress= True ) You can see in MongoDB how our documents have text, embedding (with 768 dimensions), and metadata. Example document in MongoDB Atlas that resulted from the pipeline MongoDB Atlas for vector search MongoDB Atlas was our go-to for storing both the documentation text and the embeddings themselves. It’s incredibly versatile! Setting up vector search within Atlas is a breeze, allowing us to quickly find the most relevant document chunks. Plus, LlamaIndex’s metadata parsing played perfectly with Atlas — we could easily filter results based on things like document source or topic. Setting up Vector Search:  It was remarkably simple! We just specified these few things: Path to the embedding field within our documents. Embedding dimension size. Similarity metric (e.g., cosine similarity). That it’s a vector index. Filtering Power (Optional):  For even finer control, we could add paths to fields we wanted to filter our searches by (like the company’s name). Whether you’re building a complex web app or a quick Streamlit prototype, LlamaIndex ChatEngines have you covered. They effortlessly manage conversation history, let you perform lightning-fast vector searches, and unlock a whole suite of powerful tools. We built our ChatEngine directly from our trusty MongoDB index. This integration was surprisingly simple: def   get_index ():\n   logger.info( \"Connecting to index from MongoDB...\" )\n   store = MongoDBAtlasVectorSearch(\n       db_name=os.environ[ \"MONGODB_DATABASE\" ],\n       collection_name=os.environ[ \"MONGODB_VECTORS\" ],\n       index_name=os.environ[ \"MONGODB_VECTOR_INDEX\" ],\n   )\n   index = VectorStoreIndex.from_vector_store(store)\n   logger.info( \"Finished connecting to index from MongoDB.\" )\n    return  index\n\nindex = get_index()\nindex.as_chat_engine(\n    llm = Fireworks(\n             api_key=env_vars[ 'FIREWORKS_API_KEY' ],\n             model= \"accounts/fireworks/models/mixtral-8x22b-instruct\"   #Can be changed out for Llama3 \n             )\n    chat_mode= \"best\" , \n    context_prompt=(\n            \"\"\" You are a software developer bot that is an expert at reading over documentation to answer questions.\n           Use the relevant documents for context:\n           {context_str}\n           \\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\n           \"\"\" \n           ),\n    verbose= True \n    ) create-llama : from idea to app in record time We were seriously impressed by Create-Llama. Usually, building a full-stack app takes time, but Create-Llama had us up and running in under 15 minutes! All we did was point it towards our vector database and give a few basic details. Honestly, it made development a joy!  This blog post goes into more detail about how to use create-llama. The create-llama setup screen The create-llama app, customized and ready to go Deployment: Render and Vercel To make LlamaWorksDB production-ready and easily accessible, we turned to  Render  and  Vercel . Render was a perfect fit for our Python FastAPI backend, as it focuses on ease of deployment and scalability. Vercel seamlessly handled our Next.js frontend — we loved its developer-centric approach and the effortless build process. Both platforms made deployment a breeze, letting us focus on coding rather than complex infrastructure setup. Future directions Our hackathon success is just the beginning. We envision LlamaWorksDB evolving into a powerhouse for developers seeking answers within technical documentation. Here’s how we see it growing: Enhanced Retrieval:  We’re excited to experiment with LlamaIndex’s powerful capabilities like MultiVectorSearch to further refine our results. Integrating different LLMs will open up new possibilities for how LlamaWorksDB understands and interacts with technical content. A Focus on Documentation:  We want to double down on making LlamaWorksDB the ultimate tool for navigating documentation. This means exploring specialized techniques and tools designed specifically for understanding complex technical information. LlamaWorksDB is an open-source project in Beta, and we believe in the power of collaboration! If you’re passionate about AI-powered documentation tools, we invite you to: Try it out:  Explore our  GitHub repo  and give  LlamaWorksDB a spin ! Contribute:  Help us build new features, test integrations, and refine our search capabilities. Share your feedback:  Let us know how we can make LlamaWorksDB even better. Together, let’s revolutionize the way developers interact with documentation! 🔗 Explore our project and join the innovation:  https://github.com/clab2024/clab/   https://clab-ui.vercel.app/  (front-end) (leverages free credits responds late)  https://clab.onrender.com/docs  (back-end) Meet Team CLAB! 🎉 Chris Wood : Up-and-coming tech whiz, ready to graduate with valuable insights from his internship at Tutello. Leo Walker : Data Scientist with the discipline and precision of a Military Veteran. Andrew Townsend : A Computer Science graduate from SJSU, bringing fresh academic perspectives. Barath Subramaniam : The strategic mind behind Product Security AI and Data engineering at Adobe. Twitter:  @baraths84 Team CLAB (plus Laurie)", "author": null, "date": "Apr 29, 2024", "tags": ["Mongodb", "Fireworks", "LlamaParse", "Hackathons"]},
{"title": "LlamaIndex Newsletter 2024-07-16", "content": "Hello, Llama Family! 🦙 Welcome to this week’s edition of the LlamaIndex newsletter! We’re thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons. Before we get into the details of our newsletter, we’re thrilled to share the beta launch of LlamaCloud. This new data processing layer boosts RAG workflows with sophisticated parsing, indexing, and retrieval functions. Alongside this, we’re also introducing LlamaTrace in partnership with Arize AI, which provides unmatched tracing, observability, and evaluation capabilities for LLM application workflows. Signup here:  cloud.llamaindex.ai 🤩  The highlights: LlamaCloud Launch:  We’ve launched the beta release of LlamaCloud, a data processing layer designed to enhance RAG workflows with state-of-the-art parsing, indexing, and retrieval capabilities.  Blogpost ,  Tweet . LlamaTrace Launch:  In collaboration with Arize AI, we’ve introduced LlamaTrace, offering unmatched tracing, observability, and evaluation capabilities for LLM application workflows. It features detailed call stack tracing, one-click setup through LlamaIndex, and seamless integration with LlamaCloud.  Blogpost ,  Tweet . GraphRAG Implementation:  Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation.  Notebook ,  Tweet . Redis Queue Integration with Llama-Agents:  We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance.  Notebook ,  Tweet . ✨ Feature Releases and Enhancements: We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities.  Blogpost ,  Tweet . We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries.  Notebook ,  Tweet . We have integrated Redis Queue with llama-agents to enhance coordination in multi-agent workflows, allowing for robust communication.  Notebook ,  Tweet . We have introduced LlamaTrace in collaboration with Arize AI, offering unparalleled tracing, observability, and evaluation capabilities for LLM application workflows. LlamaTrace stands out for its detailed tracing, which logs the entire call stack, one-click setup through LlamaIndex, and seamless integration with LlamaCloud for easy access and authentication.  Blogpost ,  Tweet . We have integrated NebulaGraph with LlamaIndex, enhancing PropertyGraph capabilities with sophisticated extractors, customizable properties on nodes and edges, and advanced retrieval options.  Docs ,  Tweet . 💡 Demos: Lyzrai  has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndex’s data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction. 🗺️ Guides: Guide  to Multi-Modal RAG for Document Processing that introduces a multi-modal RAG architecture using LlamaParse, LlamaIndex, and GPT-4o, designed to handle complex slide decks.  Tweet . Guide  to using LlamaParse and GPT-4o for Financial Report RAG to to effectively parse and synthesize complex financial documents, enhancing clarity and accuracy in data analysis. Guide  to Building Agentic RAG with Llama3: Explore our comprehensive cookbooks, created in collaboration with AI at Meta, featuring advanced techniques from routing and tool use to constructing complex agent reasoning loops and multi-document agents using purely local models like Llama3. ✍️ Tutorials: 1LittleCoder’s   video tutorial  demonstrates how to deploy self-hosted llama-agents using Arcee AI, MistralAI, and Ollama, including setup, local model integration, and tool development. kingzzm’s   tutorial  on using LlamaIndex to build advanced RAG flows, detailing how to compose and visualize each step from basic retrieval and prompting to advanced techniques and evaluation with RAGAS. Mervin Praison’s   tutorial  on using llama-agents, detailing the framework’s purpose, a step-by-step setup guide for multi-agent services, and how it stands out from other frameworks. 🎤 Events: Join our online hackathon  this Friday, 19th, to build AI apps with Llama 3 from Meta and win cash, credits, and prizes from us and our co-hosts  TogetherAI ,  Milvus , and  LablabAI .", "author": "LlamaIndex", "date": "Jul 16, 2024", "tags": []},
{"title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud", "content": "The Challenge: Streamlining AI Development Scaleport AI specializes in transforming emerging AI technology into tangible business results. They possess deep expertise in deploying AI across key industries such as Legal, eCommerce, Real Estate, and Finance, providing tailored generative AI solutions for production applications. Before adopting LlamaCloud and LlamaIndex, Scaleport AI faced several challenges: Long development timelines for creating technical prototypes Difficulty in demonstrating tangible value to clients during the sales process Complex setup requirements for ingestion pipelines and data processing Suboptimal OCR performance, as existing solutions were not meeting the required accuracy and efficiency standards The Solution: LlamaCloud's Comprehensive AI Development Platform Scaleport AI turned to LlamaCloud to address these challenges. LlamaCloud offered: Centralized Knowledge Interface:  Simplified data management and reduced time spent on data wrangling. LlamaParse:  Outperformed existing OCR solutions, offering superior accuracy and efficiency. Advanced Indexing and Retrieval:  Enabled flexible integration with various data sources, enhancing data management and accessibility. Rapid Prototyping and Easy Production Deployments:  LlamaCloud provides an intuitive UI for rapid prototyping and a seamless transition from UI to code for full-scale development. The Results: Accelerated Development and Enhanced Client Engagement LlamaCloud delivered remarkable improvements for  Scaleport.ai : Accelerated Development Timelines:  The team could build technical prototypes during the scoping phase, demonstrating tangible value instantly. This improved client engagement and sales outcomes. Enhanced OCR Performance:  LlamaParse outperformed GPT-4 vision on several OCR tasks, providing superior accuracy and efficiency. Flexible Data Handling:  LlamaCloud's integration with data sources and advanced indexing and retrieval capabilities allowed for quick delivery of high-quality results. Teemu Lahdenperä, CTO of Scaleport AI, shared his experience: \"LlamaCloud has really sped up our development timelines - whether it's prototyping or production deployments. Before LlamaCloud, building even a simple application took forever because we needed to write our own abstractions for everything. When building an app for a client, a LOT of the work is building the ingestion pipelines. Doing that stuff with LlamaCloud and LlamaParse is remarkably simpler. This in turn has really helped our sales outcomes since we can show tangible value instantly. We've also seen great results with LlamaParse! Specifically, we spent about 50-60% less development hours for one of our clients than we did for an equivalent application prior to LlamaCloud. The main time savings were around Llamaparse; not having to build a custom ingestion pipeline and having the indexing sorted. This helps our margins as well” Conclusion: A Game-Changer for AI Development LlamaCloud has proven to be a game-changer for  Scaleport.ai , enabling them to develop apps faster and enhance their overall AI application performance. This has accelerated their sales process! By leveraging LlamaCloud's comprehensive suite of tools,  Scaleport.ai  has positioned itself at the forefront of AI solution providers, ready to meet the evolving needs of their clients with speed, flexibility, and cutting-edge technology. Want to see what LlamaCloud can do for you? Sign up for LlamaCloud  and  get on the waitlist  for full access!", "author": "LlamaIndex", "date": "Jul 17, 2024", "tags": ["Case Study"]},
{"title": "Improving Vector Search - Reranking with PostgresML and LlamaIndex", "content": "Search and Reranking: Improving Result Relevance Search systems typically employ two main methods: keyword and semantic. Keyword search matches exact query terms to indexed database content, while semantic search uses NLP and machine learning to understand query context and intent. Many effective systems combine both approaches for optimal results. After initial retrieval, reranking can further improve result relevance. Traditional reranking relies on historical user interaction data, but this approach struggles with new content and requires substantial data to train effectively. An advanced alternative is using cross-encoders, which directly compare query-result pairs for similarity. Cross-encoders directly compare two pieces of text and compute a similarity score. Unlike traditional semantic search methods, we cannot precompute embeddings for cross-encoders and reuse them later. Instead, we must run the cross-encoder for every pair of texts we want to compare, making this method computationally expensive and impractical for large-scale searches. However, it is highly effective for reranking a subset of our dataset because it excels at evaluating new, unseen data without the need for extensive user interaction data for fine-tuning. Cross-encoders complement and enhance traditional reranking systems by addressing their limitations in deep text analysis, particularly for novel or highly specific content. They do not rely on large datasets of user interactions for training (though such data can still be beneficial) and are adept at handling new and previously unseen data. This makes cross-encoders an excellent choice for enhancing the relevance of search results in a reranking context. Implementing Reranking We are going to implement a simple reranking example using LlamaIndex and the PostgresML managed index. For more info on the PostgresML managed index. Check out our announcement with LlamaIndex:  Simplify your RAG application architecture with LlamaIndex + PostgresML . Install the required dependencies to get started: pip install llama_index llama-index-indices-managed-postgresml We will be using the Paul Graham dataset which can be downloaded with curl: mkdir  data\n\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt The PostgresML Managed Index will handle storing, splitting, embedding, and querying our documents. All we need is a database connection string. If you haven’t already,  create your PostgresML account . You’ll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Let’s create our index: from  llama_index.core.readers  import  SimpleDirectoryReader\n from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name= \"llama-index-rerank-example\" \n) Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. This is a unique quality of using the PostgresML managed index. Now let’s search! We can perform semantic search and get the top 2 results by creating a retriever from our index. retriever = index.as_retriever(limit= 2 )\ndocs = retriever.retrieve( \"What did the author do as a child?\" )\n for  doc  in  docs:\n     print ( \"---------\" )\n     print ( f\"Id:  {doc.id_} \" )\n     print ( f\"Score:  {doc.score} \" )\n     print ( f\"Text:  {doc.text} \" )\n Doing this we get: ---------\n\nId: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n\nScore: 0.7793415653313153\n\nText: Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.\n\n\n\nThis had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of essays. [11]\n\n\n\nIn the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\n\n\n\nI've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\n\n\n\n---------\n\nId: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n\nScore: 0.7770352826735559\n\nText: Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist — in the strictly technical sense of making paintings and living in New York.\n\n\n\nI was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around this time.)\n\n\n\nThe best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant. These aren’t bad results, but they aren’t perfect. Let’s try reranking with a cross-encoder. retriever = index.as_retriever(\n    limit= 2 ,\n    rerank={\n         \"model\" :  \"mixedbread-ai/mxbai-rerank-base-v1\" ,\n         \"num_documents_to_rerank\" :  100 \n    }\n)\ndocs = retriever.retrieve( \"What did the author do as a child?\" )\n for  doc  in  docs:\n     print ( \"---------\" )\n     print ( f\"Id:  {doc.id_} \" )\n     print ( f\"Score:  {doc.score} \" )\n     print ( f\"Text:  {doc.text} \" )\n Here, we configure our retriever to return the top two documents, but this time, we add a rerank parameter to use the mixedbread-ai/mxbai-rerank-base-v1 model. This means our initial semantic search will return 100 results, which will then be reranked by the mixedbread-ai/mxbai-rerank-base-v1 model, and only the top two results will be presented. Running this outputs: Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\nScore: 0.17803585529327393\nText: What I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\n\n---------\nId: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\nScore: 0.1057136133313179\nText: I wanted not just to build things, but to build things that would last.\n\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\n\nAnd moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\n\nI had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art — that it didn't just appear spontaneously — but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.\n These are much better results! We can see that the top document has the answer to the user’s question. Notice that we did not have to specify a third party API to use for reranking. Once again, PostgresML handles the reranking using cross-encoders in the database. We can use re-ranking directly in RAG: query_engine = index.as_query_engine(\n    streaming= True ,\n    vector_search_limit= 2 ,\n    vector_search_rerank={\n         \"model\" :  \"mixedbread-ai/mxbai-rerank-base-v1\" ,\n         \"num_documents_to_rerank\" :  100 ,\n    },\n)\nresults = query_engine.query( \"What did the author do as a child?\" )\n for  text  in  results.response_gen:\n     print (text, end= \"\" , flush= True )\n Running this outputs: Based on the context information, as a child, the author worked on writing (writing short stories) and programming (on the IBM 1401 using Fortran) outside of school. That is the exact answer we wanted! Reranking Leads to Better Results Search can be complicated. Reranking with cross-encoders improves search by comparing text pairs and effectively handling new data. Implementing reranking with LlamaIndex and PostgresML improves search results, providing more precise answers in retrieval-augmented generation applications. To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro  guide  to setup your account, and use the examples above with your own data.", "author": "PostgresML", "date": "Jul 19, 2024", "tags": ["reranking", "Managed Indexes"]}
]