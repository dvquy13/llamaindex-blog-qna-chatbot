{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f554fb-79c8-4201-a0c9-e7464118c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c852ac68-9eb6-46f7-b33c-a1a12f8af228",
   "metadata": {},
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "DEBUG = False\n",
    "OBSERVABILITY = True\n",
    "LOG_TO_MLFLOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ae48fa-a9cb-4f32-a91e-6a63c9b53506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721879287.498035 2834072 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "if OBSERVABILITY:\n",
    "    import phoenix as px\n",
    "    px.launch_app()\n",
    "    import llama_index.core\n",
    "    llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"exp_005_tune_eval_qa_dataset\"\n",
    "if LOG_TO_MLFLOW:\n",
    "    RUN_DESCRIPTION = \"\"\"\n",
    "# Making the RAG outputs referenced sources\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_004\n",
    "- Do not use GPT-4 because of high cost (generate 40 pairs of question-context for retrieval evaluation cost 0.4 USD already)\n",
    "\"\"\"\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc26a2-7df5-4963-855d-92ec99c958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CACHE_DP = f'data/001/{RUN_NAME}'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"NOTEBOOK_CACHE_DP\", NOTEBOOK_CACHE_DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs-v2.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'content': \"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\",\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['AI', 'Evaluation', 'Rag'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       "  'content': \"Greetings, LlamaIndex community! ü¶ô Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more. ü§©\\xa0 The highlights: DeepLearningAI Course:  JavaScript RAG Web Apps with\\xa0LlamaIndex collaborative course with DeepLearningAI.  Course ,  Tweet . RAFTDatasetPack LlamaPack : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents.  LlamaPack ,  Tweet . Memory Efficiency with Cohere Embeddings:  Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations.  Notebook ,  Tweet . Python Docs Makeover:  Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details.  API Ref ,  Tweet ,  Docs ‚ú® Feature Releases and Enhancements: We introduced RAFT - Retrieval Augmented Fine Tuning, a method from  Tianjun Zhang \\xa0and  Shishir Patil \\xa0to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation.  LlamaPack ,  Tweet . We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.  Course ,  Tweet . We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG.  Notebook ,  Tweet We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation.  API Ref ,  Tweet ,  Docs üé•\\xa0Demos: RestAI , a project by  Pedro Dias  is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI. Ragdoll  and  Ragdoll Studio  by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI. üó∫Ô∏è Guides: Guide  to Designing RAG Systems by  Micha≈Ç Oleszak  for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation. ‚úçÔ∏è Tutorials: Sujit Patil   tutorial  on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment. Florian June's  tutorial  on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch. Laurie's  video tutorial  on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries. Akriti‚Äôs   tutorial  on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant. Marco Bertelli's  tutorial  on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience. Sudarshan Koirala‚Äôs   tutorial  on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking. üé•\\xa0 Webinars: Register for a webinar  with  Tianjun Zhang \\xa0and  Shishir Patil \\xa0on how to do retrieval-augmented fine-tuning (RAFT). Webinar  with  Daniel  on  CodeGPT  - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components. Vectara‚Äôs   Panel Discussion  on 'Why RAG will Never Die?‚Äô.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Apr 2, 2024',\n",
       "  'tags': ['LLM'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-03-19',\n",
       "  'content': \"Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting weekly update from the world of LlamaVerse! We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our  launch post  for all the details! ü§©\\xa0 The highlights: New observability with Instrumentation:  Enhanced developer workflow with a new Instrumentation module for improved observability.  Docs ,  Tweet . LlamaParse accepts natural language parsing instructions : Easily extract math snippets from PDFs into LaTeX with LlamaParse.  Blogpost ,  Tweet . Financial Data Parsing:  Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis.  Notebook ,  Tweet . ‚ú® Feature Releases and Enhancements: We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application.  Docs ,  Tweet . We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline.  Blogpost ,  Tweet . We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data.  Notebook ,  Tweet . We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code.  Blogpost ,  Docs ,  Tweet . Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts.  LlamaPack ,  Tweet üé•\\xa0Demos: Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency.  Blogpost ,  Code ,  Tweet . üó∫Ô∏è Guides: Guide  to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique. ‚úçÔ∏è Tutorials: Thomas Reid ‚Äôs  tutorial  on using LlamaParse can help properly extract text from a Tesla quarterly filings. Sudarshan Koirala   video tutorial  on RAG with LlamaParse, Qdrant, and Groq. Kyosuke Morita  tutorial  showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex. Cobus Greyling   tutorial  on Agentic RAG: Context-Augmented OpenAI Agents. Roey Ben Chaim ‚Äôs  tutorial  on PII Detector: hacking privacy in RAG. üé•\\xa0 Webinars: Webinar  with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT üìÖ\\xa0Events: We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['LlamaParse', 'AI', 'LLM', 'Newsletter'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19'},\n",
       " {'title': 'One-click Open Source RAG Observability with Langfuse',\n",
       "  'content': 'This is a guest post from the team at Langfuse There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once. Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your  production  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point? Thus, we started working on  Langfuse.com  ( GitHub ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either  self-host  Langfuse or use the  cloud instance  maintained by us. We are thrilled to announce our new integration with LlamaIndex today. This feature was  highly requested  by our community and aligns with our project\\'s focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase! The challenge We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine. from  llama_index.core  import  SimpleDirectoryReader\\n from  llama_index.core  import  VectorStoreIndex\\n\\ndocuments = SimpleDirectoryReader( \"./data\" ).load_data()\\n\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nchat_engine = index.as_chat_engine()\\n\\n print (chat_engine.chat( \"What problems can I solve with RAG?\" ))\\n print (chat_engine.chat( \"How do I optimize my RAG application?\" )) In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine. This is awesome to get started, but we quickly run into questions like: ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù ‚ÄúHow is chat memory managed?‚Äù ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù One-click OSS observability to the rescue We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager. Preparation Install the community package (pip install llama-index-callbacks-langfuse) Copy/paste the environment variables from the Langfuse project settings to your Python project: \\'LANGFUSE_SECRET_KEY\\', \\'LANGFUSE_PUBLIC_KEY\\' and \\'LANGFUSE_HOST\\' Now, you only need to set the global langfuse handler: from  llama_index.core  import  set_global_handler\\n\\nset_global_handler( \"langfuse\" ) And voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns. Group multiple chat threads into a session Working with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is  session tracking  for conversational applications to see the traces in context of a full message thread. To activate it, just add an id that identifies the session as a trace param before calling the chat_engine. from  llama_index.core  import  global_handler\\n\\nglobal_handler.set_trace_params(\\n  session_id= \"your-session-id\" \\n)\\n\\nchat_engine.chat( \"What did he do growing up?\" )\\nchat_engine.chat( \"What did he do at USC?\" )\\nchat_engine.chat( \"How old is he?\" ) Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing: Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces. Trace more complex applications and use other Langfuse features for prompt management and evaluation This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable. We have also built additional features to version control and collaborate on prompts (langfuse  prompt management ), track  experiments , and  evaluate  production traces. For RAG specifically, we collaborated with the RAGAS team and it‚Äôs easy to run their popular eval suite on traces captured with Langfuse (see  cookbook ). Get started The easiest way to get started is to follow the  cookbook  and check out the  docs . Feedback? Ping us We‚Äôd love to hear any feedback. Come join us on our  community discord  or add your thoughts to this  GitHub thread .',\n",
       "  'author': 'Langfuse',\n",
       "  'date': 'Mar 18, 2024',\n",
       "  'tags': ['LLM', 'Observability'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse'},\n",
       " {'title': 'Retrieving Privacy-Safe Documents Over A Network',\n",
       "  'content': 'In a  recent blog post , we introduced our  llama-index-networks  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores‚Äîthat one may not otherwise have access to‚Äîmeans more accurate responses to an even wider breadth of queries. A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we‚Äôll be relying on some recent developments in the area of Privacy-Enhancing Techniques. The story of Alex, Bob and Beth continues To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply. We showed then how such data a collaboration could be permitted through  llama-index-networks  by taking the following steps: Bob and Beth both build their respective QueryEngine‚Äôs (RAG in llama-index lingo) Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets. Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Sidebar: differential privacy primer In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual‚Äôs inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm. By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion). A light introduction to differential privacy. A light introduction to differential privacy. Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth‚Äôs data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled ‚ÄúPrivacy-preserving in-context learning with differentially private few-shot generation‚Äù by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network! There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration. Bob and Beth create privacy-safe synthetic copies of their private datasets Bob and Beth both build their respective QueryEngine‚Äôs over their synthetic datasets Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs Creating differentially private synthetic copies of a private dataset Fortunately, for step 0., we can make use of the  DiffPrivateSimpleDataset  pack. from  llama_index.core.llama_datasets.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n\\n # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,   # OpenAI only allows for top 5 next token \\n)                     # as opposed to entire vocabulary \\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nbeth_private_dataset: LabelledSimpleDataset = ...  # a dataset that contains \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # examples with two attributes \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # `text` and `reference_label` \\n\\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,     # params for preparing required prompts \\n    simple_dataset=simple_dataset,   # to generate the synthetic examples  \\n)\\n\\nbeth_synthetic_dataset =  await  beth_synthetic_generator.arun(\\n\\t\\tsize= 3 ,   # number of synthetic observations to create \\n\\t\\tsigma= 0.5    # param that determines the level of privacy \\n) With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It‚Äôs worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.) Example: Symptom2Disease In this section of the blog post, we go over an actual example application of the privacy-safe networks over the  Symptom2Disease  dataset. This dataset consists of 1,200 examples each containing a ‚Äúsymptoms‚Äù description as well as the associated ‚Äúdisease‚Äù label ‚Äî the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network. Generate privacy-safe synthetic observations of Symptom2Disease We use the training subset and apply the  DiffPrivateSimpleDatasetPack  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a  LabelledSimpleDataset  object. import  pandas  as  pd\\n from  sklearn.model_selection  import  train_test_split\\n from  llama_index.core.llama_dataset.simple  import  (\\n    LabelledSimpleDataExample,\\n    LabelledSimpleDataset,\\n)\\n from  llama_index.core.llama_dataset.base  import  CreatedBy, CreatedByType\\n\\n # load the Symptom2Disease.csv file \\ndf = pd.read_csv( \"Symptom2Disease.csv\" )\\ntrain, test = train_test_split(df, test_size= 0.2 )\\n\\n # create a LabelledSimpleDataset (which is what the pack works with) \\nexamples = []\\n for  index, row  in  df.iterrows():\\n    example = LabelledSimpleDataExample(\\n        reference_label=row[ \"label\" ],\\n        text=row[ \"text\" ],\\n        text_by=CreatedBy( type =CreatedByType.HUMAN),\\n    )\\n    examples.append(example)\\n\\nsimple_dataset = LabelledSimpleDataset(examples=examples) Now we can use the llama-pack to create our synthetic observations. import  llama_index.core.instrumentation  as  instrument\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n from  .event_handler  import  DiffPrivacyEventHandler\\n import  asyncio\\n import  os\\n\\nNUM_SPLITS =  3 \\nT_MAX =  150 \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,\\n)\\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nprompt_bundle = PromptBundle(\\n    instruction=(\\n         \"You are a patient experiencing symptoms of a specific disease. \" \\n         \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\\\n\" \\n         \"Start your answer directly after \\'Symptoms: \\'. Begin your answer with [RESULT].\\\\n\" \\n    ),\\n    label_heading= \"Disease\" ,\\n    text_heading= \"Symptoms\" ,\\n)\\n\\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,\\n    simple_dataset=simple_dataset,\\n)\\n\\nsynthetic_dataset =  await  dp_simple_dataset_pack.arun(\\n    sizes= 3 ,\\n    t_max=T_MAX,\\n    sigma= 1.5 ,\\n    num_splits=NUM_SPLITS,\\n    num_samples_per_split= 8 ,   # number of private observations to create a \\n)                              # synthetic obsevation \\nsynthetic_dataset.save_json( \"synthetic_dataset.json\" ) Create a network with two contributors Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well. Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we‚Äôre using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user). Step 1:  Contributor‚Äôs build their Retriever over their synthetic datasets. import  os\\n from  llama_index.core  import  VectorStoreIndex\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.core.schema  import  TextNode\\n\\n\\n # load the synthetic dataset \\nsynthetic_dataset = LabelledSimpleDataset.from_json(\\n     \"./data/contributor1_synthetic_dataset.json\" \\n)\\n\\n\\nnodes = [\\n    TextNode(text=el.text, metadata={ \"reference_label\" : el.reference_label})\\n     for  el  in  synthetic_dataset[:]\\n]\\n\\nindex = VectorStoreIndex(nodes=nodes)\\nsimilarity_top_k =  int (os.environ.get( \"SIMILARITY_TOP_K\" ))\\nretriever = index.as_retriever(similarity_top_k=similarity_top_k) Step 2:  Contributor‚Äôs expose their Retrievers behind a ContributorRetrieverService from  llama_index.networks.contributor.retriever.service  import  (\\n    ContributorRetrieverService,\\n    ContributorRetrieverServiceSettings,\\n)\\n\\nsettings = ContributorRetrieverServiceSettings()  # loads from .env file \\nservice = ContributorRetrieverService(config=settings, retriever=retriever)\\napp = service.app Step 3:  Define the NetworkRetriever that connects to the ContributorRetrieverServices from  llama_index.networks.network.retriever  import  NetworkRetriever\\n from  llama_index.networks.contributor.retriever  import  ContributorRetrieverClient\\n from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\\n\\n # ContributorRetrieverClient\\'s connect to the ContributorRetrieverService \\ncontributors = [\\n    ContributorRetrieverClient.from_config_file(\\n        env_file= f\"./client-env-files/.env.contributor_ {ix} .client\" \\n    )\\n     for  ix  in   range ( 1 ,  3 )\\n]\\nreranker = CohereRerank(top_n= 5 )\\nnetwork_retriever = NetworkRetriever(\\n    contributors=contributors, node_postprocessors=[reranker]\\n) With the  NetworkRetriever  established, we can retrieve synthetic observations from the two contributors data against a query. related_records = network_retriever.aretrieve( \"Vomitting and nausea\" )\\n print (related_records)  # contain symptoms/disease records that are similar to \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  # to the queried symptoms. Evaluating the  NetworkRetriever To evaluate the efficacy of the  NetworkRetriever  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. hit rate:  a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. mean reciprocal rank:  similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. In addition to evaluating the  NetworkRetriever  we consider the two baselines that represent Retrieving only over the individual Contributor‚Äôs synthetic datasets. Retriever evaluations, with sigma equal to 1.5. In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever‚Äôs in the test set. This shouldn‚Äôt be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor‚Äôs synthetic observations‚Äîthis is the point after all of a network! Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated). Finally, this level of privacy can be controlled via the noise parameter  sigma . In the example above we used a  sigma  of 1.5, which for this dataset amounts to an  epsilon  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are  generally considered to be quite private .) Below, we share the evaluations that result from using a  sigma  of 0.5, which amounts to an  epsilon  of 15.9‚Äîhigher values of  epsilon  or privacy-loss means less privacy. # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon \\nepsilon = dp_simple_dataset_pack.sigma_to_eps(\\n\\t\\tsigma= 0.5 ,\\n\\t\\tmechanism= \"gaussian\" ,\\n\\t\\tsize= 3 * 24 ,\\n\\t\\tmax_token_cnt= 150    # number of max tokens to generate per synthetic example \\n) Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5. So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening. # synthetic example epsilon = 1.3 \\n{\\n     \"reference_label\" :  \"Psoriasis\" ,\\n     \"text\" :  \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\" ,\\n     \"text_by\" : {\\n         \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n         \"type\" :  \"ai\" \\n    }\\n},\\n\\n # synthetic example epsilon = 15.9 \\n{\\n   \"reference_label\" :  \"Migraine\" ,\\n   \"text\" :  \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\" ,\\n   \"text_by\" : {\\n     \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n     \"type\" :  \"ai\" \\n  }\\n}, We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM. In summary We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. Learn more! To delve deeper into the materials of this blog post, we share a few links below: Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( link ) Demo notebooks for the  DiffPrivateSimpleDataset  ( link ) The source code for creating the synthetic Symptom2Disease observations using the  DiffPrivateSimpleDataset  ( link )',\n",
       "  'author': 'Andrei',\n",
       "  'date': 'Mar 20, 2024',\n",
       "  'tags': ['Privacy', 'llama-index-networks'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:08.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=160\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_input_data\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags']),\n",
    "        'url': record['url']\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='4592c99d-6a97-4e0e-87a2-160423007c10', embedding=None, metadata={'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\\nThis is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       " 'author': 'LlamaIndex',\n",
       " 'date': 'Apr 2, 2024',\n",
       " 'tags': 'LLM',\n",
       " 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ee5bd7-3f29-4b0e-b2e3-19d69ed7adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_documents\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_OPTION = 'openai'\n",
    "# LLM_OPTION = 'ollama'\n",
    "LLM_OPTION = 'togetherai'\n",
    "\n",
    "# LLM_MODEL_NAME = 'llama3'\n",
    "# LLM_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "LLM_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct-Lite'\n",
    "\n",
    "# EMBED_OPTION = 'openai'\n",
    "# EMBED_OPTION = 'togetherai'\n",
    "# EMBED_OPTION = 'ollama'\n",
    "EMBED_OPTION = 'huggingface'\n",
    "\n",
    "# EMBED_MODEL_NAME = 'llama3'\n",
    "# EMBED_MODEL_NAME = 'togethercomputer/m2-bert-80M-2k-retrieval'\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)\n",
    "    mlflow.log_param(\"LLM_MODEL_NAME\", LLM_MODEL_NAME)\n",
    "    mlflow.log_param(\"EMBED_OPTION\", EMBED_OPTION)\n",
    "    mlflow.log_param(\"EMBED_MODEL_NAME\", EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831156c6-08f7-4ed6-9063-6c7d7f00b312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:14.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mLLM:\n",
      "TogetherLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x708d117bfe10>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x708d1aba49a0>, completion_to_prompt=<function default_completion_to_prompt at 0x708d1a10ad40>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Meta-Llama-3-8B-Instruct-Lite', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='3cf613093b6eb9b479c341126dc8d3761c67f9340d0a4a8e1fdc62ed41b58126', api_base='https://api.together.xyz/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=False, tokenizer=None)\u001b[0m\n",
      "\u001b[32m2024-07-25 10:48:14.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEmbed model:\n",
      "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x708d117bf650>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# LLM options\n",
    "if LLM_OPTION == 'ollama':\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    llm = Ollama(base_url=base_url, model=LLM_MODEL_NAME, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "elif LLM_OPTION == 'openai':\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    llm = OpenAI(model=LLM_MODEL_NAME)\n",
    "elif LLM_OPTION == 'togetherai':\n",
    "    from llama_index.llms.together import TogetherLLM\n",
    "    llm = TogetherLLM(model=LLM_MODEL_NAME)\n",
    "\n",
    "# Embed options\n",
    "if EMBED_OPTION == 'huggingface':\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME\n",
    "    )\n",
    "elif EMBED_OPTION == 'openai':\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embed_model = OpenAIEmbedding()\n",
    "elif EMBED_OPTION == 'togetherai':\n",
    "    from llama_index.embeddings.together import TogetherEmbedding\n",
    "    embed_model = TogetherEmbedding(EMBED_MODEL_NAME)\n",
    "elif EMBED_OPTION == 'ollama':\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    embed_model = OllamaEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "\n",
    "logger.info(f\"LLM:\\n{repr(llm)}\")\n",
    "logger.info(f\"Embed model:\\n{repr(embed_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db5ed88-2448-4308-97e0-73bc9451c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:14.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1membed_model_dim=1024\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embed_model_dim = len(embed_model.get_text_embedding('sample text to find embedding dimensions'))\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "logger.info(f\"{embed_model_dim=}\")\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"embedding_model_dim\", embed_model_dim)\n",
    "    mlflow.log_param(\"LLM_MODEL\", repr(llm))\n",
    "    mlflow.log_param(\"EMBEDDING_MODEL\", repr(embed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a05d2-4b9e-4295-9dc5-e1b3bdf22be0",
   "metadata": {},
   "source": [
    "## Qdrant as VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be2dff6-23da-448b-866b-d791bb54812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08b83e18-6391-408c-93d9-dcf225813302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    # Create a translation table that maps each punctuation character to an underscore\n",
    "    translator = str.maketrans(string.punctuation, '_' * len(string.punctuation))\n",
    "    # Translate the text using the translation table\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da75b19f-ef70-4ca7-9564-0bf663466dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:15.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mCOLLECTION='huggingface__BAAI_bge_large_en_v1_5__exp_003_reranker_flag_embedding_bge_large'\u001b[0m\n",
      "\u001b[32m2024-07-25 10:48:15.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mNODES_PERSIST_FP='data/001/exp_003_reranker_flag_embedding_bge_large/nodes.pkl'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "# collection_raw_name = f\"{EMBED_OPTION}__{EMBED_MODEL_NAME}__{RUN_NAME}\"\n",
    "# COLLECTION = substitute_punctuation(collection_raw_name)\n",
    "COLLECTION = \"huggingface__BAAI_bge_large_en_v1_5__exp_003_reranker_flag_embedding_bge_large\"\n",
    "\n",
    "logger.info(f\"{COLLECTION=}\")\n",
    "\n",
    "# NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "NODES_PERSIST_FP = 'data/001/exp_003_reranker_flag_embedding_bge_large/nodes.pkl'\n",
    "\n",
    "logger.info(f\"{NODES_PERSIST_FP=}\")\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(f\"COLLECTION\", COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec5d7b9-7ad3-427f-a423-0fee1f7f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:15.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUse existing Qdrant collection\u001b[0m\n",
      "WARNI [llama_index.vector_stores.qdrant.base] Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "collection_exists = qdrantdb.collection_exists(COLLECTION)\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing Qdrant collection...\")\n",
    "        qdrantdb.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "    logger.info(f\"Creating new Qdrant collection...\")\n",
    "    qdrantdb.create_collection(\n",
    "        COLLECTION,\n",
    "        vectors_config=VectorParams(size=embed_model_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Use existing Qdrant collection\")\n",
    "db_collection = qdrantdb.get_collection(COLLECTION)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=COLLECTION,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6e0c6f0-92a6-430b-a509-682e5884b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e60fed4-7ea5-463c-bd19-d5b6da2cb04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:15.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading index from existing DB...\u001b[0m\n",
      "\u001b[32m2024-07-25 10:48:15.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mLoading cached `nodes` at data/001/exp_003_reranker_flag_embedding_bge_large/nodes.pkl...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        logger.info(f\"Loading cached `nodes` at {NODES_PERSIST_FP}...\")\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = None\n",
    "    # Currently setting num_workers leads to error `AttributeError: 'HuggingFaceEmbedding' object has no attribute '_model'`\n",
    "    # num_workers = os.cpu_count() - 1\n",
    "    # logger.info(f\"Running Ingestion Pipeline with {num_workers=}...\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa39ef55-4f0f-46c0-8777-5b11955978cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:16.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexing 160 into VectorStoreIndex took 1s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_nodes\", len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40da3529-cf6a-41ce-853e-1526faf2e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.append_reference.custom_query_engine import ManualAppendReferenceQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 5\n",
    "RERANK_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "# APPEND_REF_MODE = 'response_synthesizer'\n",
    "APPEND_REF_MODE = 'query_engine'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    mlflow.log_param(\"RERANK_TOP_K\", RERANK_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "reranker = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=RERANK_TOP_K)\n",
    "node_postprocessors.append(reranker)\n",
    "\n",
    "if APPEND_REF_MODE == 'response_synthesizer':\n",
    "    response_synthesizer = ManualAppendReferenceSynthesizer(verbose=0)\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "elif APPEND_REF_MODE == 'query_engine':\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = ManualAppendReferenceQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "else:\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"reranker\", repr(reranker))\n",
    "    mlflow.log_param(\"response_synthesizer\", repr(response_synthesizer))\n",
    "    mlflow.log_param(\"query_engine\", repr(query_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63bb5789-dde7-465c-86c2-94aadbf5b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To address points of failures in RAG pipeline, you can leverage the live testing feature provided by RAGArch. This feature allows you to instantly test your RAG pipeline with your own data and see how different configurations affect the outcome. This enables you to identify and troubleshoot potential issues early on, ensuring that your pipeline is working as expected. Additionally, the one-click code generation feature can help you quickly generate the Python code for your custom RAG pipeline, allowing you to implement and test your pipeline more efficiently.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex](https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** cb7da65e-0eae-42f1-81a6-1a50c6e0e9a7<br>**Similarity:** -5.265542507171631<br>**Text:** RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...<br>**Metadata:** {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'author': 'Harshad Suryawanshi', 'date': 'Feb 2, 2024', 'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation', 'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** b9ae64b7-7f37-4523-ae89-7b2c9c6da607<br>**Similarity:** -5.767147541046143<br>**Text:** def   generate_rag_pipeline ( file, llm, embed_model, node_parser, response_mode, vector_store ):...<br>**Metadata:** {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'author': 'Harshad Suryawanshi', 'date': 'Feb 2, 2024', 'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation', 'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cb7da65e-0eae-42f1-81a6-1a50c6e0e9a7': {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex',\n",
       "  'author': 'Harshad Suryawanshi',\n",
       "  'date': 'Feb 2, 2024',\n",
       "  'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation',\n",
       "  'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089',\n",
       "  'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'},\n",
       " 'b9ae64b7-7f37-4523-ae89-7b2c9c6da607': {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex',\n",
       "  'author': 'Harshad Suryawanshi',\n",
       "  'date': 'Feb 2, 2024',\n",
       "  'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation',\n",
       "  'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089',\n",
       "  'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can we address points of failures in RAG pipeline?\"\n",
    "response = query_engine.query(question)\n",
    "display_response(response, show_source=True, show_metadata=True, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_RETRIEVAL_EVAL_DATASET = True\n",
    "# Currently can not reuse retrieval_eval_dataset because the retrieval evaluation is based on ids\n",
    "# RETRIEVAL_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = min(len(nodes), RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "RETRIEVAL_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "# RETRIEVAL_EVAL_LLM_MODEL = 'gpt-4'\n",
    "RETRIEVAL_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:31.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mSampling 10 nodes for retrieval evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes\n",
    "else:\n",
    "    logger.info(f\"Loading retrieval_eval_nodes from {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    with open(RETRIEVAL_EVAL_DATASET_FP, 'r') as f:\n",
    "        retrieval_eval_nodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:48:32.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                                 | 4/10 [00:03<00:05,  1.09it/s]/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/llama_index/core/llama_dataset/legacy/embedding.py:99: UserWarning: Fewer questions generated (1) than requested (2).\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "QUESTION_GEN_QUERY = \"\"\"\n",
    "You are a Retriever Evaluator. Your task is to generate {num_questions_per_chunk} questions to assess the accuracy/relevancy of an information retrieval system.\n",
    "The information retrieval system would then be asked your generated question and assessed on how well it can look up and return the correct context.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- Restrict the generated questions to the context information provided.\n",
    "- Do not mention anything about the context in the generated questions.\n",
    "- The generated questions should be diverse in nature and in difficulty across the documents.\n",
    "- When being asked the generated question, a human with no prior knowledge can still answer perfectly given the input context.\n",
    "\"\"\"\n",
    "QA_GENERATE_PROMPT_TMPL = f\"\"\"\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{{context_str}}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "{QUESTION_GEN_QUERY}\n",
    "\"\"\"\n",
    "\n",
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    retrieval_eval_llm = OpenAI(model=RETRIEVAL_EVAL_LLM_MODEL, **RETRIEVAL_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes,\n",
    "        llm=retrieval_eval_llm,\n",
    "        num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK,\n",
    "        qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef47844c-5787-458e-9860-4c1f6a0f1935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'900e3f99-47b8-4865-a707-ce8ca130abf1': 'How does the GPT-3 ReAct agent differ from the GPT-4 ReAct agent in terms of tools used?',\n",
       " 'be445808-76dd-4832-b36b-9887832a24ba': 'What is the purpose of the Simple Router agent in the given setup?',\n",
       " 'c4784b90-0bb6-4943-8f02-fdcb8d89104b': 'How does the system handle user preferences and activities to personalize article recommendations?',\n",
       " '618a20b5-bd9b-4361-9b99-39a3e61df777': 'What methods are used to perform Named Entity Recognition in the system?',\n",
       " 'b1960fe9-2cd6-4b85-9974-a1ba95a6c039': 'How can you benefit from incremental syncs in a stream that supports it?',\n",
       " '5c576f6c-5fa9-42ce-8b38-47f00ccc3034': 'What are some of the custom sources available as pip packages for Airbyte?',\n",
       " '8d9577a6-89eb-4b25-9611-63ac9e2480fa': 'What are the scopes that need to be added in order to install the Slack app?',\n",
       " '23a72032-6d40-4657-8295-208cf396639c': 'What environment variables need to be set and what values should be included in the .env file for the Slack app integration?',\n",
       " '22ef5c09-2c9a-482f-ab7f-383fe8fb3155': 'Based on the context provided, the task is to generate 2 questions as a Retriever Evaluator to assess the accuracy/relevancy of an information retrieval system. The questions should be diverse in nature and difficulty across the documents, without mentioning anything about the context in the questions. The questions should allow a human with no prior knowledge to answer perfectly given the input context.',\n",
       " '0c2060c4-2df4-4abc-9b66-8e4964cf0107': 'How does the performance of Mistral compare to Llama 2 across different metrics in the graphs?',\n",
       " 'b29b9045-af7a-45ed-b6b2-9b1ad0af5e5d': 'At what model size do Llama 2 and Mistral reach similar performance levels in the graphs?',\n",
       " 'a669b200-325b-4fa1-9046-4253b7e1aa08': 'How did Tali.AI contribute to the future of support roles at the Augment hackathon?',\n",
       " '89df09dc-18a8-4db3-8e0e-411370ae8daf': 'What types of data sources can SuperAGI process with the integration of LlamaIndex?',\n",
       " '527ecec2-5392-4934-a384-b3284fff0783': 'How does the system organize knowledge transfer sessions and make code more comprehensible?',\n",
       " 'd754f333-4004-4e3e-97c2-33f293328f0b': 'What are the four stages involved in the solution proposed by Vibhav and the author for generating video explanations paired with individual code snippets?',\n",
       " '663cef29-b232-40d1-b5a3-eea2e2d824a8': 'How does the system handle custom transformations in the ingestion pipeline?',\n",
       " 'd99a675b-7b7e-4f88-94e7-1997e74cc6cc': 'Can the system ingest documents directly into a vector database?',\n",
       " 'f1501bd1-1b25-4daa-bcca-d66e6744b390': 'How does the speed of LLM-based retrieval compare to embedding-based retrieval in terms of processing time?',\n",
       " '1edef97b-922d-4a70-b0a0-0c88ebb06d54': 'Why is using the LLM as a second-stage reranking step after a first-stage embedding pass considered helpful in the retrieval process?'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71cd5482-b261-42a0-b822-af1f1303bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    eval_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        eval_dict = {\n",
    "            \"query\": eval_result.query,\n",
    "            \"expected_ids\": eval_result.expected_ids,\n",
    "            \"retrieved_texts\": eval_result.retrieved_texts,\n",
    "            **metric_dict\n",
    "        }\n",
    "        eval_dicts.append(eval_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(eval_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df, full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_5_retrieval_eval</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.307018</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.307018</td>\n",
       "      <td>0.118228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  top_5_retrieval_eval  0.473684  0.307018   0.094737  0.473684  0.307018   \n",
       "\n",
       "       ndcg  \n",
       "0  0.118228  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df, retrieval_eval_results_full_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc7a1e36-ff9e-4dac-a383-d77f55e77e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_ids</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the GPT-3 ReAct agent differ from the...</td>\n",
       "      <td>[64af897b-400d-465d-9b44-3db5cd82401b]</td>\n",
       "      <td>[agent_chain.run(input=\"How much cash did Uber...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the Simple Router agent...</td>\n",
       "      <td>[64af897b-400d-465d-9b44-3db5cd82401b]</td>\n",
       "      <td>[agent_chain.run(input=\"How much cash did Uber...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the system handle user preferences an...</td>\n",
       "      <td>[fe557b79-4c6f-4b13-9f69-38c82723ca06]</td>\n",
       "      <td>[Sophisticated  Named-Entity Recognition, Text...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What methods are used to perform Named Entity ...</td>\n",
       "      <td>[fe557b79-4c6f-4b13-9f69-38c82723ca06]</td>\n",
       "      <td>[After the preprocessing of the data, we colle...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can you benefit from incremental syncs in ...</td>\n",
       "      <td>[7f444bd5-230c-4fb1-b39d-8358322a8064]</td>\n",
       "      <td>[For example you can still benefit from  incre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are some of the custom sources available ...</td>\n",
       "      <td>[7f444bd5-230c-4fb1-b39d-8358322a8064]</td>\n",
       "      <td>[if you have implemented your own custom Airby...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.146068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the scopes that need to be added in o...</td>\n",
       "      <td>[29dca1d0-bafd-47d8-9fad-def250d4cf27]</td>\n",
       "      <td>[Click the ‚ÄúPermissions‚Äù link in the bottom ri...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What environment variables need to be set and ...</td>\n",
       "      <td>[29dca1d0-bafd-47d8-9fad-def250d4cf27]</td>\n",
       "      <td>[Otherwise we'll do nothing. @flask_app.route(...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.213986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Based on the context provided, the task is to ...</td>\n",
       "      <td>[c25ee90d-807b-4bf1-9336-633b2060c034]</td>\n",
       "      <td>[from  llama_index.evaluation  import  QueryRe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the performance of Mistral compare to...</td>\n",
       "      <td>[3b002c76-2225-4aa1-84e0-b8227bc952a4]</td>\n",
       "      <td>[However, Mistral is not displayed on the Know...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>At what model size do Llama 2 and Mistral reac...</td>\n",
       "      <td>[3b002c76-2225-4aa1-84e0-b8227bc952a4]</td>\n",
       "      <td>[The x-axes on the graphs represent model size...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How did Tali.AI contribute to the future of su...</td>\n",
       "      <td>[7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0]</td>\n",
       "      <td>[The Building Blocks of Prosper AI: A Look int...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.146068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What types of data sources can SuperAGI proces...</td>\n",
       "      <td>[7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0]</td>\n",
       "      <td>[Understand your dataset \\n \\n  The first step...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How does the system organize knowledge transfe...</td>\n",
       "      <td>[f5ccf620-f5bb-42e2-9f08-fcb336ce5678]</td>\n",
       "      <td>[1. Code Parsing: Breaking Down the Code \\n \\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.169580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the four stages involved in the solut...</td>\n",
       "      <td>[f5ccf620-f5bb-42e2-9f08-fcb336ce5678]</td>\n",
       "      <td>[This is where D-ID\\n  comes into play.\\n \\n \\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How does the system handle custom transformati...</td>\n",
       "      <td>[744e2fef-01a5-4167-81fc-143c89772273]</td>\n",
       "      <td>[FAQ What‚Äôs the difference between a  QueryPip...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can the system ingest documents directly into ...</td>\n",
       "      <td>[744e2fef-01a5-4167-81fc-143c89772273]</td>\n",
       "      <td>[A Query Engine to Combine Structured Analytic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How does the speed of LLM-based retrieval comp...</td>\n",
       "      <td>[550a4330-b77b-4a95-976f-8c7ac0e984c1]</td>\n",
       "      <td>[LLM-based retrieval is orders of magnitude sl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why is using the LLM as a second-stage reranki...</td>\n",
       "      <td>[550a4330-b77b-4a95-976f-8c7ac0e984c1]</td>\n",
       "      <td>[Yet for a variety of reasons, embedding-based...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.213986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the GPT-3 ReAct agent differ from the...   \n",
       "1   What is the purpose of the Simple Router agent...   \n",
       "2   How does the system handle user preferences an...   \n",
       "3   What methods are used to perform Named Entity ...   \n",
       "4   How can you benefit from incremental syncs in ...   \n",
       "5   What are some of the custom sources available ...   \n",
       "6   What are the scopes that need to be added in o...   \n",
       "7   What environment variables need to be set and ...   \n",
       "8   Based on the context provided, the task is to ...   \n",
       "9   How does the performance of Mistral compare to...   \n",
       "10  At what model size do Llama 2 and Mistral reac...   \n",
       "11  How did Tali.AI contribute to the future of su...   \n",
       "12  What types of data sources can SuperAGI proces...   \n",
       "13  How does the system organize knowledge transfe...   \n",
       "14  What are the four stages involved in the solut...   \n",
       "15  How does the system handle custom transformati...   \n",
       "16  Can the system ingest documents directly into ...   \n",
       "17  How does the speed of LLM-based retrieval comp...   \n",
       "18  Why is using the LLM as a second-stage reranki...   \n",
       "\n",
       "                              expected_ids  \\\n",
       "0   [64af897b-400d-465d-9b44-3db5cd82401b]   \n",
       "1   [64af897b-400d-465d-9b44-3db5cd82401b]   \n",
       "2   [fe557b79-4c6f-4b13-9f69-38c82723ca06]   \n",
       "3   [fe557b79-4c6f-4b13-9f69-38c82723ca06]   \n",
       "4   [7f444bd5-230c-4fb1-b39d-8358322a8064]   \n",
       "5   [7f444bd5-230c-4fb1-b39d-8358322a8064]   \n",
       "6   [29dca1d0-bafd-47d8-9fad-def250d4cf27]   \n",
       "7   [29dca1d0-bafd-47d8-9fad-def250d4cf27]   \n",
       "8   [c25ee90d-807b-4bf1-9336-633b2060c034]   \n",
       "9   [3b002c76-2225-4aa1-84e0-b8227bc952a4]   \n",
       "10  [3b002c76-2225-4aa1-84e0-b8227bc952a4]   \n",
       "11  [7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0]   \n",
       "12  [7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0]   \n",
       "13  [f5ccf620-f5bb-42e2-9f08-fcb336ce5678]   \n",
       "14  [f5ccf620-f5bb-42e2-9f08-fcb336ce5678]   \n",
       "15  [744e2fef-01a5-4167-81fc-143c89772273]   \n",
       "16  [744e2fef-01a5-4167-81fc-143c89772273]   \n",
       "17  [550a4330-b77b-4a95-976f-8c7ac0e984c1]   \n",
       "18  [550a4330-b77b-4a95-976f-8c7ac0e984c1]   \n",
       "\n",
       "                                      retrieved_texts  hit_rate       mrr  \\\n",
       "0   [agent_chain.run(input=\"How much cash did Uber...       0.0  0.000000   \n",
       "1   [agent_chain.run(input=\"How much cash did Uber...       0.0  0.000000   \n",
       "2   [Sophisticated  Named-Entity Recognition, Text...       0.0  0.000000   \n",
       "3   [After the preprocessing of the data, we colle...       1.0  1.000000   \n",
       "4   [For example you can still benefit from  incre...       1.0  1.000000   \n",
       "5   [if you have implemented your own custom Airby...       1.0  0.250000   \n",
       "6   [Click the ‚ÄúPermissions‚Äù link in the bottom ri...       1.0  1.000000   \n",
       "7   [Otherwise we'll do nothing. @flask_app.route(...       1.0  0.500000   \n",
       "8   [from  llama_index.evaluation  import  QueryRe...       0.0  0.000000   \n",
       "9   [However, Mistral is not displayed on the Know...       0.0  0.000000   \n",
       "10  [The x-axes on the graphs represent model size...       0.0  0.000000   \n",
       "11  [The Building Blocks of Prosper AI: A Look int...       1.0  0.250000   \n",
       "12  [Understand your dataset \\n \\n  The first step...       0.0  0.000000   \n",
       "13  [1. Code Parsing: Breaking Down the Code \\n \\n...       1.0  0.333333   \n",
       "14  [This is where D-ID\\n  comes into play.\\n \\n \\...       0.0  0.000000   \n",
       "15  [FAQ What‚Äôs the difference between a  QueryPip...       0.0  0.000000   \n",
       "16  [A Query Engine to Combine Structured Analytic...       0.0  0.000000   \n",
       "17  [LLM-based retrieval is orders of magnitude sl...       1.0  1.000000   \n",
       "18  [Yet for a variety of reasons, embedding-based...       1.0  0.500000   \n",
       "\n",
       "    precision  recall        ap      ndcg  \n",
       "0         0.0     0.0  0.000000  0.000000  \n",
       "1         0.0     0.0  0.000000  0.000000  \n",
       "2         0.0     0.0  0.000000  0.000000  \n",
       "3         0.2     1.0  1.000000  0.339160  \n",
       "4         0.2     1.0  1.000000  0.339160  \n",
       "5         0.2     1.0  0.250000  0.146068  \n",
       "6         0.2     1.0  1.000000  0.339160  \n",
       "7         0.2     1.0  0.500000  0.213986  \n",
       "8         0.0     0.0  0.000000  0.000000  \n",
       "9         0.0     0.0  0.000000  0.000000  \n",
       "10        0.0     0.0  0.000000  0.000000  \n",
       "11        0.2     1.0  0.250000  0.146068  \n",
       "12        0.0     0.0  0.000000  0.000000  \n",
       "13        0.2     1.0  0.333333  0.169580  \n",
       "14        0.0     0.0  0.000000  0.000000  \n",
       "15        0.0     0.0  0.000000  0.000000  \n",
       "16        0.0     0.0  0.000000  0.000000  \n",
       "17        0.2     1.0  1.000000  0.339160  \n",
       "18        0.2     1.0  0.500000  0.213986  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_results_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)\n",
    "    retrieval_eval_results_full_df.to_html(f\"{NOTEBOOK_CACHE_DP}/retrieval_eval_results_full_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/retrieval_eval_results_full_df.html\", \"retrieval_eval_results_full_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cffd06-4258-4798-bd27-18636a3a9b29",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deadc982-b78c-4882-80dc-498358c14a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_ids</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the GPT-3 ReAct agent differ from the...</td>\n",
       "      <td>[64af897b-400d-465d-9b44-3db5cd82401b]</td>\n",
       "      <td>[agent_chain.run(input=\"How much cash did Uber...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the Simple Router agent...</td>\n",
       "      <td>[64af897b-400d-465d-9b44-3db5cd82401b]</td>\n",
       "      <td>[agent_chain.run(input=\"How much cash did Uber...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the system handle user preferences an...</td>\n",
       "      <td>[fe557b79-4c6f-4b13-9f69-38c82723ca06]</td>\n",
       "      <td>[Sophisticated  Named-Entity Recognition, Text...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Based on the context provided, the task is to ...</td>\n",
       "      <td>[c25ee90d-807b-4bf1-9336-633b2060c034]</td>\n",
       "      <td>[from  llama_index.evaluation  import  QueryRe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the performance of Mistral compare to...</td>\n",
       "      <td>[3b002c76-2225-4aa1-84e0-b8227bc952a4]</td>\n",
       "      <td>[However, Mistral is not displayed on the Know...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>At what model size do Llama 2 and Mistral reac...</td>\n",
       "      <td>[3b002c76-2225-4aa1-84e0-b8227bc952a4]</td>\n",
       "      <td>[The x-axes on the graphs represent model size...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What types of data sources can SuperAGI proces...</td>\n",
       "      <td>[7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0]</td>\n",
       "      <td>[Understand your dataset \\n \\n  The first step...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the four stages involved in the solut...</td>\n",
       "      <td>[f5ccf620-f5bb-42e2-9f08-fcb336ce5678]</td>\n",
       "      <td>[This is where D-ID\\n  comes into play.\\n \\n \\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How does the system handle custom transformati...</td>\n",
       "      <td>[744e2fef-01a5-4167-81fc-143c89772273]</td>\n",
       "      <td>[FAQ What‚Äôs the difference between a  QueryPip...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can the system ingest documents directly into ...</td>\n",
       "      <td>[744e2fef-01a5-4167-81fc-143c89772273]</td>\n",
       "      <td>[A Query Engine to Combine Structured Analytic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the GPT-3 ReAct agent differ from the...   \n",
       "1   What is the purpose of the Simple Router agent...   \n",
       "2   How does the system handle user preferences an...   \n",
       "8   Based on the context provided, the task is to ...   \n",
       "9   How does the performance of Mistral compare to...   \n",
       "10  At what model size do Llama 2 and Mistral reac...   \n",
       "12  What types of data sources can SuperAGI proces...   \n",
       "14  What are the four stages involved in the solut...   \n",
       "15  How does the system handle custom transformati...   \n",
       "16  Can the system ingest documents directly into ...   \n",
       "\n",
       "                              expected_ids  \\\n",
       "0   [64af897b-400d-465d-9b44-3db5cd82401b]   \n",
       "1   [64af897b-400d-465d-9b44-3db5cd82401b]   \n",
       "2   [fe557b79-4c6f-4b13-9f69-38c82723ca06]   \n",
       "8   [c25ee90d-807b-4bf1-9336-633b2060c034]   \n",
       "9   [3b002c76-2225-4aa1-84e0-b8227bc952a4]   \n",
       "10  [3b002c76-2225-4aa1-84e0-b8227bc952a4]   \n",
       "12  [7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0]   \n",
       "14  [f5ccf620-f5bb-42e2-9f08-fcb336ce5678]   \n",
       "15  [744e2fef-01a5-4167-81fc-143c89772273]   \n",
       "16  [744e2fef-01a5-4167-81fc-143c89772273]   \n",
       "\n",
       "                                      retrieved_texts  hit_rate  mrr  \\\n",
       "0   [agent_chain.run(input=\"How much cash did Uber...       0.0  0.0   \n",
       "1   [agent_chain.run(input=\"How much cash did Uber...       0.0  0.0   \n",
       "2   [Sophisticated  Named-Entity Recognition, Text...       0.0  0.0   \n",
       "8   [from  llama_index.evaluation  import  QueryRe...       0.0  0.0   \n",
       "9   [However, Mistral is not displayed on the Know...       0.0  0.0   \n",
       "10  [The x-axes on the graphs represent model size...       0.0  0.0   \n",
       "12  [Understand your dataset \\n \\n  The first step...       0.0  0.0   \n",
       "14  [This is where D-ID\\n  comes into play.\\n \\n \\...       0.0  0.0   \n",
       "15  [FAQ What‚Äôs the difference between a  QueryPip...       0.0  0.0   \n",
       "16  [A Query Engine to Combine Structured Analytic...       0.0  0.0   \n",
       "\n",
       "    precision  recall   ap  ndcg  \n",
       "0         0.0     0.0  0.0   0.0  \n",
       "1         0.0     0.0  0.0   0.0  \n",
       "2         0.0     0.0  0.0   0.0  \n",
       "8         0.0     0.0  0.0   0.0  \n",
       "9         0.0     0.0  0.0   0.0  \n",
       "10        0.0     0.0  0.0   0.0  \n",
       "12        0.0     0.0  0.0   0.0  \n",
       "14        0.0     0.0  0.0   0.0  \n",
       "15        0.0     0.0  0.0   0.0  \n",
       "16        0.0     0.0  0.0   0.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_irrelevance_df = (\n",
    "    retrieval_eval_results_full_df\n",
    "    .loc[lambda df: df['hit_rate'].lt(1)]\n",
    "    .sort_values(['hit_rate', 'mrr', 'precision', 'recall', 'ap', 'ndcg'])\n",
    ")\n",
    "retrieval_eval_irrelevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "632a0bf9-0ce7-4272-bc7b-8c2d10623ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============Error #1=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the GPT-3 ReAct agent differ from the GPT-4 ReAct agent in terms of tools used?\n",
      "\n",
      "Expected Contexts:\n",
      "graph = ComposableGraph.from_indices(\n",
      "    GPTListIndex,\n",
      "    children_indices=[march_index, june_index, sept_index],\n",
      "    index_summaries=[\n",
      "        \"Provides information about Uber quarterly financials ending March 2022\",\n",
      "        \"Provides information about Uber quarterly financials ending June 2022\",\n",
      "        \"Provides information about Uber quarterly financials ending September 2022\"\n",
      "    ]\n",
      ") The graph can be queried with a  ComposableGraphQueryEngine  : # define decompose_transform \n",
      "decompose_transform = DecomposeQueryTransform(verbose= True )\n",
      "\n",
      " # define custom query engines \n",
      "custom_query_engines = {}\n",
      " for  index  in  [march_index, june_index, sept_index]:\n",
      "    query_engine = index.as_query_engine(service_context=service_context)\n",
      "    query_engine = TransformQueryEngine(\n",
      "        query_engine,\n",
      "        query_transform=decompose_transform,\n",
      "        transform_extra_info={ 'index_summary' : index.index_struct.summary},\n",
      "    )\n",
      "    custom_query_engines[index.index_id] = query_engine\n",
      "\n",
      "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
      "    service_context=service_context,\n",
      "    streaming= True ,\n",
      ")\n",
      "\n",
      " # define graph \n",
      "g_engine = graph.as_query_engine(\n",
      "    custom_query_engines=custom_query_engines\n",
      ") We try the following agent setups: GPT-3 ReAct agent:  A zero-shot GPT-3 ReAct agent with three Tools: each Tool corresponds to the vector index over a 10-Q filing. GPT-4 ReAct agent:  Same as above but using GPT-4 instead. Simple Router agent:  A simple router ‚Äúagent‚Äù with four Tools: the three Tools listed above + the  ComposableGraphQueryEngine  explicitly setup to perform compare/contrast queries. The code snippets for initializing these agents are below. For the simple router agent, we use the native  RouterQueryEngine  within LlamaIndex, though you should also be able to achieve similar results in LangChain through either the zero-shot agent (with tweaked settings) or the router chain.\n",
      "\n",
      "Retrieved Contexts:\n",
      "agent_chain.run(input=\"How much cash did Uber have in sept 2022?\") We see that the agent makes two errors 1) it is not able to supply an action input to each Tool, and 2) ends up looking through the June and March filings which are irrelevant to the question. GPT-4 ReAct Agent Results GPT-4 ReAct agents perform a lot better than GPT-3 agents. They comprehensively go through the set of available Tools, and provide much more detailed observation extraction and response synthesis. We won‚Äôt go through all of these examples, but they can be found in the example notebook! Query 1: agent_chain_gpt4.run(input=\"Analyze Uber revenue growth over the last few quarters\") Response: Unlike the GPT-3 agent, here the GPT-4 agent at least goes through every filing and synthesizes the result. Query 2 agent_chain_gpt4.run(input=\"Analyze changes in risk factors for Uber\") Response: Here the GPT-4 agent still only looks at September and June (and skips March), but the response is way more detailed, and references concrete facts within the report. GPT-3 Router Agent Results Reminder: the router agent doesn‚Äôt do any CoT and has additional access to our ComposableGraph query engine, which can explicitly perform compare/contrast queries. Let‚Äôs take a look at how this agent does. Query 1 response = query_engine.query(\"Analyze Uber revenue growth over the last few quarters\") Response (intermediate steps): Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n",
      "> Current query: Analyze Uber revenue growth over the last few quarters\n",
      "> New query:  What was Uber's revenue growth from the last quarter ending March 2022 compared to the previous quarter?\n",
      "\n",
      "High-Level Findings The high-level finding is that  less sophisticated agents need more constraints.  More specifically, we found that using a GPT-3 powered agent in a ReAct loop did not provide good results over complex queries; it was not able to figure out the proper interaction pattern over the provided set of Tools in order to surface the results. Instead, by adding more constraints to the agent behavior and providing more sophistication in the Tool itself, we were able to get a GPT-3 agent to produce better results. Smarter agents require fewer constraints.  We did find that GPT-4 agents with ReAct were able to provide better query results than GPT-3 agents when presented with a set of simple Tools over the data. This implies that more powerful agents may not need as many tools to ‚Äúexplicitly‚Äù perform tasks when much of that logic can be handled in the agent interaction loop. Setup Our data consists of three Uber 10-Q filings (quarterly financial reports) in 2022: March, June, and September. We wish to execute different queries over this data; the bulk of these queries are around comparing different bits of information between these documents. march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\n",
      "june_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\n",
      "sept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data() We use LlamaIndex to define a vector index over each document, which just stores the document chunks + embeddings in a vector store. We can then query each vector index using a simple QueryEngine  . We create a tool for each of these QueryEngine  objects.\n",
      "\n",
      ")\n",
      "query_tool_graph = QueryEngineTool.from_defaults(\n",
      "    query_engine=g_engine,\n",
      "    description=f\"Provides comparisons between Uber financials across quarters in 2022. Can be used to answer \"\n",
      "                 \"any questions that require analysis across multiple quarters.\",\n",
      ")\n",
      "\n",
      "# our \"router\" query engine is effectively a simple agent that can only perform routing\n",
      "query_engine = RouterQueryEngine(\n",
      "    selector=LLMSingleSelector.from_defaults(),\n",
      "    query_engine_tools=[\n",
      "        query_tool_sept,\n",
      "        query_tool_june,\n",
      "        query_tool_march,\n",
      "        query_tool_graph\n",
      "    ]\n",
      ") Now that we‚Äôve described the setup, let‚Äôs take a look at the results below! Findings and Experiments At a high-level, we find using GPT-3 in ReAct agents produces suboptimal results over these queries. They tend to exhibit the following characteristics: Unpredictability in the set of chosen tools:  The set of tools chosen can differ even if the questions are semantically similar, leading to variability in the responses. Lack of coverage in the set of chosen tools:  Oftentimes we expect that a given question is able to make use of all three 10-Q statements, but only a subset of them are picked. Erroneous chain-of-thought processing:  Sometimes the agent uses tools throughout the CoT process that are irrelevant to the question. In contrast, we find that GPT-4 ReAct agents provide answers that are more relevant, predictable, and exhibit fewer errors in intermediate results. Finally, we find that using a simpler routing-only GPT-3 agent with access to an explicit ‚Äúcompare/contrast‚Äù tool allows the agent to perform better.\n",
      "\n",
      "The ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str ‚Üí output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this: ...\n",
      "You have access to the following tools:\n",
      "{tool_desc}\n",
      "\n",
      "To answer the question, please use the following format.\n",
      "\n",
      "```\n",
      "Thought: I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of {tool_names})\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n",
      "```\n",
      "Please use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n",
      "\n",
      "If this format is used, you will receive a response in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "... We implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message. Note:  the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API ‚Äî the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting. Tool Abstractions Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use.\n",
      "\n",
      "Dumber LLM Agents Need More Constraints and Better Tools\n",
      "Summary In this article, we compare how well LLM-powered agents with different degrees of complexity perform over practical data tasks (financial analysis). We compare the performance of agents with more  complex, unrestrained  interaction behavior (ReAct) with agents that contain  simpler, more constrained  interactions (routing). We specifically analyze how much complexity can be added to the agent layer vs. the tool layer. We find that the choice of the language model matters a lot. ReAct agents that are powered by ‚Äúdumber‚Äù models (in a tongue-in-cheek fashion we are referring to any non GPT-4 model as ‚Äúdumb‚Äù) struggle to return relevant results over data. We find that constraining agent interaction behavior, and giving them access to more tools that can more explicitly perform complex actions, can help improve query performance over these less sophisticated LLMs. In contrast, more sophisticated models (GPT-4) can more reliably utilize the ReAct loop to execute a variety of complex data queries. This blog post is quite detailed; we provide a  lot  of experiments and results below. Best of all, you can run this all yourself with our  example notebook ! Overview of Agents Building LLM-powered agents have gotten increasingly popular in the past few months. Frameworks like  LangChain  have made it much easier to create these agents according to a set of common abstractions. At a high-level, an ‚Äúagent‚Äù is essentially an automated decision engine, that can be used to interact with an external environment. The core agent loop looks something like the following: The agent has access to a set of ‚Äútools‚Äù, which are generic functions that it can perform. It has an awareness of each tool through some attached metadata, and it can call each tool (either as a function call or structured API). User feeds in a natural language input to the agent. Given the input, the agent  interacts with the set of tools  in some fashion and returns the response.\n",
      "\n",
      "\n",
      "\n",
      "============Error #2=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What is the purpose of the Simple Router agent in the given setup?\n",
      "\n",
      "Expected Contexts:\n",
      "graph = ComposableGraph.from_indices(\n",
      "    GPTListIndex,\n",
      "    children_indices=[march_index, june_index, sept_index],\n",
      "    index_summaries=[\n",
      "        \"Provides information about Uber quarterly financials ending March 2022\",\n",
      "        \"Provides information about Uber quarterly financials ending June 2022\",\n",
      "        \"Provides information about Uber quarterly financials ending September 2022\"\n",
      "    ]\n",
      ") The graph can be queried with a  ComposableGraphQueryEngine  : # define decompose_transform \n",
      "decompose_transform = DecomposeQueryTransform(verbose= True )\n",
      "\n",
      " # define custom query engines \n",
      "custom_query_engines = {}\n",
      " for  index  in  [march_index, june_index, sept_index]:\n",
      "    query_engine = index.as_query_engine(service_context=service_context)\n",
      "    query_engine = TransformQueryEngine(\n",
      "        query_engine,\n",
      "        query_transform=decompose_transform,\n",
      "        transform_extra_info={ 'index_summary' : index.index_struct.summary},\n",
      "    )\n",
      "    custom_query_engines[index.index_id] = query_engine\n",
      "\n",
      "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
      "    service_context=service_context,\n",
      "    streaming= True ,\n",
      ")\n",
      "\n",
      " # define graph \n",
      "g_engine = graph.as_query_engine(\n",
      "    custom_query_engines=custom_query_engines\n",
      ") We try the following agent setups: GPT-3 ReAct agent:  A zero-shot GPT-3 ReAct agent with three Tools: each Tool corresponds to the vector index over a 10-Q filing. GPT-4 ReAct agent:  Same as above but using GPT-4 instead. Simple Router agent:  A simple router ‚Äúagent‚Äù with four Tools: the three Tools listed above + the  ComposableGraphQueryEngine  explicitly setup to perform compare/contrast queries. The code snippets for initializing these agents are below. For the simple router agent, we use the native  RouterQueryEngine  within LlamaIndex, though you should also be able to achieve similar results in LangChain through either the zero-shot agent (with tweaked settings) or the router chain.\n",
      "\n",
      "Retrieved Contexts:\n",
      "agent_chain.run(input=\"How much cash did Uber have in sept 2022?\") We see that the agent makes two errors 1) it is not able to supply an action input to each Tool, and 2) ends up looking through the June and March filings which are irrelevant to the question. GPT-4 ReAct Agent Results GPT-4 ReAct agents perform a lot better than GPT-3 agents. They comprehensively go through the set of available Tools, and provide much more detailed observation extraction and response synthesis. We won‚Äôt go through all of these examples, but they can be found in the example notebook! Query 1: agent_chain_gpt4.run(input=\"Analyze Uber revenue growth over the last few quarters\") Response: Unlike the GPT-3 agent, here the GPT-4 agent at least goes through every filing and synthesizes the result. Query 2 agent_chain_gpt4.run(input=\"Analyze changes in risk factors for Uber\") Response: Here the GPT-4 agent still only looks at September and June (and skips March), but the response is way more detailed, and references concrete facts within the report. GPT-3 Router Agent Results Reminder: the router agent doesn‚Äôt do any CoT and has additional access to our ComposableGraph query engine, which can explicitly perform compare/contrast queries. Let‚Äôs take a look at how this agent does. Query 1 response = query_engine.query(\"Analyze Uber revenue growth over the last few quarters\") Response (intermediate steps): Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n",
      "> Current query: Analyze Uber revenue growth over the last few quarters\n",
      "> New query:  What was Uber's revenue growth from the last quarter ending March 2022 compared to the previous quarter?\n",
      "\n",
      ")\n",
      "query_tool_graph = QueryEngineTool.from_defaults(\n",
      "    query_engine=g_engine,\n",
      "    description=f\"Provides comparisons between Uber financials across quarters in 2022. Can be used to answer \"\n",
      "                 \"any questions that require analysis across multiple quarters.\",\n",
      ")\n",
      "\n",
      "# our \"router\" query engine is effectively a simple agent that can only perform routing\n",
      "query_engine = RouterQueryEngine(\n",
      "    selector=LLMSingleSelector.from_defaults(),\n",
      "    query_engine_tools=[\n",
      "        query_tool_sept,\n",
      "        query_tool_june,\n",
      "        query_tool_march,\n",
      "        query_tool_graph\n",
      "    ]\n",
      ") Now that we‚Äôve described the setup, let‚Äôs take a look at the results below! Findings and Experiments At a high-level, we find using GPT-3 in ReAct agents produces suboptimal results over these queries. They tend to exhibit the following characteristics: Unpredictability in the set of chosen tools:  The set of tools chosen can differ even if the questions are semantically similar, leading to variability in the responses. Lack of coverage in the set of chosen tools:  Oftentimes we expect that a given question is able to make use of all three 10-Q statements, but only a subset of them are picked. Erroneous chain-of-thought processing:  Sometimes the agent uses tools throughout the CoT process that are irrelevant to the question. In contrast, we find that GPT-4 ReAct agents provide answers that are more relevant, predictable, and exhibit fewer errors in intermediate results. Finally, we find that using a simpler routing-only GPT-3 agent with access to an explicit ‚Äúcompare/contrast‚Äù tool allows the agent to perform better.\n",
      "\n",
      "High-Level Findings The high-level finding is that  less sophisticated agents need more constraints.  More specifically, we found that using a GPT-3 powered agent in a ReAct loop did not provide good results over complex queries; it was not able to figure out the proper interaction pattern over the provided set of Tools in order to surface the results. Instead, by adding more constraints to the agent behavior and providing more sophistication in the Tool itself, we were able to get a GPT-3 agent to produce better results. Smarter agents require fewer constraints.  We did find that GPT-4 agents with ReAct were able to provide better query results than GPT-3 agents when presented with a set of simple Tools over the data. This implies that more powerful agents may not need as many tools to ‚Äúexplicitly‚Äù perform tasks when much of that logic can be handled in the agent interaction loop. Setup Our data consists of three Uber 10-Q filings (quarterly financial reports) in 2022: March, June, and September. We wish to execute different queries over this data; the bulk of these queries are around comparing different bits of information between these documents. march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\n",
      "june_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\n",
      "sept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data() We use LlamaIndex to define a vector index over each document, which just stores the document chunks + embeddings in a vector store. We can then query each vector index using a simple QueryEngine  . We create a tool for each of these QueryEngine  objects.\n",
      "\n",
      "These query engines can be used in an overall agent setting. from  llama_index.tools  import  QueryEngineTool\n",
      "\n",
      "query_engine_tools = [\n",
      "    QueryEngineTool(\n",
      "        query_engine=query_engine, \n",
      "        metadata=ToolMetadata(\n",
      "            name= '&lt;tool_name&gt;' , \n",
      "            description= \"Queries over X data source.\" \n",
      "        )\n",
      "    ),\n",
      " ...\n",
      "] Tool Specs A  tool spec  is a Python class that represents a full API specification that an agent can interact with, and a tool spec can be converted into a list of tools that an agent can be initialized with. This class allows users to define entire services, not just single tools that perform individual tasks. Each tool spec may contain read/write endpoints that allow an agent to interact with a service in meaningful ways. For instance, a Slack tool spec could allow the user to both read existing messages and channels ( load_data ,  fetch_channels ) as well as write messages ( send_message ). It would be roughly defined as the following: class   SlackToolSpec ( BaseToolSpec ):\n",
      "     \"\"\"Slack tool spec.\"\"\" \n",
      "    spec_functions = [ \"load_data\" ,  \"send_message\" ,  \"fetch_channels\" ]\n",
      "\n",
      "     def   load_data ( \n",
      "          self,\n",
      "          channel_ids:  List [ str ],\n",
      "          reverse_chronological:  bool  =  True ,\n",
      "       ) -&gt;  List [Document]:\n",
      "           \"\"\"Load data from the input directory.\"\"\" \n",
      "          ...\n",
      "       def   send_message ( \n",
      "          self,\n",
      "          channel_id:  str ,\n",
      "          message:  str ,\n",
      "       ) -&gt;  None :\n",
      "           \"\"\"Send a message to a channel given the channel ID.\"\"\" \n",
      "          ...\n",
      "       def   fetch_channels ( \n",
      "          self,\n",
      "       ) -&gt;  List [ str ]:\n",
      "           \"\"\"Fetch a list of relevant channels.\"\"\" \n",
      "          ... If a tool spec is initialized, it can be converted into a list of tools that can be fed into an agent with  to_tool_list .\n",
      "\n",
      "We allow users to define both a single Tool as well as a ‚ÄúToolSpec‚Äù containing a series of functions under the hood. We describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools. Base Tool Abstraction The base tool defines a very generic interface. The  __call__  function can take in any series of arguments, and return a generic  ToolOutput  container that can capture any response. A tool also has metadata containing its name, description, and function schema. @dataclass \n",
      " class   ToolMetadata :\n",
      "     description:  str\n",
      "     name:   Optional [str] =  None \n",
      "     fn_schema:   Optional [ Type [ BaseModel ]] =  DefaultToolFnSchema \n",
      "\n",
      " class   BaseTool :\n",
      "     @property \n",
      "     @abstractmethod \n",
      "     def   metadata ( self ) -&gt;  ToolMetadata :\n",
      "        pass\n",
      "     @abstractmethod \n",
      "     def   __call__ ( self ,  input:   Any ) -&gt;  ToolOutput :\n",
      "        pass Function Tool A function tool allows users to easily convert any function into a Tool. It takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface. It can also ‚Äúauto-infer‚Äù the function schema if it isn‚Äôt specified beforehand. Our  ToolSpec  classes make use of this  FunctionTool  abstraction to convert functions defined in the tool spec into a set of agent tools (see below). Here‚Äôs a trivial example of defining a FunctionTool. from llama_index.tools.function_tool import FunctionTool\n",
      "\n",
      "def  multiply (a:  int , b:  int )  -&gt;  int :\n",
      "     \"\" \"Multiple two integers and returns the result integer\" \"\" \n",
      "     return  a * b\n",
      "multiply_tool = FunctionTool.from_defaults(fn=multiply) QueryEngineTool Of course, we also provide Tool abstractions to wrap our existing query engines. This provides a seamless transition from working on query engines to working on agents. Our query engines can be thought of ‚Äúconstrained‚Äù agents meant for the read/write setting and centered around retrieval purposes.\n",
      "\n",
      "\n",
      "\n",
      "============Error #3=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the system handle user preferences and activities to personalize article recommendations?\n",
      "\n",
      "Expected Contexts:\n",
      "After the preprocessing of the data, we collect the metadata, including keywords, NER results, summary, body, title, author, and so on, in the payload and push the payload with embedding to the Qdrant Vector Database. We will skip the part on how to create embeddings with the OpenAI Ada model, as there are many existing tutorials available. To perform Named Entity Recognition, we utilize the  dslim/bert-base-NER  model from HuggingFace. from  transformers  import  pipeline\n",
      " from  transformers  import  AutoTokenizer, AutoModelForTokenClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained( \"dslim/bert-base-NER\" )\n",
      "model = AutoModelForTokenClassification.from_pretrained( \"dslim/bert-base-NER\" )\n",
      "nlp = pipeline( \"ner\" , model=model, tokenizer=tokenizer, batch_size=batch_size)\n",
      "ner_results = nlp(texts) Personalization When accessing NewsGPT, we have two options: we can either create a new account or use a guest login to test the service. It should be noted, however, that using the guest login will not provide personalization. If we choose to sign up, we will be asked about our preferred news categories, which will help the service make initial recommendations during the cold start. Recommendation Pipeline for Personalization After users sign up and log in, we query the preferences and activities from Google Firebase, if any; otherwise, use the favorite categories for recommendation cold start. If the activities and preferences are available, we will call the recommendation API hosted on AWS Lambda and generate personalized articles for the users. The activities related to the user reading history. Users can indicate their preference for articles by clicking thumbs-up or down buttons on the article page. Some of the recommendation logic can be found in the  code . Besides the personalized feeds, users can also choose different categories or search for specific topics using the search bar.\n",
      "\n",
      "Retrieved Contexts:\n",
      "Sophisticated  Named-Entity Recognition, Text Embedding with OpenAI API,  and  asynchronous article embedding processes  are incorporated. This data is systematically stored in the  Qdrant Vector Database, promoting accuracy and efficiency. ‚úÖ  Tailored News Recommendations: Our system does more than just present news; it learns from you. By analyzing your reading habits, we leverage article embeddings to curate a personal news feed tailored to your interests. A versatile  search bar  is always at your disposal, letting users explore any news topics that capture their interest. ‚úÖ  Efficient Information Retrieval: With just a single click on an article of interest, NewsGPT gets to work. It collates  similar news from multiple sources (3‚Äì5)  and activates a Streamlit chatbot. Your engagement begins immediately: the first query is autonomously forwarded to our chatbot to fetch a concise news summary. For ease of user experience, we display  predefined prompts  as clickable buttons. This means users can receive information without the need for manual input. Curiosity welcomed: any questions users may have about the news article will be addressed as long as the answers are detailed within the source articles. ‚úÖ  Time-Saving Reminder & Category Distribution Chart: To keep you informed, our sidebar displays the time saved using NewsGPT and visually represents news category distribution. Delving Deep into Architecture Data Pipeline We start with a reliable and sustainable data pipeline to support the users to get fresh news with two powerful libraries,  pygooglenews,  and  newspaper3k. pip install pygooglenews --upgrade\n",
      "pip install newspaper3k By utilizing  Spark batch processing , we efficiently process data with  NER(Named-Entity Recognition)  and create embeddings via  OpenAI API (Ada model) .\n",
      "\n",
      "The adjustability features mean I can tailor it to my needs, ensuring optimal posture and comfort throughout the day.\"},\n",
      "    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"I was initially drawn to its aesthetic appeal, but the functional benefits have been profound. The breathable material ensures no discomfort even after prolonged use, and the robust build gives me confidence that it's a chair built to last.\"},\n",
      "] Setting up an In-Memory Database To process our data, we‚Äôre using an in-memory SQLite database. SQLAlchemy provides an efficient way to model, create, and interact with this database. Here‚Äôs how our  product_reviews  table structure looks: id  (Integer, Primary Key) category  (String) product_name  (String) review  (String, Not Null) Once we‚Äôve defined our table structure, we populate it with our sample dataset. engine = create_engine( \"sqlite:///:memory:\" )\n",
      "metadata_obj = MetaData()\n",
      "\n",
      " # create product reviews SQL table \n",
      "table_name =  \"product_reviews\" \n",
      "city_stats_table = Table(\n",
      "    table_name,\n",
      "    metadata_obj,\n",
      "    Column( \"id\" , Integer(), primary_key= True ),\n",
      "    Column( \"category\" , String( 16 ), primary_key= True ),\n",
      "    Column( \"product_name\" , Integer),\n",
      "    Column( \"review\" , String( 16 ), nullable= False )\n",
      ")\n",
      "metadata_obj.create_all(engine)\n",
      "\n",
      "sql_database = SQLDatabase(engine, include_tables=[ \"product_reviews\" ])\n",
      "\n",
      " for  row  in  rows:\n",
      "    stmt = insert(city_stats_table).values(**row)\n",
      "     with  engine.connect()  as  connection:\n",
      "        cursor = connection.execute(stmt)\n",
      "        connection.commit() Analysing Product Reviews ‚Äî Text2SQL + RAG Deriving insights from data often requires intricate questioning.\n",
      "\n",
      "2. Question that needs to be asked on the top of the result from the first question to provide the final answer.\n",
      "\n",
      "  Example:\n",
      "\n",
      "  Input:\n",
      "  How is the culture of countries whose population is more than 5000000\n",
      "\n",
      "  Output:\n",
      "  1. Get the reviews of countries whose population is more than 5000000\n",
      "  2. Provide the culture of countries\n",
      "  '''\n",
      "\n",
      "  messages = [\n",
      "      ChatMessage(role=\"system\", content=system_message),\n",
      "      ChatMessage(role=\"user\", content=user_query),\n",
      "  ]\n",
      "  generated_questions = llm.chat(messages).message.content.split('\\n')\n",
      "\n",
      "  return generated_questions\n",
      "\n",
      "user_query = \"Get the summary of reviews of Iphone13\"\n",
      "\n",
      "text_to_sql_query, rag_query = generate_questions(user_query) Data Retrieval ‚Äî Executing the Primary Query When we decompose a user‚Äôs question into its constituent parts, the first step is to convert the ‚ÄúDatabase Query in Natural Language‚Äù into an actual SQL query that can be run against our database. In this section, we‚Äôll use the LlamaIndex‚Äôs  NLSQLTableQueryEngine  to handle the conversion and execution of this SQL query. Setting up the NLSQLTableQueryEngine: The  NLSQLTableQueryEngine  is a powerful tool that takes natural language queries and converts them into SQL queries. We initiate this by providing the necessary details: sql_database : This represents our SQL database connection details. tables : We specify which table(s) our query will be run against. In this scenario, we're targeting the  product_reviews  table. synthesize_response : When set to  False , this ensures we receive raw SQL responses without additional synthesis. service_context : This is an optional parameter, which could be used to provide service-specific settings or plugins.\n",
      "\n",
      "response = st.session_state[ \"chat_engine\" ].query(prompt)\n",
      " def   stream_example ():\n",
      "     for  word  in  response.response_gen:\n",
      "         yield  word\n",
      "write(stream_example) Streaming Demo Predefined Prompts Three different prompts are predefined in the article chat page, allowing users to select from a drop-down menu to ask questions without typing. The prompts are 5W1H, Similar Viewpoints, and Discrepancy Viewpoints. prompt_content = {\n",
      "     \"5W1H\" :  'Summarize the content details in the \"5W1H\" approach (Who, What, When, Where, Why, and How) in bullet points' ,\n",
      "     \"Similar Viewpoints\" :  \"Compare between the articles and provide the similar viewpoints in bullet points\" ,\n",
      "     \"Discrepency Viewpoints\" :  \"Compare between the articles and provide the discrepency viewpoints in bullet points\" \n",
      "} What‚Äôs Next Huge thanks to LlamaIndex and Streamlit for generously providing a massive platform that allows more people to gain awareness of the organic news digest and save valuable time through  NewsGPT . If you enjoyed reading the article and agree with our concept, please do not hesitate to leave a clap for the article and join  Neotice , the production app of NewsGPT, to support us. We are confident in our mission and look forward to having you on board with us. Thank you! You can also connect us on LinkedIn:   Kang-Chi Ho ,  Jian-An Wang üéâ Click Here to Join Neotice üëâ  Neotice\n",
      "\n",
      "Chat with Article Powered by LlamaIndex LlamaIndex Pipeline Unlike other news aggregator apps, NewsGPT offers a unique service by providing users with a summary and discrepancy key takeaway of a topic from various news sources. This feature not only saves users time by eliminating the need to read multiple articles but also helps identify which information can be trusted by comparing discrepancies. What is under the hood is when the user clicks on the ‚Äúchat with article‚Äù button, the system first uses a search API hosted on AWS Lambda to find related articles from various sources. Then, the system utilizes the  LlamaIndex  library to create a vector store and a query engine, which can be integrated with the  Streamlit  chat component to create an  RAG  application for information retrieval. Streaming Output with LlamaIndex and Streamlit Thanks to the powerful library, S treamlit-Extras , which provides additional functionality not officially supported. To enhance the user experience and make it more like chatting with ChatGPT, we use the  streaming_write  functions from the Streamlit-Extras library. Additionally, we set  streaming=True  for the  query_engine  to ensure a smoother experience. Let‚Äôs take a look at the code. pip install streamlit-extras To begin with, we set up the  service_context .\n",
      "\n",
      "\n",
      "\n",
      "============Error #4=============\n",
      "\n",
      "\n",
      "Query:\n",
      "Based on the context provided, the task is to generate 2 questions as a Retriever Evaluator to assess the accuracy/relevancy of an information retrieval system. The questions should be diverse in nature and difficulty across the documents, without mentioning anything about the context in the questions. The questions should allow a human with no prior knowledge to answer perfectly given the input context.\n",
      "\n",
      "Expected Contexts:\n",
      "To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs. Let‚Äôs initialize a prompt template to generate question-context pairs. # Prompt to generate questions\n",
      "qa_generate_prompt_tmpl = \"\"\"\\\n",
      "Context information is below.\n",
      "\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "\n",
      "Given the context information and not prior knowledge.\n",
      "generate only questions based on the below query.\n",
      "\n",
      "You are a Professor. Your task is to setup \\\n",
      "{num_questions_per_chunk} questions for an upcoming \\\n",
      "quiz/examination. The questions should be diverse in nature \\\n",
      "across the document. The questions should not contain options, not start with Q1/ Q2. \\\n",
      "Restrict the questions to the context information provided.\\\n",
      "\"\"\" llm = Anthropic(api_key=anthropic_api_key)\n",
      "qa_dataset = generate_question_context_pairs(\n",
      "    nodes, llm=llm, num_questions_per_chunk= 2 \n",
      ") Function to filter out sentences such as ‚Äî  Here are 2 questions based on provided context # function to clean the dataset \n",
      " def   filter_qa_dataset ( qa_dataset ):\n",
      "     \"\"\"\n",
      "    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n",
      "    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n",
      "    the filtered data.\n",
      "\n",
      "    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n",
      "    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n",
      "    \"\"\"\n",
      "\n",
      "Retrieved Contexts:\n",
      "from  llama_index.evaluation  import  QueryResponseEvaluator\n",
      "\n",
      " # build service context \n",
      "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature= 0 , model_name= \"gpt-4\" ))\n",
      "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
      "\n",
      " # Build index and get response object \n",
      "...\n",
      "\n",
      " # define evaluator \n",
      "evaluator = QueryResponseEvaluator(service_context=service_context)\n",
      "\n",
      " # evaluate using the response object \n",
      "eval_result = evaluator.evaluate(query, response) Query + Response + Individual Source Nodes (Context) This function answers the question: Which source nodes of the retrieved source nodes are used to generate a response? Often in the real world, the source nodes can be nodes from different documents. In these cases, it‚Äôs important to understand which source nodes are relevant and show those documents to the users. This mode of evaluation will look at each source node and see if each source node contains an answer to the query. from  llama_index.evaluation  import  QueryResponseEvaluator\n",
      "\n",
      " # build service context \n",
      "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature= 0 , model_name= \"gpt-4\" ))\n",
      "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
      "\n",
      " # build index and get response object  \n",
      "...\n",
      "\n",
      " # define evaluator \n",
      "evaluator = QueryResponseEvaluator(service_context=service_context)\n",
      "\n",
      " # evaluate using the response object \n",
      "eval_result = evaluator.evaluate_source_nodes(response) Google Colab notebook for Evaluating QA systems using LlamaIndex ‚Äî Google Colaboratory Evaluating QA systems using LlamaIndex Conclusion LlamaIndex provides a comprehensive solution for building and evaluating QA systems without the need for ground-truth labels. By using the Question Generation and Evaluation modules, you can ensure that your system is accurate and reliable, making it suitable for production environments.\n",
      "\n",
      "The response object for a given query returns both the response and source nodes (context) with which it generated the response. We can now evaluate the response against the retrieved sources ‚Äî without taking into account the query! This allows you to measure hallucination ‚Äî if the response does not match the retrieved sources, this means that the model may be ‚Äúhallucinating‚Äù an answer since it is not rooting the answer in the context provided to it in the prompt. The result is a binary response ‚Äî either ‚ÄúYES/NO‚Äù. YES ‚Äî Response and Source Nodes (Context) are matching. NO ‚Äî Response and Source Nodes (Context) are not matching. from  llama_index.evaluation  import  ResponseEvaluator\n",
      "\n",
      " # build service context \n",
      "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature= 0 , model_name= \"gpt-4\" ))\n",
      "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
      "\n",
      " # Build index and get response object \n",
      "...\n",
      "\n",
      " # define evaluator \n",
      "evaluator = ResponseEvaluator(service_context=service_context)\n",
      "\n",
      " # evaluate using the response object \n",
      "eval_result = evaluator.evaluate(response) Query + Response + Source Nodes (Context) This function answers the question: Are response generated, source nodes (context), and query matching? Often with the ‚ÄúResponse + Source Nodes (Context)‚Äù approach, the response generated is in line with the source nodes but may not be the answer to the query. Therefore, considering the query along with the response and source nodes is a good approach for a more accurate analysis. The goal is to determine if the response + source context answers the query. The result is a binary response ‚Äî either ‚ÄúYES/NO‚Äù. YES ‚Äî Query, Response, and Source Nodes (Context) are matching. NO ‚Äî Query, Response, and Source Nodes (Context) are not matching.\n",
      "\n",
      "The first two of these metrics recall and hit rate, don‚Äôt consider the position (or ranking) of the relevant documents, whereas all the others do in their own respective ways. Generator Evaluation:  does the response use the retrieved documents to sufficiently answer the user query? In abstractive question-answering systems, like the kinds we‚Äôre talking about in this blog, measuring the generated response is made more tricky due to the fact that there isn‚Äôt just one way to sufficiently answer a query in written language ‚Äî there‚Äôs plenty! So, in this case, our measurement relies on subjective judgement, which can be performed by humans, though this is costly and unscalable. An alternative approach, is to use an LLM judge to measure things like  relevancy  and  faithfulness . Relevancy: considers textual context and evaluates how much the generated response matches the query. Faithfulness: evaluates how much the generated response matches the retrieved textual context. For both of these, the retrieved context as well as the query and generated response are passed to the LLM judge. (This pattern of using an LLM to judge the responses has been termed by some researchers of the space as LLM-As-A-Judge ( Zheng et al., 2023 ).) Currently, the  llama-index (v0.9.2)  library supports hit-rate and mean reciprocal rank for retrieval evaluation, as well as relevancy, faithfulness and a few others for generator evaluation. (Check out our Evaluation guides in our  docs !). Evaluation Of Multi-Modal RAG For the multi-modal case, the evaluation can (and should) still be carried out with respect to the different stages of retrieval and generation. Separating Out Retrieval Evaluation For Text and Image Modalities Now that retrieved documents can come in two forms, it would seem most sensible to consider computing the usual retrieval evaluation metrics separately for images and text. In this way, you have more knowledge as to which aspect of the multi-modal retriever is working well and what isn‚Äôt.\n",
      "\n",
      "Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores.\n",
      "\n",
      "from  llama_index.evaluation  import  DatasetGenerator\n",
      " from  llama_index  import  SimpleDirectoryReader\n",
      "\n",
      " # Load documents \n",
      "reader = SimpleDirectoryReader( \"./data\" )\n",
      "documents = reader.load_data()\n",
      "\n",
      " # Generate Question \n",
      "data_generator = DatasetGenerator.from_documents(documents)\n",
      "question = data_generator.generate_questions_from_nodes() 2. Generate Answers/Source Nodes (Context) Using List Index, we generate answers and source nodes for the generated questions in the response object. \n",
      " from  llama_index  import  GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n",
      "\n",
      " # load documents \n",
      "documents = SimpleDirectoryReader( './data' ).load_data()\n",
      "\n",
      " # Create Index \n",
      "index = GPTVectorStoreIndex.from_documents(documents)\n",
      "\n",
      " # save index to disk \n",
      "index.set_index_id( \"vector_index\" )\n",
      "index.storage_context.persist( 'storage' )\n",
      "\n",
      " # rebuild storage context \n",
      "storage_context = StorageContext.from_defaults(persist_dir= 'storage' )\n",
      " # load index \n",
      "index = load_index_from_storage(storage_context, index_id= \"vector_index\" )\n",
      "\n",
      " # Query the index \n",
      "query_engine = index.as_query_engine(similarity_top_k= 3 )\n",
      "response = query_engine.query(&lt;Query&gt;)\n",
      "\n",
      " # Response object has both response and source nodes. \n",
      " 3. Evaluation The evaluation module can be used to answer the following three questions: Are the response generated and source nodes (context) matching? ‚Äî Response + Source Nodes (Context) Are response generated, source nodes (context), and query matching? ‚Äî Query + Response + Source Nodes (Context) Which source nodes of the retrieved source nodes are used to generate a response? ‚Äî Query + Response + Individual Source Nodes (Context) Evaluation can be done with some combination of the query, context, and response, combining these with LLM calls. Response + Source Nodes (Context) This function answers the question: Are the response generated and source nodes (context) matching?\n",
      "\n",
      "\n",
      "\n",
      "============Error #5=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the performance of Mistral compare to Llama 2 across different metrics in the graphs?\n",
      "\n",
      "Expected Contexts:\n",
      "Answer: Examine the Image: The image contains four graphs, each graph compares the performance of three different models ‚Äî Llama 2, Mistral, and an unnamed third model ‚Äî across different metrics: mAP@L (mean Average Precision at L), Reason@L (presumably a reasoning score at L), Knowledge@L, and Comprehension@L. Each graph shows performance as a function of model size (in terms of billion parameters). Identify Relevant Data: We need to focus on the Mistral and Llama 2 models across all four graphs to extract the relevant data. For mAP@L: ‚Äî Llama 2 reaches above 65% when reaching 70 billion parameters. ‚Äî Mistral starts at around 5 billion parameters with about 48% and reaches almost 65% by the time it gets to 34 billion parameters. For Reason@L: ‚Äî Llama 2 reaches about 68% when reaching 70 billion parameters. ‚Äî Mistral starts at around 5 billion parameters with about 52% and reaches almost 68% by the time it gets to 34 billion parameters. For Knowledge@L: ‚Äî Llama 2 starts at about 45% with 1.3 billion parameters and reaches just slightly under 60% by 70 billion parameters. ‚Äî Mistral is not plotted in this graph, so no direct comparison can be made. For Comprehension@L: ‚Äî Llama 2 starts just above 40% with 1.3 billion parameters and reaches just under 60% by 70 billion parameters. ‚Äî Mistral starts around 13 billion parameters with about 52% and continues above 62% by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral and Llama 2 are very comparable in performance in the metrics where both are shown.\n",
      "\n",
      "Retrieved Contexts:\n",
      "However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that particular metric. Draw a Conclusion: In the graphs where both models can be compared (mAP@L, Reason@L, and Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching similar levels of performance at smaller parameter sizes relative to the plotted sizes for Llama 2. This suggests that Mistral could be more parameter-efficient. However, since there is no data for Mistral in the Knowledge@L graph, we cannot draw a complete comparison Observation: There is a hallucination with a number of model parameters and percentage points though the final conclusion is partially correct. Image 3 ‚Äî Performances of different LLMs across various NLP tasks. General Question: Question:  Analyze the Image. Answer: The image appears to be a table containing numerical data, which seems to be a comparative analysis of various models across multiple parameters or tests. Each row represents a different model or configuration as indicated by names such as ‚ÄúMPT‚Äù and ‚ÄúFalcon‚Äù followed by a size specification like ‚Äú70B,‚Äù ‚Äú7B,‚Äù or some other parameter. The columns are labeled with test names or evaluation metrics, such as ‚ÄúSize,‚Äù ‚ÄúAvg,‚Äù ‚ÄúAQuA-RAT,‚Äù ‚ÄúLogiQA,‚Äù ‚ÄúL-SAT-AR,‚Äù ‚ÄúL-SAT-IC,‚Äù ‚ÄúL-SAT-RC,‚Äù ‚ÄúSAT-en (w/o Psg.),‚Äù and ‚ÄúSAT-math.‚Äù The data is likely related to performance scores of these models on these tests, where higher numbers probably indicate better performance.\n",
      "\n",
      "The x-axes on the graphs represent model size in terms of the number of parameters, with three points that likely correspond to the sizes of the models being compared. The y-axes represent performance as a percentage, which could mean accuracy, precision, recall, or another relevant performance metric depending on the specific task. The graphs appear to show that Mistral performs better than LLaMA-2 and the Size 7B model across all metrics. This indicates that within the context of these measurements and tasks, Mistral is a stronger model. The exact nature of the tasks or what ‚ÄúMLM U‚Äù, ‚ÄúReasoning‚Äù, ‚ÄúKnowledge‚Äù, and ‚ÄúCommonsense‚Äù specifically refer to are not detailed in the image, but they likely correspond to standard NLP tasks designed to test understanding of language, ability to reason, knowledge recall, and commonsense reasoning, respectively. The performance improvements are depicted as increasingly significant with larger model sizes. Observation: Now by giving the detail that mistral is available in 7B series, it is able to answer correctly. Chain of thought prompting: Question:  Based on the image provided. Follow the steps and answer the query ‚Äî Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?. Examine the Image: Look at the mentioned category in the query in the Image. Identify Relevant Data: Note the respective percentages. Evaluate: Compare if there is any comparision required as per the query. Draw a Conclusion: Now draw the conclusion based on the whole data.\n",
      "\n",
      "Based on these graphs, Mistral appears to be the top-performing model in all four metrics, suggesting that it might be more efficient or effective than LLaMA 2, LLaMA 13B, and GPT-3, at least within the scope of the parameters and tasks represented here. On a technical note, it‚Äôs important to mention that the y-axis shows different scales for the metrics (ranging from 45‚Äì75% for NLU, Knowledge, and Commonsense, and from 50‚Äì72% for Reasoning), which suggests that these tasks might have different levels of difficulty or different benchmarks for success. Observation: It did answer the query but hallucinated with the NLU task which is the MMLU task and assumed Mistral is available across all different model parameters. Specific Questions: Question:  How well does mistral model compared to llama2 model? Answer: The image you‚Äôve provided contains four graphs, each plotting different performance metrics for evaluating language models. It compares two models: Mistral and LLaMA2 across four different aspects ‚Äî MMLU (Multi-tasking multiple-choice), Reasoning, Knowledge, and Commonsense. In each graph, the performance metric is plotted on the y-axis while the effective model size in billion parameters is plotted on the x-axis. Here‚Äôs a summary based on the trends observed in the graphs: 1. **MMLU (Top Left Graph)**: LLaMA2 shows a steeper performance curve as the model size increases, starting at around 61% for the smallest size and reaching upwards of 66% for the largest model. Mistral also shows performance gains with model size but plateaus earlier, reaching a level just shy of 62%. 2.\n",
      "\n",
      "2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement curve, starting from about 61% and surpassing 72% for the largest model. Mistral, while improving, seems to plateau near 70% for the largest model. 3. **Knowledge (Bottom Left Graph)**: This graph reflects a similar trend to the previous ones, with LLaMA2 beginning at a lower performance around 46% and eclipsing 52%. Mistral starts higher at around 48% and appears to plateau near 52%. 4. **Commonsense (Bottom Right Graph)**: Here, LLaMA2 starts its performance at approximately 62% and reaches just above 66%. Mistral seems to start at a slightly higher point than LLaMA2 but ends at a similar level to LLaMA2‚Äôs largest model. Overall, the LLaMA2 model appears to show a greater degree of improvement in performance as the model size increases compared to Mistral across these metrics. Meanwhile, Mistral starts at a higher performance for some metrics but tends to plateau earlier, suggesting that LLaMA2 may scale better with size in terms of performance gains. Observation: Incorrect answers and percentages are not accurate enough and again assumed mistral is available across all parameter models. Question:  Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model? Answer: This image appears to be a set of four graphs, each comparing the performance of three different language models on various tasks: Mistral, LLaMA-2, and a hypothetical ‚ÄúSize 7B‚Äù model. The graphs are labeled with task-specific performance metrics: ‚ÄúMLM U‚Äù, ‚ÄúReasoning‚Äù, ‚ÄúKnowledge‚Äù, and ‚ÄúCommonsense‚Äù.\n",
      "\n",
      "Observation: With chain of thought prompting it did hallicunate with bar colours but answered correctly saying Llama2 has lower violation compared to vicuna in Hateful and harmful though for a section Llama2 has higher violation compared to vicuna. Image2 ‚Äî Llama2 vs Mistral model performances across various NLP tasks. General Question: Question:  Analyze the Image. Answer: The image appears to be a set of four line graphs that compare the performance of different natural language processing (NLP) models across four different metrics: Natural Language Understanding (NLU), Reasoning, Knowledge, and Commonsense. The models compared are LLaMA 2, LLaMA 13B, Mistral, and GPT-3. Each graph plots the performance metric (on the y-axis) against the model size measured in billions of effective parameters (on the x-axis). In general, these plots demonstrate that as the model size increases, the performance on each metric improves. Here are specific observations for each graph: 1. Natural Language Understanding (Top left): All models show an increase in NLU performance as the model size increases. Mistral is depicted as the highest-performing model at each size benchmark. 2. Reasoning (Top right): Again, larger models perform better on reasoning tasks, with Mistral consistently outperforming the other models mentioned. 3. Knowledge (Bottom left): Similar to the NLU and reasoning tasks, the trend is for better performance with larger models, with Mistral leading across the different sizes. 4. Commonsense (Bottom right): This graph follows the same trend with the Mistral model performing better at each size level compared to the other models.\n",
      "\n",
      "\n",
      "\n",
      "============Error #6=============\n",
      "\n",
      "\n",
      "Query:\n",
      "At what model size do Llama 2 and Mistral reach similar performance levels in the graphs?\n",
      "\n",
      "Expected Contexts:\n",
      "Answer: Examine the Image: The image contains four graphs, each graph compares the performance of three different models ‚Äî Llama 2, Mistral, and an unnamed third model ‚Äî across different metrics: mAP@L (mean Average Precision at L), Reason@L (presumably a reasoning score at L), Knowledge@L, and Comprehension@L. Each graph shows performance as a function of model size (in terms of billion parameters). Identify Relevant Data: We need to focus on the Mistral and Llama 2 models across all four graphs to extract the relevant data. For mAP@L: ‚Äî Llama 2 reaches above 65% when reaching 70 billion parameters. ‚Äî Mistral starts at around 5 billion parameters with about 48% and reaches almost 65% by the time it gets to 34 billion parameters. For Reason@L: ‚Äî Llama 2 reaches about 68% when reaching 70 billion parameters. ‚Äî Mistral starts at around 5 billion parameters with about 52% and reaches almost 68% by the time it gets to 34 billion parameters. For Knowledge@L: ‚Äî Llama 2 starts at about 45% with 1.3 billion parameters and reaches just slightly under 60% by 70 billion parameters. ‚Äî Mistral is not plotted in this graph, so no direct comparison can be made. For Comprehension@L: ‚Äî Llama 2 starts just above 40% with 1.3 billion parameters and reaches just under 60% by 70 billion parameters. ‚Äî Mistral starts around 13 billion parameters with about 52% and continues above 62% by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral and Llama 2 are very comparable in performance in the metrics where both are shown.\n",
      "\n",
      "Retrieved Contexts:\n",
      "The x-axes on the graphs represent model size in terms of the number of parameters, with three points that likely correspond to the sizes of the models being compared. The y-axes represent performance as a percentage, which could mean accuracy, precision, recall, or another relevant performance metric depending on the specific task. The graphs appear to show that Mistral performs better than LLaMA-2 and the Size 7B model across all metrics. This indicates that within the context of these measurements and tasks, Mistral is a stronger model. The exact nature of the tasks or what ‚ÄúMLM U‚Äù, ‚ÄúReasoning‚Äù, ‚ÄúKnowledge‚Äù, and ‚ÄúCommonsense‚Äù specifically refer to are not detailed in the image, but they likely correspond to standard NLP tasks designed to test understanding of language, ability to reason, knowledge recall, and commonsense reasoning, respectively. The performance improvements are depicted as increasingly significant with larger model sizes. Observation: Now by giving the detail that mistral is available in 7B series, it is able to answer correctly. Chain of thought prompting: Question:  Based on the image provided. Follow the steps and answer the query ‚Äî Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?. Examine the Image: Look at the mentioned category in the query in the Image. Identify Relevant Data: Note the respective percentages. Evaluate: Compare if there is any comparision required as per the query. Draw a Conclusion: Now draw the conclusion based on the whole data.\n",
      "\n",
      "However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that particular metric. Draw a Conclusion: In the graphs where both models can be compared (mAP@L, Reason@L, and Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching similar levels of performance at smaller parameter sizes relative to the plotted sizes for Llama 2. This suggests that Mistral could be more parameter-efficient. However, since there is no data for Mistral in the Knowledge@L graph, we cannot draw a complete comparison Observation: There is a hallucination with a number of model parameters and percentage points though the final conclusion is partially correct. Image 3 ‚Äî Performances of different LLMs across various NLP tasks. General Question: Question:  Analyze the Image. Answer: The image appears to be a table containing numerical data, which seems to be a comparative analysis of various models across multiple parameters or tests. Each row represents a different model or configuration as indicated by names such as ‚ÄúMPT‚Äù and ‚ÄúFalcon‚Äù followed by a size specification like ‚Äú70B,‚Äù ‚Äú7B,‚Äù or some other parameter. The columns are labeled with test names or evaluation metrics, such as ‚ÄúSize,‚Äù ‚ÄúAvg,‚Äù ‚ÄúAQuA-RAT,‚Äù ‚ÄúLogiQA,‚Äù ‚ÄúL-SAT-AR,‚Äù ‚ÄúL-SAT-IC,‚Äù ‚ÄúL-SAT-RC,‚Äù ‚ÄúSAT-en (w/o Psg.),‚Äù and ‚ÄúSAT-math.‚Äù The data is likely related to performance scores of these models on these tests, where higher numbers probably indicate better performance.\n",
      "\n",
      "2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement curve, starting from about 61% and surpassing 72% for the largest model. Mistral, while improving, seems to plateau near 70% for the largest model. 3. **Knowledge (Bottom Left Graph)**: This graph reflects a similar trend to the previous ones, with LLaMA2 beginning at a lower performance around 46% and eclipsing 52%. Mistral starts higher at around 48% and appears to plateau near 52%. 4. **Commonsense (Bottom Right Graph)**: Here, LLaMA2 starts its performance at approximately 62% and reaches just above 66%. Mistral seems to start at a slightly higher point than LLaMA2 but ends at a similar level to LLaMA2‚Äôs largest model. Overall, the LLaMA2 model appears to show a greater degree of improvement in performance as the model size increases compared to Mistral across these metrics. Meanwhile, Mistral starts at a higher performance for some metrics but tends to plateau earlier, suggesting that LLaMA2 may scale better with size in terms of performance gains. Observation: Incorrect answers and percentages are not accurate enough and again assumed mistral is available across all parameter models. Question:  Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model? Answer: This image appears to be a set of four graphs, each comparing the performance of three different language models on various tasks: Mistral, LLaMA-2, and a hypothetical ‚ÄúSize 7B‚Äù model. The graphs are labeled with task-specific performance metrics: ‚ÄúMLM U‚Äù, ‚ÄúReasoning‚Äù, ‚ÄúKnowledge‚Äù, and ‚ÄúCommonsense‚Äù.\n",
      "\n",
      "Running Mixtral 8x7 locally with LlamaIndex and Ollama\n",
      "You may have heard the fuss about the latest release from European AI powerhouse  Mistral AI : it‚Äôs called Mixtral 8x7b, a ‚Äúmixture of experts‚Äù model ‚Äî eight of them, each trained with 7 billion parameters, hence the name. Released originally as a  mic-drop tweet  they followed up a few days later with a  blog post  that showed it matching or exceeding GPT-3.5 as well as the much larger Llama2 70b on a number of benchmarks. Here at LlamaIndex we‚Äôre naturally fans of open source software, so open models with permissive licenses like Mixtral are right up our alley. We‚Äôve had a few questions about how to get Mixtral working with LlamaIndex, so this post is here to get you up and running with a totally local model. Step 1: Install Ollama Previously getting a local model installed and working was a huge pain, but with the release of  Ollama , it‚Äôs suddenly a snap! Available for MacOS and Linux (and soon on Windows, though you can use it today on Windows via  Windows Subsystem For Linux ), it is itself open source and a  free download . Once downloaded, you can get Mixtral with a single command: ollama run mixtral The first time you run this command it will have to download the model, which can take a long time, so go get a snack. Also note that it requires a hefty 48GB of RAM to run smoothly! If that‚Äôs too much for your machine, consider using its smaller but still very capable cousin  Mistral 7b , which you install and run the same way: ollama run mistral We‚Äôll assume you‚Äôre using Mixtral for the rest of this tutorial, but Mistral will also work.\n",
      "\n",
      "Based on these graphs, Mistral appears to be the top-performing model in all four metrics, suggesting that it might be more efficient or effective than LLaMA 2, LLaMA 13B, and GPT-3, at least within the scope of the parameters and tasks represented here. On a technical note, it‚Äôs important to mention that the y-axis shows different scales for the metrics (ranging from 45‚Äì75% for NLU, Knowledge, and Commonsense, and from 50‚Äì72% for Reasoning), which suggests that these tasks might have different levels of difficulty or different benchmarks for success. Observation: It did answer the query but hallucinated with the NLU task which is the MMLU task and assumed Mistral is available across all different model parameters. Specific Questions: Question:  How well does mistral model compared to llama2 model? Answer: The image you‚Äôve provided contains four graphs, each plotting different performance metrics for evaluating language models. It compares two models: Mistral and LLaMA2 across four different aspects ‚Äî MMLU (Multi-tasking multiple-choice), Reasoning, Knowledge, and Commonsense. In each graph, the performance metric is plotted on the y-axis while the effective model size in billion parameters is plotted on the x-axis. Here‚Äôs a summary based on the trends observed in the graphs: 1. **MMLU (Top Left Graph)**: LLaMA2 shows a steeper performance curve as the model size increases, starting at around 61% for the smallest size and reaching upwards of 66% for the largest model. Mistral also shows performance gains with model size but plateaus earlier, reaching a level just shy of 62%. 2.\n",
      "\n",
      "\n",
      "\n",
      "============Error #7=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What types of data sources can SuperAGI process with the integration of LlamaIndex?\n",
      "\n",
      "Expected Contexts:\n",
      "Data Agents session at TPF X Nexus VC  Buildathon  by  Ravi Theja . Demos: Tali.AI  at the Augment hackathon dove into the future of support roles by developing an Autonomous Support Bot using LlamaIndex.  Tweet SuperAGI  integrated with LlamaIndex which enables AI agents to process a wide variety of data from both structured and unstructured sources including Docx, PDF, CSV files, videos, and images.  Tweet\n",
      "\n",
      "Retrieved Contexts:\n",
      "Understand your dataset \n",
      " \n",
      "  The first step in doing data analysis is to get a better understanding of your\n",
      "  dataset.\n",
      " \n",
      " You can start asking questions to your agent: \n",
      " agent.chat(\"Describe my database\") \n",
      " The output of which is: \n",
      " STARTING TURN 1 --------------- === Calling Function === Calling function: describe_dataset with args:    {\"ask\":\"Can you describe the whole database?\"} ... ======================== STARTING TURN 2 --------------- The database I have access to consists of several schemas, each with its  own set of tables and domains: 1. **CRUNCHBASE_2016**: This schema includes tables related to acquisitions,     companies, investments, and funding rounds in the Crunchbase database for    the year 2016. It's useful for analyzing acquisition trends, company     information, investment activity, and funding trends. 2. **RETAIL_DATA**: This schema contains tables related to retail operations,     such as call centers, customers, addresses, demographics, inventory,     items, promotions, stores, returns, sales, and warehouses. It can be used     to analyze call center performance, customer demographics, inventory     management, sales performance, and other retail operations. 3. ... \n",
      " \n",
      "  As you can see, the agent understood the request, called\n",
      "   describe_dataset  function provided by Waii, and generated a\n",
      "  summary of the dataset.\n",
      " \n",
      " \n",
      "  I‚Äôm interested in the\n",
      "   RETAIL_DATA  schema, so let me ask more of the schema -\n",
      " \n",
      " agent.chat(\"What can i do with the retail_data schema\") \n",
      " And I get this: \n",
      " The RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed  to support a wide range of analyses related to retail operations. Here  are some of the capabilities and types of analyses you can perform with  this schema: 1.\n",
      "\n",
      "For example, if the question indicates information needs to be retrieved\n",
      "  from the ‚Äúinternet‚Äù, the Google search tool will be chosen. Internally it uses\n",
      "  LLM which returns selected functions with parameters for a given context.\n",
      " \n",
      " \n",
      "  When the Waii tool is chosen, whether for describing a dataset, generating a\n",
      "  query, or running a query, it sends the API request to the Waii Service.\n",
      " \n",
      " \n",
      "  The Waii Service can be deployed as a hosted SaaS or as Docker containers\n",
      "  running in your on-premises environment. The components of the Waii Service\n",
      "  include:\n",
      " \n",
      " \n",
      "   \n",
      "     The Query Generator:  coordinates the entire workflow of\n",
      "    query generation and communicates with the LLM for this purpose.\n",
      "   \n",
      "   \n",
      "     Knowledge Graph / Metadata Management : connects to\n",
      "    databases, extracting metadata and query history as a knowledge graph to\n",
      "    assist the Query Generator in choosing the right tables and schemas.\n",
      "   \n",
      "   \n",
      "     Semantic Rules : These aid the Query Generator in producing\n",
      "    semantically correct queries.\n",
      "   \n",
      "   \n",
      "     Waii Compiler : After a query is generated by the LLM, the\n",
      "    Waii Compiler patches identified issues in the query. If a compilation issue\n",
      "    is not fixable, it regenerates the query with an articulated error message.\n",
      "   \n",
      " \n",
      " Create LlamaIndex agent with Waii + PDF Loader \n",
      " \n",
      "  Let‚Äôs first create two LlamaHub tools ‚Äî Waii and PDF Loader. LlamaHub tools\n",
      "  include specs to identify available functions along with their parameters, the\n",
      "  agent will select and execute which function to use based on available\n",
      "  functions and context.\n",
      " \n",
      " Let‚Äôs start with creating an agent which includes the Waii tool: \n",
      " from llama_hub.tools.google_search import GoogleSearchToolSpec from llama_hub.tools.waii import WaiiToolSpec from llama_index.agent import OpenAIAgent from llama_index.\n",
      "\n",
      "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis\n",
      "Introduction In many enterprises, data primarily resides in databases and generally, it‚Äôs very difficult to combine database data with other forms of data, such as PDFs, when trying to generate actionable insights. We envision the development of an agent that empowers anyone to leverage data from all of these data sources for informed decision-making. Imagine an agent proficient in creating documents by merging data from diverse sources, including JIRA and databases, further enriched with the latest internet-sourced information. At  waii.ai , we are committed to delivering an enterprise text-to-SQL API with the most complete and accurate translation of plain English to SQL available. Waii allows companies to build text-to-SQL right into their products as well as enable no-code analytics for their internal data/business teams. Waii works out of the box and can be self-hosted/on-prem. LlamaIndex introduces a remarkable RAG framework, facilitating the connection of various customer data sources, such as PDFs, Notion, and internal knowledge bases, to large language models (LLMs). This advancement simplifies the creation of data-augmented chatbots and analysis agents. This opens up a prime opportunity to develop an enterprise agent that can access data from multiple sources, including your preferred database. We will explore this further in the rest of the blog. Why a New Text-to-SQL LlamaIndex Plugin? To enable the Llama Index agent to utilize text-to-SQL APIs, a plugin is essential. LlamaIndex already has a built-in text-to-SQL plugin, but why did we decide to create a new LlamaHub plugin? The existing text-to-SQL plugin in LlamaIndex has been suitable for handling simple databases (less than 10 tables, 100 columns) with straightforward SQL queries.\n",
      "\n",
      "However, managing medium to large databases, which can include 100s of tables and 1000s of columns, presents a complex challenge. Limitations arise due to the restricted context windows of LLMs, and even those with large context windows, like GPT-4-turbo with its 128K tokens, can suffer from inaccuracies and regression in task retrieval when overloaded with content. This issue is discussed in a  LlamaIndex study . In contrast, Waii focuses on making query generation more efficient. We have developed a built-in compiler to deal with compilation errors from LLMs to support multiple dialects. Our internal knowledge graph, created from database metadata, constraints, and query history, aids in table/schema selection. Users can also apply semantic rules to schema/table/column, or integrate with their data catalog services, ensuring the semantic correctness of generated queries, in addition to syntactic correctness. To utilize our service, users simply need to connect their database to Waii and copy a Waii API key to create a LlamaIndex agent. LlamaIndex + Waii Agent We are thrilled to showcase the integration of Waii with LlamaIndex to create an agent capable of executing various text-to-SQL tasks and validating the data based on a PDF. We‚Äôll be analyzing customers‚Äô top-purchased categories during Christmas time, and compare it with  Deloitte‚Äôs holiday retail survey  report. Architecture of LlamaIndex + Waii Before diving into the code example, let‚Äôs look at the architecture first: \n",
      "\n",
      " \n",
      "   \n",
      " \n",
      "\n",
      " \n",
      "  The LlamaIndex agent operates on the client side, accompanied by a number of\n",
      "  tools: Each tool provides function specifications and allows functions to be\n",
      "  selected based on context and the user‚Äôs input to\n",
      "   chat(\"‚Ä¶\") .\n",
      "\n",
      "- \"Gift Cards & Other\" and \"Food & Beverage\" from Deloitte's survey do    not have a direct match in the top categories from your database. ... \n",
      " \n",
      "  Bingo! Now we can compare the results from our database with PDFs. And I love\n",
      "  seeing how the agent can correlate the two lists and tell me that my store\n",
      "  doesn‚Äôt have the ‚ÄúGift Cards & Other‚Äù and ‚ÄúFood & Beverage‚Äù\n",
      "  categories!\n",
      " \n",
      " \n",
      "  You can find the code from the Colab notebook\n",
      "   link \n",
      " \n",
      " Wrapping up \n",
      " \n",
      "  The integration of Waii‚Äôs text-to-SQL API with LlamaIndex‚Äôs RAG framework\n",
      "  marks a significant advancement in enterprise data analytics. This powerful\n",
      "  combination enables companies to effortlessly merge and analyze data from\n",
      "  various sources, including databases, PDFs, and the Internet. We demonstrated\n",
      "  the agent‚Äôs capability to generate SQL queries, understand complex datasets,\n",
      "  and correlate findings with external reports. This innovation not only\n",
      "  simplifies data analysis but also opens new avenues for informed\n",
      "  decision-making in the digital era.\n",
      " \n",
      " \n",
      "  To learn more about Waii, please contact us here:\n",
      "   https://www.waii.ai/#request-demo\n",
      "\n",
      "\n",
      "\n",
      "============Error #8=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What are the four stages involved in the solution proposed by Vibhav and the author for generating video explanations paired with individual code snippets?\n",
      "\n",
      "Expected Contexts:\n",
      "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the world of IT and Software Development, knowledge transfer (KT) stands out as a big challenge. Whether it‚Äôs new hires trying to understand their roles, folks on their notice periods aiming for a smooth handover, or the daily tasks of developers and product specialists adapting to ever-changing projects ‚Äî the KT process often leads to stress and worry. This gets more complicated with information spread out everywhere, the mix of new and old tech, and the fast pace of IT and Software Development projects. In this situation, broken bits of knowledge become the norm, causing delays, misunderstandings, and making learning harder. But amidst these challenges, might there be a beacon of optimism shining through? Vibhav  and I have developed a system that seamlessly organizes KT sessions. By leveraging personal images, we generate video explanations that are paired with individual code snippets, making the code far more comprehensible. Our innovative approach was recognized when we secured the First Prize at the Google Cloud, Searce, and LifeSight hackathon. With the combined strengths of LlamaIndex and D-ID, our aim is not just to consolidate information but also to simplify tasks and elevate the KT process. In doing so, we‚Äôre transforming a daunting industry challenge into a straightforward and manageable endeavor. Want to see how LlamaIndex plays a key role in this change? Let‚Äôs dive in together! Solution: The solution has four stages: Code Parsing: Break down the code base into individual code snippets or blocks. Summary and Explanation Generation with LlamaIndex: Produce a comprehensive summary of the entire code base. Create detailed explanations for each individual code block using LlamaIndex. Video Creation with D-ID: Generate videos using text-to-speech capabilities provided by D-ID. Video-Code Integration: Seamlessly stitch together the individual code blocks with their corresponding generated videos. Let‚Äôs dive into each stage in detail. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "   \n",
      " \n",
      "\n",
      " 1.\n",
      "\n",
      "Retrieved Contexts:\n",
      "This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  With the prowess of D-ID‚Äôs cutting-edge technology, we‚Äôre able to create\n",
      "  realistic videos where avatars ‚Äî whether they‚Äôre of us or another chosen\n",
      "  figure ‚Äî articulate each code block. Now, what brings these avatars to life?\n",
      "  The answer lies in Microsoft‚Äôs text-to-speech synthesizer. This tool takes our\n",
      "  detailed textual explanations and transforms them into natural, fluent speech.\n",
      "  Thus, with D-ID, we‚Äôre not just generating video but also integrating audio,\n",
      "  culminating in a comprehensive and fluid video explanation.\n",
      " \n",
      " To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration: \n",
      " \n",
      "  After generating insightful videos with avatars elucidating the code and\n",
      "  having our individual code snippets ready, the next crucial step is to marry\n",
      "  these two elements. This fusion ensures that viewers receive an immersive\n",
      "  visual experience, where they can simultaneously watch the explanation and\n",
      "  observe the related code.\n",
      " \n",
      " \n",
      "  To achieve this, we employed the\n",
      "   carbon  library, which transforms our code snippets into visually\n",
      "  appealing images. These images, when presented side-by-side with our\n",
      "  explanatory videos, offer a clearer understanding of the code in focus. The\n",
      "  final touch is added with the  moviepy  library, which seamlessly\n",
      "  stitches the video and code images together, ensuring a smooth and integrated\n",
      "  visual flow. Below, you'll find a sample illustrating this compelling\n",
      "  combination.\n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " Final Automatic Knowledge Transfer (KT) Generated Video \n",
      " \n",
      "  Following our detailed process, we‚Äôve crafted a KT video where Jerry explains\n",
      "  the ChatEngine code base of LlamaIndex. Watch the video below to see it all\n",
      "  come together!\n",
      "\n",
      "1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "   Code Parser \n",
      " \n",
      " \n",
      "  Understanding a code base starts with a high-level summary, but the true depth\n",
      "  lies in individual snippets or blocks. However, using entire code bases for\n",
      "  explanations can overwhelm language models like LLMs, causing them to either\n",
      "  exceed token limits or miss key details.\n",
      " \n",
      " \n",
      "  Our approach is simple yet efficient: break the code into digestible sections\n",
      "  like import statements, classes, initializer functions, and methods without\n",
      "  losing the code‚Äôs flow. This segmentation is done through a dependency graph\n",
      "  approach, utilizing Python‚Äôs\n",
      "   ast  library. By analyzing the code's structure, we can extract\n",
      "  classes, their docstrings, initializers, and other methods. This method not\n",
      "  only captures the essence of each segment but is also flexible, allowing for\n",
      "  further rules to extract additional code components.\n",
      " \n",
      " \n",
      "  The  code_parser  class embodies this strategy. It navigates the\n",
      "  code, distinguishing module-level functions from class-nested ones, and\n",
      "  arranges them systematically. The result? A granular yet comprehensive view of\n",
      "  the code, paving the way for precise and context-rich explanations.\n",
      " \n",
      " 2. Summary and Explanation Generation with LlamaIndex \n",
      " \n",
      "   Producing a Comprehensive Summary: \n",
      " \n",
      " \n",
      "  The initial step in understanding a code base is to grasp its overall essence.\n",
      "  This is achieved by generating a concise summary that gives a bird‚Äôs-eye view\n",
      "  of the entire code. LlamaIndex‚Äôs Summary Index \n",
      "  has been tailored for this exact task. In SummaryIndex, each block of code is\n",
      "  treated as a node. By inputting the structured blocks obtained from our code\n",
      "  parsing phase into SummaryIndex, we can produce a comprehensive snapshot that\n",
      "  serves as a summary of the entire code base.\n",
      "\n",
      "Detailed Explanations for Individual Code Blocks: \n",
      " \n",
      " \n",
      "  With a general understanding established, the next step is to delve into the\n",
      "  finer details. Starting from import statements, progressing to functions, and\n",
      "  eventually diving into classes and initializer functions, every block gets its\n",
      "  due attention. Here, LlamaIndex‚Äôs\n",
      "   accumulate response mode is a valuable asset, providing in-depth\n",
      "  explanations for each block.\n",
      " \n",
      " \n",
      "  However, a challenge arises. While\n",
      "   accumulate  provides in-depth insights into each block, it can\n",
      "  occasionally miss the broader context offered by preceding blocks. To address\n",
      "  this limitation, we‚Äôve adopted a two-pronged approach. As depicted in the\n",
      "  subsequent architecture, we employ two SummaryIndices for this endeavor.\n",
      " \n",
      " \n",
      "   \n",
      "    We utilize the first SummaryIndex to generate a concise summary for each\n",
      "    block, treating each block as a Node in SummaryIndex.\n",
      "   \n",
      "   \n",
      "    For the second SummaaryIndex in the stack, we feed the summarized context\n",
      "    from one node into the next. This ensures every node benefits from the\n",
      "    context of its predecessor. We then harness the\n",
      "     accumulate  mode to provide detailed explanations, making\n",
      "    certain that every segment of the code is explained comprehensively,\n",
      "    preserving the broader perspective. The outcome? A deep, contextually rich\n",
      "    interpretation of each code section.\n",
      "   \n",
      " \n",
      " \n",
      "  Note: We utilized Google‚Äôs PaLM API in conjunction with LlamaIndex to generate\n",
      "  summaries and explanations. Alternatively, models like GPT-3.5, GPT-4, or\n",
      "  other LLM‚Äôs can be employed for this purpose.\n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " 3. Video Creation with D-ID: \n",
      " \n",
      "  After carefully crafting summaries and detailed explanations for each code\n",
      "  block, it‚Äôs essential to convey this information in a captivating and\n",
      "  accessible manner. Videos, given their dynamic appeal, have the power to make\n",
      "  intricate code explanations clearer and more engaging.\n",
      "\n",
      "embed_model=embed_model, node_parser=node_parser)\\n\\n\" \n",
      "\n",
      "    code_snippet +=  \"_file = 'path_to_your_file'  # Replace with the path to your file\\n\" \n",
      "    code_snippet +=  \"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\" \n",
      "    code_snippet +=  \"if storage_context:\\n\" \n",
      "    code_snippet +=  \"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\" \n",
      "\n",
      "    code_snippet +=  \"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\" \n",
      "\n",
      "     return  code_snippet Conclusion RAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It‚Äôs designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation. Your insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let‚Äôs start a conversation on Linkedin. I‚Äôm always eager to collaborate and share knowledge with fellow tech adventurers. GitHub Repo Connect with Me on LinkedIn Live Demo\n",
      "\n",
      "Code Repository:\n",
      "   https://github.com/ravi03071991/KT_Generator \n",
      " \n",
      " Conclusion \n",
      " \n",
      "  Through this post, we‚Äôve showcased the transformative potential of LlamaIndex\n",
      "  in creating Knowledge Transfer (KT) Videos for code bases. It‚Äôs genuinely\n",
      "  remarkable to envision the advancements we‚Äôre making in this space. The\n",
      "  methodology we‚Äôve adopted is language-neutral, allowing flexibility in\n",
      "  adapting to various code bases. With some tweaks to the code parsing phase, we\n",
      "  believe it‚Äôs feasible to scale this to cover expansive code repositories\n",
      "  within organizations. Imagine a platform akin to YouTube, perhaps\n",
      "   KodeTube(KT) , where an organization‚Äôs entire codebase is\n",
      "  cataloged through explanatory videos. The horizon is bright with the\n",
      "  opportunities LlamaIndex brings, and we‚Äôre thrilled about the journey ahead.\n",
      "\n",
      "\n",
      "\n",
      "============Error #9=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the system handle custom transformations in the ingestion pipeline?\n",
      "\n",
      "Expected Contexts:\n",
      "from  llama_index  import  Document\n",
      " from  llama_index.embeddings  import  OpenAIEmbedding\n",
      " from  llama_index.text_splitter  import  SentenceSplitter\n",
      " from  llama_index.extractors  import  TitleExtractor\n",
      " from  llama_index.ingestion  import  IngestionPipeline, IngestionCache\n",
      " from  llama_index.ingestion.cache  import  RedisCache\n",
      " from  llama_index.vector_stores.qdrant  import  QdrantVectorStore\n",
      "\n",
      " import  qdrant_client\n",
      "client = qdrant_client.QdrantClient(location= \":memory:\" )\n",
      "vector_store = QdrantVectorStore(client=client, collection_name= \"test_store\" )\n",
      "pipeline = IngestionPipeline(\n",
      "    transformations=[\n",
      "        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n",
      "        TitleExtractor(),\n",
      "        OpenAIEmbedding(),\n",
      "    ],\n",
      "    cache=IngestionCache(cache=RedisCache(), collection= \"test_cache\" ),\n",
      "    vector_store=vector_store,\n",
      ")\n",
      " # Ingest directly into a vector db \n",
      "pipeline.run(documents=[Document.example()])\n",
      " # Create your index \n",
      " from  llama_index  import  VectorStoreIndex\n",
      "index = VectorStoreIndex.from_vector_store(vector_store) Custom Transformations Implementing custom transformations is easy! Let‚Äôs add a transform to remove special characters from the text before calling embeddings. The only real requirement for transformations is that they must accept a list of nodes and return a list of nodes.\n",
      "\n",
      "Retrieved Contexts:\n",
      "FAQ What‚Äôs the difference between a  QueryPipeline  and  IngestionPipeline  ? Great question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there‚Äôs potentially some shared abstractions we‚Äôll develop for both! Conclusion + Resources That‚Äôs it! As mentioned above we‚Äôll be adding a lot more resources and guides soon. In the meantime check out our current guides: Query Pipelines Guide Query Pipelines Walkthrough Query Pipeline Usage Pattern Query Pipelines Module Usage Guide\n",
      "\n",
      "What is a  Transformation  though? It could be a: text splitter node parser metadata extractor embeddings model Here‚Äôs a quick example of the basic usage pattern: from  llama_index  import  Document\n",
      " from  llama_index.embeddings  import  OpenAIEmbedding\n",
      " from  llama_index.text_splitter  import  SentenceSplitter\n",
      " from  llama_index.extractors  import  TitleExtractor\n",
      " from  llama_index.ingestion  import  IngestionPipeline, IngestionCache\n",
      "\n",
      "pipeline = IngestionPipeline(\n",
      "    transformations=[\n",
      "        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n",
      "        TitleExtractor(),\n",
      "        OpenAIEmbedding(),\n",
      "    ]\n",
      ")\n",
      "nodes = pipeline.run(documents=[Document.example()]) Transformation Caching Each time you run the same  IngestionPipeline  object, it caches a hash of the input nodes + transformations and the output of that transformation for each transformation in the pipeline. In subsequent runs, if there is a cache hit, that transformation will be skipped and the cached result will be used instead. The greatly speeds up duplicate runs, and can help improve iteration times when deciding which transformations to use.\n",
      "\n",
      "We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud. More specifically, we showcase a very relevant use case ‚Äî highlighting Ray features that are present in both the documentation as well as the Ray blog posts! Data Ingestion and Embedding Pipeline We use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below. Code for this part of the blog is  available here . Sequential pipeline with ‚Äúingest‚Äù, ‚Äúparse‚Äù and ‚Äúembed‚Äù stages. Files are processed sequentially resulting in poor hardware utilization and long computation time. Parallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized. Load Data We start by ingesting these two sources of data. We first fetch both data sources and download the HTML files. We then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API‚Äôs, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by  Unstructured . from  typing  import   Dict ,  List \n",
      " from  pathlib  import  Path\n",
      "\n",
      " from  llama_index  import  download_loader\n",
      " from  llama_index  import  Document\n",
      "\n",
      " # Step 1: Logic for loading and parsing the files into llama_index documents.\n",
      "\n",
      "Using the previous example, output = p.run(topic=\"YC\")\n",
      "# output type is Response\n",
      "type(output) If the pipeline has multiple root nodes and/or multiple output nodes, use  run_multi  . output_dict = p. run_multi ({ \"llm\" : { \"topic\" :  \"YC\" }})\n",
      " print (output_dict) Defining a Custom Query Component It‚Äôs super easy to subclass  CustomQueryComponent  so you can plug it into the QueryPipeline. Check out  our walkthrough  for more details. Supported Modules Currently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own! LLMs (both completion and chat) (  LLM  ) Prompts (  PromptTemplate  ) Query Engines (  BaseQueryEngine  ) Query Transforms (  BaseQueryTransform  ) Retrievers (  BaseRetriever  ) Output Parsers (  BaseOutputParser  ) Postprocessors/Rerankers (  BaseNodePostprocessor ) Response Synthesizers (  BaseSynthesizer  ) Other  QueryPipeline objects Custom components (  CustomQueryComponent  ) Check out the  module usage guide  for more details. Walkthrough Example Make sure to check out our  Introduction to Query Pipelines guide  for full details. We go over all the steps above with concrete examples! The notebook guide also logs traces through  Arize Phoenix . You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider. Related Work The idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include  Haystack  as well as the  LangChain Expression Language . Other related works include pipelines that are setup in the no-code/low-code setting such as  Langflow  /  Flowise . Our main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There‚Äôs a lot of optimizations/guides to be done here!\n",
      "\n",
      "import  re\n",
      " from  llama_index  import  Document\n",
      " from  llama_index.embeddings  import  OpenAIEmbedding\n",
      " from  llama_index.text_splitter  import  SentenceSplitter\n",
      " from  llama_index.ingestion  import  IngestionPipeline\n",
      " from  llama_index.schema  import  TransformComponent\n",
      "\n",
      " class   TextCleaner ( TransformComponent ):\n",
      "   def   __call__ ( self, nodes, **kwargs ):\n",
      "     for  node  in  nodes:\n",
      "      node.text = re.sub( r'[^0-9A-Za-z ]' ,  \"\" , node.text)\n",
      "     return  nodes\n",
      "pipeline = IngestionPipeline(\n",
      "    transformations=[\n",
      "        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n",
      "        TextCleaner(),\n",
      "        OpenAIEmbedding(),\n",
      "    ],\n",
      ")\n",
      "nodes = pipeline.run(documents=[Document.example()]) Node Parsing/Text Splitting ‚Äî Flattened and Simplified Interface We‚Äôve made our interface for parsing and splitting text a lot cleaner. Before: from  llama_index.node_parser  import  SimpleNodeParser\n",
      " from  llama_index.node_parser.extractors  import  (\n",
      "\tMetadataExtractor, TitleExtractor\n",
      ") \n",
      " from  llama_index.text_splitter  import  SentenceSplitter\n",
      "\n",
      "node_parser = SimpleNodeParser(\n",
      "  text_splitter=SentenceSplitter(chunk_size= 512 ),\n",
      "  metadata_extractor=MetadataExtractor(\n",
      "  extractors=[TitleExtractor()]\n",
      " ),\n",
      ")\n",
      "nodes = node_parser.get_nodes_from_documents(documents) After: from  llama_index.text_splitter  import  SentenceSplitter\n",
      " from  llama_index.extractors  import  TitleExtractor \n",
      "\n",
      "node_parser = SentenceSplitter(chunk_size= 512 )\n",
      "extractor = TitleExtractor()\n",
      "\n",
      " # use transforms directly \n",
      "nodes = node_parser(documents)\n",
      "nodes = extractor(nodes) Previously, the  NodeParser  object in LlamaIndex had become extremely bloated, holding both text splitters and metadata extractors, which caused both pains for users when changing these components, and pains for us trying to maintain and develop them.\n",
      "\n",
      "\n",
      "\n",
      "============Error #10=============\n",
      "\n",
      "\n",
      "Query:\n",
      "Can the system ingest documents directly into a vector database?\n",
      "\n",
      "Expected Contexts:\n",
      "from  llama_index  import  Document\n",
      " from  llama_index.embeddings  import  OpenAIEmbedding\n",
      " from  llama_index.text_splitter  import  SentenceSplitter\n",
      " from  llama_index.extractors  import  TitleExtractor\n",
      " from  llama_index.ingestion  import  IngestionPipeline, IngestionCache\n",
      " from  llama_index.ingestion.cache  import  RedisCache\n",
      " from  llama_index.vector_stores.qdrant  import  QdrantVectorStore\n",
      "\n",
      " import  qdrant_client\n",
      "client = qdrant_client.QdrantClient(location= \":memory:\" )\n",
      "vector_store = QdrantVectorStore(client=client, collection_name= \"test_store\" )\n",
      "pipeline = IngestionPipeline(\n",
      "    transformations=[\n",
      "        SentenceSplitter(chunk_size= 25 , chunk_overlap= 0 ),\n",
      "        TitleExtractor(),\n",
      "        OpenAIEmbedding(),\n",
      "    ],\n",
      "    cache=IngestionCache(cache=RedisCache(), collection= \"test_cache\" ),\n",
      "    vector_store=vector_store,\n",
      ")\n",
      " # Ingest directly into a vector db \n",
      "pipeline.run(documents=[Document.example()])\n",
      " # Create your index \n",
      " from  llama_index  import  VectorStoreIndex\n",
      "index = VectorStoreIndex.from_vector_store(vector_store) Custom Transformations Implementing custom transformations is easy! Let‚Äôs add a transform to remove special characters from the text before calling embeddings. The only real requirement for transformations is that they must accept a list of nodes and return a list of nodes.\n",
      "\n",
      "Retrieved Contexts:\n",
      "A Query Engine to Combine Structured Analytics and Semantic Search We have created a brand-new query engine (  SQLAutoVectorQueryEngine  ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer. The  SQLAutoVectorQueryEngine  is initialized through passing in a SQL query engine (  GPTNLStructStoreQueryEngine  ) as well as a query engine that uses our vector store  auto-retriever module  (  VectorIndexAutoRetriever  ). Both the SQL query engine and vector query engines are wrapped as ‚ÄúTool‚Äù objects containing a  name  and  description  field. Reminder: the  VectorIndexAutoRetriever  takes in a natural language query as input. Given some knowledge of the metadata schema of the vector database, the auto retriever first  infers  the other necessary query parameters to pass in (e.g. top-k value, and metadata filters), and executes a query against the vector database with all the query parameters. Diagram of the flow for SQLAutoVectorQueryEngine During query-time, we run the following steps: A selector prompt (similarly used in our  RouterQueryEngine  , see  guide ) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the  RetrieverQueryEngine  with  VectorIndexAutoRetriever  . If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output. A  query transformation  is run, to convert the original question into a more detailed question given the results from the SQL query.\n",
      "\n",
      "Example queries suited for Retrieval Augmented Generation ‚ÄúTell me about the historical museums in Berlin‚Äù ‚ÄúWhat does Jordan ask from Nick on behalf of Gatsby?‚Äù Combining These Two Systems For some queries, we may want to make use of knowledge in  both structured tables as well as vector databases/document stores  in order to give the best answer to the query. Ideally this can give us the best of both worlds: the analytics capabilities over structured data, and semantic understanding over unstructured data. Here‚Äôs an example use case: You have access to a collection of articles about different cities, stored in a vector database You also have access to a structured table containing statistics for each city. Given this data collection, let‚Äôs take an example query: ‚ÄúTell me about the arts and culture of the city with the highest population.‚Äù The ‚Äúproper‚Äù way to answer this question is roughly as follows: Query the structured table for the city with the highest population. SELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1 Convert the original question into a more detailed question: ‚ÄúTell me about the arts and culture of Tokyo.‚Äù Ask the new question over your vector database. Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer. Let‚Äôs think about some of the high-level implications of such a sequence: Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first ‚Äúretrieval‚Äù step. We want to make sure that we can somehow ‚Äújoin‚Äù the results from the SQL query with the context stored in the vector database. There is no existing language to ‚Äújoin‚Äù information between a SQL and vector database. We will have to implement this behavior ourselves. Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population.\n",
      "\n",
      "These data connectors are primarily hosted on [LlamaHub]( https://llamahub.ai/ ). This makes it easy for users to integrate data from their existing files and applications. Data Indexing Once the data is loaded, LlamaIndex offers the ability to index this data with a wide variety of data structures and storage integration options (including Weaviate). LlamaIndex supports indexing unstructured, semi-structured, and structured data. A standard way to index unstructured data is to split the source documents into text ‚Äúchunks‚Äù, embed each chunk, and store each chunk/embedding in a vector database. Data Querying Once your data is ingested/stored, LlamaIndex provides the tools to define an advanced retrieval / query ‚Äúengine‚Äù over your data. Our retriever constructs allow you to retrieve data from your knowledge base given an input prompt. A query engine construct allows you to define an interface that can take in an input prompt, and output a knowledge-augmented response ‚Äî it can use retrieval and synthesis (LLM) modules under the hood. Some examples of query engine ‚Äútasks‚Äù are given below, in rough order from easy to advanced: Semantic Search: Retrieve the top-k most similar items from the knowledge corpus by embedding similarity to the query, and synthesize a response over these contexts. Structured Analytics: Convert natural language to a SQL query that can be executed Query Decomposition over Documents: Break down a query into sub-questions, each over a subset of underlying documents. Each sub-question can be executed against its own query engine. Demo Notebook Walkthrough Let‚Äôs walk through a simple example of how LlamaIndex can be used with Weaviate to build a simple Question-Answering (QA) system over the Weaviate blogs! The full code can be found in the  Weaviate recipes repo . The first step is to setup your Weaviate client.\n",
      "\n",
      "For instance if the original question is ‚ÄúTell me about the arts and culture of the city with the highest population.‚Äù, and the SQL query returns Tokyo as the city with the highest population, then the new query is ‚ÄúTell me about the arts and culture of Tokyo.‚Äù The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response. The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a  VectorIndexAutoRetriever  module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like  query_str=\"arts and culture\"  and  filters={\"title\": \"Tokyo\"}  . The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer. Taking a step back, here are some general comments about this approach: Using our auto-retrieval module is our way of  simulating  a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with. This also implies that there doesn‚Äôt need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and document store metadata though; that way we don‚Äôt need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters. Experiments So how well does this work?\n",
      "\n",
      "Experiments So how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection. Setup Our experiment setup is very simple. We have a SQL table called  city_stats  which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin. We also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate ‚ÄúNode‚Äù object; each chunk also contains a  title  metadata attribute containing the city name. We then derive the  VectorIndexAutoRetriever  and  RetrieverQueryEngine  from the Pinecone vector index. from  llama_index.indices.vector_store.retrievers  import  VectorIndexAutoRetriever\n",
      " from  llama_index.vector_stores.types  import  MetadataInfo, VectorStoreInfo\n",
      " from  llama_index.query_engine.retriever_query_engine  import  RetrieverQueryEngine\n",
      "\n",
      "\n",
      "vector_store_info = VectorStoreInfo(\n",
      "    content_info= 'articles about different cities' ,\n",
      "    metadata_info=[\n",
      "        MetadataInfo(\n",
      "            name= 'city' , \n",
      "             type = 'str' , \n",
      "            description= 'The name of the city' ),\n",
      "    ]\n",
      ")\n",
      "vector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n",
      "\n",
      "retriever_query_engine = RetrieverQueryEngine.from_args(\n",
      "    vector_auto_retriever, service_context=service_context\n",
      ") You can also get the SQL query engine as follows sql_query_engine = sql_index.as_query_engine() Both the SQL query engine and vector query engine can be wrapped as  QueryEngineTool  objects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in retrieval_eval_irrelevance_df.reset_index(drop=True).iterrows():\n",
    "    print(f\"\\n\\n============Error #{i+1}=============\\n\\n\")\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    expected_contexts = [json.loads(record.payload['_node_content'])['text'] for record in qdrantdb.retrieve(COLLECTION, ids=row.expected_ids)]\n",
    "    expected_contexts = '\\n\\n'.join(expected_contexts)\n",
    "    print(f\"Expected Contexts:\\n{expected_contexts}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.retrieved_texts)\n",
    "    print(f\"Retrieved Contexts:\\n{contexts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    "),\n",
    "# Below question is hard because LLM needs to follow the URL in the blog to get the information to answer\n",
    "(\"How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.\",\n",
    "\"\"\"\n",
    "The MemoryCache project by Mozilla aims to transform local desktop environments into on-device AI agents, utilizing PrivateGPT_AI and LlamaIndex to enhance personal knowledge management. It saves browser history and other local files to the user‚Äôs machine, allowing a local AI model to ingest and augment responses. This approach maintains privacy by avoiding cloud-based processing, focusing instead on generating insights from personal data. The project emphasizes creating a personalized AI experience that mirrors the original vision of personal computers as companions for thought.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_rag_dataset(response_eval_dataset, response_eval_prediction_dataset, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "        evals[\"contexts\"].append(prediction.contexts)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "        \"contexts\": evals['contexts'],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "        pd.Series(evals['contexts'], name='contexts')\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_SYNTHETIC_EVAL_DATASET = True\n",
    "RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "# RESPONSE_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "# RESPONSE_EVAL_LLM_MODEL = 'gpt-4'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 1\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = min(len(documents), RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:49:06.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 10:49:08.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mCreating synthetic response eval dataset...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf002d73b0c42ca86cd9dd50be42f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:05<00:00,  4.89it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.05s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.56s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.26s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.52s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.38s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.46s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.80s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.93s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.61s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.45s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.80s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.08s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.74s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # Use good model to generate the eval dataset\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=response_eval_llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        question_gen_query=QUESTION_GEN_QUERY,  # Reuse the same format from the above Retrieval Question Gen Query\n",
    "        show_progress=True,\n",
    "        workers=(os.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "940feda2-1aad-427c-895f-2d4cdf43918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:24<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_prediction_dataset = await synthetic_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e566c49226845ceb85de8ca15b9437c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    synthetic_response_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>3.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   3.758621\n",
       "mean_relevancy_score     0.793103\n",
       "mean_faithfulness_score  0.793103"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the property graph index different from...</td>\n",
       "      <td>\\nThe property graph index is different from t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Customizing property graph index in LlamaInde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of using the SchemaLLMPath...</td>\n",
       "      <td>\\nThe purpose of using the SchemaLLMPathExtrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[In this example, we will use the  SchemaLLMPa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you accelerate the process of extracti...</td>\n",
       "      <td>\\nYou can accelerate the process of extracting...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now that we have defined the graph schema, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you adjust the similarity_threshold an...</td>\n",
       "      <td>\\nYou can adjust the similarity_threshold and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[size(allCombinedResults)-1, 1) as combinedRes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What methods are used in the custom retriever ...</td>\n",
       "      <td>\\nThe custom retriever uses both Vector search...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[results_df = pd.DataFrame()\\n\\nembed_name =  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the OpenAI Cookbook aim to enhance th...</td>\n",
       "      <td>\\nThe OpenAI Cookbook aims to enhance the effe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the project integrate KOSMOS-2 and Pa...</td>\n",
       "      <td>\\nThe project integrates KOSMOS-2 and PaLM wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Google  PaLM API  adds the layer of linguisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How is the sidebar enhanced to improve credibi...</td>\n",
       "      <td>\\nThe sidebar is not mentioned in the provided...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It contains a detailed analysis of all risk f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the application handle user interacti...</td>\n",
       "      <td>\\nThe application handles user interaction and...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This triggers the outer loop to run the conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How has the field of AI and large language mod...</td>\n",
       "      <td>\\nThe field of AI and large language models ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Docs ,  Tweet . RA-DIT:  We drew inspiration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of using Medium articles f...</td>\n",
       "      <td>\\nThe purpose of using Medium articles from 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the purpose of using a vision model to...</td>\n",
       "      <td>\\nThe purpose of using a vision model to filte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The  SimpleMultiModalQueryEngine  first retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does the integration between Create-llama ...</td>\n",
       "      <td>\\nThe integration between Create-llama and E2B...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Code ,  Tweet . We have introduced  return_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of using LLMs for retrieva...</td>\n",
       "      <td>\\nTo enhance document retrieval beyond naive t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the two ways of feeding text into the...</td>\n",
       "      <td>\\nYou can directly feed in the raw text corres...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Example format:\\n  Document 1:\\n  &lt;summary of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the benefit of using LLMs for rerankin...</td>\n",
       "      <td>\\nThe benefit of using LLMs for reranking in i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the length of the Lyft SEC 10-K document?</td>\n",
       "      <td>\\nThe length of the Lyft SEC 10-K document is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[If Uber is unable to attract or maintain a cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is Tonic Validate and what does it provid...</td>\n",
       "      <td>\\nTonic Validate is a RAG benchmarking and eva...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate x LlamaIndex: Implementing int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What steps are involved in setting up Tonic Va...</td>\n",
       "      <td>\\nTo set up Tonic Validate, first install it v...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Setting up Tonic Validate To set up Tonic Val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the purpose of setting up Tonic Validate?</td>\n",
       "      <td>\\nTo create tests for Tonic Validate and to im...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Setting up Tonic Validate To set up Tonic Val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What metrics are used in the scoring process f...</td>\n",
       "      <td>\\nThe metrics used in the scoring process for ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let‚Äôs score them: def   score_run ( questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the recommended approach for adding QA...</td>\n",
       "      <td>\\nOne approach for adding QA testing to a RAG ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[One approach, the one that we recommend, for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What challenges do enterprises face when adopt...</td>\n",
       "      <td>\\nEnterprises face challenges around protectin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of Llama Datasets in bench...</td>\n",
       "      <td>\\nThe purpose of Llama Datasets is to allow us...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Datasets ü¶ôüìù\\n(Authors: Andr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What new offering was launched by LlamaIndex t...</td>\n",
       "      <td>\\nThe new offering launched by LlamaIndex that...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Each dataset, designed as a QA set, integrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Zephyr 7b LLM address the obstacles f...</td>\n",
       "      <td>\\nZephyr 7b LLM employs advanced machine learn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Becoming Proficient in Document Extraction\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the integration of Zephyr and LlamaIn...</td>\n",
       "      <td>\\nThe integration of Zephyr and LlamaIndex enh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2. Contextual Comprehension: \\n \\n \\n  Diverg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does a tree index organize data?</td>\n",
       "      <td>\\nA tree index organizes data by building a tr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if child_branch_factor=2, a query will choose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How can one explore the author's Huggingface p...</td>\n",
       "      <td>\\nTo explore the author's HuggingFace profile,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[)\\n\\nquery_engine = index. as_query_engine (\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How is the property graph index different from...   \n",
       "1   What is the purpose of using the SchemaLLMPath...   \n",
       "2   How can you accelerate the process of extracti...   \n",
       "3   How can you adjust the similarity_threshold an...   \n",
       "4   What methods are used in the custom retriever ...   \n",
       "5   How does the OpenAI Cookbook aim to enhance th...   \n",
       "6   How does the project integrate KOSMOS-2 and Pa...   \n",
       "7   How is the sidebar enhanced to improve credibi...   \n",
       "8   How does the application handle user interacti...   \n",
       "9   How has the field of AI and large language mod...   \n",
       "10  What is the purpose of using Medium articles f...   \n",
       "11  What is the purpose of using a vision model to...   \n",
       "12  How does the integration between Create-llama ...   \n",
       "13  What is the purpose of using LLMs for retrieva...   \n",
       "14  What are the two ways of feeding text into the...   \n",
       "15  What is the benefit of using LLMs for rerankin...   \n",
       "16  What is the length of the Lyft SEC 10-K document?   \n",
       "17  What is Tonic Validate and what does it provid...   \n",
       "18  What steps are involved in setting up Tonic Va...   \n",
       "19  What is the purpose of setting up Tonic Validate?   \n",
       "20  What metrics are used in the scoring process f...   \n",
       "21  What is the recommended approach for adding QA...   \n",
       "22  What challenges do enterprises face when adopt...   \n",
       "23  What is the purpose of Llama Datasets in bench...   \n",
       "24  What new offering was launched by LlamaIndex t...   \n",
       "25  How does Zephyr 7b LLM address the obstacles f...   \n",
       "26  How does the integration of Zephyr and LlamaIn...   \n",
       "27               How does a tree index organize data?   \n",
       "28  How can one explore the author's Huggingface p...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   \\nThe property graph index is different from t...              1.0   \n",
       "1   \\nThe purpose of using the SchemaLLMPathExtrac...              1.0   \n",
       "2   \\nYou can accelerate the process of extracting...              1.0   \n",
       "3   \\nYou can adjust the similarity_threshold and ...              1.0   \n",
       "4   \\nThe custom retriever uses both Vector search...              1.0   \n",
       "5   \\nThe OpenAI Cookbook aims to enhance the effe...              1.0   \n",
       "6   \\nThe project integrates KOSMOS-2 and PaLM wit...              1.0   \n",
       "7   \\nThe sidebar is not mentioned in the provided...              0.0   \n",
       "8   \\nThe application handles user interaction and...              1.0   \n",
       "9   \\nThe field of AI and large language models ha...              1.0   \n",
       "10  \\nThe purpose of using Medium articles from 20...              1.0   \n",
       "11  \\nThe purpose of using a vision model to filte...              0.0   \n",
       "12  \\nThe integration between Create-llama and E2B...              0.0   \n",
       "13  \\nTo enhance document retrieval beyond naive t...              1.0   \n",
       "14  \\nYou can directly feed in the raw text corres...              1.0   \n",
       "15  \\nThe benefit of using LLMs for reranking in i...              1.0   \n",
       "16  \\nThe length of the Lyft SEC 10-K document is ...              0.0   \n",
       "17  \\nTonic Validate is a RAG benchmarking and eva...              1.0   \n",
       "18  \\nTo set up Tonic Validate, first install it v...              1.0   \n",
       "19  \\nTo create tests for Tonic Validate and to im...              0.0   \n",
       "20  \\nThe metrics used in the scoring process for ...              1.0   \n",
       "21  \\nOne approach for adding QA testing to a RAG ...              1.0   \n",
       "22  \\nEnterprises face challenges around protectin...              1.0   \n",
       "23  \\nThe purpose of Llama Datasets is to allow us...              1.0   \n",
       "24  \\nThe new offering launched by LlamaIndex that...              1.0   \n",
       "25  \\nZephyr 7b LLM employs advanced machine learn...              1.0   \n",
       "26  \\nThe integration of Zephyr and LlamaIndex enh...              1.0   \n",
       "27  \\nA tree index organizes data by building a tr...              1.0   \n",
       "28  \\nTo explore the author's HuggingFace profile,...              0.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 4.5                 1.0   \n",
       "1                 4.5                 1.0   \n",
       "2                 4.5                 1.0   \n",
       "3                 4.5                 1.0   \n",
       "4                 3.5                 1.0   \n",
       "5                 4.5                 1.0   \n",
       "6                 4.5                 1.0   \n",
       "7                 1.0                 0.0   \n",
       "8                 2.5                 1.0   \n",
       "9                 4.0                 1.0   \n",
       "10                4.0                 1.0   \n",
       "11                2.0                 1.0   \n",
       "12                2.0                 0.0   \n",
       "13                4.0                 1.0   \n",
       "14                4.5                 0.0   \n",
       "15                4.0                 1.0   \n",
       "16                1.0                 0.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.5                 1.0   \n",
       "19                3.0                 1.0   \n",
       "20                4.5                 1.0   \n",
       "21                5.0                 1.0   \n",
       "22                5.0                 1.0   \n",
       "23                4.5                 1.0   \n",
       "24                3.0                 1.0   \n",
       "25                4.0                 1.0   \n",
       "26                4.5                 1.0   \n",
       "27                4.0                 0.0   \n",
       "28                3.0                 0.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [Customizing property graph index in LlamaInde...  \n",
       "1   [In this example, we will use the  SchemaLLMPa...  \n",
       "2   [Now that we have defined the graph schema, we...  \n",
       "3   [size(allCombinedResults)-1, 1) as combinedRes...  \n",
       "4   [results_df = pd.DataFrame()\\n\\nembed_name =  ...  \n",
       "5   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "6   [Google  PaLM API  adds the layer of linguisti...  \n",
       "7   [It contains a detailed analysis of all risk f...  \n",
       "8   [This triggers the outer loop to run the conti...  \n",
       "9   [Docs ,  Tweet . RA-DIT:  We drew inspiration ...  \n",
       "10  [LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...  \n",
       "11  [The  SimpleMultiModalQueryEngine  first retri...  \n",
       "12  [Code ,  Tweet . We have introduced  return_di...  \n",
       "13  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "14  [Example format:\\n  Document 1:\\n  <summary of...  \n",
       "15  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "16  [If Uber is unable to attract or maintain a cr...  \n",
       "17  [Tonic Validate x LlamaIndex: Implementing int...  \n",
       "18  [Setting up Tonic Validate To set up Tonic Val...  \n",
       "19  [Setting up Tonic Validate To set up Tonic Val...  \n",
       "20  [Let‚Äôs score them: def   score_run ( questions...  \n",
       "21  [One approach, the one that we recommend, for ...  \n",
       "22  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "23  [Introducing Llama Datasets ü¶ôüìù\\n(Authors: Andr...  \n",
       "24  [Each dataset, designed as a QA set, integrate...  \n",
       "25  [Becoming Proficient in Document Extraction\\nI...  \n",
       "26  [2. Contextual Comprehension: \\n \\n \\n  Diverg...  \n",
       "27  [if child_branch_factor=2, a query will choose...  \n",
       "28  [)\\n\\nquery_engine = index. as_query_engine (\\...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14e8cb-d4fc-4e60-9c11-4af8fe304bb0",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a36d56f-6210-42b1-a878-606b4f0e4c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How is the sidebar enhanced to improve credibi...</td>\n",
       "      <td>\\nThe sidebar is not mentioned in the provided...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It contains a detailed analysis of all risk f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the length of the Lyft SEC 10-K document?</td>\n",
       "      <td>\\nThe length of the Lyft SEC 10-K document is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[If Uber is unable to attract or maintain a cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does the integration between Create-llama ...</td>\n",
       "      <td>\\nThe integration between Create-llama and E2B...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Code ,  Tweet . We have introduced  return_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the purpose of using a vision model to...</td>\n",
       "      <td>\\nThe purpose of using a vision model to filte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The  SimpleMultiModalQueryEngine  first retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How can one explore the author's Huggingface p...</td>\n",
       "      <td>\\nTo explore the author's HuggingFace profile,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[)\\n\\nquery_engine = index. as_query_engine (\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the purpose of setting up Tonic Validate?</td>\n",
       "      <td>\\nTo create tests for Tonic Validate and to im...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Setting up Tonic Validate To set up Tonic Val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the application handle user interacti...</td>\n",
       "      <td>\\nThe application handles user interaction and...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This triggers the outer loop to run the conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What new offering was launched by LlamaIndex t...</td>\n",
       "      <td>\\nThe new offering launched by LlamaIndex that...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Each dataset, designed as a QA set, integrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What methods are used in the custom retriever ...</td>\n",
       "      <td>\\nThe custom retriever uses both Vector search...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[results_df = pd.DataFrame()\\n\\nembed_name =  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does a tree index organize data?</td>\n",
       "      <td>\\nA tree index organizes data by building a tr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if child_branch_factor=2, a query will choose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How has the field of AI and large language mod...</td>\n",
       "      <td>\\nThe field of AI and large language models ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Docs ,  Tweet . RA-DIT:  We drew inspiration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of using Medium articles f...</td>\n",
       "      <td>\\nThe purpose of using Medium articles from 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of using LLMs for retrieva...</td>\n",
       "      <td>\\nTo enhance document retrieval beyond naive t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the benefit of using LLMs for rerankin...</td>\n",
       "      <td>\\nThe benefit of using LLMs for reranking in i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Zephyr 7b LLM address the obstacles f...</td>\n",
       "      <td>\\nZephyr 7b LLM employs advanced machine learn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Becoming Proficient in Document Extraction\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the two ways of feeding text into the...</td>\n",
       "      <td>\\nYou can directly feed in the raw text corres...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Example format:\\n  Document 1:\\n  &lt;summary of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the property graph index different from...</td>\n",
       "      <td>\\nThe property graph index is different from t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Customizing property graph index in LlamaInde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of using the SchemaLLMPath...</td>\n",
       "      <td>\\nThe purpose of using the SchemaLLMPathExtrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[In this example, we will use the  SchemaLLMPa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you accelerate the process of extracti...</td>\n",
       "      <td>\\nYou can accelerate the process of extracting...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now that we have defined the graph schema, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you adjust the similarity_threshold an...</td>\n",
       "      <td>\\nYou can adjust the similarity_threshold and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[size(allCombinedResults)-1, 1) as combinedRes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the OpenAI Cookbook aim to enhance th...</td>\n",
       "      <td>\\nThe OpenAI Cookbook aims to enhance the effe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the project integrate KOSMOS-2 and Pa...</td>\n",
       "      <td>\\nThe project integrates KOSMOS-2 and PaLM wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Google  PaLM API  adds the layer of linguisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is Tonic Validate and what does it provid...</td>\n",
       "      <td>\\nTonic Validate is a RAG benchmarking and eva...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate x LlamaIndex: Implementing int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What steps are involved in setting up Tonic Va...</td>\n",
       "      <td>\\nTo set up Tonic Validate, first install it v...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Setting up Tonic Validate To set up Tonic Val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What metrics are used in the scoring process f...</td>\n",
       "      <td>\\nThe metrics used in the scoring process for ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let‚Äôs score them: def   score_run ( questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of Llama Datasets in bench...</td>\n",
       "      <td>\\nThe purpose of Llama Datasets is to allow us...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Datasets ü¶ôüìù\\n(Authors: Andr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the integration of Zephyr and LlamaIn...</td>\n",
       "      <td>\\nThe integration of Zephyr and LlamaIndex enh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2. Contextual Comprehension: \\n \\n \\n  Diverg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the recommended approach for adding QA...</td>\n",
       "      <td>\\nOne approach for adding QA testing to a RAG ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[One approach, the one that we recommend, for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What challenges do enterprises face when adopt...</td>\n",
       "      <td>\\nEnterprises face challenges around protectin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "7   How is the sidebar enhanced to improve credibi...   \n",
       "16  What is the length of the Lyft SEC 10-K document?   \n",
       "12  How does the integration between Create-llama ...   \n",
       "11  What is the purpose of using a vision model to...   \n",
       "28  How can one explore the author's Huggingface p...   \n",
       "19  What is the purpose of setting up Tonic Validate?   \n",
       "8   How does the application handle user interacti...   \n",
       "24  What new offering was launched by LlamaIndex t...   \n",
       "4   What methods are used in the custom retriever ...   \n",
       "27               How does a tree index organize data?   \n",
       "9   How has the field of AI and large language mod...   \n",
       "10  What is the purpose of using Medium articles f...   \n",
       "13  What is the purpose of using LLMs for retrieva...   \n",
       "15  What is the benefit of using LLMs for rerankin...   \n",
       "25  How does Zephyr 7b LLM address the obstacles f...   \n",
       "14  What are the two ways of feeding text into the...   \n",
       "0   How is the property graph index different from...   \n",
       "1   What is the purpose of using the SchemaLLMPath...   \n",
       "2   How can you accelerate the process of extracti...   \n",
       "3   How can you adjust the similarity_threshold an...   \n",
       "5   How does the OpenAI Cookbook aim to enhance th...   \n",
       "6   How does the project integrate KOSMOS-2 and Pa...   \n",
       "17  What is Tonic Validate and what does it provid...   \n",
       "18  What steps are involved in setting up Tonic Va...   \n",
       "20  What metrics are used in the scoring process f...   \n",
       "23  What is the purpose of Llama Datasets in bench...   \n",
       "26  How does the integration of Zephyr and LlamaIn...   \n",
       "21  What is the recommended approach for adding QA...   \n",
       "22  What challenges do enterprises face when adopt...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "7   \\nThe sidebar is not mentioned in the provided...              0.0   \n",
       "16  \\nThe length of the Lyft SEC 10-K document is ...              0.0   \n",
       "12  \\nThe integration between Create-llama and E2B...              0.0   \n",
       "11  \\nThe purpose of using a vision model to filte...              0.0   \n",
       "28  \\nTo explore the author's HuggingFace profile,...              0.0   \n",
       "19  \\nTo create tests for Tonic Validate and to im...              0.0   \n",
       "8   \\nThe application handles user interaction and...              1.0   \n",
       "24  \\nThe new offering launched by LlamaIndex that...              1.0   \n",
       "4   \\nThe custom retriever uses both Vector search...              1.0   \n",
       "27  \\nA tree index organizes data by building a tr...              1.0   \n",
       "9   \\nThe field of AI and large language models ha...              1.0   \n",
       "10  \\nThe purpose of using Medium articles from 20...              1.0   \n",
       "13  \\nTo enhance document retrieval beyond naive t...              1.0   \n",
       "15  \\nThe benefit of using LLMs for reranking in i...              1.0   \n",
       "25  \\nZephyr 7b LLM employs advanced machine learn...              1.0   \n",
       "14  \\nYou can directly feed in the raw text corres...              1.0   \n",
       "0   \\nThe property graph index is different from t...              1.0   \n",
       "1   \\nThe purpose of using the SchemaLLMPathExtrac...              1.0   \n",
       "2   \\nYou can accelerate the process of extracting...              1.0   \n",
       "3   \\nYou can adjust the similarity_threshold and ...              1.0   \n",
       "5   \\nThe OpenAI Cookbook aims to enhance the effe...              1.0   \n",
       "6   \\nThe project integrates KOSMOS-2 and PaLM wit...              1.0   \n",
       "17  \\nTonic Validate is a RAG benchmarking and eva...              1.0   \n",
       "18  \\nTo set up Tonic Validate, first install it v...              1.0   \n",
       "20  \\nThe metrics used in the scoring process for ...              1.0   \n",
       "23  \\nThe purpose of Llama Datasets is to allow us...              1.0   \n",
       "26  \\nThe integration of Zephyr and LlamaIndex enh...              1.0   \n",
       "21  \\nOne approach for adding QA testing to a RAG ...              1.0   \n",
       "22  \\nEnterprises face challenges around protectin...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "7                 1.0                 0.0   \n",
       "16                1.0                 0.0   \n",
       "12                2.0                 0.0   \n",
       "11                2.0                 1.0   \n",
       "28                3.0                 0.0   \n",
       "19                3.0                 1.0   \n",
       "8                 2.5                 1.0   \n",
       "24                3.0                 1.0   \n",
       "4                 3.5                 1.0   \n",
       "27                4.0                 0.0   \n",
       "9                 4.0                 1.0   \n",
       "10                4.0                 1.0   \n",
       "13                4.0                 1.0   \n",
       "15                4.0                 1.0   \n",
       "25                4.0                 1.0   \n",
       "14                4.5                 0.0   \n",
       "0                 4.5                 1.0   \n",
       "1                 4.5                 1.0   \n",
       "2                 4.5                 1.0   \n",
       "3                 4.5                 1.0   \n",
       "5                 4.5                 1.0   \n",
       "6                 4.5                 1.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.5                 1.0   \n",
       "20                4.5                 1.0   \n",
       "23                4.5                 1.0   \n",
       "26                4.5                 1.0   \n",
       "21                5.0                 1.0   \n",
       "22                5.0                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "7   [It contains a detailed analysis of all risk f...  \n",
       "16  [If Uber is unable to attract or maintain a cr...  \n",
       "12  [Code ,  Tweet . We have introduced  return_di...  \n",
       "11  [The  SimpleMultiModalQueryEngine  first retri...  \n",
       "28  [)\\n\\nquery_engine = index. as_query_engine (\\...  \n",
       "19  [Setting up Tonic Validate To set up Tonic Val...  \n",
       "8   [This triggers the outer loop to run the conti...  \n",
       "24  [Each dataset, designed as a QA set, integrate...  \n",
       "4   [results_df = pd.DataFrame()\\n\\nembed_name =  ...  \n",
       "27  [if child_branch_factor=2, a query will choose...  \n",
       "9   [Docs ,  Tweet . RA-DIT:  We drew inspiration ...  \n",
       "10  [LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...  \n",
       "13  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "15  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "25  [Becoming Proficient in Document Extraction\\nI...  \n",
       "14  [Example format:\\n  Document 1:\\n  <summary of...  \n",
       "0   [Customizing property graph index in LlamaInde...  \n",
       "1   [In this example, we will use the  SchemaLLMPa...  \n",
       "2   [Now that we have defined the graph schema, we...  \n",
       "3   [size(allCombinedResults)-1, 1) as combinedRes...  \n",
       "5   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "6   [Google  PaLM API  adds the layer of linguisti...  \n",
       "17  [Tonic Validate x LlamaIndex: Implementing int...  \n",
       "18  [Setting up Tonic Validate To set up Tonic Val...  \n",
       "20  [Let‚Äôs score them: def   score_run ( questions...  \n",
       "23  [Introducing Llama Datasets ü¶ôüìù\\n(Authors: Andr...  \n",
       "26  [2. Contextual Comprehension: \\n \\n \\n  Diverg...  \n",
       "21  [One approach, the one that we recommend, for ...  \n",
       "22  [LlamaIndex Accelerates Enterprise Generative ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df.sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e56956b8-7c12-4548-b913-ae3bc0f77fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How is the sidebar enhanced to improve credibility and engagement?\n",
      "\n",
      "Context:\n",
      "It contains a detailed analysis of all risk factors in bullet points, and offers a comparison across time for all bullet points. Query 3 response = query_engine.query(\"Analyze Uber revenue growth and risk factors over the last few quarters\") Response (intermediate steps): > Current query: Analyze Uber revenue growth and risk factors over quarters\n",
      "> New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n",
      "> Current query: Analyze Uber revenue growth and risk factors over quarters\n",
      "> New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n",
      "> Current query: Analyze Uber revenue growth and risk factors over quarters\n",
      "> New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n",
      "> Current query: Analyze Uber revenue growth and risk factors over quarters\n",
      "> New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n",
      "> Current query: Analyze Uber revenue growth and risk factors over quarters\n",
      "> New query:  What were Uber's total revenues for the quarter ending September 2022?\n",
      "> Current query: Analyze Uber revenue growth and risk factors over quarters\n",
      "> New query:  What were Uber's total revenues for the quarter ending September 2022? Final Response Uber 's revenue growth has been impressive over the past few quarters, with the quarter ending March 2022 showing a 136% year-over-year growth and the quarter ending June 2022 showing a 105% year-over-year growth. The revenue for the quarter ending June 2022 was $8,343 million. \n",
      "\n",
      "The risk factors for Uber' s business have been largely related to the ongoing impacts of the COVID- 19  pandemic, including reduced  global  demand  for  Mobility rides, supply constraints,  and  potential permanent changes to end-user behavior.\n",
      "\n",
      "As a reminder, full results are in the notebook:  https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing GPT-3 ReAct Agent Results Query 1 agent_chain.run(input=\"Analyze Uber revenue growth over the last few quarters\") Response: We see that only the September 10-Q filing is chosen to answer the question. The September 10-Q does contain some information about revenue growth compared to the same period in 2021, but that doesn‚Äôt explicitly answer the question, which is about revenue growth the past few quarters. Query 2 agent_chain.run(input=\"Analyze changes in risk factors for Uber\") Response: The September and June 10-Q filings are chosen, but not March. Moreover, the answer is vague and doesn‚Äôt provide much detail regarding concrete risk factors for Uber (and also mentions that the risk factors ‚Äúhave changed over the past three quarters‚Äù even though it‚Äôs only using two Tools). Query 3 In this query, we more explicitly showcase how slight changes in prompts can induce different chain-of-thought paths through different Tools, and as a result produce different answers. # Prompt variation 1 \n",
      "agent_chain.run(input=\"Analyze Uber revenue growth and risk factors over time\") Response: # Prompt variation 2\n",
      "agent_chain.run(input=\"Analyze Uber revenue growth and risk factors over quarters\") The main difference between these two queries is ‚Äúover time‚Äù versus ‚Äúover quarters.‚Äù As we can see, not only are the selected Tools different between the two variations, but the inputs are different as well ‚Äî in the first it‚Äôs ‚Äúfinancials‚Äù, and in the second it‚Äôs ‚ÄúRevenue growth and risk factors.‚Äù Since the Tool input in the first variant is unrelated to the question, the answer is similarly vague: ‚ÄúUber‚Äôs revenue growth and risk factors can be analyzed by comparing the financials‚Ä¶‚Äù Query 4: Here instead of asking a compare/contrast question let‚Äôs just ask a question about a given statement.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The sidebar is not mentioned in the provided context.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Dumber LLM Agents Need More Constraints and Better Tools](https://www.llamaindex.ai/blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12)\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Query:\n",
      "What is the length of the Lyft SEC 10-K document?\n",
      "\n",
      "Context:\n",
      "If Uber is unable to attract or maintain a critical mass of Drivers and Restaurants, its platform will become less appealing to platform users and its financial results would be adversely impacted. The answer is much better. It goes into details about the US-China trade war, slowing economic growth, Brexit, and more (keep in mind 2019 is pre-COVID). Token Usage and Latency The document contains around ~170K tokens. For some reason, this number is not reflected on the Anthropic usage logs (the ‚ÄúPrompt Tokens‚Äù section seems capped at 10240). But the Prompt Length (in characters) is logged, as well as the model latency. Given the pricing, ~170K tokens would be equivalent to $1.5‚Äì2 USD. A query through one Uber SEC-10K takes around  150   seconds , including all LLM calls. This is actually a bit faster than repeated calls to ChatGPT/davinci. Each ChatGPT/davinci call (with the 4K token window maximized), empirically can take 6‚Äì10 seconds to complete ‚Üí  125‚Äì250 seconds ( or more). Analyzing Multiple Documents A popular example in our  previous blog post  was showcasing that you could compare/contrast different documents with LlamaIndex graph structures. We test whether we can do that here as well, by feeding in multiple SEC reports into Claude-v1 100k. Caveat:  Considering that one UBER SEC-10K filing doesn‚Äôt even fit in the context window, we‚Äôll of course also need to implement response synthesis strategies in order to handle ingesting multiple 10K filings. We build a list index over all 4 10K filings: 2019, 2020, 2021, and 2022. list_index = GPTListIndex.from_documents(all_docs, service_context=service_context)\n",
      " print ( len (list_index.index_struct.nodes)) We then ask our question using our Tree Summarize response mode.\n",
      "\n",
      "If these third parties failed to provide services  or  increased costs, it could adversely impact Ube r's business.\n",
      "\n",
      "‚Ä¢ Macroeconomic conditions. Uber' s business was sensitive to economic conditions  and  changes that impacted discretionary consumer spending. A decline  in  the economy could reduce demand  for  Ube r's products and services.  \n",
      "\n",
      "‚Ä¢ Reliance on Drivers and Restaurants. Uber' s success depended on attracting  and  retaining enough Drivers  and  Restaurants to its platform. If Uber was unable to attract  or  retain enough Drivers  and  Restaurants, it could negatively impact its operations.\n",
      "\n",
      "‚Ä¢ Intellectual  property . If Uber was unable to protect its intellectual  property ,  or   if  third parties claimed Uber was infringing on their intellectual  property , it could harm Ube r's business. Uber relied on a combination of copyright, trademark, patent, and trade secret laws to establish its intellectual property rights.\n",
      "\n",
      "Refined answer:\n",
      "\n",
      "Some of the biggest risk factors for Uber in 2019 included:\n",
      "\n",
      "‚Ä¢ Regulatory challenges and uncertainty. Uber faced significant regulatory challenges in 2019, including AB5 in California which increased the risk of Drivers being classified as employees and regulatory scrutiny and temporary bans in London and other markets. These regulatory issues created uncertainty and posed risks to Uber' s business model  and  financial results.\n",
      "\n",
      "‚Ä¢ Competition. The ridesharing  and  meal delivery markets are highly competitive,  and  competitors offered significant incentives  and  discounts to take market share  from  Uber  in   2019.  This competition could negatively impact Ube r's growth and profitability.\n",
      "\n",
      "‚Ä¢ Safety and security. There were risks related to the safety and security of Uber' s platform, including risks  from  vehicle  or  scooter accidents, assaults,  and  other incidents. Ube r's safety report detailing sexual assault reports brought additional scrutiny and risks. Any failure to ensure safety could significantly damage Uber' s reputation  and  business.\n",
      "\n",
      "‚Ä¢ Financial performance  and  profitability.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The length of the Lyft SEC 10-K document is not mentioned in the provided context. The context only discusses the testing of Anthropic's 100K model on Uber's 10-K filings, not Lyft's.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Testing Anthropic Claude‚Äôs 100k-token window on SEC 10-K Filings](https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba)\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Query:\n",
      "How does the integration between Create-llama and E2B enhance agent capabilities?\n",
      "\n",
      "Context:\n",
      "Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controllability by allowing direct output returns as final responses. This optimizes for reduced latency and costs, and effectively halts the agent after crucial actions like booking confirmations or answering queries.  Docs ,  Tweet . üé•¬†Demos: RAG-enhanced MetaGPT : A robust multi-agent framework that features structured team dynamics for problem-solving, now supercharged with domain-specific knowledge from LlamaIndex modules. This framework supports diverse data inputs, sophisticated retrieval options, and efficient data management for enhanced agent performance. üó∫Ô∏è Guides: Guide  to Building and Evaluating Advanced RAG by Hamza Gharbi for setting up a basic RAG pipeline, defining custom evaluation functions, and optimizing retrieval techniques. Paper  by Prof.  Markus J. Buehler : Using LLM-Generated Knowledge Graphs to Accelerate Biomaterials Discovery - This study showcases how a comprehensive knowledge graph from over 1000 scientific papers reveals novel insights and connections, driving innovation in biomaterials through art as inspiration. The KG construction was done with the help of LlamaIndex modules. Guide  to Full-Stack RAG Application with AWS Bedrock: Set up Bedrock embeddings, use LlamaIndex for PDF retrieval, and build an interactive Streamlit interface, an ideal resource for enterprises starting with AWS services. Guide  to Building a Lightweight ColBERT Retrieval Agent: Learn how to create an agent capable of advanced document retrieval and maintaining conversation memory, without the complexity of heavyweight agent frameworks. Guide  to the Best RAG Techniques: 'ARAGOG'  paper  by Matous Eibich is a comprehensive evaluation survey exploring various RAG methods from classic vector databases to LlamaIndex's advanced techniques. Key findings highlight the effectiveness of HyDE, LLM reranking, and sentence window retrieval for improving precision and answer similarity.\n",
      "\n",
      "LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIndex Family! ü¶ô Welcome to another thrilling weekly update from LlamaGalaxy! We're excited to bring you a variety of outstanding updates, including the Chain of Abstraction LlamaPack, create-tsi, demos, guides, tutorials, and much more. Before we delve into these updates, we have an exciting tutorial series on Agents and Tools for you to check out. Perfect for beginners, this series covers everything from advanced QA/RAG implementations to step-wise execution. By the end, you‚Äôll have gained a deeper understanding of how to use agent reasoning with tool use to build simple applications. Check them out: Overview ReAct agents Function Calling agents Retrieval-Augmented agent Controlling tool outputs Agents with step-by-step execution ü§©¬† The highlights: Chain of Abstraction LlamaPack:  Chain of Abstraction technique as llamapack a method enabling multi-step reasoning for enhanced tool use introduced by Silin Gao's team.  LlamaPack ,  Tweet . Create-tsi Toolkit:  Launched a toolkit for building full-stack RAG applications with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers.  Code ,  Tweet . Improved Agent Control :  return_direct  feature in tools allows direct output returns, reducing costs and enhancing response efficiency.  Docs ,  Tweet . ‚ú® Feature Releases and Enhancements: We have introduced the Chain of Abstraction Technique Developed by Silin Gao, and team as LlamaPack, this new method enables LLMs to generate multi-step reasoning chains for efficient sequence planning, enhancing tool use beyond single-shot functions.  LlamaPack ,  Tweet . We have launched create-tsi: A toolkit in collaboration with T-Systems and Marcus Schiesser to generate GDPR-compliant, full-stack AI applications via a CLI interface. Build enterprise-grade RAG bots with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers.  Code ,  Tweet .\n",
      "\n",
      "Answer:\n",
      "\n",
      "The integration between Create-llama and E2B does not enhance agent capabilities. The context does not mention the integration of Create-llama and E2B. Instead, it highlights the introduction of the return_direct feature in tools, which enhances agent controllability by allowing direct output returns as final responses, reducing latency and costs.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [LlamaIndex Newsletter 2024-04-16](https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-16)\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Query:\n",
      "What is the purpose of using a vision model to filter out irrelevant photos in the pipeline?\n",
      "\n",
      "Context:\n",
      "The  SimpleMultiModalQueryEngine  first retrieves the set of relevant images/text, and feeds the input to a vision model in order to synthesize a response. from  llama_index.query_engine  import  SimpleMultiModalQueryEngine\n",
      "\n",
      "query_engine = index.as_query_engine(\n",
      "    multi_modal_llm=openai_mm_llm,\n",
      "    text_qa_template=qa_tmpl\n",
      ")\n",
      "\n",
      "query_str =  \"Tell me more about the Porsche\" \n",
      "response = query_engine.query(query_str) The generated result + sources are shown below:\n",
      "\n",
      "This includes  model_name ,  temperature ,  max_tokens , and  generate_kwargs . As an example, you can do: llm = Gemini(model=\"models/gemini-ultra\") Multi-modal Model Full Notebook Guide Here In this notebook, we test out the  gemini-pro-vision  variant that features  multi-modal inputs.  It contains the following features: supports both  complete  and  chat  capabilities supports streaming and async Supports feeding in  multiple images  in addition to text in the completion endpoint Future work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision. Let‚Äôs walk through a concrete example. Let‚Äôs say we are given a picture of the  following scene : Scene from a street in New York City We can then initialize our Gemini Vision model, and ask it a question: ‚ÄúIdentify the city where this photo was taken‚Äù: from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n",
      "from llama_index.multi_modal_llms.generic_utils import (\n",
      "    load_image_urls,\n",
      ")\n",
      "\n",
      "image_urls = [\n",
      "    \" &lt; https://storage.googleapis.com/generativeai-downloads/data/scene.jpg &gt; \",\n",
      "    # Add yours here!\n",
      "]\n",
      "image_documents = load_image_urls(image_urls)\n",
      "gemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\n",
      "complete_response = gemini_pro.complete(\n",
      "    prompt=\"Identify the city where this photo was taken.\",\n",
      "    image_documents=image_documents,\n",
      ") Our response is the following: New York City We can insert multiple images too. Here‚Äôs an example with an image of Messi and the Colosseum.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The purpose of using a vision model to filter out irrelevant photos in the pipeline is to synthesize a response.\n",
      "\n",
      "\n",
      "Sources:\n",
      "[Multi-Modal RAG](https://www.llamaindex.ai/blog/multi-modal-rag-621de7525fea)\n",
      "- [LlamaIndex + Gemini](https://www.llamaindex.ai/blog/llamaindex-gemini-8d7c3b9ea97e)\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Query:\n",
      "How can one explore the author's Huggingface profile?\n",
      "\n",
      "Context:\n",
      ")\n",
      "\n",
      "query_engine = index. as_query_engine (\n",
      "    text_qa_template=text_qa_template, \n",
      "    refine_template=refine_template\n",
      ")\n",
      "\n",
      "response = query_engine. query ( \"Draw me a picture of a happy dog\" ) Snag #1 One main drawback of Transformers Agents currently is that they will only pick one tool to solve each prompt. So if we want to augment the image-generator tool, we need to replace it! In our tool implementation, we actually load the original image-generator tool and call it after running LlamaIndex to generate a new text-to-image prompt. Snag #2 The next bump in our journey is how Hugging Face downloads tools from the space. Initially, it only downloading the  tool_config.json  file and the source code for the tool. But we also need to download the prompts we spent time indexing! To get around this, during the  setup()  of the tool, we call  hf_hub_download()  to download the files we need to load the index. Back on Track With the index created and the general processes figured out, the actual tool implementation is fairly straightforward.\n",
      "\n",
      "Bring in our dependencies: import  os\n",
      "\n",
      " from  llama_index.embeddings.huggingface  import  HuggingFaceEmbedding\n",
      " from  llama_index.core.indices.vector_store  import  VectorStoreIndex\n",
      " from  llama_index.core.settings  import  Settings\n",
      " from  llama_index.core.readers  import  SimpleDirectoryReader\n",
      " from  llama_index.llms.ollama  import  Ollama Configure the embedding model and Llama3 model embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-base-en-v1.5\" )\n",
      "llm = Ollama(model= \"llama3\" , request_timeout= 300.0 ) Update settings for the indexing pipeline: Settings.llm = llm\n",
      "Settings.embed_model = embed_model\n",
      "Settings.chunk_size =  512   # This parameter defines the size of text chunks for embedding \n",
      "\n",
      "documents = SimpleDirectoryReader( \"reviews_1_5.json\" ).load_data()  #Modify path for your case Now create our index, our query engine and run a query: index = VectorStoreIndex.from_documents(documents, show_progress= True )\n",
      "\n",
      "query_engine = index.as_query_engine(similarity_top_k= 3 )\n",
      "\n",
      "response = query_engine.query( \"What is the least favourite movie?\" )\n",
      " print (response) Output: Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10. Now we know that the review 1 is the least favorite movie among these reviews. Next Steps This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.  Get started with MyMagic AI‚Äôs API  today!\n",
      "\n",
      "Answer:\n",
      "\n",
      "To explore the author's HuggingFace profile, you can visit the Hugging Face Hub and search for the author's name.\n",
      "\n",
      "\n",
      "Sources:\n",
      "[LlamaIndex and Transformers Agents](https://www.llamaindex.ai/blog/llamaindex-and-transformers-agents-67042ee1d8d6)\n",
      "- [Batch inference with MyMagic AI and LlamaIndex](https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex)\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Query:\n",
      "What is the purpose of setting up Tonic Validate?\n",
      "\n",
      "Context:\n",
      "Setting up Tonic Validate To set up Tonic Validate, first install it via poetry: poetry add tonic-validate Now, we can create our tests for Tonic Validate. To get started, create a file inside  llama-validate-demo/tests  called  validate_test.py . We will also need to create a list of test questions and answers which you can find  here . Alternatively, you can also use the Tonic Validate UI to create the test set and call it via the SDK ‚Äî we‚Äôll be adding a feature to help generate these benchmarks using synthetic data to make this process even easier. Download the  qa_pairs.json  file from the link and paste it into  llama-validate-demo/tests . Once we have both of these files, we can add the following code into  validate_test.py . import  json\n",
      " import  os\n",
      " from  tonic_validate  import  ValidateApi\n",
      " from  tonic_validate.metrics  import  AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric\n",
      " from  llama_index.evaluation  import  TonicValidateEvaluator\n",
      " import  requests\n",
      "\n",
      " from  dotenv  import  load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      " def   get_llm_response ( prompt ):\n",
      "    url =  \"http://localhost:8000/api/chat\" \n",
      "\n",
      "    payload = json.dumps({\n",
      "         \"messages\" : [\n",
      "            {\n",
      "                 \"role\" :  \"user\" ,\n",
      "                 \"content\" : prompt\n",
      "            }\n",
      "        ]\n",
      "    })\n",
      "    headers = {  'Content-Type' :  'application/json'  }\n",
      "    response = requests.request( \"POST\" , url, headers=headers, data=payload).json()\n",
      "    result = response[ 'result' ]\n",
      "     return  result[ 'content' ], result[ 'context' ] This code sets up the dependency imports and also specifies a  get_llm_response  function which sends a request to the LlamaIndex API server we set up earlier to get a response.\n",
      "\n",
      "One approach, the one that we recommend, for adding QA testing for your RAG system is to use GitHub Actions to establish an integration test using Tonic Validate that checks the LLM response quality of your RAG system, allowing you to catch and rectify any performance degradation before it is pushed into production. To set up Tonic Validate to run in GitHub Actions, we can create a folder  llama-validate-demo/.github/workflows  with a file called  python-app.yml .\n",
      "\n",
      "Answer:\n",
      "\n",
      "To create tests for Tonic Validate and to implement integration tests for LlamaIndex in production.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex](https://www.llamaindex.ai/blog/tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9)\n",
      "\n",
      "\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_irrelevance_df = (\n",
    "    synthetic_deep_eval_df\n",
    "    .loc[lambda df: df['relevancy_score'].lt(1)]\n",
    "    .sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])\n",
    ")\n",
    "\n",
    "for i, row in synthetic_response_eval_irrelevance_df.iterrows():\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.contexts)\n",
    "    print(f\"Context:\\n{contexts}\\n\")\n",
    "    print(f\"Answer:\\n{row.answer}\\n----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2876bc3c-251b-4686-9321-6a7a0c081019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "curated_response_eval_prediction_dataset = await curated_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4ca4973a3341bea8d452cb6949240b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    curated_response_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score       4.00\n",
       "mean_relevancy_score         0.75\n",
       "mean_faithfulness_score      0.75"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>\\nDistributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via Standardized API Interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of Deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\n\\nSources:\\n- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # create our agents \\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\\nagent1 = worker1.as_agent()\\nagent2 = worker2.as_agent() We turn those agents into services: agent_server_1 = AgentService(\\n    agent=agent1,\\n    message_queue=message_queue,\\n    description= \"Useful for getting the secret fact.\" ,\\n    service_name= \"secret_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8003 \\n)\\nagent_server_2 = AgentService(\\n    agent=agent2,\\n    message_queue=message_queue,\\n    description= \"Useful for getting random dumb facts.\" ,\\n    service_name= \"dumb_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8004 \\n) And finally we launch each service as an independent agent. Here we‚Äôre doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently: from  llama_agents  import  ServerLauncher, CallableMessageConsumer\\n\\n # Additional human consumer \\n def   handle_result ( message ) -&gt;  None :\\n     print ( f\"Got result:\" , message.data)\\n\\n\\n # the final result is published to a \"human\" consumer \\n # so we define one to handle it! \\nhuman_consumer = CallableMessageConsumer(\\n    handler=handle_result, message_type= \"human\" \\n)\\n\\n # Define Launcher \\nlauncher = ServerLauncher(\\n    [agent_server_1, agent_server_2],\\n    control_plane,\\n    message_queue,\\n    additional_consumers=[human_consumer]\\n)\\n\\nlauncher.launch_servers() Real-time monitoring One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this: llama-agents monitor --control-plane-url http://127.0.0.1:8000 Once launched, you get an intuitive, point-and-click terminal application.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>\\nThe two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\\n\\n\\nSources:\\n[OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)\\n- [Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, Faithfulness Evaluator  ‚Äî It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. Relevancy Evaluator  ‚Äî It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. # We will use GPT-4 for evaluating the responses\\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\\n\\n# Define service context for GPT-4 for evaluation\\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\\n\\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4) Response Evaluation For A Chunk Size We evaluate each chunk_size based on 3 metrics. Average Response Time. Average Faithfulness. Average Relevancy. Here‚Äôs a function,  evaluate_response_time_and_accuracy , that does just that which has: VectorIndex Creation. Building the Query Engine. Metrics Calculation. # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size \\n # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it. \\n def   evaluate_response_time_and_accuracy ( chunk_size, eval_questions ):\\n     \"\"\"\\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\\n    \\n    Parameters:\\n    chunk_size (int): The size of data chunks being processed.\\n    \\n    Returns:\\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\\n    \"\"\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>\\nThe two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\\n\\n\\nSources:\\n- [Boosting RAG: Picking the Best Embedding &amp; Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results., Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.</td>\n",
       "      <td>\\nThe MemoryCache project by Mozilla is not mentioned in the provided context. The context only discusses the integration of LlamaIndex and MongoDB to build a ChatGPT with private data, focusing on large language models, knowledge retrieval, and generation. There is no information about the MemoryCache project, PrivateGPT_AI, or their relationship with LlamaIndex.\\n\\n\\nSources:\\n- [Build a ChatGPT with your Private Data using LlamaIndex and MongoDB](https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c)\\n\\n</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Example question: Who is the most recent UK prime minister? There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation capabilities of LLMs: Model finetuning and in-context learning. Model Finetuning can be more complex and expensive to operationalize. There are also some open questions like how to delete information from a fine-tuned model to ensure you comply with local laws (ex. GDPR in Europe), and for changing data you need to fine-tune again constantly. In-context learning requires inserting the new data as part of the input prompts to the LLM. To perform this data augmentation in a secure, high performance and cost-effective manner is where tools like LlamaIndex and MongoDB Developer Data Platform can help. Introduction to LlamaIndex LlamaIndex provides a simple, flexible interface to connect LLMs with external data. Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc). Provides indices over the unstructured and structured data for use with LLMs. Structures external information so that it can be used with the prompt window limitations of any LLM. Exposes a query interface which takes in an input prompt and returns a knowledge-augmented output. MongoDB as the Datastore It is effortless to store the ingested documents (i.e. Node objects), index metadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There is an option to store the ‚Äúdocuments‚Äù as an actual collection in MongoDB using  MongoDocumentStore . There is an option to persist the ‚ÄúIndexes‚Äù using the  MongoIndexStore  . Storing LlamaIndex‚Äôs documents and indexes in a database becomes necessary in a couple of scenarios: Use cases with large datasets may require more than in-memory storage. Ingesting and processing data from various sources (for example, PDFs, Google Docs, Slack). The requirement to continuously maintain updates from the underlying data sources., Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\\nCo-authors: Prakul Agarwal ‚Äî Senior Product Manager, Machine Learning at MongoDB Jerry Liu ‚Äî co-founder at LlamaIndex Update (6/22/2023):  The preferred way to use LlamaIndex + MongoDB is now with our MongoDBAtlasVectorSearch class. Take a look at our guide here:  https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html Summary Large Language Models (LLMs) like ChatGPT have revolutionized the way users can get answers to their questions. However, the ‚Äúknowledge‚Äù of LLMs is restricted by what they were trained on, which for ChatGPT means publicly available information on the internet till September 2021. How can LLMs answer questions using private knowledge sources like your company‚Äôs data and unlock its true transformative power? This blog will discuss how LlamaIndex and MongoDB can enable you to achieve this outcome quickly. The  attached notebook  provides a code walkthrough on how to query any PDF document using English queries. Background Traditionally, AI has been used to analyze data, identify patterns and make predictions based on existing data. The recent advancements have led to AI becoming better at generating new things (rather than just analyzing existing things). This is referred to as Generative AI. Generative AI is powered mainly by machine learning models called Large Language Models (LLM). LLMs are pre-trained on large quantities of publicly available text. There are various proprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of emerging open-source LLMs like Llama, Dolly, etc. There are 2 main scenarios where the knowledge of LLMs falls short: Private data such as your company‚Äôs internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and Slack Newer data than when the LLMs were last trained.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                query  \\\n",
       "0                                                                                                                                                                              What are key features of llama-agents?   \n",
       "1                                                                 What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                                                                                        What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "3  How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         answer  \\\n",
       "0  \\nDistributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via Standardized API Interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of Deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\n\\nSources:\\n- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\\n\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\nThe two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\\n\\n\\nSources:\\n[OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)\\n- [Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\\n\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\nThe two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\\n\\n\\nSources:\\n- [Boosting RAG: Picking the Best Embedding & Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\\n\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                    \\nThe MemoryCache project by Mozilla is not mentioned in the provided context. The context only discusses the integration of LlamaIndex and MongoDB to build a ChatGPT with private data, focusing on large language models, knowledge retrieval, and generation. There is no information about the MemoryCache project, PrivateGPT_AI, or their relationship with LlamaIndex.\\n\\n\\nSources:\\n- [Build a ChatGPT with your Private Data using LlamaIndex and MongoDB](https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c)\\n\\n   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \\\n",
       "0              1.0                4.0                 1.0   \n",
       "1              1.0                5.0                 1.0   \n",
       "2              1.0                5.0                 1.0   \n",
       "3              0.0                2.0                 0.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 contexts  \n",
       "0                                                                                                                                                                                                                                       [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # create our agents \\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\\nagent1 = worker1.as_agent()\\nagent2 = worker2.as_agent() We turn those agents into services: agent_server_1 = AgentService(\\n    agent=agent1,\\n    message_queue=message_queue,\\n    description= \"Useful for getting the secret fact.\" ,\\n    service_name= \"secret_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8003 \\n)\\nagent_server_2 = AgentService(\\n    agent=agent2,\\n    message_queue=message_queue,\\n    description= \"Useful for getting random dumb facts.\" ,\\n    service_name= \"dumb_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8004 \\n) And finally we launch each service as an independent agent. Here we‚Äôre doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently: from  llama_agents  import  ServerLauncher, CallableMessageConsumer\\n\\n # Additional human consumer \\n def   handle_result ( message ) ->  None :\\n     print ( f\"Got result:\" , message.data)\\n\\n\\n # the final result is published to a \"human\" consumer \\n # so we define one to handle it! \\nhuman_consumer = CallableMessageConsumer(\\n    handler=handle_result, message_type= \"human\" \\n)\\n\\n # Define Launcher \\nlauncher = ServerLauncher(\\n    [agent_server_1, agent_server_2],\\n    control_plane,\\n    message_queue,\\n    additional_consumers=[human_consumer]\\n)\\n\\nlauncher.launch_servers() Real-time monitoring One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this: llama-agents monitor --control-plane-url http://127.0.0.1:8000 Once launched, you get an intuitive, point-and-click terminal application.]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, Faithfulness Evaluator  ‚Äî It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. Relevancy Evaluator  ‚Äî It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. # We will use GPT-4 for evaluating the responses\\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\\n\\n# Define service context for GPT-4 for evaluation\\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\\n\\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4) Response Evaluation For A Chunk Size We evaluate each chunk_size based on 3 metrics. Average Response Time. Average Faithfulness. Average Relevancy. Here‚Äôs a function,  evaluate_response_time_and_accuracy , that does just that which has: VectorIndex Creation. Building the Query Engine. Metrics Calculation. # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size \\n # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it. \\n def   evaluate_response_time_and_accuracy ( chunk_size, eval_questions ):\\n     \"\"\"\\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\\n    \\n    Parameters:\\n    chunk_size (int): The size of data chunks being processed.\\n    \\n    Returns:\\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\\n    \"\"\"]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results., Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.]  \n",
       "3  [Example question: Who is the most recent UK prime minister? There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation capabilities of LLMs: Model finetuning and in-context learning. Model Finetuning can be more complex and expensive to operationalize. There are also some open questions like how to delete information from a fine-tuned model to ensure you comply with local laws (ex. GDPR in Europe), and for changing data you need to fine-tune again constantly. In-context learning requires inserting the new data as part of the input prompts to the LLM. To perform this data augmentation in a secure, high performance and cost-effective manner is where tools like LlamaIndex and MongoDB Developer Data Platform can help. Introduction to LlamaIndex LlamaIndex provides a simple, flexible interface to connect LLMs with external data. Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc). Provides indices over the unstructured and structured data for use with LLMs. Structures external information so that it can be used with the prompt window limitations of any LLM. Exposes a query interface which takes in an input prompt and returns a knowledge-augmented output. MongoDB as the Datastore It is effortless to store the ingested documents (i.e. Node objects), index metadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There is an option to store the ‚Äúdocuments‚Äù as an actual collection in MongoDB using  MongoDocumentStore . There is an option to persist the ‚ÄúIndexes‚Äù using the  MongoIndexStore  . Storing LlamaIndex‚Äôs documents and indexes in a database becomes necessary in a couple of scenarios: Use cases with large datasets may require more than in-memory storage. Ingesting and processing data from various sources (for example, PDFs, Google Docs, Slack). The requirement to continuously maintain updates from the underlying data sources., Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\\nCo-authors: Prakul Agarwal ‚Äî Senior Product Manager, Machine Learning at MongoDB Jerry Liu ‚Äî co-founder at LlamaIndex Update (6/22/2023):  The preferred way to use LlamaIndex + MongoDB is now with our MongoDBAtlasVectorSearch class. Take a look at our guide here:  https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html Summary Large Language Models (LLMs) like ChatGPT have revolutionized the way users can get answers to their questions. However, the ‚Äúknowledge‚Äù of LLMs is restricted by what they were trained on, which for ChatGPT means publicly available information on the internet till September 2021. How can LLMs answer questions using private knowledge sources like your company‚Äôs data and unlock its true transformative power? This blog will discuss how LlamaIndex and MongoDB can enable you to achieve this outcome quickly. The  attached notebook  provides a code walkthrough on how to query any PDF document using English queries. Background Traditionally, AI has been used to analyze data, identify patterns and make predictions based on existing data. The recent advancements have led to AI becoming better at generating new things (rather than just analyzing existing things). This is referred to as Generative AI. Generative AI is powered mainly by machine learning models called Large Language Models (LLM). LLMs are pre-trained on large quantities of publicly available text. There are various proprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of emerging open-source LLMs like Llama, Dolly, etc. There are 2 main scenarios where the knowledge of LLMs falls short: Private data such as your company‚Äôs internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and Slack Newer data than when the LLMs were last trained.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "113a264d-1cea-4e41-82ac-22135f12f729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results.\n",
      "----------\n",
      "Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for context in curated_deep_eval_df.iloc[2]['contexts']:\n",
    "    print(context)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
