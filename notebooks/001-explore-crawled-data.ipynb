{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f554fb-79c8-4201-a0c9-e7464118c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c852ac68-9eb6-46f7-b33c-a1a12f8af228",
   "metadata": {},
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "DEBUG = True\n",
    "OBSERVABILITY = True\n",
    "LOG_TO_MLFLOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ae48fa-a9cb-4f32-a91e-6a63c9b53506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721823468.132393  582382 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "if OBSERVABILITY:\n",
    "    import phoenix as px\n",
    "    px.launch_app()\n",
    "    import llama_index.core\n",
    "    llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"exp_003_reranker_flag_embedding_bge_large\"\n",
    "if LOG_TO_MLFLOW:\n",
    "    RUN_DESCRIPTION = \"\"\"\n",
    "# Making the RAG outputs referenced sources\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_002\n",
    "- Update source data to have URLs as well\n",
    "- First attempt to add reranker\n",
    "\"\"\"\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc26a2-7df5-4963-855d-92ec99c958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CACHE_DP = f'data/001/{RUN_NAME}'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"NOTEBOOK_CACHE_DP\", NOTEBOOK_CACHE_DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs-v2.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'content': \"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\",\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['AI', 'Evaluation', 'Rag'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       "  'content': \"Greetings, LlamaIndex community! ü¶ô Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more. ü§©\\xa0 The highlights: DeepLearningAI Course:  JavaScript RAG Web Apps with\\xa0LlamaIndex collaborative course with DeepLearningAI.  Course ,  Tweet . RAFTDatasetPack LlamaPack : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents.  LlamaPack ,  Tweet . Memory Efficiency with Cohere Embeddings:  Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations.  Notebook ,  Tweet . Python Docs Makeover:  Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details.  API Ref ,  Tweet ,  Docs ‚ú® Feature Releases and Enhancements: We introduced RAFT - Retrieval Augmented Fine Tuning, a method from  Tianjun Zhang \\xa0and  Shishir Patil \\xa0to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation.  LlamaPack ,  Tweet . We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.  Course ,  Tweet . We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG.  Notebook ,  Tweet We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation.  API Ref ,  Tweet ,  Docs üé•\\xa0Demos: RestAI , a project by  Pedro Dias  is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI. Ragdoll  and  Ragdoll Studio  by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI. üó∫Ô∏è Guides: Guide  to Designing RAG Systems by  Micha≈Ç Oleszak  for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation. ‚úçÔ∏è Tutorials: Sujit Patil   tutorial  on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment. Florian June's  tutorial  on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch. Laurie's  video tutorial  on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries. Akriti‚Äôs   tutorial  on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant. Marco Bertelli's  tutorial  on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience. Sudarshan Koirala‚Äôs   tutorial  on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking. üé•\\xa0 Webinars: Register for a webinar  with  Tianjun Zhang \\xa0and  Shishir Patil \\xa0on how to do retrieval-augmented fine-tuning (RAFT). Webinar  with  Daniel  on  CodeGPT  - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components. Vectara‚Äôs   Panel Discussion  on 'Why RAG will Never Die?‚Äô.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Apr 2, 2024',\n",
       "  'tags': ['LLM'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-03-19',\n",
       "  'content': \"Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting weekly update from the world of LlamaVerse! We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our  launch post  for all the details! ü§©\\xa0 The highlights: New observability with Instrumentation:  Enhanced developer workflow with a new Instrumentation module for improved observability.  Docs ,  Tweet . LlamaParse accepts natural language parsing instructions : Easily extract math snippets from PDFs into LaTeX with LlamaParse.  Blogpost ,  Tweet . Financial Data Parsing:  Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis.  Notebook ,  Tweet . ‚ú® Feature Releases and Enhancements: We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application.  Docs ,  Tweet . We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline.  Blogpost ,  Tweet . We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data.  Notebook ,  Tweet . We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code.  Blogpost ,  Docs ,  Tweet . Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts.  LlamaPack ,  Tweet üé•\\xa0Demos: Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency.  Blogpost ,  Code ,  Tweet . üó∫Ô∏è Guides: Guide  to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique. ‚úçÔ∏è Tutorials: Thomas Reid ‚Äôs  tutorial  on using LlamaParse can help properly extract text from a Tesla quarterly filings. Sudarshan Koirala   video tutorial  on RAG with LlamaParse, Qdrant, and Groq. Kyosuke Morita  tutorial  showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex. Cobus Greyling   tutorial  on Agentic RAG: Context-Augmented OpenAI Agents. Roey Ben Chaim ‚Äôs  tutorial  on PII Detector: hacking privacy in RAG. üé•\\xa0 Webinars: Webinar  with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT üìÖ\\xa0Events: We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['LlamaParse', 'AI', 'LLM', 'Newsletter'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19'},\n",
       " {'title': 'One-click Open Source RAG Observability with Langfuse',\n",
       "  'content': 'This is a guest post from the team at Langfuse There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once. Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your  production  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point? Thus, we started working on  Langfuse.com  ( GitHub ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either  self-host  Langfuse or use the  cloud instance  maintained by us. We are thrilled to announce our new integration with LlamaIndex today. This feature was  highly requested  by our community and aligns with our project\\'s focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase! The challenge We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine. from  llama_index.core  import  SimpleDirectoryReader\\n from  llama_index.core  import  VectorStoreIndex\\n\\ndocuments = SimpleDirectoryReader( \"./data\" ).load_data()\\n\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nchat_engine = index.as_chat_engine()\\n\\n print (chat_engine.chat( \"What problems can I solve with RAG?\" ))\\n print (chat_engine.chat( \"How do I optimize my RAG application?\" )) In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine. This is awesome to get started, but we quickly run into questions like: ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù ‚ÄúHow is chat memory managed?‚Äù ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù One-click OSS observability to the rescue We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager. Preparation Install the community package (pip install llama-index-callbacks-langfuse) Copy/paste the environment variables from the Langfuse project settings to your Python project: \\'LANGFUSE_SECRET_KEY\\', \\'LANGFUSE_PUBLIC_KEY\\' and \\'LANGFUSE_HOST\\' Now, you only need to set the global langfuse handler: from  llama_index.core  import  set_global_handler\\n\\nset_global_handler( \"langfuse\" ) And voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns. Group multiple chat threads into a session Working with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is  session tracking  for conversational applications to see the traces in context of a full message thread. To activate it, just add an id that identifies the session as a trace param before calling the chat_engine. from  llama_index.core  import  global_handler\\n\\nglobal_handler.set_trace_params(\\n  session_id= \"your-session-id\" \\n)\\n\\nchat_engine.chat( \"What did he do growing up?\" )\\nchat_engine.chat( \"What did he do at USC?\" )\\nchat_engine.chat( \"How old is he?\" ) Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing: Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces. Trace more complex applications and use other Langfuse features for prompt management and evaluation This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable. We have also built additional features to version control and collaborate on prompts (langfuse  prompt management ), track  experiments , and  evaluate  production traces. For RAG specifically, we collaborated with the RAGAS team and it‚Äôs easy to run their popular eval suite on traces captured with Langfuse (see  cookbook ). Get started The easiest way to get started is to follow the  cookbook  and check out the  docs . Feedback? Ping us We‚Äôd love to hear any feedback. Come join us on our  community discord  or add your thoughts to this  GitHub thread .',\n",
       "  'author': 'Langfuse',\n",
       "  'date': 'Mar 18, 2024',\n",
       "  'tags': ['LLM', 'Observability'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse'},\n",
       " {'title': 'Retrieving Privacy-Safe Documents Over A Network',\n",
       "  'content': 'In a  recent blog post , we introduced our  llama-index-networks  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores‚Äîthat one may not otherwise have access to‚Äîmeans more accurate responses to an even wider breadth of queries. A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we‚Äôll be relying on some recent developments in the area of Privacy-Enhancing Techniques. The story of Alex, Bob and Beth continues To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply. We showed then how such data a collaboration could be permitted through  llama-index-networks  by taking the following steps: Bob and Beth both build their respective QueryEngine‚Äôs (RAG in llama-index lingo) Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets. Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Sidebar: differential privacy primer In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual‚Äôs inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm. By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion). A light introduction to differential privacy. A light introduction to differential privacy. Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth‚Äôs data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled ‚ÄúPrivacy-preserving in-context learning with differentially private few-shot generation‚Äù by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network! There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration. Bob and Beth create privacy-safe synthetic copies of their private datasets Bob and Beth both build their respective QueryEngine‚Äôs over their synthetic datasets Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs Creating differentially private synthetic copies of a private dataset Fortunately, for step 0., we can make use of the  DiffPrivateSimpleDataset  pack. from  llama_index.core.llama_datasets.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n\\n # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,   # OpenAI only allows for top 5 next token \\n)                     # as opposed to entire vocabulary \\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nbeth_private_dataset: LabelledSimpleDataset = ...  # a dataset that contains \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # examples with two attributes \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # `text` and `reference_label` \\n\\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,     # params for preparing required prompts \\n    simple_dataset=simple_dataset,   # to generate the synthetic examples  \\n)\\n\\nbeth_synthetic_dataset =  await  beth_synthetic_generator.arun(\\n\\t\\tsize= 3 ,   # number of synthetic observations to create \\n\\t\\tsigma= 0.5    # param that determines the level of privacy \\n) With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It‚Äôs worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.) Example: Symptom2Disease In this section of the blog post, we go over an actual example application of the privacy-safe networks over the  Symptom2Disease  dataset. This dataset consists of 1,200 examples each containing a ‚Äúsymptoms‚Äù description as well as the associated ‚Äúdisease‚Äù label ‚Äî the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network. Generate privacy-safe synthetic observations of Symptom2Disease We use the training subset and apply the  DiffPrivateSimpleDatasetPack  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a  LabelledSimpleDataset  object. import  pandas  as  pd\\n from  sklearn.model_selection  import  train_test_split\\n from  llama_index.core.llama_dataset.simple  import  (\\n    LabelledSimpleDataExample,\\n    LabelledSimpleDataset,\\n)\\n from  llama_index.core.llama_dataset.base  import  CreatedBy, CreatedByType\\n\\n # load the Symptom2Disease.csv file \\ndf = pd.read_csv( \"Symptom2Disease.csv\" )\\ntrain, test = train_test_split(df, test_size= 0.2 )\\n\\n # create a LabelledSimpleDataset (which is what the pack works with) \\nexamples = []\\n for  index, row  in  df.iterrows():\\n    example = LabelledSimpleDataExample(\\n        reference_label=row[ \"label\" ],\\n        text=row[ \"text\" ],\\n        text_by=CreatedBy( type =CreatedByType.HUMAN),\\n    )\\n    examples.append(example)\\n\\nsimple_dataset = LabelledSimpleDataset(examples=examples) Now we can use the llama-pack to create our synthetic observations. import  llama_index.core.instrumentation  as  instrument\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n from  .event_handler  import  DiffPrivacyEventHandler\\n import  asyncio\\n import  os\\n\\nNUM_SPLITS =  3 \\nT_MAX =  150 \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,\\n)\\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nprompt_bundle = PromptBundle(\\n    instruction=(\\n         \"You are a patient experiencing symptoms of a specific disease. \" \\n         \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\\\n\" \\n         \"Start your answer directly after \\'Symptoms: \\'. Begin your answer with [RESULT].\\\\n\" \\n    ),\\n    label_heading= \"Disease\" ,\\n    text_heading= \"Symptoms\" ,\\n)\\n\\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,\\n    simple_dataset=simple_dataset,\\n)\\n\\nsynthetic_dataset =  await  dp_simple_dataset_pack.arun(\\n    sizes= 3 ,\\n    t_max=T_MAX,\\n    sigma= 1.5 ,\\n    num_splits=NUM_SPLITS,\\n    num_samples_per_split= 8 ,   # number of private observations to create a \\n)                              # synthetic obsevation \\nsynthetic_dataset.save_json( \"synthetic_dataset.json\" ) Create a network with two contributors Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well. Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we‚Äôre using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user). Step 1:  Contributor‚Äôs build their Retriever over their synthetic datasets. import  os\\n from  llama_index.core  import  VectorStoreIndex\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.core.schema  import  TextNode\\n\\n\\n # load the synthetic dataset \\nsynthetic_dataset = LabelledSimpleDataset.from_json(\\n     \"./data/contributor1_synthetic_dataset.json\" \\n)\\n\\n\\nnodes = [\\n    TextNode(text=el.text, metadata={ \"reference_label\" : el.reference_label})\\n     for  el  in  synthetic_dataset[:]\\n]\\n\\nindex = VectorStoreIndex(nodes=nodes)\\nsimilarity_top_k =  int (os.environ.get( \"SIMILARITY_TOP_K\" ))\\nretriever = index.as_retriever(similarity_top_k=similarity_top_k) Step 2:  Contributor‚Äôs expose their Retrievers behind a ContributorRetrieverService from  llama_index.networks.contributor.retriever.service  import  (\\n    ContributorRetrieverService,\\n    ContributorRetrieverServiceSettings,\\n)\\n\\nsettings = ContributorRetrieverServiceSettings()  # loads from .env file \\nservice = ContributorRetrieverService(config=settings, retriever=retriever)\\napp = service.app Step 3:  Define the NetworkRetriever that connects to the ContributorRetrieverServices from  llama_index.networks.network.retriever  import  NetworkRetriever\\n from  llama_index.networks.contributor.retriever  import  ContributorRetrieverClient\\n from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\\n\\n # ContributorRetrieverClient\\'s connect to the ContributorRetrieverService \\ncontributors = [\\n    ContributorRetrieverClient.from_config_file(\\n        env_file= f\"./client-env-files/.env.contributor_ {ix} .client\" \\n    )\\n     for  ix  in   range ( 1 ,  3 )\\n]\\nreranker = CohereRerank(top_n= 5 )\\nnetwork_retriever = NetworkRetriever(\\n    contributors=contributors, node_postprocessors=[reranker]\\n) With the  NetworkRetriever  established, we can retrieve synthetic observations from the two contributors data against a query. related_records = network_retriever.aretrieve( \"Vomitting and nausea\" )\\n print (related_records)  # contain symptoms/disease records that are similar to \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  # to the queried symptoms. Evaluating the  NetworkRetriever To evaluate the efficacy of the  NetworkRetriever  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. hit rate:  a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. mean reciprocal rank:  similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. In addition to evaluating the  NetworkRetriever  we consider the two baselines that represent Retrieving only over the individual Contributor‚Äôs synthetic datasets. Retriever evaluations, with sigma equal to 1.5. In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever‚Äôs in the test set. This shouldn‚Äôt be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor‚Äôs synthetic observations‚Äîthis is the point after all of a network! Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated). Finally, this level of privacy can be controlled via the noise parameter  sigma . In the example above we used a  sigma  of 1.5, which for this dataset amounts to an  epsilon  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are  generally considered to be quite private .) Below, we share the evaluations that result from using a  sigma  of 0.5, which amounts to an  epsilon  of 15.9‚Äîhigher values of  epsilon  or privacy-loss means less privacy. # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon \\nepsilon = dp_simple_dataset_pack.sigma_to_eps(\\n\\t\\tsigma= 0.5 ,\\n\\t\\tmechanism= \"gaussian\" ,\\n\\t\\tsize= 3 * 24 ,\\n\\t\\tmax_token_cnt= 150    # number of max tokens to generate per synthetic example \\n) Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5. So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening. # synthetic example epsilon = 1.3 \\n{\\n     \"reference_label\" :  \"Psoriasis\" ,\\n     \"text\" :  \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\" ,\\n     \"text_by\" : {\\n         \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n         \"type\" :  \"ai\" \\n    }\\n},\\n\\n # synthetic example epsilon = 15.9 \\n{\\n   \"reference_label\" :  \"Migraine\" ,\\n   \"text\" :  \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\" ,\\n   \"text_by\" : {\\n     \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n     \"type\" :  \"ai\" \\n  }\\n}, We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM. In summary We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. Learn more! To delve deeper into the materials of this blog post, we share a few links below: Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( link ) Demo notebooks for the  DiffPrivateSimpleDataset  ( link ) The source code for creating the synthetic Symptom2Disease observations using the  DiffPrivateSimpleDataset  ( link )',\n",
       "  'author': 'Andrei',\n",
       "  'date': 'Mar 20, 2024',\n",
       "  'tags': ['Privacy', 'llama-index-networks'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:17:51.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=160\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_input_data\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags']),\n",
    "        'url': record['url']\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='dd4374f1-3b6e-4dce-b664-ac47324a99b8', embedding=None, metadata={'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\\nThis is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       " 'author': 'LlamaIndex',\n",
       " 'date': 'Apr 2, 2024',\n",
       " 'tags': 'LLM',\n",
       " 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ee5bd7-3f29-4b0e-b2e3-19d69ed7adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_documents\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_OPTION = 'openai'\n",
    "# LLM_OPTION = 'ollama'\n",
    "LLM_OPTION = 'togetherai'\n",
    "\n",
    "# LLM_MODEL_NAME = 'llama3'\n",
    "# LLM_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "LLM_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct-Lite'\n",
    "\n",
    "# EMBED_OPTION = 'openai'\n",
    "# EMBED_OPTION = 'togetherai'\n",
    "# EMBED_OPTION = 'ollama'\n",
    "EMBED_OPTION = 'huggingface'\n",
    "\n",
    "# EMBED_MODEL_NAME = 'llama3'\n",
    "# EMBED_MODEL_NAME = 'togethercomputer/m2-bert-80M-2k-retrieval'\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)\n",
    "    mlflow.log_param(\"LLM_MODEL_NAME\", LLM_MODEL_NAME)\n",
    "    mlflow.log_param(\"EMBED_OPTION\", EMBED_OPTION)\n",
    "    mlflow.log_param(\"EMBED_MODEL_NAME\", EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831156c6-08f7-4ed6-9063-6c7d7f00b312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:18:08.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mLLM:\n",
      "TogetherLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x731e21766210>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x731e32b4cd60>, completion_to_prompt=<function default_completion_to_prompt at 0x731e32bb3100>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Meta-Llama-3-8B-Instruct-Lite', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='3cf613093b6eb9b479c341126dc8d3761c67f9340d0a4a8e1fdc62ed41b58126', api_base='https://api.together.xyz/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=False, tokenizer=None)\u001b[0m\n",
      "\u001b[32m2024-07-24 19:18:08.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEmbed model:\n",
      "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x731df004ca50>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# LLM options\n",
    "if LLM_OPTION == 'ollama':\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    llm = Ollama(base_url=base_url, model=LLM_MODEL_NAME, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "elif LLM_OPTION == 'openai':\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    llm = OpenAI(model=LLM_MODEL_NAME)\n",
    "elif LLM_OPTION == 'togetherai':\n",
    "    from llama_index.llms.together import TogetherLLM\n",
    "    llm = TogetherLLM(model=LLM_MODEL_NAME)\n",
    "\n",
    "# Embed options\n",
    "if EMBED_OPTION == 'huggingface':\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME\n",
    "    )\n",
    "elif EMBED_OPTION == 'openai':\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embed_model = OpenAIEmbedding()\n",
    "elif EMBED_OPTION == 'togetherai':\n",
    "    from llama_index.embeddings.together import TogetherEmbedding\n",
    "    embed_model = TogetherEmbedding(EMBED_MODEL_NAME)\n",
    "elif EMBED_OPTION == 'ollama':\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    embed_model = OllamaEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "\n",
    "logger.info(f\"LLM:\\n{repr(llm)}\")\n",
    "logger.info(f\"Embed model:\\n{repr(embed_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db5ed88-2448-4308-97e0-73bc9451c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:18:08.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1membed_model_dim=1024\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embed_model_dim = len(embed_model.get_text_embedding('sample text to find embedding dimensions'))\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "logger.info(f\"{embed_model_dim=}\")\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"embedding_model_dim\", embed_model_dim)\n",
    "    mlflow.log_param(\"LLM_MODEL\", repr(llm))\n",
    "    mlflow.log_param(\"EMBEDDING_MODEL\", repr(embed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a05d2-4b9e-4295-9dc5-e1b3bdf22be0",
   "metadata": {},
   "source": [
    "## Qdrant as VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be2dff6-23da-448b-866b-d791bb54812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08b83e18-6391-408c-93d9-dcf225813302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:18:10.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1msubstitute_punctuation(collection_raw_name)='huggingface__BAAI_bge_large_en_v1_5__exp_003_reranker_flag_embedding_bge_large'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    # Create a translation table that maps each punctuation character to an underscore\n",
    "    translator = str.maketrans(string.punctuation, '_' * len(string.punctuation))\n",
    "    # Translate the text using the translation table\n",
    "    return text.translate(translator)\n",
    "\n",
    "collection_raw_name = f\"{EMBED_OPTION}__{EMBED_MODEL_NAME}__{RUN_NAME}\"\n",
    "logger.info(f\"{substitute_punctuation(collection_raw_name)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da75b19f-ef70-4ca7-9564-0bf663466dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "COLLECTION = substitute_punctuation(collection_raw_name)\n",
    "\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "# NODES_PERSIST_FP = 'data/001/exp_001_qdrant_togetherai_llama3/nodes.pkl'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(f\"COLLECTION\", COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec5d7b9-7ad3-427f-a423-0fee1f7f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:18:14.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mCreating new Qdrant collection...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [llama_index.vector_stores.qdrant.base] Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "collection_exists = qdrantdb.collection_exists(COLLECTION)\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing Qdrant collection...\")\n",
    "        qdrantdb.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "    logger.info(f\"Creating new Qdrant collection...\")\n",
    "    qdrantdb.create_collection(\n",
    "        COLLECTION,\n",
    "        vectors_config=VectorParams(size=embed_model_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Use existing Qdrant collection\")\n",
    "db_collection = qdrantdb.get_collection(COLLECTION)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=COLLECTION,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6e0c6f0-92a6-430b-a509-682e5884b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e60fed4-7ea5-463c-bd19-d5b6da2cb04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:18:24.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mCreating new DB index...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Supercharge your LlamaIndex RAG Pipeline with U...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Supercharge your LlamaIndex RAG Pipeline with U...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the following sections, we will illustrate h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the following sections, we will illustrate h...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Context Relevance : However informative the doc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Context Relevance : However informative the doc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: RAG pipelines retrieve documents based on seman...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: RAG pipelines retrieve documents based on seman...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Then, we moved to advanced RAG concepts like th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Then, we moved to advanced RAG concepts like th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-04-02\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-04-02\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This addresses the high memory usage and costs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This addresses the high memory usage and costs ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Webinar  with  Daniel  on  CodeGPT  - a platfor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Webinar  with  Daniel  on  CodeGPT  - a platfor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Notebook ,  Tweet . We collaborated with langfu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Notebook ,  Tweet . We collaborated with langfu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: üé•¬† Webinars: Webinar  with Charles Packer, lead...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: üé•¬† Webinars: Webinar  with Charles Packer, lead...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: One-click Open Source RAG Observability with La...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: One-click Open Source RAG Observability with La...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.core  import  SimpleDirectory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.core  import  SimpleDirectory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.core  import  global_handler\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.core  import  global_handler\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This time Bob and Beth have sensitive data that...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This time Bob and Beth have sensitive data that...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.core.llama_datasets.simple  i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.core.llama_datasets.simple  i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This dataset consists of 1,200 examples each co...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This dataset consists of 1,200 examples each co...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: import  llama_index.core.instrumentation  as  i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: import  llama_index.core.instrumentation  as  i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In particular, we split the 24 categories of di...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In particular, we split the 24 categories of di...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: nodes = [\n",
      "    TextNode(text=el.text, metadata={...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: nodes = [\n",
      "    TextNode(text=el.text, metadata={...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluating the  NetworkRetriever To evaluate th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluating the  NetworkRetriever To evaluate th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the example above we used a  sigma  of 1.5, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the example above we used a  sigma  of 1.5, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # synthetic example epsilon = 1.3 \n",
      "{\n",
      "     \"refe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # synthetic example epsilon = 1.3 \n",
      "{\n",
      "     \"refe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ( link ) Demo notebooks for the  DiffPrivateSim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ( link ) Demo notebooks for the  DiffPrivateSim...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-04-09\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-04-09\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: üó∫Ô∏è Guides: Guide  to Building Advanced RAG with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: üó∫Ô∏è Guides: Guide  to Building Advanced RAG with...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-04-23\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-04-23\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: DREAM : A Distributed RAG Experimentation Frame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: DREAM : A Distributed RAG Experimentation Frame...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Andy Singal 's  tutorial  on Building a ColBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Andy Singal 's  tutorial  on Building a ColBERT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-04-30\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-04-30\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet Michael of KX Systems demonstrated making...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet Michael of KX Systems demonstrated making...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Streamlining knowledge work with LlamaIndex, Fi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Streamlining knowledge work with LlamaIndex, Fi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.readers.web  import  SimpleWe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.readers.web  import  SimpleWe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # FireworksEmbedding defaults to using model \n",
      "e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # FireworksEmbedding defaults to using model \n",
      "e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Example document in MongoDB Atlas that resulted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Example document in MongoDB Atlas that resulted...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "    return  index\n",
      "\n",
      "index = get_index()\n",
      "index....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "    return  index\n",
      "\n",
      "index = get_index()\n",
      "index....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Integrating different LLMs will open up new pos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Integrating different LLMs will open up new pos...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-05-14\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-05-14\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Using LlamaIndex and llamafile to build a local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Using LlamaIndex and llamafile to build a local...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Each llamafile bundles 1) model weights & metad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Each llamafile bundles 1) model weights & metad...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Now, open your computer's terminal and, if nece...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Now, open your computer's terminal and, if nece...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the rest of this walkthrough, we'll be using...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the rest of this walkthrough, we'll be using...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Our vector store will include Wikipedia pages, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Our vector store will include Wikipedia pages, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To start the llamafile server, open a terminal ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To start the llamafile server, open a terminal ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Load local data \n",
      " from  llama_index.core  imp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Load local data \n",
      " from  llama_index.core  imp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Your assistant ran 100% locally: you didn't hav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Your assistant ran 100% locally: you didn't hav...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building and Evaluating a QA System with LlamaI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building and Evaluating a QA System with LlamaI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.evaluation  import  DatasetGe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.evaluation  import  DatasetGe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The response object for a given query returns b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The response object for a given query returns b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.evaluation  import  QueryResp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.evaluation  import  QueryResp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Testing Anthropic Claude‚Äôs 100k-token window on...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Testing Anthropic Claude‚Äôs 100k-token window on...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Anthropic‚Äôs model is able to crunch an entire U...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Anthropic‚Äôs model is able to crunch an entire U...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We then create a Document object for each SEC f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We then create a Document object for each SEC f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index  import  PromptHelper, LLMPre...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index  import  PromptHelper, LLMPre...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Good for summarization purposes. Claude-v1 100K...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Good for summarization purposes. Claude-v1 100K...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If these third parties failed to provide servic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If these third parties failed to provide servic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚Ä¢ Financial performance  and  profitability. Ub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚Ä¢ Financial performance  and  profitability. Ub...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: query_engine = list_index.as_query_engine(servi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: query_engine = list_index.as_query_engine(servi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If Uber is unable to attract or maintain a crit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If Uber is unable to attract or maintain a crit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: query = \"How are the risk factors changing acro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: query = \"How are the risk factors changing acro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The changes over time show how an innovative co...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The changes over time show how an innovative co...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Part of this is potentially due to our tree sum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Part of this is potentially due to our tree sum...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Secure RAG with LlamaIndex and LLM Guard by Pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Secure RAG with LlamaIndex and LLM Guard by Pro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The nature of the attack manifests as an embedd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The nature of the attack manifests as an embedd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Please recommend me an experienced person. Retu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Please recommend me an experienced person. Retu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-26\n",
      "Hi there, Llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-03-26\n",
      "Hi there, Llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaPack ,  Tweet . BAM Elevate  integrated Da...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaPack ,  Tweet . BAM Elevate  integrated Da...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Ravi Theja   tutorial  on showcasing RAG with L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Ravi Theja   tutorial  on showcasing RAG with L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build a ChatGPT with your Private Data using Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Example question: Who is the most recent UK pri...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Example question: Who is the most recent UK pri...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Being able to persist this data enables process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Being able to persist this data enables process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here‚Äôs the summary of the various queries we ca...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here‚Äôs the summary of the various queries we ca...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Also check out the reference notebook below! Re...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Also check out the reference notebook below! Re...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Dumber LLM Agents Need More Constraints and Bet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Dumber LLM Agents Need More Constraints and Bet...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: There‚Äôs a variety of ways to perform  agent-too...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: There‚Äôs a variety of ways to perform  agent-too...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This includes not only asking questions over a ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This includes not only asking questions over a ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: High-Level Findings The high-level finding is t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: High-Level Findings The high-level finding is t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # define indices\n",
      "march_index = GPTVectorStoreIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # define indices\n",
      "march_index = GPTVectorStoreIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: graph = ComposableGraph.from_indices(\n",
      "    GPTLi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: graph = ComposableGraph.from_indices(\n",
      "    GPTLi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: GPT-3/GPT-4 ReAct Agent Setup # initializing ze...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: GPT-3/GPT-4 ReAct Agent Setup # initializing ze...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "query_tool_graph = QueryEngineTool.from_defau...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "query_tool_graph = QueryEngineTool.from_defau...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As a reminder, full results are in the notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: As a reminder, full results are in the notebook...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: agent_chain.run(input=\"How much cash did Uber h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: agent_chain.run(input=\"How much cash did Uber h...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: > Current query: Analyze Uber revenue growth ov...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: > Current query: Analyze Uber revenue growth ov...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Can  be used to answer any questions that  requ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Can  be used to answer any questions that  requ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 4.  Government restrictions  and  regulations: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 4.  Government restrictions  and  regulations: ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It contains a detailed analysis of all risk fac...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It contains a detailed analysis of all risk fac...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: These risk factors have been present in both th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: These risk factors have been present in both th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Code ,  Tweet . We have introduced  return_dire...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Code ,  Tweet . We have introduced  return_dire...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚úçÔ∏è Tutorials: Akash Mathur ‚Äôs  tutorial  on Dat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚úçÔ∏è Tutorials: Akash Mathur ‚Äôs  tutorial  on Dat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-05-07\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-05-07\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  Slides Plaban Nayak sets up a local, o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  Slides Plaban Nayak sets up a local, o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-07-23\n",
      "Hello, Llama F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-07-23\n",
      "Hello, Llama F...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Notebook ,  Tweet . We have released a cookbook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Notebook ,  Tweet . We have released a cookbook...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Lakshmi Narayana  tutorial  on Agentic RAG with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Lakshmi Narayana  tutorial  on Agentic RAG with...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex and Weaviate\n",
      "Co-authors: Jerry Liu (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex and Weaviate\n",
      "Co-authors: Jerry Liu (...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: These data connectors are primarily hosted on [...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: These data connectors are primarily hosted on [...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this example, we connect to a local Weaviate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this example, we connect to a local Weaviate...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚Äã‚Äãquery_engine = index.as_query_engine()\n",
      "respon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚Äã‚Äãquery_engine = index.as_query_engine()\n",
      "respon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex and Transformers Agents\n",
      "Summary Agen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex and Transformers Agents\n",
      "Summary Agen...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  datasets  import  load_dataset\n",
      " from  lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  datasets  import  load_dataset\n",
      " from  lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "\n",
      "query_engine = index. as_query_engine (\n",
      "    ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "\n",
      "query_engine = index. as_query_engine (\n",
      "    ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: class   Text2ImagePromptAssistant ( Tool ):\n",
      "   ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: class   Text2ImagePromptAssistant ( Tool ):\n",
      "   ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "        service_context = ServiceContext.from...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "        service_context = ServiceContext.from...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: When asked to draw a picture of a mountain, thi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: When asked to draw a picture of a mountain, thi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Custom tools in Transformers Agents are easily ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Custom tools in Transformers Agents are easily ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex on TWIML AI: A Distilled Summary (us...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex on TWIML AI: A Distilled Summary (us...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Additionally, LlamaIndex offers more advanced p...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Additionally, LlamaIndex offers more advanced p...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Additionally, LlamaIndex is looking into ways t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Additionally, LlamaIndex is looking into ways t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This allows users to get the best results for a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This allows users to get the best results for a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For example, does it evolve to look like the ch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For example, does it evolve to look like the ch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A New Document Summary Index for LLM-powered QA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A New Document Summary Index for LLM-powered QA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This index can help enhance retrieval performan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This index can help enhance retrieval performan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Why should we do this? This retrieval method gi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Why should we do this? This retrieval method gi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.indices.document_summary  imp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.indices.document_summary  imp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      " print (response) Lower-level API # use retri...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      " print (response) Lower-level API # use retri...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Llama Index & Prem AI Join Forces\n",
      "Co-authors:  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Llama Index & Prem AI Join Forces\n",
      "Co-authors:  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You can check the port on which the service is ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You can check the port on which the service is ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: doc3 = Document(text=\"\"\"\n",
      "Prem Benefits\n",
      "\n",
      "Effortl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: doc3 = Document(text=\"\"\"\n",
      "Prem Benefits\n",
      "\n",
      "Effortl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Perform an example query query_engine = index.a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Perform an example query query_engine = index.a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: He also studied philosophy in college, but swit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: He also studied philosophy in college, but swit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  trulens_eval  import  TruLlama, Tru, Quer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  trulens_eval  import  TruLlama, Tru, Quer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚ÄúWhat did the author do growing up?‚Äù In this ex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚ÄúWhat did the author do growing up?‚Äù In this ex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Using LLM‚Äôs for Retrieval and Reranking\n",
      "Summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Using LLM‚Äôs for Retrieval and Reranking\n",
      "Summary...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Yet for a variety of reasons, embedding-based r...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Yet for a variety of reasons, embedding-based r...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Example format:\n",
      "  Document 1:\n",
      "  <summary of doc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Example format:\n",
      "  Document 1:\n",
      "  <summary of doc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index  import  GPTListIndex\n",
      " from  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index  import  GPTListIndex\n",
      " from  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LLM-based retrieval is orders of magnitude slow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LLM-based retrieval is orders of magnitude slow...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # LLM Predictor (gpt-3.5-turbo) + service conte...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # LLM Predictor (gpt-3.5-turbo) + service conte...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We see that in embedding-based retrieval, the t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We see that in embedding-based retrieval, the t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôve added some initial functionality to help ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôve added some initial functionality to help ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Scale a Powerful Query Engine with Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Scale a Powerful Query Engine with Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We can use it to dramatically accelerate ingest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We can use it to dramatically accelerate ingest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: UnstructuredReader = download_loader( \"Unstruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: UnstructuredReader = download_loader( \"Unstruct...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Node objects can be used in the input prompt as...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Node objects can be used in the input prompt as...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: model_name= \"sentence-transformers/all-mpnet-ba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: model_name= \"sentence-transformers/all-mpnet-ba...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ray_docs_nodes = []\n",
      " for  row  in  embedded_nod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ray_docs_nodes = []\n",
      " for  row  in  embedded_nod...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Each index has a ‚Äúdefault‚Äù corresponding query ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Each index has a ‚Äúdefault‚Äù corresponding query ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As a next step, what if we could seamlessly dep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: As a next step, what if we could seamlessly dep...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It also provides a command line interface\n",
      "(CLI)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It also provides a command line interface\n",
      "(CLI)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It provides a Quick Start guide, a User Guide, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It provides a Quick Start guide, a User Guide, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Enriching LlamaIndex Models with GraphQL and Gr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Enriching LlamaIndex Models with GraphQL and Gr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In our Notebook we download the  countries.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In our Notebook we download the  countries.csv ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Initializing CSV Loader and GPTVectorStoreIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Initializing CSV Loader and GPTVectorStoreIndex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Question: Was ist die Hauptstadt von Albanien?\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Question: Was ist die Hauptstadt von Albanien?\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: SOURCE: '1' &gt; : '22e11ac6-8375-4d0c-91c6-475...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: SOURCE: '1' &gt; : '22e11ac6-8375-4d0c-91c6-475...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Below is the example code for the Graph Databas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Below is the example code for the Graph Databas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "            self.client = GraphDatabase.drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "            self.client = GraphDatabase.drive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To query a graph database you can use the  Cyph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To query a graph database you can use the  Cyph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from_documents(documents, service_context=servi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from_documents(documents, service_context=servi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The answer shows that the LLM can utilize the i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The answer shows that the LLM can utilize the i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \\nrelation: IN_GENRE\\ntitle: Matrix, The\\nyear:...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: \\nrelation: IN_GENRE\\ntitle: Matrix, The\\nyear:...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: There are a number of GraphQL libraries, most n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: There are a number of GraphQL libraries, most n...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: for authentication. GraphQLReader = download_lo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: for authentication. GraphQLReader = download_lo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Die Hauptst√§dte, die in Nordamerika liegen, sin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Die Hauptst√§dte, die in Nordamerika liegen, sin...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Data Agents\n",
      "Today we‚Äôre incredibly excited to a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Data Agents\n",
      "Today we‚Äôre incredibly excited to a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In order to do this, we had to improve the ‚Äúrea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In order to do this, we had to improve the ‚Äúrea...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: These components are described in more detail b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: These components are described in more detail b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The ReAct agent uses general text completion en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The ReAct agent uses general text completion en...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We allow users to define both a single Tool as ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We allow users to define both a single Tool as ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: These query engines can be used in an overall a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: These query engines can be used in an overall a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For instance, tool_spec = SlackToolSpec()\n",
      "# ini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For instance, tool_spec = SlackToolSpec()\n",
      "# ini...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As mentioned above, the spec consists of tools ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: As mentioned above, the spec consists of tools ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: All three of these steps happen in a single too...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: All three of these steps happen in a single too...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You  can now search the information using read_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You  can now search the information using read_...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If you are using agents for search/retrieval, b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If you are using agents for search/retrieval, b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Resources We‚Äôve written a comprehensive section...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Resources We‚Äôve written a comprehensive section...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Vellum <> LlamaIndex Integration\n",
      "Co-Authors: Ak...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Vellum <> LlamaIndex Integration\n",
      "Co-Authors: Ak...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is expected, so no stress! Through the Vel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is expected, so no stress! Through the Vel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Create 5‚Äì10 input scenarios to test performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Create 5‚Äì10 input scenarios to test performance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You get a clear indication of which test cases ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You get a clear indication of which test cases ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building Better Tools for LLM Agents\n",
      "Over the p...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building Better Tools for LLM Agents\n",
      "Over the p...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: self.token = app_id\n",
      "  \n",
      "   # Our function to be ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: self.token = app_id\n",
      "  \n",
      "   # Our function to be ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ,\n",
      "   'name' :  'wolfram_alpha_query' ,\n",
      "   'para...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ,\n",
      "   'name' :  'wolfram_alpha_query' ,\n",
      "   'para...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Step 1: Calculate the exponent 12^10.\n",
      "12^10 = 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Step 1: Calculate the exponent 12^10.\n",
      "12^10 = 6...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Args:\n",
      "            to (List[str]): The email add...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Args:\n",
      "            to (List[str]): The email add...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Args:\n",
      "            draft_id (str): the id of the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Args:\n",
      "            draft_id (str): the id of the...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We could simply pass along the null value and r...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We could simply pass along the null value and r...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agents won‚Äôt know what the current date is, and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agents won‚Äôt know what the current date is, and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We can still return ids as part of these prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We can still return ids as part of these prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The responses of the Agent are useful in determ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The responses of the Agent are useful in determ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Concluding thoughts Building tools for Agents r...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Concluding thoughts Building tools for Agents r...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building the data framework for LLMs\n",
      "Today is a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building the data framework for LLMs\n",
      "Today is a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It gained recognition within the AI community, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It gained recognition within the AI community, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Data Querying:  Data retrieval, response synthe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Data Querying:  Data retrieval, response synthe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Some  libraries  for handling this). This becom...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Some  libraries  for handling this). This becom...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We are not just building tools for ML practitio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We are not just building tools for ML practitio...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Data Agents + Zapier NLA\n",
      "Joint blog by LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Data Agents + Zapier NLA\n",
      "Joint blog by LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Combining Text-to-SQL with Semantic Search for ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Combining Text-to-SQL with Semantic Search for ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Each of these stacks solves particular use case...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Each of these stacks solves particular use case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Example queries suited for Retrieval Augmented ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Example queries suited for Retrieval Augmented ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A Query Engine to Combine Structured Analytics ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A Query Engine to Combine Structured Analytics ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For instance if the original question is ‚ÄúTell ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For instance if the original question is ‚ÄúTell ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Experiments So how well does this work? It work...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Experiments So how well does this work? It work...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: sql_tool = QueryEngineTool.from_defaults(\n",
      "    q...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: sql_tool = QueryEngineTool.from_defaults(\n",
      "    q...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The results are combined into a final response....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The results are combined into a final response....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This query can be answered by just querying the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This query can be answered by just querying the...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Zep and LlamaIndex: A Vector Store Walkthrough\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Zep and LlamaIndex: A Vector Store Walkthrough\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.vector_stores  import  ZepVec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.vector_stores  import  ZepVec...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: print ( str (response)) But one of the most sig...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: print ( str (response)) But one of the most sig...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: filters = MetadataFilters(filters=[ExactMatchFi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: filters = MetadataFilters(filters=[ExactMatchFi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Easily Finetune Llama 2 for Your Text-to-SQL Ap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Easily Finetune Llama 2 for Your Text-to-SQL Ap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Unlike ChatGPT and GPT-4, Llama 2 does not reli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Unlike ChatGPT and GPT-4, Llama 2 does not reli...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: All of our materials can be found in our Github...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: All of our materials can be found in our Github...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: modal run src.load_data_sql --data-dir \"data_sq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: modal run src.load_data_sql --data-dir \"data_sq...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Splits the dataset into training and validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Splits the dataset into training and validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ,  'context' :  'CREATE TABLE table_name_12 (re...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ,  'context' :  'CREATE TABLE table_name_12 (re...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We first define a test SQL database that we can...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We first define a test SQL database that we can...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tutorial repo:  https://github.com/run-llama/mo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tutorial repo:  https://github.com/run-llama/mo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Update ‚Äî 06/26/2023\n",
      "Greetings, Llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Update ‚Äî 06/26/2023\n",
      "Greetings, Llama...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This standalone module allows any LLM input to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This standalone module allows any LLM input to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs:  OpenAI Agent + Query Engine ,  Retrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs:  OpenAI Agent + Query Engine ,  Retrieval...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet The TruLens team has introduced tracing f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet The TruLens team has introduced tracing f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Hackathons: The LlamaIndex team has presented a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Hackathons: The LlamaIndex team has presented a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Special Feature: Berkeley Hackathon Projects (L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Special Feature: Berkeley Hackathon Projects (L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Additionally, Helmet AI offers a Chat interface...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Additionally, Helmet AI offers a Chat interface...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Future Prospects Looking ahead, Helmet AI aims ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Future Prospects Looking ahead, Helmet AI aims ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The current interface was developed using React...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The current interface was developed using React...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The rich have always had access to knowledge an...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The rich have always had access to knowledge an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The Building Blocks of Prosper AI: A Look into ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The Building Blocks of Prosper AI: A Look into ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To tackle this, we had to design a system that ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To tackle this, we had to design a system that ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As we continue to harness cutting-edge technolo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: As we continue to harness cutting-edge technolo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Update ‚Äî 07/11/2023\n",
      "Greetings once a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Update ‚Äî 07/11/2023\n",
      "Greetings once a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Blogpost ,  Tweet LlamaIndex now incorporates t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Blogpost ,  Tweet LlamaIndex now incorporates t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Additionally, LlamaIndex now also integrates wi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Additionally, LlamaIndex now also integrates wi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet Tutorials: Anyscale tutorial  on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet Tutorials: Anyscale tutorial  on ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Fine-Tuning Embeddings for RAG with Synthetic D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Fine-Tuning Embeddings for RAG with Synthetic D...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: (Of course RAG can be much more advanced than t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: (Of course RAG can be much more advanced than t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: With LlamaIndex you can! We use LlamaIndex modu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: With LlamaIndex you can! We use LlamaIndex modu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \"\" \"\n",
      "\n",
      "# for a given node, extract questions (do...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: \"\" \"\n",
      "\n",
      "# for a given node, extract questions (do...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # define model \n",
      "model_id =  \"BAAI/bge-small-en\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # define model \n",
      "model_id =  \"BAAI/bge-small-en\"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluation suite from `InformationRetrievalEval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluation suite from `InformationRetrievalEval...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex: Automatic Knowledge Transfer (KT) G...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex: Automatic Knowledge Transfer (KT) G...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Detailed Explanations for Individual Code Block...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Detailed Explanations for Individual Code Block...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  Wit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  Wit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Code Repository:\n",
      "   https://github.com/ravi0307...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Code Repository:\n",
      "   https://github.com/ravi0307...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Airbyte sources within LlamaIndex\n",
      "A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Airbyte sources within LlamaIndex\n",
      "A...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: But if you are just getting started and are run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: But if you are just getting started and are run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For example you can still benefit from  increme...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For example you can still benefit from  increme...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: if you have implemented your own custom Airbyte...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: if you have implemented your own custom Airbyte...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ChatGPT‚Äôs Knowledge is Two Years Old: What to d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ChatGPT‚Äôs Knowledge is Two Years Old: What to d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Search, Give, Get. For those of us coming from ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Search, Give, Get. For those of us coming from ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A few of the ones I‚Äôve personally tried are Met...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A few of the ones I‚Äôve personally tried are Met...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex 0.7.0: Better Enabling Bottoms-Up LL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex 0.7.0: Better Enabling Bottoms-Up LL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Before, our  LLMPredictor  class had a ton of l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Before, our  LLMPredictor  class had a ton of l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here‚Äôs on how you can use the LLM abstractions ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here‚Äôs on how you can use the LLM abstractions ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here‚Äôs some resources to show both the LLM abst...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here‚Äôs some resources to show both the LLM abst...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Accumulate  - Query an LLM with the same prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Accumulate  - Query an LLM with the same prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Defining Metadata Fields document = Document(\n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Defining Metadata Fields document = Document(\n",
      " ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This format is configurable at multiple levels:...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This format is configurable at multiple levels:...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 5 )\n",
      "    ]\n",
      ")\n",
      " # assemble query engine \n",
      "query_eng...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 5 )\n",
      "    ]\n",
      ")\n",
      " # assemble query engine \n",
      "query_eng...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This makes the data model consistent with the i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This makes the data model consistent with the i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Fine-Tuning a Linear Adapter for Any Embedding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Fine-Tuning a Linear Adapter for Any Embedding ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The  linear adapter  is simply a linear transfo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The  linear adapter  is simply a linear transfo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As with the previous post, we use the UBER and ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: As with the previous post, we use the UBER and ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.finetuning  import  Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.finetuning  import  Embedding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Mean Reciprocal Rank : A slightly more granular...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Mean Reciprocal Rank : A slightly more granular...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Resources Guide:  https://gpt-index.readthedocs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Resources Guide:  https://gpt-index.readthedocs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Update ‚Äî 09/03/2023\n",
      "Hello LlamaIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Update ‚Äî 09/03/2023\n",
      "Hello LlamaIndex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet . LlamaIndex, in collaboration with Predi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet . LlamaIndex, in collaboration with Predi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . LlamaIndex has updated the LLM ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . LlamaIndex has updated the LLM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This boosts querying across varied documents, o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This boosts querying across varied documents, o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . LlamaIndex introduces the  Auto...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . LlamaIndex introduces the  Auto...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . LlamaIndex is integrated with R...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . LlamaIndex is integrated with R...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet . RAG Tips: LlamaIndex shares  four tacti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet . RAG Tips: LlamaIndex shares  four tacti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Webinars: Webinar  with members from Docugami o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Webinars: Webinar  with members from Docugami o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex + Vectara\n",
      "(co-authored by Ofer Mende...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex + Vectara\n",
      "(co-authored by Ofer Mende...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Security and Privacy.  Vectara‚Äôs API is fully e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Security and Privacy.  Vectara‚Äôs API is fully e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Step 1: Setup your Vectara account and Index To...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Step 1: Setup your Vectara account and Index To...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The first programs I tried writing were on the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The first programs I tried writing were on the ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For example, in the following code we use the c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For example, in the following code we use the c...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing LlamaIndex.TS\n",
      "We are beyond excited...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing LlamaIndex.TS\n",
      "We are beyond excited...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Playground We are building an open source playg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Playground We are building an open source playg...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Update ‚Äî 08/01/2023\n",
      "Greetings once a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Update ‚Äî 08/01/2023\n",
      "Greetings once a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  Blogpost LlamaIndex‚Äôs  ContextChatEngi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  Blogpost LlamaIndex‚Äôs  ContextChatEngi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This integration allows for more advanced multi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This integration allows for more advanced multi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Trulens ‚Äôs  tutorial  on using LlamaIndex Yelp ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Trulens ‚Äôs  tutorial  on using LlamaIndex Yelp ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Data Agents session at TPF X Nexus VC  Buildath...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Data Agents session at TPF X Nexus VC  Buildath...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . RA-DIT:  We drew inspiration fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . RA-DIT:  We drew inspiration fr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Enjoy local data storage, track LLM input/outpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Enjoy local data storage, track LLM input/outpu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Jerry Liu spoke on Evals/ Benchmarking  and Adv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Jerry Liu spoke on Evals/ Benchmarking  and Adv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Update ‚Äî 20/09/2023\n",
      "Hello LlamaIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Update ‚Äî 20/09/2023\n",
      "Hello LlamaIndex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  Replit Templates . LlamaIndex.TS: Laun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  Replit Templates . LlamaIndex.TS: Laun...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tutorials from the LlamaIndex Team. Sourabh   t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tutorials from the LlamaIndex Team. Sourabh   t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Webinars : Webinar  on How to Win an LLM Hackat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Webinars : Webinar  on How to Win an LLM Hackat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex: Harnessing the Power of Text2SQL an...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex: Harnessing the Power of Text2SQL an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It's remarkable how seamless the integration fe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It's remarkable how seamless the integration fe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The adjustability features mean I can tailor it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The adjustability features mean I can tailor it...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: SQL + RAG in LlamaIndex simplifies this by brea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: SQL + RAG in LlamaIndex simplifies this by brea...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 2. Question that needs to be asked on the top o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 2. Question that needs to be asked on the top o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: sql_query_engine = NLSQLTableQueryEngine(\n",
      "    s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: sql_query_engine = NLSQLTableQueryEngine(\n",
      "    s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: listindex = ListIndex([Document(text=text)])\n",
      "li...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: listindex = ListIndex([Document(text=text)])\n",
      "li...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Conclusion In the era of e-commerce, where user...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Conclusion In the era of e-commerce, where user...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì10‚Äì17\n",
      "Hello Llama En...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì10‚Äì17\n",
      "Hello Llama En...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build: Demostify Stick with Fit ,  SafeQuery , ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build: Demostify Stick with Fit ,  SafeQuery , ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Mayo Oshin  has a  tutorial  on How to Compare ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Mayo Oshin  has a  tutorial  on How to Compare ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . Nougat ‚Äî MetaAI:  We integrated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . Nougat ‚Äî MetaAI:  We integrated...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: How I built the Streamlit LLM Hackathon winning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: How I built the Streamlit LLM Hackathon winning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here‚Äôs\n",
      "  how it works behind the scenes:\n",
      " \n",
      "\n",
      " \n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here‚Äôs\n",
      "  how it works behind the scenes:\n",
      " \n",
      "\n",
      " \n",
      " ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from pypdf import PdfReader from llama_index.sc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from pypdf import PdfReader from llama_index.sc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Calling the functions through streamlit front...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Calling the functions through streamlit front...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is especially useful\n",
      "  when you have more ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is especially useful\n",
      "  when you have more ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Using the  PydanticOutputParser  we write the d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Using the  PydanticOutputParser  we write the d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: So let‚Äôs write a function that allows us to plu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: So let‚Äôs write a function that allows us to plu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: write(\"Strategy Outlook and Future Direction.\")...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: write(\"Strategy Outlook and Future Direction.\")...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: write(st.session_state.risk_management.risk_fac...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: write(st.session_state.risk_management.risk_fac...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If you like what you‚Äôve read, please do leave a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If you like what you‚Äôve read, please do leave a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Mastering PDFs: Extracting Sections, Headings, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Mastering PDFs: Extracting Sections, Headings, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Do we need an efficient parser? \n",
      " \n",
      "   \n",
      "    In t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Do we need an efficient parser? \n",
      " \n",
      "   \n",
      "    In t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is the most common and\n",
      "    straightforward...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is the most common and\n",
      "    straightforward...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from llmsherpa.readers import LayoutPDFReader l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from llmsherpa.readers import LayoutPDFReader l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The tool exclusively supports PDFs equipped wit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The tool exclusively supports PDFs equipped wit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: NewsGPT(Neotice): Summarize news articles with ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: NewsGPT(Neotice): Summarize news articles with ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Sophisticated  Named-Entity Recognition, Text E...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Sophisticated  Named-Entity Recognition, Text E...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: After the preprocessing of the data, we collect...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: After the preprocessing of the data, we collect...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Chat with Article Powered by LlamaIndex LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Chat with Article Powered by LlamaIndex LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index  import  ServiceContext\n",
      "\n",
      "st.s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index  import  ServiceContext\n",
      "\n",
      "st.s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: response = st.session_state[ \"chat_engine\" ].qu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: response = st.session_state[ \"chat_engine\" ].qu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex + Metaphor: Towards Automating Knowl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex + Metaphor: Towards Automating Knowl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A complete data agent consists of both a reason...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A complete data agent consists of both a reason...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Companies, articles, people. You can find conte...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Companies, articles, people. You can find conte...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Set up Metaphor tool \n",
      " from  llama_hub.tools....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Set up Metaphor tool \n",
      " from  llama_hub.tools....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Setting up an OpenAI Function Calling Agent wit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Setting up an OpenAI Function Calling Agent wit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: thestockyards.ca/', 'id': 'Pffz-DQlOepqVgKQDmW5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: thestockyards.ca/', 'id': 'Pffz-DQlOepqVgKQDmW5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: [ Kit   Kat   Italian   Bar  &amp;  Grill ]( ht...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: [ Kit   Kat   Italian   Bar  &amp;  Grill ]( ht...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: They also have a sister restaurant called Giuli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: They also have a sister restaurant called Giuli...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Creating the  LoadAndSearchToolSpec : from  lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Creating the  LoadAndSearchToolSpec : from  lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: On July 22, scientists in South Korea published...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: On July 22, scientists in South Korea published...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Timescale Vector x LlamaIndex: Making PostgreSQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Timescale Vector x LlamaIndex: Making PostgreSQ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Timescale Vector also offers pgvector‚Äôs Hierarc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Timescale Vector also offers pgvector‚Äôs Hierarc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: üéâ  LlamaIndex Users Get 3 Months of Timescale V...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: üéâ  LlamaIndex Users Get 3 Months of Timescale V...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Simply create a Timescale Vector vector store a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Simply create a Timescale Vector vector store a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The ability to conveniently create ANN search i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The ability to conveniently create ANN search i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For example, chatbot chat history. Let‚Äôs take a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For example, chatbot chat history. Let‚Äôs take a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.schema  import  TextNode, Nod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.schema  import  TextNode, Nod...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  timescale_vector  import  client\n",
      " # Funct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  timescale_vector  import  client\n",
      " # Funct...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll use the `OpenAIEmbedding` model to create...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll use the `OpenAIEmbedding` model to create...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: and enabled automatic time-based partitioning o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: and enabled automatic time-based partitioning o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: -----------------------------------------------...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: -----------------------------------------------...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 12 all the functions present in _timescaledb_in...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 12 all the functions present in _timescaledb_in...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Timescale Vector partitions the data by time an...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Timescale Vector partitions the data by time an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: When creating the query engine, we use Timescal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: When creating the query engine, we use Timescal...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Resources and next steps Now that you‚Äôve learne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Resources and next steps Now that you‚Äôve learne...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex + Laurie Voss: an alpaca joins the l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex + Laurie Voss: an alpaca joins the l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: So in the same way, I‚Äôm especially interested i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: So in the same way, I‚Äôm especially interested i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluating the Ideal Chunk Size for a RAG Syste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluating the Ideal Chunk Size for a RAG Syste...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It's vital to undergo thorough testing with var...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It's vital to undergo thorough testing with var...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Faithfulness Evaluator  ‚Äî It is useful for meas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Faithfulness Evaluator  ‚Äî It is useful for meas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: total_response_time =  0 \n",
      "    total_faithfulnes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: total_response_time =  0 \n",
      "    total_faithfulnes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: model= \"gpt-4\" )\n",
      "\n",
      " # Define service context for...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: model= \"gpt-4\" )\n",
      "\n",
      " # Define service context for...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: average_faithfulness, average_relevancy\n",
      "\n",
      " # Ite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: average_faithfulness, average_relevancy\n",
      "\n",
      " # Ite...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Improving RAG effectiveness with Retrieval-Augm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Improving RAG effectiveness with Retrieval-Augm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We use the Q/A pairs through the QueryResponseD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We use the Q/A pairs through the QueryResponseD...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: They contain questions and scenarios that typic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: They contain questions and scenarios that typic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this experiment without retrieval augmentati...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this experiment without retrieval augmentati...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex news special edition: OpenAI develop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex news special edition: OpenAI develop...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  Docs . SQLRetriever:  We introduce SQL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  Docs . SQLRetriever:  We introduce SQL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet . Amazon Bedrock & AI21 Labs LLMs:  We‚Äôve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet . Amazon Bedrock & AI21 Labs LLMs:  We‚Äôve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: NVIDIA Research: RAG with Long Context LLMs\n",
      "Int...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: NVIDIA Research: RAG with Long Context LLMs\n",
      "Int...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: One hypothesis for this difference centers on t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: One hypothesis for this difference centers on t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Contriever:  Utilizes a basic contrastive learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Contriever:  Utilizes a basic contrastive learn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Results Baseline models without retrieval, havi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Results Baseline models without retrieval, havi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Efficiency of 4K Models with Retrieval : 4K con...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Efficiency of 4K Models with Retrieval : 4K con...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building My Own ChatGPT Vision with PaLM, KOSMO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building My Own ChatGPT Vision with PaLM, KOSMO...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Google  PaLM API  adds the layer of linguistic ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Google  PaLM API  adds the layer of linguistic ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Image Upload and Processing Upon uploading an i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Image Upload and Processing Upon uploading an i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: @st.cache_resource \n",
      " def   create_chat_engine (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: @st.cache_resource \n",
      " def   create_chat_engine (...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "     # Disabling the uploader and input by no...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "     # Disabling the uploader and input by no...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I invite you to dive into the demo, tinker with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I invite you to dive into the demo, tinker with...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Multi-Modal RAG\n",
      "(co-authored by Haotian Zhang, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Multi-Modal RAG\n",
      "(co-authored by Haotian Zhang, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is just a small part of the overall space ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is just a small part of the overall space ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We also have a docs page for multi-modal models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We also have a docs page for multi-modal models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Currently the way to deal with long text contex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Currently the way to deal with long text contex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The  SimpleMultiModalQueryEngine  first retriev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The  SimpleMultiModalQueryEngine  first retriev...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Fr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Users can now upload images for enhanced chatbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Users can now upload images for enhanced chatbo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: David Garnitz ‚Äôs tutorial blog explores the use...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: David Garnitz ‚Äôs tutorial blog explores the use...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Improving Retrieval Performance by Fine-tuning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Improving Retrieval Performance by Fine-tuning ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Setting Up the Environment !pip install llama-i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Setting Up the Environment !pip install llama-i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: llm = OpenAI(api_key=openai_api_key, temperatur...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: llm = OpenAI(api_key=openai_api_key, temperatur...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Training dataset \n",
      "qa_dataset_lyft_train = gen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Training dataset \n",
      "qa_dataset_lyft_train = gen...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: embed_model = CohereEmbedding(\n",
      "    cohere_api_k...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: embed_model = CohereEmbedding(\n",
      "    cohere_api_k...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: generate_cohere_reranker_finetuning_dataset(\n",
      "  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: generate_cohere_reranker_finetuning_dataset(\n",
      "  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: finetune_model_no_hard_negatives = CohereRerank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: finetune_model_no_hard_negatives = CohereRerank...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: reranker_base = CohereRerank(top_n= 5 )\n",
      "reranke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: reranker_base = CohereRerank(top_n= 5 )\n",
      "reranke...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: index_embed_model = CohereEmbedding(\n",
      "    cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: index_embed_model = CohereEmbedding(\n",
      "    cohere...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: results_df = pd.DataFrame()\n",
      "\n",
      "embed_name =  'Coh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: results_df = pd.DataFrame()\n",
      "\n",
      "embed_name =  'Coh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: return  self._retrieve(query_bundle)\n",
      "\n",
      "         ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: return  self._retrieve(query_bundle)\n",
      "\n",
      "         ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluating Multi-Modal Retrieval-Augmented Gene...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluating Multi-Modal Retrieval-Augmented Gene...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The first two of these metrics recall and hit r...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The first two of these metrics recall and hit r...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: One can then apply a desired weighting scheme t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: One can then apply a desired weighting scheme t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.evaluation.multi_modal  impor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.evaluation.multi_modal  impor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We believe that separating out the retrieval ev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We believe that separating out the retrieval ev...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì10‚Äì31\n",
      "Greetings Llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì10‚Äì31\n",
      "Greetings Llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Notebook ,  Tweet . üé•  Demos: Harshad Suryawans...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Notebook ,  Tweet . üé•  Demos: Harshad Suryawans...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Ravi Theja‚Äôs   blog post  delves into NVIDIA Re...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Ravi Theja‚Äôs   blog post  delves into NVIDIA Re...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Mean Reciprocal Rank (MRR): For each query, MRR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Mean Reciprocal Rank (MRR): For each query, MRR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To remove bias for the evaluation of embedding(...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To remove bias for the evaluation of embedding(...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Extract keys from queries and relevant_docs t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Extract keys from queries and relevant_docs t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: embed_model = OpenAIEmbedding()\n",
      "service_context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: embed_model = OpenAIEmbedding()\n",
      "service_context...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "        ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "        ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: bge-large : Experiences significant improvement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: bge-large : Experiences significant improvement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Necessity of Rerankers : The data clearly indic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Necessity of Rerankers : The data clearly indic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Foundation is Key : Choosing the right embeddin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Foundation is Key : Choosing the right embeddin...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Demo ,  Tweet . We introduced a method for fine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Demo ,  Tweet . We introduced a method for fine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Sophia Yang‚Äôs   tutorial  on Zephyr-7b-beta sho...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Sophia Yang‚Äôs   tutorial  on Zephyr-7b-beta sho...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LongLLMLingua: Bye-bye to Middle Loss and Save ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LongLLMLingua: Bye-bye to Middle Loss and Save ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: These issues become more apparent in real-world...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: These issues become more apparent in real-world...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Results show that this approach significantly o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Results show that this approach significantly o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Question-aware Fine-grained Prompt Compression ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Question-aware Fine-grained Prompt Compression ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: But this can actually be recovered through a si...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: But this can actually be recovered through a si...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.query_engine  import  Retriev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.query_engine  import  Retriev...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Guide  on using  Fleet Context  to download the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Guide  on using  Fleet Context  to download the...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing RAGs: Your Personalized ChatGPT Exp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing RAGs: Your Personalized ChatGPT Exp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: See below for the full list of parameters. [2] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: See below for the full list of parameters. [2] ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Launch the app: streamlit run 1_üè†_Home.py Build...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Launch the app: streamlit run 1_üè†_Home.py Build...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex‚Äôs founder, Jerry Liu, says: Our comm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex‚Äôs founder, Jerry Liu, says: Our comm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: September 2023: We  launched   secinsights.ai  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: September 2023: We  launched   secinsights.ai  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: create-llama, a command line tool to generate L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: create-llama, a command line tool to generate L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This also uses LlamaIndex.TS. Python FastAPI : ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This also uses LlamaIndex.TS. Python FastAPI : ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì28\n",
      "Hello to Our L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì28\n",
      "Hello to Our L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Project ,  Tweet . We introduced a LlamaPack th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Project ,  Tweet . We introduced a LlamaPack th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This introductory guide explains retrieval-augm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This introductory guide explains retrieval-augm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Announcing LlamaIndex 0.9\n",
      "Our hard-working team...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Announcing LlamaIndex 0.9\n",
      "Our hard-working team...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: What is a  Transformation  though? It could be ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: What is a  Transformation  though? It could be ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here‚Äôs an example with a saving and loading a l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here‚Äôs an example with a saving and loading a l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index  import  Document\n",
      " from  llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index  import  Document\n",
      " from  llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: import  re\n",
      " from  llama_index  import  Document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: import  re\n",
      " from  llama_index  import  Document...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In V0.9, we have  flattened  the entire interfa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In V0.9, we have  flattened  the entire interfa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Packaging ‚Äî Reduced Bloat In an effort to moder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Packaging ‚Äî Reduced Bloat In an effort to moder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To help users use these features, we‚Äôve started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To help users use these features, we‚Äôve started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Llama Datasets ü¶ôüìù\n",
      "(Authors: Andrei ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Llama Datasets ü¶ôüìù\n",
      "(Authors: Andrei ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Llama Datasets on LlamaHub Overview Today‚Äôs lau...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Llama Datasets on LlamaHub Overview Today‚Äôs lau...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You can easily plug in any query engine into  a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You can easily plug in any query engine into  a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: query_by = CreatedBy(type=CreatedByType.AI, mod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: query_by = CreatedBy(type=CreatedByType.AI, mod...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Prepare the dataset card (  card.json  ) and  R...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Prepare the dataset card (  card.json  ) and  R...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex + Waii: Combining Structured Data fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex + Waii: Combining Structured Data fr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: However, managing medium to large databases, wh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: However, managing medium to large databases, wh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For example, if the question indicates informat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For example, if the question indicates informat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: agent import OpenAIAgent from llama_index.llms ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: agent import OpenAIAgent from llama_index.llms ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Understand your dataset \n",
      " \n",
      "  The first step in ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Understand your dataset \n",
      " \n",
      "  The first step in ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: **Call Center Analysis**: Evaluate the performa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: **Call Center Analysis**: Evaluate the performa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: And got a SQL like: \n",
      " WITH christmas_sales AS (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: And got a SQL like: \n",
      " WITH christmas_sales AS (...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Men: 1,451,285 \n",
      " Use with a PDF report \n",
      " \n",
      "  Let...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Men: 1,451,285 \n",
      " Use with a PDF report \n",
      " \n",
      "  Let...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: - \"Gift Cards & Other\" and \"Food & Beverage\" fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: - \"Gift Cards & Other\" and \"Food & Beverage\" fr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: GPT4-V Experiments with General, Specific quest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: GPT4-V Experiments with General, Specific quest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: (Llama2 paper) Let‚Äôs go through each of these d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: (Llama2 paper) Let‚Äôs go through each of these d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚Äî ‚ÄúCommercial Websites‚Äù seem to have lower perc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚Äî ‚ÄúCommercial Websites‚Äù seem to have lower perc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If you provide the actual percentages or descri...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If you provide the actual percentages or descri...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Conversely, in other subsections, the Vicuna ba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Conversely, in other subsections, the Vicuna ba...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Observation: With chain of thought prompting it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Observation: With chain of thought prompting it...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Based on these graphs, Mistral appears to be th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Based on these graphs, Mistral appears to be th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 2. **Reasoning (Top Right Graph)**: In the reas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 2. **Reasoning (Top Right Graph)**: In the reas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The x-axes on the graphs represent model size i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The x-axes on the graphs represent model size i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Answer: Examine the Image: The image contains f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Answer: Examine the Image: The image contains f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: However, Mistral is not displayed on the Knowle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: However, Mistral is not displayed on the Knowle...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Without additional context, it is difficult to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Without additional context, it is difficult to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Answer: The table you‚Äôve provided shows perform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Answer: The table you‚Äôve provided shows perform...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluate: I will compare the SAT-en percentages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluate: I will compare the SAT-en percentages...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì12\n",
      "Howdy, Llama E...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì12\n",
      "Howdy, Llama E...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Each dataset, designed as a QA set, integrates ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Each dataset, designed as a QA set, integrates ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet . We launched the Ollama LlamaPack, a new...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet . We launched the Ollama LlamaPack, a new...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: An hour comprehensive workshop  tutorial  by  A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: An hour comprehensive workshop  tutorial  by  A...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Shipping your Retrieval-Augmented Generation ap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Shipping your Retrieval-Augmented Generation ap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Step 5: Deploy! That‚Äôs it! Deploying a Next.js ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Step 5: Deploy! That‚Äôs it! Deploying a Next.js ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You should now have a site live at a URL someth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You should now have a site live at a URL someth...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Becoming Proficient in Document Extraction\n",
      "Intr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Becoming Proficient in Document Extraction\n",
      "Intr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 2. Contextual Comprehension: \n",
      " \n",
      " \n",
      "  Diverging f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 2. Contextual Comprehension: \n",
      " \n",
      " \n",
      "  Diverging f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Source: Image created by Author using MidJourne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Source: Image created by Author using MidJourne...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: messages_to_prompt=messages_to_prompt,     devi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: messages_to_prompt=messages_to_prompt,     devi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: if child_branch_factor=2, a query will choose t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: if child_branch_factor=2, a query will choose t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Conclusion \n",
      " \n",
      "  In summary, the fusion of Zephy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Conclusion \n",
      " \n",
      "  In summary, the fusion of Zephy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I‚Äôm always looking for new ideas for future Not...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I‚Äôm always looking for new ideas for future Not...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The Ultra variants (which are not yet publicly ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The Ultra variants (which are not yet publicly ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This includes  model_name ,  temperature ,  max...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This includes  model_name ,  temperature ,  max...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: image_urls = [\n",
      "    \" &lt; https://www.sportsnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: image_urls = [\n",
      "    \" &lt; https://www.sportsnet...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It has a rating of 4 and a review score of 4.3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It has a rating of 4 and a review score of 4.3 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The notebook guide highlights three  advanced R...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The notebook guide highlights three  advanced R...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex: RAG Evaluation Showdown with GPT-4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex: RAG Evaluation Showdown with GPT-4 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Score 4: If the generated answer is relevant to...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Score 4: If the generated answer is relevant to...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.llama_dataset  import  downlo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.llama_dataset  import  downlo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 3. The output format should look as follows: 'F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 3. The output format should look as follows: 'F...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 4. The output format should look as follows: \" ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 4. The output format should look as follows: \" ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ### Existing   answer : {existing_answer}\n",
      "\n",
      "### ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ### Existing   answer : {existing_answer}\n",
      "\n",
      "### ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ### Feedback :  \"\" \"\n",
      "\n",
      "prometheus_relevancy_refi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ### Feedback :  \"\" \"\n",
      "\n",
      "prometheus_relevancy_refi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ### Feedback :  \"\" \" Define Correctness, FaithF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ### Feedback :  \"\" \" Define Correctness, FaithF...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: token_counter = TokenCountingHandler(\n",
      "    token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: token_counter = TokenCountingHandler(\n",
      "    token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: async   def   batch_eval_runner ( \n",
      "    evaluato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: async   def   batch_eval_runner ( \n",
      "    evaluato...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Feedback comparison between Prometheus and GPT-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Feedback comparison between Prometheus and GPT-...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: So the overall score is 3. Prometheus Score:  3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: So the overall score is 3. Prometheus Score:  3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Feedback comparison between Prometheus and GPT-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Feedback comparison between Prometheus and GPT-...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Context-1:  Llama 2 : Open Foundation and Fine-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Context-1:  Llama 2 : Open Foundation and Fine-...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We provide a detailed description of our approa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We provide a detailed description of our approa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 7 Conclusion Inthisstudy,wehaveintroduced Llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 7 Conclusion Inthisstudy,wehaveintroduced Llama...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The query asked for the two primary objectives ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The query asked for the two primary objectives ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: GPT-4: Evaluates it correctly, unlike the Prome...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: GPT-4: Evaluates it correctly, unlike the Prome...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Multimodal RAG pipeline with LlamaIndex and Neo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Multimodal RAG pipeline with LlamaIndex and Neo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: When a question is posed by an end user, two ve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: When a question is posed by an end user, two ve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: sections = []\n",
      "    current_section = { \"header\" ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: sections = []\n",
      "    current_section = { \"header\" ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: all_documents = []\n",
      "all_images = []\n",
      "\n",
      " # Director...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: all_documents = []\n",
      "all_images = []\n",
      "\n",
      " # Director...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: text_store = Neo4jVectorStore(\n",
      "    url=NEO4J_UR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: text_store = Neo4jVectorStore(\n",
      "    url=NEO4J_UR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: openai_mm_llm =  OpenAIMultiModal (\n",
      "    model= ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: openai_mm_llm =  OpenAIMultiModal (\n",
      "    model= ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The multimodal RAG pipelines implementation wit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The multimodal RAG pipelines implementation wit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . üëÄ Community Demos : MemoryCache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . üëÄ Community Demos : MemoryCache...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Guide on  Qdrant‚Äôs Multitenancy with LlamaIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Guide on  Qdrant‚Äôs Multitenancy with LlamaIndex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: They can be downloaded either through our  llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: They can be downloaded either through our  llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Take a look at this folder for a full set of ex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Take a look at this folder for a full set of ex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Multimodal RAG: Building ‚ÄòAInimal Go!‚Äô, a Pok√©m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Multimodal RAG: Building ‚ÄòAInimal Go!‚Äô, a Pok√©m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex:  Seamlessly orchestrates the workfl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex:  Seamlessly orchestrates the workfl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Image upload section. \n",
      "    image_file = st.fi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Image upload section. \n",
      "    image_file = st.fi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   load_model_and_labels ():\n",
      "     # Load ani...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   load_model_and_labels ():\n",
      "     # Load ani...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This name is then displayed to the user. def   ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This name is then displayed to the user. def   ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The function sets up the environment in which t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The function sets up the environment in which t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 7. Bringing It All Together With all the techni...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 7. Bringing It All Together With all the techni...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Running Mixtral 8x7 locally with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Running Mixtral 8x7 locally with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Once the model is running Ollama will automatic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Once the model is running Ollama will automatic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Create a new python file, and load in all our d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Create a new python file, and load in all our d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: llm = Ollama(model=\"mixtral\")\n",
      "service_context =...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: llm = Ollama(model=\"mixtral\")\n",
      "service_context =...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôre also passing  similarity_top_k=20  to the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôre also passing  similarity_top_k=20  to the...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll need two new dependencies: pip install fl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll need two new dependencies: pip install fl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: And add a route that accepts a query (as form d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: And add a route that accepts a query (as form d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Two new llama-datasets and a Gemini vs. GPT sho...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Two new llama-datasets and a Gemini vs. GPT sho...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The way to do that with our llama-dataset abstr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The way to do that with our llama-dataset abstr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: However, there is a subtle difference in the ev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: However, there is a subtle difference in the ev...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The below snippet of code is how you can replic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The below snippet of code is how you can replic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Observations It seems that Gemini-Pro and GPT-3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Observations It seems that Gemini-Pro and GPT-3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Inconclusive‚Äôs represent the case when an LLM e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Inconclusive‚Äôs represent the case when an LLM e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Scaling LlamaIndex with AWS and Hugging Face\n",
      "Ov...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Scaling LlamaIndex with AWS and Hugging Face\n",
      "Ov...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: My AWS console favourites Note on how deploymen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: My AWS console favourites Note on how deploymen...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Once you have a quota for GPU instances (like G...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Once you have a quota for GPU instances (like G...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: First, you need to create your cluster. For wha...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: First, you need to create your cluster. For wha...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: NOTE:  Make sure to write down the URL printed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: NOTE:  Make sure to write down the URL printed ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: docker tag <image_name>:latest <image_name>:<ve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: docker tag <image_name>:latest <image_name>:<ve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Create a python file called  lambda_function.py...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Create a python file called  lambda_function.py...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Select  Create function Give it a name, select ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Select  Create function Give it a name, select ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To ingest data, you can run: import  requests\n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To ingest data, you can run: import  requests\n",
      " ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To delete my clusters, I ran the following: eks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To delete my clusters, I ran the following: eks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: These packs simplify the process to almost a si...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: These packs simplify the process to almost a si...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . üëÄ Demo: AInimal Go ‚Äî an innovat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . üëÄ Demo: AInimal Go ‚Äî an innovat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Key findings reveal that all models incorrectly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Key findings reveal that all models incorrectly...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building An Intelligent Query-Response System w...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building An Intelligent Query-Response System w...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Setting up the environment The first step is to...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Setting up the environment The first step is to...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: v1: Creating a simple completion service Before...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: v1: Creating a simple completion service Before...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 2.  Support  for  a variety of datasets: OpenLL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 2.  Support  for  a variety of datasets: OpenLL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is a good start as we proceed with buildin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is a good start as we proceed with buildin...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Create a folder and let‚Äôs import the GitHub REA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Create a folder and let‚Äôs import the GitHub REA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: response.print_response_stream() Your directory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: response.print_response_stream() Your directory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Generation must be able to make good use of the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Generation must be able to make good use of the...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Define objective function \n",
      " def   objective_fun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Define objective function \n",
      " def   objective_fun...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build ParamTuner object \n",
      "param_dict = { \"chunk_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build ParamTuner object \n",
      "param_dict = { \"chunk_...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: chunk_overlap= 20 )  for  c  in  sub_chunk_size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: chunk_overlap= 20 )  for  c  in  sub_chunk_size...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here are links to a select few of them: Buildin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here are links to a select few of them: Buildin...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: irrelevant information). LlamaIndex Information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: irrelevant information). LlamaIndex Information...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Re-Ranking For Better Generation Rec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Re-Ranking For Better Generation Rec...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Generator-Enhanced Retrieval Recipe ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Generator-Enhanced Retrieval Recipe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Iterative Retrieval-Generator Recipe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Iterative Retrieval-Generator Recipe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Bridging the Language Gap in Programming: Intro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Bridging the Language Gap in Programming: Intro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You will need a  GitHub Personal Access Token  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You will need a  GitHub Personal Access Token  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Recognizing this, we‚Äôve integrated a robust sys...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Recognizing this, we‚Äôve integrated a robust sys...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì09\n",
      "Hola, LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì09\n",
      "Hola, LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Multimodal ReAct Agent : Launch of an agent cap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Multimodal ReAct Agent : Launch of an agent cap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . We have integrated with Pathway...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . We have integrated with Pathway...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Guide  to building advanced RAG CHATBOT with NV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Guide  to building advanced RAG CHATBOT with NV...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Free Advanced RAG Certification course with Act...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Free Advanced RAG Certification course with Act...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Combine cutting-edge NLP and computer vision te...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Combine cutting-edge NLP and computer vision te...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Here‚Äôs a brief introduction to the course by Lo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Here‚Äôs a brief introduction to the course by Lo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: AI Voice Assistant: Enhancing Accessibility in ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: AI Voice Assistant: Enhancing Accessibility in ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The backend, on the other hand, is deployed on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The backend, on the other hand, is deployed on ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: current . interimResults  =  false ;\n",
      "\n",
      "      wak...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: current . interimResults  =  false ;\n",
      "\n",
      "      wak...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Handling User Speech and Response toggleRecordi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Handling User Speech and Response toggleRecordi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: error ( \"Error communicating with LLM:\" , error...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: error ( \"Error communicating with LLM:\" , error...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Follow the prompts to set up a Python FastAPI b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Follow the prompts to set up a Python FastAPI b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building Multi-Tenancy RAG System with LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building Multi-Tenancy RAG System with LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: By implementing this approach, we effectively p...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: By implementing this approach, we effectively p...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # For user Jerry \n",
      " for  document  in  documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # For user Jerry \n",
      " for  document  in  documents...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LLMCompiler consists of three key components: a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LLMCompiler consists of three key components: a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building a Slack bot that learns with LlamaInde...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building a Slack bot that learns with LlamaInde...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Otherwise we'll do nothing. @flask_app.route( \"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Otherwise we'll do nothing. @flask_app.route( \"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Click the ‚ÄúPermissions‚Äù link in the bottom righ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Click the ‚ÄúPermissions‚Äù link in the bottom righ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: SLACK_SIGNING_SECRET : you can find this in the...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: SLACK_SIGNING_SECRET : you can find this in the...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Let‚Äôs try it out! Stop running  1_flask.py  and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Let‚Äôs try it out! Stop running  1_flask.py  and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: @app.message() \n",
      " def   reply ( message, say ):\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: @app.message() \n",
      " def   reply ( message, say ):\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Real documents can be huge blocks of text, whol...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Real documents can be huge blocks of text, whol...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Initialize your index while you're at it: from ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Initialize your index while you're at it: from ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Your bot now survives reboots, and remembers th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Your bot now survives reboots, and remembers th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To do that we are going to stop inserting  Docu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To do that we are going to stop inserting  Docu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \\n\" \n",
      "     \"Given the most relevant chat message...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: \\n\" \n",
      "     \"Given the most relevant chat message...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Step 8: draw the rest of the owl This bot is wo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Step 8: draw the rest of the owl This bot is wo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Connect it to the repo on GitHub you just creat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Connect it to the repo on GitHub you just creat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: How to train a custom GPT on your data with Emb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: How to train a custom GPT on your data with Emb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The steps to do this are: Extract all the URLs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The steps to do this are: Extract all the URLs ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The steps are: Find your Channel ID Install  sc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The steps are: Find your Channel ID Install  sc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying over tabular data in EmbedAI is a majo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Querying over tabular data in EmbedAI is a majo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Transforming Natural Language to SQL and Insigh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This powerful tool translates natural language ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This powerful tool translates natural language ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: StreamlitChatPack Class The  StreamlitChatPack ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: StreamlitChatPack Class The  StreamlitChatPack ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Sidebar for database schema viewer \n",
      "st.sideba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Sidebar for database schema viewer \n",
      "st.sideba...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is where the app sets up the engine respon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is where the app sets up the engine respon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It embodies the potential of AI in simplifying ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It embodies the potential of AI in simplifying ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building Scalable RAG Applications with LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building Scalable RAG Applications with LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It hides the technical complexity behind a few ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It hides the technical complexity behind a few ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: zcp_index.insert_doc_url(\n",
      "    url= \"https://pub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: zcp_index.insert_doc_url(\n",
      "    url= \"https://pub...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # The chatbot will only answer with the retriev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # The chatbot will only answer with the retriev...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing the LlamaIndex retrieval-augmented ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing the LlamaIndex retrieval-augmented ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This approach overcomes the limitations of fine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This approach overcomes the limitations of fine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tonic Validate x LlamaIndex: Implementing integ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tonic Validate x LlamaIndex: Implementing integ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this case, LlamaIndex provides a tool called...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this case, LlamaIndex provides a tool called...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Then you can run the following commands in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Then you can run the following commands in the ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Setting up Tonic Validate To set up Tonic Valid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Setting up Tonic Validate To set up Tonic Valid...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Now, let‚Äôs create a function that gets the list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Now, let‚Äôs create a function that gets the list...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Let‚Äôs score them: def   score_run ( questions, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Let‚Äôs score them: def   score_run ( questions, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Once you sign up, you will be taken through a s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Once you sign up, you will be taken through a s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: One approach, the one that we recommend, for ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: One approach, the one that we recommend, for ad...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this file, we will include the following cod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this file, we will include the following cod...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The GitHub Actions configuration downloads the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The GitHub Actions configuration downloads the ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì30\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì30\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚ú® Feature Releases and Enhancements: We have la...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚ú® Feature Releases and Enhancements: We have la...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Twitter . üó∫Ô∏è Guides: Guide  to Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Twitter . üó∫Ô∏è Guides: Guide  to Building...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The system prompt for the meta-agent /top-agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The system prompt for the meta-agent /top-agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚≠êÔ∏è Follow me on  LinkedIn  for updates on Large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚≠êÔ∏è Follow me on  LinkedIn  for updates on Large...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building a Fully Open Source Retriever with Nom...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building a Fully Open Source Retriever with Nom...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: How To To build an open source retriever with L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: How To To build an open source retriever with L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Using LlamaIndex, this is as simple as a few li...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Using LlamaIndex, this is as simple as a few li...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It was like a mini Bond villain 's lair down th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It was like a mini Bond villain 's lair down th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì02\n",
      "Hello, Llama L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì02\n",
      "Hello, Llama L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . We have implemented the LLMComp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . We have implemented the LLMComp...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It involves extracting and summarizing LeetCode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It involves extracting and summarizing LeetCode...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This involves comparing LLM judge predictions (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This involves comparing LLM judge predictions (...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Interested?  Get in touch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Interested?  Get in touch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì02‚Äì06\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì02‚Äì06\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet . üó∫Ô∏è Guides: Guide  to Building a Fully O...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet . üó∫Ô∏è Guides: Guide  to Building a Fully O...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We are working hard to make LlamaIndex, even mo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We are working hard to make LlamaIndex, even mo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: RAGArch: Building a No-Code RAG Pipeline Config...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: RAGArch: Building a No-Code RAG Pipeline Config...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tools and Technologies The creation of RAGArch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tools and Technologies The creation of RAGArch ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Total documents loaded:  { len (loaded_file)} \"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Total documents loaded:  { len (loaded_file)} \"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You can choose from Google‚Äôs Gemini Pro, Cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You can choose from Google‚Äôs Gemini Pro, Cohere...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: header ( \"Choose Embedding Model\" )\n",
      "    col1, c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: header ( \"Choose Embedding Model\" )\n",
      "    col1, c...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I have included some of the most commonly used ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I have included some of the most commonly used ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: number_input( \"Chunk Lines Overlap\" , min_value...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: number_input( \"Chunk Lines Overlap\" , min_value...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \" )\n",
      "             return   None ,  None \n",
      "       ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: \" )\n",
      "             return   None ,  None \n",
      "       ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I have included varioud response synthesis meth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I have included varioud response synthesis meth...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   select_vector_store ():\n",
      "    st.header( \"C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   select_vector_store ():\n",
      "    st.header( \"C...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   generate_rag_pipeline ( file, llm, embed_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   generate_rag_pipeline ( file, llm, embed_...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: node_parser import SentenceSplitter, CodeSplitt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: node_parser import SentenceSplitter, CodeSplitt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: chunk_overlap= {node_parser_params.get( 'chunk_...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: chunk_overlap= {node_parser_params.get( 'chunk_...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \"MarkdownNodeParser\" :  \"MarkdownNodeParser()\" ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: \"MarkdownNodeParser\" :  \"MarkdownNodeParser()\" ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: embed_model=embed_model, node_parser=node_parse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: embed_model=embed_model, node_parser=node_parse...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex v0.10\n",
      "Today we‚Äôre excited to launch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex v0.10\n",
      "Today we‚Äôre excited to launch ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This includes data loaders, LLMs, embedding mod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This includes data loaders, LLMs, embedding mod...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: There are 19 folders in here. The main integrat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: There are 19 folders in here. The main integrat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: pip install llama-index-readers-notion from  ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: pip install llama-index-readers-notion from  ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This expands beyond its existing domain of load...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This expands beyond its existing domain of load...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Passing in an entire  service_context  containe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Passing in an entire  service_context  containe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: See our usage example below as well. Usage Exam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: See our usage example below as well. Usage Exam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Migration to v0.10 If you want to use LlamaInde...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Migration to v0.10 If you want to use LlamaInde...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Query Pipelines\n",
      "Today we introduce ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Query Pipelines\n",
      "Today we introduce ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Source: ‚ÄúAdvanced RAG Techniques: an Illustrate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Source: ‚ÄúAdvanced RAG Techniques: an Illustrate...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: [In the future] Caching:  This interface also a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: [In the future] Caching:  This interface also a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.postprocessor  import  Cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.postprocessor  import  Cohere...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Using the previous example, output = p.run(topi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Using the previous example, output = p.run(topi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: FAQ What‚Äôs the difference between a  QueryPipel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: FAQ What‚Äôs the difference between a  QueryPipel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Pioneering the Future of Housing: Introducing G...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Pioneering the Future of Housing: Introducing G...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôre just getting started, and this recognitio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôre just getting started, and this recognitio...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Flowchart Flowchart Code Samples Querying PDFs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Flowchart Flowchart Code Samples Querying PDFs ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "\n",
      "\n",
      "&gt;&gt;&gt;  'For detached ADUs in Saratog...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "\n",
      "\n",
      "&gt;&gt;&gt;  'For detached ADUs in Saratog...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Please analyze the image, describle the layout,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Please analyze the image, describle the layout,...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Please note that without a broader view  or  ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Please note that without a broader view  or  ad...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì02‚Äì13\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì02‚Äì13\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Utilize Mistral-medium for enhanced RAG functio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Utilize Mistral-medium for enhanced RAG functio...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Krish Naik   tutorial  on Step-by-Step Guide to...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Krish Naik   tutorial  on Step-by-Step Guide to...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì16\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì16\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaPack ,  Tweet . We launched LLM Self-Consi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaPack ,  Tweet . We launched LLM Self-Consi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: NVIDIA  tutorial  on Building AI apps with loca...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: NVIDIA  tutorial  on Building AI apps with loca...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì23\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì23\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Long-Context Embedding Models:  Explore models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Long-Context Embedding Models:  Explore models ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Guide  to a Five-Part Series on Building a Full...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Guide  to a Five-Part Series on Building a Full...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì02‚Äì20: introducing L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì02‚Äì20: introducing L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  LlamaPack . SELF-DISCOVER LlamaPack : ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  LlamaPack . SELF-DISCOVER LlamaPack : ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Guide  to creating a RAG-powered research agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Guide  to creating a RAG-powered research agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing LlamaCloud and LlamaParse\n",
      "Today is ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing LlamaCloud and LlamaParse\n",
      "Today is ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This enables downstream orchestration of retrie...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This enables downstream orchestration of retrie...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaParse Demo. Given a PDF file, returns a pa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaParse Demo. Given a PDF file, returns a pa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_parse  import  LlamaParse\n",
      "\n",
      "parser =...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_parse  import  LlamaParse\n",
      "\n",
      "parser =...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Playground:  Interactive UI to test and refine ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Playground:  Interactive UI to test and refine ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: By incorporating LlamaIndex into RAGStack, we a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: By incorporating LlamaIndex into RAGStack, we a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Next Steps As mentioned in the above sections, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Next Steps As mentioned in the above sections, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì02‚Äì27\n",
      "Yo, LlamaIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024‚Äì02‚Äì27\n",
      "Yo, LlamaIndex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . Gemma Cookbook:  A comprehensiv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . Gemma Cookbook:  A comprehensiv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: create-llama ,  Tweet . üé• Demos: Counselor Copi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: create-llama ,  Tweet . üé• Demos: Counselor Copi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Unlocking the 3rd Dimension for Generative AI (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Unlocking the 3rd Dimension for Generative AI (...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Unlike traditional graphical CAD interfaces, wh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Unlike traditional graphical CAD interfaces, wh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For those familiar with the whirlwind nature of...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For those familiar with the whirlwind nature of...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Examples of neThing.xyz in Action: \n",
      " Text: \n",
      " \n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Examples of neThing.xyz in Action: \n",
      " Text: \n",
      " \n",
      " ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Towards Long Context RAG \n",
      "Google recently relea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Towards Long Context RAG \n",
      "Google recently relea...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: But we‚Äôre also excited about Gemini Pro, and we...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: But we‚Äôre also excited about Gemini Pro, and we...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: There are some parts where we noticed Gemini st...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: There are some parts where we noticed Gemini st...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: For these use cases, developers will no longer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: For these use cases, developers will no longer ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Nevertheless, stuffing a 1M context window take...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Nevertheless, stuffing a 1M context window take...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: On the other hand, we‚Äôve found that embedding m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: On the other hand, we‚Äôve found that embedding m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: At a high-level, a KV cache stores activations ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: At a high-level, a KV cache stores activations ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-05\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-03-05\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: keyword search is tuned on a per-query basis by...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: keyword search is tuned on a per-query basis by...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Check out this webinar in which we invited the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Check out this webinar in which we invited the ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex: Enhancing Retrieval Performance wit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex: Enhancing Retrieval Performance wit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Concept Seeking Queries:  Abstract questions th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Concept Seeking Queries:  Abstract questions th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The remainder of this blog post is divided into...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The remainder of this blog post is divided into...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   get_weaviate_client ( api_key, url ):\n",
      "  a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   get_weaviate_client ( api_key, url ):\n",
      "  a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: documents1 = load_documents(\"llm_compiler.pdf\",...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: documents1 = load_documents(\"llm_compiler.pdf\",...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: self._vector_retriever = vector_retriever\n",
      "     ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: self._vector_retriever = vector_retriever\n",
      "     ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Alpha values and datasets to test \n",
      "alpha_valu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Alpha values and datasets to test \n",
      "alpha_valu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: keys():\n",
      "         for  alpha  in  alpha_values:\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: keys():\n",
      "         for  alpha  in  alpha_values:\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: delaxes(axes[- 1 , - 1 ])   # Remove the last s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: delaxes(axes[- 1 , - 1 ])   # Remove the last s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Concept Seeking Queries: ‚Äî MRR and Hit Rate are...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Concept Seeking Queries: ‚Äî MRR and Hit Rate are...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Should you come across any noteworthy observati...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Should you come across any noteworthy observati...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: PII Detector: hacking privacy in RAG\n",
      "A couple o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: PII Detector: hacking privacy in RAG\n",
      "A couple o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Model Language models, are trained on large dat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Model Language models, are trained on large dat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It‚Äôs customizable, so you can create your own a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It‚Äôs customizable, so you can create your own a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: What's your last name? [PER_286], it's [PER_286...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: What's your last name? [PER_286], it's [PER_286...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I could not have that. So, I added a counter an...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I could not have that. So, I added a counter an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Have you been to a P√°lmi Einarsson concert befo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Have you been to a P√°lmi Einarsson concert befo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I was curious to see how the parsing of these s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I was curious to see how the parsing of these s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: How to build LLM Agents in TypeScript with Llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: How to build LLM Agents in TypeScript with Llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI provides us a function calling API which...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI provides us a function calling API which...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: // Sum properties to give to the LLM \n",
      " const  s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: // Sum properties to give to the LLM \n",
      " const  s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Then you should see an output as: ===  Calling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Then you should see an output as: ===  Calling ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: asQueryEngine ();\n",
      "\n",
      " // Create a QueryEngineTool...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: asQueryEngine ();\n",
      "\n",
      " // Create a QueryEngineTool...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: There is still a long way to go before they are...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: There is still a long way to go before they are...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Secure code execution in LlamaIndex with Azure ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Secure code execution in LlamaIndex with Azure ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Set up Azure Container Apps dynamic sessions Fi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Set up Azure Container Apps dynamic sessions Fi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You‚Äôre now ready to set up your agent: # Import...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You‚Äôre now ready to set up your agent: # Import...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Modifying files would not be useful if you coul...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Modifying files would not be useful if you coul...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Now, you can easily process complex documents l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Now, you can easily process complex documents l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: All parameters are required\n",
      "        \n",
      "        If...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: All parameters are required\n",
      "        \n",
      "        If...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: print (agent.chat( \"browse to the latest email ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The email also included contact information for...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The email also included contact information for...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Check out the  LlamaHub page  here. If you‚Äôre i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Check out the  LlamaHub page  here. If you‚Äôre i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Bridging the Gap in Crisis Counseling: Introduc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Bridging the Gap in Crisis Counseling: Introduc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: There is no prompting that is needed from couns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: There is no prompting that is needed from couns...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tools at the ReAct Agent‚Äôs disposal include: Es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tools at the ReAct Agent‚Äôs disposal include: Es...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As we continue to refine and expand this techno...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: As we continue to refine and expand this techno...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Moving forward, you can access the full code on...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Moving forward, you can access the full code on...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   video_to_images ( video_path, output_fold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   video_to_images ( video_path, output_fold...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: )\n",
      "         except  sr.RequestError  as  e:\n",
      "    ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: )\n",
      "         except  sr.RequestError  as  e:\n",
      "    ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index  import  (\n",
      "    SimpleDirector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index  import  (\n",
      "    SimpleDirector...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   retrieve ( retriever_engine, query_str ):...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   retrieve ( retriever_engine, query_str ):...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Below is the prompt template : qa_tmpl_str = (\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Below is the prompt template : qa_tmpl_str = (\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Gaussian Function:  Details the Gaussian functi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Gaussian Function:  Details the Gaussian functi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Querying a network of knowledge with llama-inde...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To illustrate how the llama-index-networks pack...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To illustrate how the llama-index-networks pack...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Note, that the dotenv file  .env.contributor  c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Note, that the dotenv file  .env.contributor  c...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ), the query is sent to all contributors. Their...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ), the query is sent to all contributors. Their...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Internal networks: another potential use case I...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Internal networks: another potential use case I...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Simplify your RAG application architecture with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Simplify your RAG application architecture with...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: About PostgresML PostgresML [ github  ||  websi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: About PostgresML PostgresML [ github  ||  websi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Set the PGML_DATABASE_URL environment variable:...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Set the PGML_DATABASE_URL environment variable:...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Compare this query against other common LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Compare this query against other common LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing the Property Graph Index: A Powerfu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing the Property Graph Index: A Powerfu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.indices.property_graph  impor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.indices.property_graph  impor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: index = PropertyGraphIndex(..., vector_store=ve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: index = PropertyGraphIndex(..., vector_store=ve...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  llama_index.core.indices.property_graph  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  llama_index.core.indices.property_graph  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The store supports: Inserting and updating node...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The store supports: Inserting and updating node...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Customizing property graph index in LlamaIndex\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Customizing property graph index in LlamaIndex\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this blog post you will learn how to: Constr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this blog post you will learn how to: Constr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this example, we will use the  SchemaLLMPath...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this example, we will use the  SchemaLLMPath...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Now that we have defined the graph schema, we c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Now that we have defined the graph schema, we c...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: graph_store.structured_query( \"\"\"\n",
      "CREATE VECTOR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: graph_store.structured_query( \"\"\"\n",
      "CREATE VECTOR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: size(allCombinedResults)-1, 1) as combinedResul...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: size(allCombinedResults)-1, 1) as combinedResul...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: VectorContextRetriever : retrieves nodes based ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: VectorContextRetriever : retrieves nodes based ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: def   init ( \n",
      "        self,\n",
      "         ## vector ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: def   init ( \n",
      "        self,\n",
      "         ## vector ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If any entities are found, we iterate and execu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If any entities are found, we iterate and execu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: O‚ÄôBrien expressed confidence that the dispute c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: O‚ÄôBrien expressed confidence that the dispute c...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Launching the first GenAI-native document parsi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Launching the first GenAI-native document parsi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: (You can see the full code in our  demonstratio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: (You can see the full code in our  demonstratio...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôve also added support for a large array of d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôve also added support for a large array of d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-06-25\n",
      "Hello to All L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-06-25\n",
      "Hello to All L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-12\n",
      "Salutations, L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-03-12\n",
      "Salutations, L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet . üé•¬†Demos: Build an AI Browser Copilot : ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet . üé•¬†Demos: Build an AI Browser Copilot : ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: üé•¬† Webinars: Webinar  with  Parth Sarthi , lead...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: üé•¬† Webinars: Webinar  with  Parth Sarthi , lead...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: First we‚Äôll bring in our dependencies and set u...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: First we‚Äôll bring in our dependencies and set u...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Set up the launcher for local testing \n",
      " from ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Set up the launcher for local testing \n",
      " from ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: tool = FunctionTool.from_defaults(fn=get_the_se...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: tool = FunctionTool.from_defaults(fn=get_the_se...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You can see both of the agents running, and at ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You can see both of the agents running, and at ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: His name is Peter.\" )]\n",
      "index = VectorStoreIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: His name is Peter.\" )]\n",
      "index = VectorStoreIndex...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: input ,\n",
      "    )\n",
      "    query_engine = RetrieverQuery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: input ,\n",
      "    )\n",
      "    query_engine = RetrieverQuery...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Check out  the repo  to learn more, especially ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Check out  the repo  to learn more, especially ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-07-02\n",
      "Hello, Llama e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-07-02\n",
      "Hello, Llama e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Cookbook ,  Tweet . üí°¬†Demos: Automating Code Re...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Cookbook ,  Tweet . üí°¬†Demos: Automating Code Re...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaCloud - Built for Enterprise LLM App Build...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaCloud - Built for Enterprise LLM App Build...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Its ability to preserve nested tables, extract ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Its ability to preserve nested tables, extract ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Try it yourself We‚Äôve opened up an official wai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Try it yourself We‚Äôve opened up an official wai...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Batch inference with MyMagic AI and LlamaIndex\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Batch inference with MyMagic AI and LlamaIndex\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: import  asyncio\n",
      " from  llama_index.llms.mymagic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: import  asyncio\n",
      " from  llama_index.llms.mymagic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: However, we are planning to add those methods i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: However, we are planning to add those methods i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Please ensure the following prerequisites are i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Please ensure the following prerequisites are i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: \",\n",
      "    \"output\": \"Petter Mattei's 'Love in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: \",\n",
      "    \"output\": \"Petter Mattei's 'Love in the ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Bring in our dependencies: import  os\n",
      "\n",
      " from  l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Bring in our dependencies: import  os\n",
      "\n",
      " from  l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-05-28\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-05-28\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to building a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to building a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Case study: Lyzr: Taking autonomous AI agents t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Case study: Lyzr: Taking autonomous AI agents t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: 3. Scalability : The flexibility provided by Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: 3. Scalability : The flexibility provided by Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: By continuing to use LlamaIndex as their RAG pa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: By continuing to use LlamaIndex as their RAG pa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Arize AI and LlamaIndex Roll Out Joint Platform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Arize AI and LlamaIndex Roll Out Joint Platform...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚ÄúAs leaders in our respective spaces with a com...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚ÄúAs leaders in our respective spaces with a com...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-06-11\n",
      "Hello Llama Fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-06-11\n",
      "Hello Llama Fa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Notebook . üó∫Ô∏è Guides: Guide  to Integrating Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Notebook . üó∫Ô∏è Guides: Guide  to Integrating Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Pavan Mantha 's  tutorial  on securing RAG apps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Pavan Mantha 's  tutorial  on securing RAG apps...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-06-18\n",
      "Hey Llama Foll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-06-18\n",
      "Hey Llama Foll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaPack ,  Tweet . PingCap  has integrated th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaPack ,  Tweet . PingCap  has integrated th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Kingzzm ‚Äôs  tutorial  on Advanced RAG Patterns ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Kingzzm ‚Äôs  tutorial  on Advanced RAG Patterns ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-06-04\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-06-04\n",
      "Hello, LlamaIn...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: üìπ¬†Webinar: Webinar  with authors of memary - Ju...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: üìπ¬†Webinar: Webinar  with authors of memary - Ju...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-07-09\n",
      "Hello, Llama L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-07-09\n",
      "Hello, Llama L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Notebook ,  Tweet . [ Yi-01.AI ]( http://Yi-01....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Notebook ,  Tweet . [ Yi-01.AI ]( http://Yi-01....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Ross A.‚Äôs  tutorial  on retrieval evaluations f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Ross A.‚Äôs  tutorial  on retrieval evaluations f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-07-16\n",
      "Hello, Llama F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-07-16\n",
      "Hello, Llama F...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Blogpost ,  Tweet . We have launched an impleme...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Blogpost ,  Tweet . We have launched an impleme...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚úçÔ∏è Tutorials: 1LittleCoder‚Äôs   video tutorial  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚úçÔ∏è Tutorials: 1LittleCoder‚Äôs   video tutorial  ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Case Study: How Scaleport.ai Accelerated Develo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Case Study: How Scaleport.ai Accelerated Develo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Before LlamaCloud, building even a simple appli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Before LlamaCloud, building even a simple appli...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the past each developer would spend time re-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the past each developer would spend time re-...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Building a multi-agent concierge system\n",
      "Why bui...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Building a multi-agent concierge system\n",
      "Why bui...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Instead, it looks at what the user is currently...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Instead, it looks at what the user is currently...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Could you please provide your username and pass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Could you please provide your username and pass...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Another agent will assist you with transferring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Another agent will assist you with transferring...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The continuation agent sees that there are no f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The continuation agent sees that there are no f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: If there is no current_speaker value, look at t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: If there is no current_speaker value, look at t...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This is when the state object gets passed to ea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This is when the state object gets passed to ea...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This triggers the outer loop to run the continu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This triggers the outer loop to run the continu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Improving Vector Search - Reranking with Postgr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Improving Vector Search - Reranking with Postgr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Install the required dependencies to get starte...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Install the required dependencies to get starte...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: documents = SimpleDirectoryReader( \"data\" ).loa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: documents = SimpleDirectoryReader( \"data\" ).loa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This had been possible in principle since 1993,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This had been possible in principle since 1993,...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: I was nervous about money, because I could sens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: I was nervous about money, because I could sens...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This means our initial semantic search will ret...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This means our initial semantic search will ret...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In this dissatisfied state I went in 1988 to vi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In this dissatisfied state I went in 1988 to vi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We can use re-ranking directly in RAG: query_en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We can use re-ranking directly in RAG: query_en...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.49it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.74it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.85it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.03s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.85it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.40it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.62it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.66it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.02s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.90it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.16it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.92it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.45it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.52it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.08it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.60it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.76it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:06<00:00, 25.35s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.60it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.17it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.78it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.95it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.09s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.66it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.27it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.40it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.34it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.92it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.38it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.53it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.96it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.93it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.49it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.16it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.77it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.68it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.80it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.50it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.78it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.60it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.27it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:05<00:00,  1.13s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.33it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.42it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.98it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.01it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.82it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.34it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.58it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.00s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.73it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.77it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.51it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.72it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.30it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.08it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.70it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.87it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.30it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.93it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.59it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.87it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.85it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.30it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.60it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.17it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.23it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.03it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.51it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.02it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.21it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.69it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.16it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.87it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  1.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.00it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.57it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.63it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.36it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.68it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.00it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.53it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.91it/s]\n",
      "\u001b[32m2024-07-24 19:32:27.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mIndexing 160 into VectorStoreIndex took 843s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = None\n",
    "    # Currently setting num_workers leads to error `AttributeError: 'HuggingFaceEmbedding' object has no attribute '_model'`\n",
    "    # num_workers = os.cpu_count() - 1\n",
    "    # logger.info(f\"Running Ingestion Pipeline with {num_workers=}...\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae9a042-75d5-4b9a-8dec-42f75c86c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 19:32:27.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexing 160 into VectorStoreIndex took 843s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_nodes\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f72b3d52-7be9-49e1-bf4e-4cd586635d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5a5fb8a-636a-463c-a938-1864cff81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "COLLECTION = 'togetherai'\n",
    "NOTEBOOK_CACHE_DP = 'data/001/togetherai'\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4a1d0d1-142d-4e7a-b626-ef154ba08e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:16:44.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mUse existing ChromaDB collection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = chromadb.PersistentClient(path=f\"{NOTEBOOK_CACHE_DP}/chroma_db\")\n",
    "collection_exists = COLLECTION in [c.name for c in db.list_collections()]\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    logger.info(f\"Creating new ChromaDB collection...\")\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing ChromaDB collection...\")\n",
    "        db.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "else:\n",
    "    logger.info(f\"Use existing ChromaDB collection\")\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ecdec51-2720-4628-a43f-caaa98310630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baa0a955-fbdf-4029-8e41-2efdea6ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0f36679-603e-40d6-862b-5d9313b3c1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:16:44.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mLoading index from existing ChromaDB...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if chroma_collection.count() > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing ChromaDB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new ChromaDB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embedding,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "    \n",
    "    # Need to use await and arun here to run the pipeline else error\n",
    "    # Ref: https://docs.llamaindex.ai/en/stable/examples/ingestion/async_ingestion_pipeline/\n",
    "    # Ref: https://github.com/run-llama/llama_index/issues/13904#issuecomment-2145561710\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a338d-469e-4461-9511-553dd6c1f766",
   "metadata": {},
   "source": [
    "#### Inspect nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f68dde-bd70-49b2-a62b-40a527fc44ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# embeddings are excluded by default for performance, if need then explicitly ask for it in `include`\n",
    "# chroma_collection.get(include=['embeddings'])\n",
    "chroma_collection.get()['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40da3529-cf6a-41ce-853e-1526faf2e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.append_reference.custom_query_engine import ManualAppendReferenceQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 5\n",
    "RERANK_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "# APPEND_REF_MODE = 'response_synthesizer'\n",
    "APPEND_REF_MODE = 'query_engine'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    mlflow.log_param(\"RERANK_TOP_K\", RERANK_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "reranker = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=RERANK_TOP_K)\n",
    "node_postprocessors.append(reranker)\n",
    "\n",
    "if APPEND_REF_MODE == 'response_synthesizer':\n",
    "    response_synthesizer = ManualAppendReferenceSynthesizer(verbose=0)\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "elif APPEND_REF_MODE == 'query_engine':\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = ManualAppendReferenceQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "else:\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"reranker\", repr(reranker))\n",
    "    mlflow.log_param(\"response_synthesizer\", repr(response_synthesizer))\n",
    "    mlflow.log_param(\"query_engine\", repr(query_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63bb5789-dde7-465c-86c2-94aadbf5b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 1f8a95ea-06cc-4e05-b71b-3c3145d559d4] [Similarity score:             0.694795] \" )\n",
      "             return   None ,  None \n",
      "        \n",
      "        embed_model = st.session_state[ 'embed_m...\n",
      "> [Node ae8f1af1-9df3-48ce-8523-8050161bed54] [Similarity score:             0.692144] I have included varioud response synthesis methods supported by Llamaindex including  refine ,  t...\n",
      "> [Node b9ae64b7-7f37-4523-ae89-7b2c9c6da607] [Similarity score:             0.692125] def   generate_rag_pipeline ( file, llm, embed_model, node_parser, response_mode, vector_store ):...\n",
      "> [Node cb7da65e-0eae-42f1-81a6-1a50c6e0e9a7] [Similarity score:             0.691156] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> [Node e199f568-edb2-42d7-bc2c-fbb1fef8c779] [Similarity score:             0.690548] def   select_vector_store ():\n",
      "    st.header( \"Choose Vector Store\" )\n",
      "    vector_stores = [ \"Simpl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 1f8a95ea-06cc-4e05-b71b-3c3145d559d4] [Similarity score:             0.694795] \" )\n",
      "             return   None ,  None \n",
      "        \n",
      "        embed_model = st.session_state[ 'embed_m...\n",
      "> [Node ae8f1af1-9df3-48ce-8523-8050161bed54] [Similarity score:             0.692144] I have included varioud response synthesis methods supported by Llamaindex including  refine ,  t...\n",
      "> [Node b9ae64b7-7f37-4523-ae89-7b2c9c6da607] [Similarity score:             0.692125] def   generate_rag_pipeline ( file, llm, embed_model, node_parser, response_mode, vector_store ):...\n",
      "> [Node cb7da65e-0eae-42f1-81a6-1a50c6e0e9a7] [Similarity score:             0.691156] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> [Node e199f568-edb2-42d7-bc2c-fbb1fef8c779] [Similarity score:             0.690548] def   select_vector_store ():\n",
      "    st.header( \"Choose Vector Store\" )\n",
      "    vector_stores = [ \"Simpl...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To address points of failures in RAG pipeline, RAGArch provides a robust and intuitive interface for testing and debugging. The tool's live testing feature allows you to instantly test your RAG pipeline with your own data, enabling you to identify and troubleshoot any issues that may arise. Additionally, the code generation feature enables you to generate Python code for your custom RAG pipeline, making it easier to integrate and test your pipeline in your application. By leveraging LlamaIndex's powerful LLM orchestration capabilities, RAGArch provides a seamless experience and granular control over your RAG pipeline, allowing you to fine-tune and optimize your pipeline to achieve the desired results.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex](https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** cb7da65e-0eae-42f1-81a6-1a50c6e0e9a7<br>**Similarity:** -5.265542507171631<br>**Text:** RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...<br>**Metadata:** {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'author': 'Harshad Suryawanshi', 'date': 'Feb 2, 2024', 'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation', 'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** b9ae64b7-7f37-4523-ae89-7b2c9c6da607<br>**Similarity:** -5.767147541046143<br>**Text:** def   generate_rag_pipeline ( file, llm, embed_model, node_parser, response_mode, vector_store ):...<br>**Metadata:** {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'author': 'Harshad Suryawanshi', 'date': 'Feb 2, 2024', 'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation', 'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'cb7da65e-0eae-42f1-81a6-1a50c6e0e9a7': {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex',\n",
       "  'author': 'Harshad Suryawanshi',\n",
       "  'date': 'Feb 2, 2024',\n",
       "  'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation',\n",
       "  'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089',\n",
       "  'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'},\n",
       " 'b9ae64b7-7f37-4523-ae89-7b2c9c6da607': {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex',\n",
       "  'author': 'Harshad Suryawanshi',\n",
       "  'date': 'Feb 2, 2024',\n",
       "  'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation',\n",
       "  'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089',\n",
       "  'document_title': 'Based on the provided context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool for Streamlined AI Development, File Handling, and Large Language Model Selection for Natural Language Processing\"\\n\\nThis title captures the main entities and themes present in the context, including:\\n\\n* RAGArch: the tool itself\\n* No-code pipeline configuration\\n* One-click code generation\\n* AI development\\n* File handling\\n* Large Language Model selection\\n* Natural Language Processing (NLP)\\n\\nThis title provides a clear and concise overview of the document\\'s content, making it easy for readers to understand the main topics and themes discussed.'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can we address points of failures in RAG pipeline?\"\n",
    "response = query_engine.query(question)\n",
    "display_response(response, show_source=True, show_metadata=True, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_RETRIEVAL_EVAL_DATASET = True\n",
    "# Currently can not reuse retrieval_eval_dataset because the retrieval evaluation is based on ids\n",
    "# RETRIEVAL_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = min(len(nodes), RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "RETRIEVAL_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RETRIEVAL_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 22:54:51.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mSampling 10 nodes for retrieval evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes\n",
    "else:\n",
    "    logger.info(f\"Loading retrieval_eval_nodes from {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    with open(RETRIEVAL_EVAL_DATASET_FP, 'r') as f:\n",
    "        retrieval_eval_nodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 22:54:53.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                                          | 4/10 [00:06<00:08,  1.48s/it]/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/llama_index/core/llama_dataset/legacy/embedding.py:99: UserWarning: Fewer questions generated (1) than requested (2).\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:15<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    retrieval_eval_llm = OpenAI(model=RETRIEVAL_EVAL_LLM_MODEL, **RETRIEVAL_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes, llm=retrieval_eval_llm, num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef47844c-5787-458e-9860-4c1f6a0f1935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'91843b68-ac1a-4878-8df4-9a82f0fe9408': 'Explain the process of setting up custom query engines for a ComposableGraph using the provided code snippets. How does the TransformQueryEngine and DecomposeQueryTransform contribute to this setup?',\n",
       " '469bd344-561b-4c25-9f49-f9a5da7dd0eb': 'Compare and contrast the functionality of the GPT-3 ReAct agent, GPT-4 ReAct agent, and Simple Router agent in the context of querying a ComposableGraph. How do the tools used by each agent differ in their approach to accessing information from the graph?',\n",
       " 'f7f7c25a-d3f8-49fe-98de-a19a5a430741': 'How does the process of Named Entity Recognition (NER) contribute to the personalization of news recommendations in NewsGPT? Explain the role of the dslim/bert-base-NER model from HuggingFace in this process.',\n",
       " '99b6df62-f04b-4475-a166-a5ce919485a7': 'Describe the recommendation pipeline for personalization in NewsGPT after users sign up and log in. Include details on how user preferences and activities are utilized to generate personalized articles, as well as the role of the recommendation API hosted on AWS Lambda.',\n",
       " '93d58c4a-54bc-46ef-af07-fb7b673e71fa': 'How can incremental syncs be utilized in updating an existing vector database effectively? Provide an example using the provided code snippet.',\n",
       " '278aa89b-72bf-4c25-9a56-b158309b32ca': 'How can the mapping of Airbyte records to LlamaIndex documents be customized using a record handler? Explain the default behavior and how it can be modified to suit specific data requirements.',\n",
       " '89c745c8-a8b4-4e4f-93f4-520363d2754f': \"What are the necessary scopes that need to be added in the Slack app in order to see channels, join channels, send messages, and view people's names?\",\n",
       " '5ac78ea0-6907-4ebd-83d4-1f31229f558b': 'How can a Slack app receive messages and join a channel, according to the instructions provided in the document?',\n",
       " '37069ddf-fca5-4551-9bd8-b90307156b86': 'The context provided is about using Anthropic LLM to generate question-context pairs for evaluating embedding and reranker models. The goal is to create diverse questions for an upcoming quiz/examination based on the given context information. The prompt template instructs the user to generate questions without options, starting with Q1/Q2, and to ensure the questions are relevant to the provided context. Additionally, there is a function mentioned to filter out certain phrases from the generated questions dataset.',\n",
       " 'f39e68bd-4731-42d8-8d33-933f41da335b': 'In terms of mAP@L, how does the performance of Mistral compare to Llama 2 across different model sizes as shown in the graphs?',\n",
       " 'e265e54b-162f-4dbb-93d9-42ffc6b09e64': 'What can be inferred about the performance of Mistral and Llama 2 in the Reason@L metric based on the data provided in the graphs?',\n",
       " 'e5d867ab-3851-4fde-97cb-eb708f2347ab': 'How did Tali.AI contribute to the future of support roles at the Augment hackathon?',\n",
       " '195b8b6b-9bb7-42e4-9102-4e18229ba8c7': 'How does SuperAGI integrate with LlamaIndex to process a wide variety of data from structured and unstructured sources?',\n",
       " 'b661dc27-7447-49f6-be55-041dc1f2afe3': 'How does the LlamaIndex system aim to simplify the knowledge transfer process in IT and Software Development?',\n",
       " '6b0d666c-e82d-4e3c-9d07-9ab1a4e007a6': 'Can you explain the four stages of the solution proposed by Vibhav and the author for organizing knowledge transfer sessions using LlamaIndex and D-ID?',\n",
       " '35b7e19b-ab98-4a3c-a1f7-18a7e93d7941': 'How does the IngestionPipeline in the provided context transform documents before ingesting them into the vector database?',\n",
       " '8599c044-c126-4ea8-9f02-74257594d9cd': 'Can you explain the process of creating an index using the VectorStoreIndex in the context of the given code snippet?',\n",
       " '8bdde93d-d3aa-46dc-99ba-dda0c2f2b144': 'Explain the difference in speed between LLM-based retrieval and embedding-based retrieval, and discuss the potential drawbacks of using LLM for retrieval tasks.',\n",
       " '62349442-b5e5-49b3-b66f-dc87f05f03b4': 'Compare and contrast the results of naive top-k embedding-based retrieval with the two-stage retrieval pipeline involving LLM reranking. How does the use of LLM as a second-stage reranking step impact the overall retrieval performance?'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71cd5482-b261-42a0-b822-af1f1303bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 378f3d9c-b23a-48cd-8a98-9cd5f69b9ddb] [Similarity score:             0.69874] Using the previous example, output = p.run(topic=\"YC\")\n",
      "# output type is Response\n",
      "type(output) If ...\n",
      "> [Node 0ff41e8b-9bfc-47b7-abd3-0096fd05d382] [Similarity score:             0.694699] If any entities are found, we iterate and execute the vector context retriever for each entity. O...\n",
      "> [Node f8126886-be98-4eba-91a6-e198e64e7fd2] [Similarity score:             0.689692] graph_store.structured_query( \"\"\"\n",
      "CREATE VECTOR INDEX entity IF NOT EXISTS\n",
      "FOR (m:`__Entity__`)\n",
      "O...\n",
      "> [Node 3cbfcd7d-bdc8-45f4-80f7-3db9245da480] [Similarity score:             0.689627] def   init ( \n",
      "        self,\n",
      "         ## vector context retriever params \n",
      "        embed_model:  Op...\n",
      "> [Node d295f976-4218-46ca-8b10-4eb9d415f401] [Similarity score:             0.688716] Customizing property graph index in LlamaIndex\n",
      "Learn how to implement entity deduplication and cu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 378f3d9c-b23a-48cd-8a98-9cd5f69b9ddb] [Similarity score:             0.69874] Using the previous example, output = p.run(topic=\"YC\")\n",
      "# output type is Response\n",
      "type(output) If ...\n",
      "> [Node 0ff41e8b-9bfc-47b7-abd3-0096fd05d382] [Similarity score:             0.694699] If any entities are found, we iterate and execute the vector context retriever for each entity. O...\n",
      "> [Node f8126886-be98-4eba-91a6-e198e64e7fd2] [Similarity score:             0.689692] graph_store.structured_query( \"\"\"\n",
      "CREATE VECTOR INDEX entity IF NOT EXISTS\n",
      "FOR (m:`__Entity__`)\n",
      "O...\n",
      "> [Node 3cbfcd7d-bdc8-45f4-80f7-3db9245da480] [Similarity score:             0.689627] def   init ( \n",
      "        self,\n",
      "         ## vector context retriever params \n",
      "        embed_model:  Op...\n",
      "> [Node d295f976-4218-46ca-8b10-4eb9d415f401] [Similarity score:             0.688716] Customizing property graph index in LlamaIndex\n",
      "Learn how to implement entity deduplication and cu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 8762af05-573e-4218-8c37-61c103ff5c50] [Similarity score:             0.741513] agent_chain.run(input=\"How much cash did Uber have in sept 2022?\") We see that the agent makes tw...\n",
      "> [Node b121f089-28e9-4bb6-bf18-35bde674ac74] [Similarity score:             0.734254] High-Level Findings The high-level finding is that  less sophisticated agents need more constrain...\n",
      "> [Node d7b1f3dc-479b-45f5-af9b-e6c4afcffede] [Similarity score:             0.728069] )\n",
      "query_tool_graph = QueryEngineTool.from_defaults(\n",
      "    query_engine=g_engine,\n",
      "    description=f\"...\n",
      "> [Node 0118fb96-b70f-4363-83ed-f5161c3672c2] [Similarity score:             0.715617] # define indices\n",
      "march_index = GPTVectorStoreIndex.from_documents(march_2022)\n",
      "june_index = GPTVec...\n",
      "> [Node d42ced9c-0dc7-44e6-b164-34bb9547da34] [Similarity score:             0.715541] Dumber LLM Agents Need More Constraints and Better Tools\n",
      "Summary In this article, we compare how ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 8762af05-573e-4218-8c37-61c103ff5c50] [Similarity score:             0.741513] agent_chain.run(input=\"How much cash did Uber have in sept 2022?\") We see that the agent makes tw...\n",
      "> [Node b121f089-28e9-4bb6-bf18-35bde674ac74] [Similarity score:             0.734254] High-Level Findings The high-level finding is that  less sophisticated agents need more constrain...\n",
      "> [Node d7b1f3dc-479b-45f5-af9b-e6c4afcffede] [Similarity score:             0.728069] )\n",
      "query_tool_graph = QueryEngineTool.from_defaults(\n",
      "    query_engine=g_engine,\n",
      "    description=f\"...\n",
      "> [Node 0118fb96-b70f-4363-83ed-f5161c3672c2] [Similarity score:             0.715617] # define indices\n",
      "march_index = GPTVectorStoreIndex.from_documents(march_2022)\n",
      "june_index = GPTVec...\n",
      "> [Node d42ced9c-0dc7-44e6-b164-34bb9547da34] [Similarity score:             0.715541] Dumber LLM Agents Need More Constraints and Better Tools\n",
      "Summary In this article, we compare how ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node fe557b79-4c6f-4b13-9f69-38c82723ca06] [Similarity score:             0.79376] After the preprocessing of the data, we collect the metadata, including keywords, NER results, su...\n",
      "> [Node e46a803e-c18b-4dea-99e0-182a48c60339] [Similarity score:             0.763455] Sophisticated  Named-Entity Recognition, Text Embedding with OpenAI API,  and  asynchronous artic...\n",
      "> [Node 6a47aa8c-c22a-4cf0-9069-42b50a317d7e] [Similarity score:             0.743386] Chat with Article Powered by LlamaIndex LlamaIndex Pipeline Unlike other news aggregator apps, Ne...\n",
      "> [Node ba03f8e0-20c5-49e9-a934-edad75b471c0] [Similarity score:             0.740407] NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app\n",
      "We‚Äôre excited t...\n",
      "> [Node 65b19540-5b23-480a-8883-e0c59736b599] [Similarity score:             0.739633] from  llama_index  import  ServiceContext\n",
      "\n",
      "st.session_state[ \"service_context\" ] = ServiceContext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node fe557b79-4c6f-4b13-9f69-38c82723ca06] [Similarity score:             0.79376] After the preprocessing of the data, we collect the metadata, including keywords, NER results, su...\n",
      "> [Node e46a803e-c18b-4dea-99e0-182a48c60339] [Similarity score:             0.763455] Sophisticated  Named-Entity Recognition, Text Embedding with OpenAI API,  and  asynchronous artic...\n",
      "> [Node 6a47aa8c-c22a-4cf0-9069-42b50a317d7e] [Similarity score:             0.743386] Chat with Article Powered by LlamaIndex LlamaIndex Pipeline Unlike other news aggregator apps, Ne...\n",
      "> [Node ba03f8e0-20c5-49e9-a934-edad75b471c0] [Similarity score:             0.740407] NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app\n",
      "We‚Äôre excited t...\n",
      "> [Node 65b19540-5b23-480a-8883-e0c59736b599] [Similarity score:             0.739633] from  llama_index  import  ServiceContext\n",
      "\n",
      "st.session_state[ \"service_context\" ] = ServiceContext...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 6a47aa8c-c22a-4cf0-9069-42b50a317d7e] [Similarity score:             0.705137] Chat with Article Powered by LlamaIndex LlamaIndex Pipeline Unlike other news aggregator apps, Ne...\n",
      "> [Node e46a803e-c18b-4dea-99e0-182a48c60339] [Similarity score:             0.698638] Sophisticated  Named-Entity Recognition, Text Embedding with OpenAI API,  and  asynchronous artic...\n",
      "> [Node ba03f8e0-20c5-49e9-a934-edad75b471c0] [Similarity score:             0.682706] NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app\n",
      "We‚Äôre excited t...\n",
      "> [Node af232d37-5cc8-49e4-b476-4bd649fe6141] [Similarity score:             0.681119] response = st.session_state[ \"chat_engine\" ].query(prompt)\n",
      " def   stream_example ():\n",
      "     for  wo...\n",
      "> [Node 65b19540-5b23-480a-8883-e0c59736b599] [Similarity score:             0.673862] from  llama_index  import  ServiceContext\n",
      "\n",
      "st.session_state[ \"service_context\" ] = ServiceContext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 6a47aa8c-c22a-4cf0-9069-42b50a317d7e] [Similarity score:             0.705137] Chat with Article Powered by LlamaIndex LlamaIndex Pipeline Unlike other news aggregator apps, Ne...\n",
      "> [Node e46a803e-c18b-4dea-99e0-182a48c60339] [Similarity score:             0.698638] Sophisticated  Named-Entity Recognition, Text Embedding with OpenAI API,  and  asynchronous artic...\n",
      "> [Node ba03f8e0-20c5-49e9-a934-edad75b471c0] [Similarity score:             0.682706] NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app\n",
      "We‚Äôre excited t...\n",
      "> [Node af232d37-5cc8-49e4-b476-4bd649fe6141] [Similarity score:             0.681119] response = st.session_state[ \"chat_engine\" ].query(prompt)\n",
      " def   stream_example ():\n",
      "     for  wo...\n",
      "> [Node 65b19540-5b23-480a-8883-e0c59736b599] [Similarity score:             0.673862] from  llama_index  import  ServiceContext\n",
      "\n",
      "st.session_state[ \"service_context\" ] = ServiceContext...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 7f444bd5-230c-4fb1-b39d-8358322a8064] [Similarity score:             0.801989] For example you can still benefit from  incremental syncs  if your stream supports it by accessin...\n",
      "> [Node 295873b5-b963-4cac-b308-18eb1dcdf502] [Similarity score:             0.769418] Introducing Airbyte sources within LlamaIndex\n",
      "Authored by Joe Reuter, Software Engineer at Airbyt...\n",
      "> [Node b43afcfa-722a-4886-aa86-7662595a90a6] [Similarity score:             0.768482] if you have implemented your own custom Airbyte sources, it‚Äôs also possible to integrate them by ...\n",
      "> [Node a69cc820-538e-4aa8-8497-a48c2f956ed9] [Similarity score:             0.765949] But if you are just getting started and are running everything locally, using a full Airbyte inst...\n",
      "> [Node 0ff41e8b-9bfc-47b7-abd3-0096fd05d382] [Similarity score:             0.739814] If any entities are found, we iterate and execute the vector context retriever for each entity. O...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 7f444bd5-230c-4fb1-b39d-8358322a8064] [Similarity score:             0.801989] For example you can still benefit from  incremental syncs  if your stream supports it by accessin...\n",
      "> [Node 295873b5-b963-4cac-b308-18eb1dcdf502] [Similarity score:             0.769418] Introducing Airbyte sources within LlamaIndex\n",
      "Authored by Joe Reuter, Software Engineer at Airbyt...\n",
      "> [Node b43afcfa-722a-4886-aa86-7662595a90a6] [Similarity score:             0.768482] if you have implemented your own custom Airbyte sources, it‚Äôs also possible to integrate them by ...\n",
      "> [Node a69cc820-538e-4aa8-8497-a48c2f956ed9] [Similarity score:             0.765949] But if you are just getting started and are running everything locally, using a full Airbyte inst...\n",
      "> [Node 0ff41e8b-9bfc-47b7-abd3-0096fd05d382] [Similarity score:             0.739814] If any entities are found, we iterate and execute the vector context retriever for each entity. O...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 7f444bd5-230c-4fb1-b39d-8358322a8064] [Similarity score:             0.672314] For example you can still benefit from  incremental syncs  if your stream supports it by accessin...\n",
      "> [Node 9637a4dd-ce30-4d71-91f5-a9f397ccd01d] [Similarity score:             0.67173] For example, chatbot chat history. Let‚Äôs take a look at an example of performing time-based searc...\n",
      "> [Node b3f21c8e-d13a-4b5f-9272-6d715ca85ed7] [Similarity score:             0.671071] Simply create a Timescale Vector vector store and add the  data nodes  you want to query as shown...\n",
      "> [Node 62dee34e-c941-422c-a558-bfdef666a824] [Similarity score:             0.67002] Timescale Vector partitions the data by time and creates ANN indexes on each partition individual...\n",
      "> [Node 3a044929-f516-4c4c-86ac-17d6acf8ad10] [Similarity score:             0.665354] Timescale Vector also offers pgvector‚Äôs Hierarchical Navigable Small Worlds (HNSW) and Inverted F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 7f444bd5-230c-4fb1-b39d-8358322a8064] [Similarity score:             0.672314] For example you can still benefit from  incremental syncs  if your stream supports it by accessin...\n",
      "> [Node 9637a4dd-ce30-4d71-91f5-a9f397ccd01d] [Similarity score:             0.67173] For example, chatbot chat history. Let‚Äôs take a look at an example of performing time-based searc...\n",
      "> [Node b3f21c8e-d13a-4b5f-9272-6d715ca85ed7] [Similarity score:             0.671071] Simply create a Timescale Vector vector store and add the  data nodes  you want to query as shown...\n",
      "> [Node 62dee34e-c941-422c-a558-bfdef666a824] [Similarity score:             0.67002] Timescale Vector partitions the data by time and creates ANN indexes on each partition individual...\n",
      "> [Node 3a044929-f516-4c4c-86ac-17d6acf8ad10] [Similarity score:             0.665354] Timescale Vector also offers pgvector‚Äôs Hierarchical Navigable Small Worlds (HNSW) and Inverted F...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 29dca1d0-bafd-47d8-9fad-def250d4cf27] [Similarity score:             0.750523] Click the ‚ÄúPermissions‚Äù link in the bottom right: This will bring you to the ‚Äúscopes‚Äù screen wher...\n",
      "> [Node 514659da-cc22-4fb0-977f-381ee7b995a9] [Similarity score:             0.709097] SLACK_SIGNING_SECRET : you can find this in the \"Basic Information\" section of your Slack app. We...\n",
      "> [Node 0b7d38cb-cc03-4cc2-bbad-cacfb81504bb] [Similarity score:             0.694875] Let‚Äôs try it out! Stop running  1_flask.py  and run  python 2_join_and_reply.py  instead. You don...\n",
      "> [Node beb380b1-18f5-4956-94ca-1e3cb303cdc2] [Similarity score:             0.684127] To do that we are going to stop inserting  Documents  into the index and instead insert  Nodes , ...\n",
      "> [Node 222f2305-8916-43a7-aa1e-a339d07499df] [Similarity score:             0.681211] @app.message() \n",
      " def   reply ( message, say ):\n",
      "     if  message.get( 'blocks' ):\n",
      "         for  bl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 29dca1d0-bafd-47d8-9fad-def250d4cf27] [Similarity score:             0.750523] Click the ‚ÄúPermissions‚Äù link in the bottom right: This will bring you to the ‚Äúscopes‚Äù screen wher...\n",
      "> [Node 514659da-cc22-4fb0-977f-381ee7b995a9] [Similarity score:             0.709097] SLACK_SIGNING_SECRET : you can find this in the \"Basic Information\" section of your Slack app. We...\n",
      "> [Node 0b7d38cb-cc03-4cc2-bbad-cacfb81504bb] [Similarity score:             0.694875] Let‚Äôs try it out! Stop running  1_flask.py  and run  python 2_join_and_reply.py  instead. You don...\n",
      "> [Node beb380b1-18f5-4956-94ca-1e3cb303cdc2] [Similarity score:             0.684127] To do that we are going to stop inserting  Documents  into the index and instead insert  Nodes , ...\n",
      "> [Node 222f2305-8916-43a7-aa1e-a339d07499df] [Similarity score:             0.681211] @app.message() \n",
      " def   reply ( message, say ):\n",
      "     if  message.get( 'blocks' ):\n",
      "         for  bl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 29dca1d0-bafd-47d8-9fad-def250d4cf27] [Similarity score:             0.722634] Click the ‚ÄúPermissions‚Äù link in the bottom right: This will bring you to the ‚Äúscopes‚Äù screen wher...\n",
      "> [Node 514659da-cc22-4fb0-977f-381ee7b995a9] [Similarity score:             0.656255] SLACK_SIGNING_SECRET : you can find this in the \"Basic Information\" section of your Slack app. We...\n",
      "> [Node 0b7d38cb-cc03-4cc2-bbad-cacfb81504bb] [Similarity score:             0.647589] Let‚Äôs try it out! Stop running  1_flask.py  and run  python 2_join_and_reply.py  instead. You don...\n",
      "> [Node 3c2661f5-b4a3-413a-8217-79794217eb59] [Similarity score:             0.63594] Otherwise we'll do nothing. @flask_app.route( \"/\" , methods=[ \"POST\" ] ) \n",
      " def   slack_challenge ...\n",
      "> [Node 19da8ef3-18d3-46a7-ad3a-48a097d7e496] [Similarity score:             0.626236] Your bot now survives reboots, and remembers that I typoed ‚ÄúDoug‚Äù as ‚ÄúDough‚Äù and was too lazy to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 29dca1d0-bafd-47d8-9fad-def250d4cf27] [Similarity score:             0.722634] Click the ‚ÄúPermissions‚Äù link in the bottom right: This will bring you to the ‚Äúscopes‚Äù screen wher...\n",
      "> [Node 514659da-cc22-4fb0-977f-381ee7b995a9] [Similarity score:             0.656255] SLACK_SIGNING_SECRET : you can find this in the \"Basic Information\" section of your Slack app. We...\n",
      "> [Node 0b7d38cb-cc03-4cc2-bbad-cacfb81504bb] [Similarity score:             0.647589] Let‚Äôs try it out! Stop running  1_flask.py  and run  python 2_join_and_reply.py  instead. You don...\n",
      "> [Node 3c2661f5-b4a3-413a-8217-79794217eb59] [Similarity score:             0.63594] Otherwise we'll do nothing. @flask_app.route( \"/\" , methods=[ \"POST\" ] ) \n",
      " def   slack_challenge ...\n",
      "> [Node 19da8ef3-18d3-46a7-ad3a-48a097d7e496] [Similarity score:             0.626236] Your bot now survives reboots, and remembers that I typoed ‚ÄúDoug‚Äù as ‚ÄúDough‚Äù and was too lazy to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node c25ee90d-807b-4bf1-9336-633b2060c034] [Similarity score:             0.784335] To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use ...\n",
      "> [Node fb900e12-904c-47c7-8971-47195c7a922b] [Similarity score:             0.769176] Part of this is potentially due to our tree summarize response synthesis strategy. Nevertheless, ...\n",
      "> [Node f486cd4c-3bc0-4f4f-89ea-7baac026c7b8] [Similarity score:             0.766599] from  llama_index  import  PromptHelper, LLMPredictor, ServiceContext\n",
      " from  langchain.llms  impo...\n",
      "> [Node bc8748d2-5c7d-40c7-8119-677e9c717429] [Similarity score:             0.756038] We then create a Document object for each SEC filing. You can access Unstructured data loaders on...\n",
      "> [Node 9e986a59-1fe4-490a-8c77-094f6282747c] [Similarity score:             0.74083] Testing Anthropic Claude‚Äôs 100k-token window on SEC 10-K Filings\n",
      "Anthropic‚Äôs  100K Context Window...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node c25ee90d-807b-4bf1-9336-633b2060c034] [Similarity score:             0.784335] To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use ...\n",
      "> [Node fb900e12-904c-47c7-8971-47195c7a922b] [Similarity score:             0.769176] Part of this is potentially due to our tree summarize response synthesis strategy. Nevertheless, ...\n",
      "> [Node f486cd4c-3bc0-4f4f-89ea-7baac026c7b8] [Similarity score:             0.766599] from  llama_index  import  PromptHelper, LLMPredictor, ServiceContext\n",
      " from  langchain.llms  impo...\n",
      "> [Node bc8748d2-5c7d-40c7-8119-677e9c717429] [Similarity score:             0.756038] We then create a Document object for each SEC filing. You can access Unstructured data loaders on...\n",
      "> [Node 9e986a59-1fe4-490a-8c77-094f6282747c] [Similarity score:             0.74083] Testing Anthropic Claude‚Äôs 100k-token window on SEC 10-K Filings\n",
      "Anthropic‚Äôs  100K Context Window...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 213b8371-4b42-4baa-8b82-69ec91ea6763] [Similarity score:             0.757659] Running Mixtral 8x7 locally with LlamaIndex and Ollama\n",
      "You may have heard the fuss about the late...\n",
      "> [Node 836281f1-d0b5-4e80-ba2f-81ae0e80bb57] [Similarity score:             0.756082] However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that...\n",
      "> [Node 101c3b72-dd19-40ba-8b8c-3fd4125751c4] [Similarity score:             0.755345] The x-axes on the graphs represent model size in terms of the number of parameters, with three po...\n",
      "> [Node 25277fca-a381-4c8b-8a8a-132543c069ea] [Similarity score:             0.748454] Based on these graphs, Mistral appears to be the top-performing model in all four metrics, sugges...\n",
      "> [Node f47cd96f-b16f-4e17-9aa4-a165e0cad38b] [Similarity score:             0.745371] 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 213b8371-4b42-4baa-8b82-69ec91ea6763] [Similarity score:             0.757659] Running Mixtral 8x7 locally with LlamaIndex and Ollama\n",
      "You may have heard the fuss about the late...\n",
      "> [Node 836281f1-d0b5-4e80-ba2f-81ae0e80bb57] [Similarity score:             0.756082] However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that...\n",
      "> [Node 101c3b72-dd19-40ba-8b8c-3fd4125751c4] [Similarity score:             0.755345] The x-axes on the graphs represent model size in terms of the number of parameters, with three po...\n",
      "> [Node 25277fca-a381-4c8b-8a8a-132543c069ea] [Similarity score:             0.748454] Based on these graphs, Mistral appears to be the top-performing model in all four metrics, sugges...\n",
      "> [Node f47cd96f-b16f-4e17-9aa4-a165e0cad38b] [Similarity score:             0.745371] 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 101c3b72-dd19-40ba-8b8c-3fd4125751c4] [Similarity score:             0.785649] The x-axes on the graphs represent model size in terms of the number of parameters, with three po...\n",
      "> [Node 836281f1-d0b5-4e80-ba2f-81ae0e80bb57] [Similarity score:             0.784112] However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that...\n",
      "> [Node f47cd96f-b16f-4e17-9aa4-a165e0cad38b] [Similarity score:             0.778956] 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement...\n",
      "> [Node 25277fca-a381-4c8b-8a8a-132543c069ea] [Similarity score:             0.776885] Based on these graphs, Mistral appears to be the top-performing model in all four metrics, sugges...\n",
      "> [Node 3b002c76-2225-4aa1-84e0-b8227bc952a4] [Similarity score:             0.768617] Answer: Examine the Image: The image contains four graphs, each graph compares the performance of...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 101c3b72-dd19-40ba-8b8c-3fd4125751c4] [Similarity score:             0.785649] The x-axes on the graphs represent model size in terms of the number of parameters, with three po...\n",
      "> [Node 836281f1-d0b5-4e80-ba2f-81ae0e80bb57] [Similarity score:             0.784112] However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that...\n",
      "> [Node f47cd96f-b16f-4e17-9aa4-a165e0cad38b] [Similarity score:             0.778956] 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement...\n",
      "> [Node 25277fca-a381-4c8b-8a8a-132543c069ea] [Similarity score:             0.776885] Based on these graphs, Mistral appears to be the top-performing model in all four metrics, sugges...\n",
      "> [Node 3b002c76-2225-4aa1-84e0-b8227bc952a4] [Similarity score:             0.768617] Answer: Examine the Image: The image contains four graphs, each graph compares the performance of...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 705e2318-710c-4b43-80d1-e93fde2aaf78] [Similarity score:             0.67134] The Building Blocks of Prosper AI: A Look into Our Tech Stack Backend:  Our backend, the engine t...\n",
      "> [Node 7c313e23-5698-4770-8a54-c68a3e5ce563] [Similarity score:             0.664139] Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)\n",
      "We had an awesome time at...\n",
      "> [Node 31b80661-7af6-4dff-b5d4-9949815dcb9e] [Similarity score:             0.663564] The current interface was developed using React.js for the website and a Flask API to interact wi...\n",
      "> [Node 7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0] [Similarity score:             0.663191] Data Agents session at TPF X Nexus VC  Buildathon  by  Ravi Theja . Demos: Tali.AI  at the Augmen...\n",
      "> [Node b9766278-d8ae-467e-8086-609c45b91b13] [Similarity score:             0.663132] Future Prospects Looking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 705e2318-710c-4b43-80d1-e93fde2aaf78] [Similarity score:             0.67134] The Building Blocks of Prosper AI: A Look into Our Tech Stack Backend:  Our backend, the engine t...\n",
      "> [Node 7c313e23-5698-4770-8a54-c68a3e5ce563] [Similarity score:             0.664139] Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)\n",
      "We had an awesome time at...\n",
      "> [Node 31b80661-7af6-4dff-b5d4-9949815dcb9e] [Similarity score:             0.663564] The current interface was developed using React.js for the website and a Flask API to interact wi...\n",
      "> [Node 7b7ae6d2-0d60-47a8-8f69-83331cfcc7e0] [Similarity score:             0.663191] Data Agents session at TPF X Nexus VC  Buildathon  by  Ravi Theja . Demos: Tali.AI  at the Augmen...\n",
      "> [Node b9766278-d8ae-467e-8086-609c45b91b13] [Similarity score:             0.663132] Future Prospects Looking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the ent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node a5828a62-c3cd-4ba2-85cb-94c1921cfa75] [Similarity score:             0.781817] These data connectors are primarily hosted on [LlamaHub]( https://llamahub.ai/ ). This makes it e...\n",
      "> [Node c9aae507-5cbc-4f28-a1c4-6415e8853329] [Similarity score:             0.77644] Understand your dataset \n",
      " \n",
      "  The first step in doing data analysis is to get a better understandi...\n",
      "> [Node 9ceadb9e-b341-4036-9bdb-ce0387a6f687] [Similarity score:             0.767788] However, managing medium to large databases, which can include 100s of tables and 1000s of column...\n",
      "> [Node d307bfef-b376-4b32-beec-a62885127139] [Similarity score:             0.765028] For example, if the question indicates information needs to be retrieved\n",
      "  from the ‚Äúinternet‚Äù, t...\n",
      "> [Node 300ebaee-89ec-4470-ae6f-895849bba78e] [Similarity score:             0.764095] - \"Gift Cards & Other\" and \"Food & Beverage\" from Deloitte's survey do    not have a direct match...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node a5828a62-c3cd-4ba2-85cb-94c1921cfa75] [Similarity score:             0.781817] These data connectors are primarily hosted on [LlamaHub]( https://llamahub.ai/ ). This makes it e...\n",
      "> [Node c9aae507-5cbc-4f28-a1c4-6415e8853329] [Similarity score:             0.77644] Understand your dataset \n",
      " \n",
      "  The first step in doing data analysis is to get a better understandi...\n",
      "> [Node 9ceadb9e-b341-4036-9bdb-ce0387a6f687] [Similarity score:             0.767788] However, managing medium to large databases, which can include 100s of tables and 1000s of column...\n",
      "> [Node d307bfef-b376-4b32-beec-a62885127139] [Similarity score:             0.765028] For example, if the question indicates information needs to be retrieved\n",
      "  from the ‚Äúinternet‚Äù, t...\n",
      "> [Node 300ebaee-89ec-4470-ae6f-895849bba78e] [Similarity score:             0.764095] - \"Gift Cards & Other\" and \"Food & Beverage\" from Deloitte's survey do    not have a direct match...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node f5ccf620-f5bb-42e2-9f08-fcb336ce5678] [Similarity score:             0.840962] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> [Node 7573ed8a-9659-450a-bad5-52a5647211f0] [Similarity score:             0.835525] Detailed Explanations for Individual Code Blocks: \n",
      " \n",
      " \n",
      "  With a general understanding established...\n",
      "> [Node 52ed2b97-690d-4bed-920a-4505b22e9b95] [Similarity score:             0.821962] Code Repository:\n",
      "   https://github.com/ravi03071991/KT_Generator \n",
      " \n",
      " Conclusion \n",
      " \n",
      "  Through this...\n",
      "> [Node 1b998d14-346c-44d8-bcae-f2391f13f82c] [Similarity score:             0.819965] 1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "   Code Parser \n",
      " \n",
      " \n",
      "  Understanding a code base st...\n",
      "> [Node 976e3314-9169-4f5e-a64d-558eb2442c4f] [Similarity score:             0.804402] This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  With the prowess of D-ID‚Äôs cutting-edge technology, w...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node f5ccf620-f5bb-42e2-9f08-fcb336ce5678] [Similarity score:             0.840962] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> [Node 7573ed8a-9659-450a-bad5-52a5647211f0] [Similarity score:             0.835525] Detailed Explanations for Individual Code Blocks: \n",
      " \n",
      " \n",
      "  With a general understanding established...\n",
      "> [Node 52ed2b97-690d-4bed-920a-4505b22e9b95] [Similarity score:             0.821962] Code Repository:\n",
      "   https://github.com/ravi03071991/KT_Generator \n",
      " \n",
      " Conclusion \n",
      " \n",
      "  Through this...\n",
      "> [Node 1b998d14-346c-44d8-bcae-f2391f13f82c] [Similarity score:             0.819965] 1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "   Code Parser \n",
      " \n",
      " \n",
      "  Understanding a code base st...\n",
      "> [Node 976e3314-9169-4f5e-a64d-558eb2442c4f] [Similarity score:             0.804402] This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  With the prowess of D-ID‚Äôs cutting-edge technology, w...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node f5ccf620-f5bb-42e2-9f08-fcb336ce5678] [Similarity score:             0.783909] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> [Node 976e3314-9169-4f5e-a64d-558eb2442c4f] [Similarity score:             0.782011] This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  With the prowess of D-ID‚Äôs cutting-edge technology, w...\n",
      "> [Node 7573ed8a-9659-450a-bad5-52a5647211f0] [Similarity score:             0.774397] Detailed Explanations for Individual Code Blocks: \n",
      " \n",
      " \n",
      "  With a general understanding established...\n",
      "> [Node 52ed2b97-690d-4bed-920a-4505b22e9b95] [Similarity score:             0.769068] Code Repository:\n",
      "   https://github.com/ravi03071991/KT_Generator \n",
      " \n",
      " Conclusion \n",
      " \n",
      "  Through this...\n",
      "> [Node 1b998d14-346c-44d8-bcae-f2391f13f82c] [Similarity score:             0.762281] 1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "   Code Parser \n",
      " \n",
      " \n",
      "  Understanding a code base st...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node f5ccf620-f5bb-42e2-9f08-fcb336ce5678] [Similarity score:             0.783909] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> [Node 976e3314-9169-4f5e-a64d-558eb2442c4f] [Similarity score:             0.782011] This is where D-ID\n",
      "  comes into play.\n",
      " \n",
      " \n",
      "  With the prowess of D-ID‚Äôs cutting-edge technology, w...\n",
      "> [Node 7573ed8a-9659-450a-bad5-52a5647211f0] [Similarity score:             0.774397] Detailed Explanations for Individual Code Blocks: \n",
      " \n",
      " \n",
      "  With a general understanding established...\n",
      "> [Node 52ed2b97-690d-4bed-920a-4505b22e9b95] [Similarity score:             0.769068] Code Repository:\n",
      "   https://github.com/ravi03071991/KT_Generator \n",
      " \n",
      " Conclusion \n",
      " \n",
      "  Through this...\n",
      "> [Node 1b998d14-346c-44d8-bcae-f2391f13f82c] [Similarity score:             0.762281] 1. Code Parsing: Breaking Down the Code \n",
      " \n",
      "   \n",
      "   Code Parser \n",
      " \n",
      " \n",
      "  Understanding a code base st...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 0bbfea6e-b924-44eb-9e31-97d36f7d9c20] [Similarity score:             0.727138] FAQ What‚Äôs the difference between a  QueryPipeline  and  IngestionPipeline  ? Great question. Cur...\n",
      "> [Node 28cfbe65-7d54-4c30-8090-dfab0d86bf35] [Similarity score:             0.714025] What is a  Transformation  though? It could be a: text splitter node parser metadata extractor em...\n",
      "> [Node 4ce0d88a-b48b-4663-8bcf-8ad5c01c8b9e] [Similarity score:             0.693701] import  re\n",
      " from  llama_index  import  Document\n",
      " from  llama_index.embeddings  import  OpenAIEmbe...\n",
      "> [Node 9c2e5b83-98f2-42df-beb6-27844a280777] [Similarity score:             0.690805] Here‚Äôs an example with a saving and loading a local cache: from  llama_index  import  Document\n",
      " f...\n",
      "> [Node 1a529c58-ba84-4705-8904-9d3fc49d3885] [Similarity score:             0.685317] # Calling the functions through streamlit frontend import streamlit as st if \"index\" not in st.se...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 0bbfea6e-b924-44eb-9e31-97d36f7d9c20] [Similarity score:             0.727138] FAQ What‚Äôs the difference between a  QueryPipeline  and  IngestionPipeline  ? Great question. Cur...\n",
      "> [Node 28cfbe65-7d54-4c30-8090-dfab0d86bf35] [Similarity score:             0.714025] What is a  Transformation  though? It could be a: text splitter node parser metadata extractor em...\n",
      "> [Node 4ce0d88a-b48b-4663-8bcf-8ad5c01c8b9e] [Similarity score:             0.693701] import  re\n",
      " from  llama_index  import  Document\n",
      " from  llama_index.embeddings  import  OpenAIEmbe...\n",
      "> [Node 9c2e5b83-98f2-42df-beb6-27844a280777] [Similarity score:             0.690805] Here‚Äôs an example with a saving and loading a local cache: from  llama_index  import  Document\n",
      " f...\n",
      "> [Node 1a529c58-ba84-4705-8904-9d3fc49d3885] [Similarity score:             0.685317] # Calling the functions through streamlit frontend import streamlit as st if \"index\" not in st.se...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node b220413a-0ac9-43fd-9ffa-1eede85f5efa] [Similarity score:             0.710011] from  llama_index.vector_stores  import  ZepVectorStore\n",
      "zep_api_url =  \"http://localhost:8000\" \n",
      "z...\n",
      "> [Node 06a2f9bc-9a47-4363-9010-38a896efe735] [Similarity score:             0.685134] print ( str (response)) But one of the most signal examples of this kind, of which we are aware, ...\n",
      "> [Node b3f21c8e-d13a-4b5f-9272-6d715ca85ed7] [Similarity score:             0.684357] Simply create a Timescale Vector vector store and add the  data nodes  you want to query as shown...\n",
      "> [Node 78161534-e09e-4ae1-a0ff-a9b703ae54f0] [Similarity score:             0.678696] from pypdf import PdfReader from llama_index.schema import Document def process_pdf(pdf):     fil...\n",
      "> [Node 4479325e-d44c-40f4-8982-cc3309945418] [Similarity score:             0.673028] See our usage example below as well. Usage Example To build a  VectorStoreIndex  and then query i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node b220413a-0ac9-43fd-9ffa-1eede85f5efa] [Similarity score:             0.710011] from  llama_index.vector_stores  import  ZepVectorStore\n",
      "zep_api_url =  \"http://localhost:8000\" \n",
      "z...\n",
      "> [Node 06a2f9bc-9a47-4363-9010-38a896efe735] [Similarity score:             0.685134] print ( str (response)) But one of the most signal examples of this kind, of which we are aware, ...\n",
      "> [Node b3f21c8e-d13a-4b5f-9272-6d715ca85ed7] [Similarity score:             0.684357] Simply create a Timescale Vector vector store and add the  data nodes  you want to query as shown...\n",
      "> [Node 78161534-e09e-4ae1-a0ff-a9b703ae54f0] [Similarity score:             0.678696] from pypdf import PdfReader from llama_index.schema import Document def process_pdf(pdf):     fil...\n",
      "> [Node 4479325e-d44c-40f4-8982-cc3309945418] [Similarity score:             0.673028] See our usage example below as well. Usage Example To build a  VectorStoreIndex  and then query i...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 550a4330-b77b-4a95-976f-8c7ac0e984c1] [Similarity score:             0.839029] LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding searc...\n",
      "> [Node 36e508bc-cb26-43b5-9ab4-ac344a03bfcb] [Similarity score:             0.831561] Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant co...\n",
      "> [Node 124ae013-39ea-49ab-a870-f5f98d46a71d] [Similarity score:             0.808496] We‚Äôve added some initial functionality to help support LLM-augmented retrieval pipelines, but of ...\n",
      "> [Node 511ef310-c1e9-4b5f-b760-3a3e97512825] [Similarity score:             0.807362] Using LLM‚Äôs for Retrieval and Reranking\n",
      "Summary This blog post outlines some of the core abstract...\n",
      "> [Node 17fbe4dd-c34d-4c03-9f71-bd68136e2caf] [Similarity score:             0.803881] We see that in embedding-based retrieval, the top two texts contain semantics of the car crash bu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 550a4330-b77b-4a95-976f-8c7ac0e984c1] [Similarity score:             0.839029] LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding searc...\n",
      "> [Node 36e508bc-cb26-43b5-9ab4-ac344a03bfcb] [Similarity score:             0.831561] Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant co...\n",
      "> [Node 124ae013-39ea-49ab-a870-f5f98d46a71d] [Similarity score:             0.808496] We‚Äôve added some initial functionality to help support LLM-augmented retrieval pipelines, but of ...\n",
      "> [Node 511ef310-c1e9-4b5f-b760-3a3e97512825] [Similarity score:             0.807362] Using LLM‚Äôs for Retrieval and Reranking\n",
      "Summary This blog post outlines some of the core abstract...\n",
      "> [Node 17fbe4dd-c34d-4c03-9f71-bd68136e2caf] [Similarity score:             0.803881] We see that in embedding-based retrieval, the top two texts contain semantics of the car crash bu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 550a4330-b77b-4a95-976f-8c7ac0e984c1] [Similarity score:             0.832421] LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding searc...\n",
      "> [Node 36e508bc-cb26-43b5-9ab4-ac344a03bfcb] [Similarity score:             0.829715] Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant co...\n",
      "> [Node 124ae013-39ea-49ab-a870-f5f98d46a71d] [Similarity score:             0.822108] We‚Äôve added some initial functionality to help support LLM-augmented retrieval pipelines, but of ...\n",
      "> [Node 17fbe4dd-c34d-4c03-9f71-bd68136e2caf] [Similarity score:             0.814088] We see that in embedding-based retrieval, the top two texts contain semantics of the car crash bu...\n",
      "> [Node 511ef310-c1e9-4b5f-b760-3a3e97512825] [Similarity score:             0.811899] Using LLM‚Äôs for Retrieval and Reranking\n",
      "Summary This blog post outlines some of the core abstract...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 550a4330-b77b-4a95-976f-8c7ac0e984c1] [Similarity score:             0.832421] LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding searc...\n",
      "> [Node 36e508bc-cb26-43b5-9ab4-ac344a03bfcb] [Similarity score:             0.829715] Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant co...\n",
      "> [Node 124ae013-39ea-49ab-a870-f5f98d46a71d] [Similarity score:             0.822108] We‚Äôve added some initial functionality to help support LLM-augmented retrieval pipelines, but of ...\n",
      "> [Node 17fbe4dd-c34d-4c03-9f71-bd68136e2caf] [Similarity score:             0.814088] We see that in embedding-based retrieval, the top two texts contain semantics of the car crash bu...\n",
      "> [Node 511ef310-c1e9-4b5f-b760-3a3e97512825] [Similarity score:             0.811899] Using LLM‚Äôs for Retrieval and Reranking\n",
      "Summary This blog post outlines some of the core abstract...\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_5_retrieval_eval</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.193099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate   mrr  precision    recall    ap      ndcg\n",
       "0  top_5_retrieval_eval  0.631579  0.55   0.126316  0.631579  0.55  0.193099"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_rag_dataset(response_eval_dataset, response_eval_prediction_dataset, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "        evals[\"contexts\"].append(prediction.contexts)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "        \"contexts\": evals['contexts'],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "        pd.Series(evals['contexts'], name='contexts')\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_SYNTHETIC_EVAL_DATASET = False\n",
    "# RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = min(len(documents), RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 22:56:51.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 22:56:53.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading existing synthetic response eval dataset at data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # Use good model to generate the eval dataset\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=response_eval_llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        show_progress=True,\n",
    "        workers=(os.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "940feda2-1aad-427c-895f-2d4cdf43918f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node c4e47e3e-d056-4ef1-bff1-9be8b3180892] [Similarity score:             0.854614] Docs ,  Tweet . RA-DIT:  We drew inspiration from the RA-DIT paper, which introduced LLM fine-tun...\n",
      "> [Node 3ad42e4f-17b2-4d0d-8ba2-297acbc1adf0] [Similarity score:             0.853496] This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, u...\n",
      "> [Node 1b7d4deb-9abd-46f3-8e9f-9d12f81b9918] [Similarity score:             0.846551] Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines\n",
      "Introduction We are...\n",
      "> [Node 6212e071-f6ee-454f-bb70-e8c7e1704811] [Similarity score:             0.841555] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node 1ce6a371-f31a-456c-a184-7b970012fce8] [Similarity score:             0.841511] LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly look at developments across the LLM space and RAG ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node c4e47e3e-d056-4ef1-bff1-9be8b3180892] [Similarity score:             0.854614] Docs ,  Tweet . RA-DIT:  We drew inspiration from the RA-DIT paper, which introduced LLM fine-tun...\n",
      "> [Node 3ad42e4f-17b2-4d0d-8ba2-297acbc1adf0] [Similarity score:             0.853496] This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, u...\n",
      "> [Node 1b7d4deb-9abd-46f3-8e9f-9d12f81b9918] [Similarity score:             0.846551] Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines\n",
      "Introduction We are...\n",
      "> [Node 6212e071-f6ee-454f-bb70-e8c7e1704811] [Similarity score:             0.841555] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node 1ce6a371-f31a-456c-a184-7b970012fce8] [Similarity score:             0.841511] LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly look at developments across the LLM space and RAG ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node f85e8372-aade-4846-aa4c-d8c874a1dab6] [Similarity score:             0.807304] LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting...\n",
      "> [Node 92348901-97fe-4a64-918c-7f706507f4a6] [Similarity score:             0.80093] In V0.9, we have  flattened  the entire interface into a single  TransformComponent  abstraction,...\n",
      "> [Node d10877d0-5a7a-43a2-9019-12235b516604] [Similarity score:             0.800342] Launching the first GenAI-native document parsing platform\n",
      "Our mission at LlamaIndex is to connec...\n",
      "> [Node 20d10f16-820e-46a4-9c64-233181488e36] [Similarity score:             0.799856] LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structu...\n",
      "> [Node e41335df-9543-4738-8397-dbb472a1db68] [Similarity score:             0.796163] from  llama_parse  import  LlamaParse\n",
      "\n",
      "parser = LlamaParse(\n",
      "    api_key= \"llx-...\" ,   # can also...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node f85e8372-aade-4846-aa4c-d8c874a1dab6] [Similarity score:             0.807304] LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting...\n",
      "> [Node 92348901-97fe-4a64-918c-7f706507f4a6] [Similarity score:             0.80093] In V0.9, we have  flattened  the entire interface into a single  TransformComponent  abstraction,...\n",
      "> [Node d10877d0-5a7a-43a2-9019-12235b516604] [Similarity score:             0.800342] Launching the first GenAI-native document parsing platform\n",
      "Our mission at LlamaIndex is to connec...\n",
      "> [Node 20d10f16-820e-46a4-9c64-233181488e36] [Similarity score:             0.799856] LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structu...\n",
      "> [Node e41335df-9543-4738-8397-dbb472a1db68] [Similarity score:             0.796163] from  llama_parse  import  LlamaParse\n",
      "\n",
      "parser = LlamaParse(\n",
      "    api_key= \"llx-...\" ,   # can also...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 8dc576ac-2aff-4298-9f3b-d20a0c1d75eb] [Similarity score:             0.774369] Long-Context Embedding Models:  Explore models like M2-BERT-80M-32k-retrieval tackling the embedd...\n",
      "> [Node 025e96e5-fb70-48e9-9aa2-3f15993e7ab8] [Similarity score:             0.77205] Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing ...\n",
      "> [Node fee183ea-24df-4e3a-bf8e-19a7c1a06d0b] [Similarity score:             0.763075] ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"Building Advanced RAG over Complex PDFs with Llama...\n",
      "> [Node aba30086-7f10-4266-9bff-908ffb633096] [Similarity score:             0.762987] query_by = CreatedBy(type=CreatedByType.AI, model_name=\"gpt-4\")\n",
      "reference_answer = \"Yes it is.\"\n",
      "r...\n",
      "> [Node 5b4425ba-54c9-450d-9643-7a0462a1a717] [Similarity score:             0.761947] Project ,  Tweet . We introduced a LlamaPack that enables the setup of a fully local RAG pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 8dc576ac-2aff-4298-9f3b-d20a0c1d75eb] [Similarity score:             0.774369] Long-Context Embedding Models:  Explore models like M2-BERT-80M-32k-retrieval tackling the embedd...\n",
      "> [Node 025e96e5-fb70-48e9-9aa2-3f15993e7ab8] [Similarity score:             0.77205] Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing ...\n",
      "> [Node fee183ea-24df-4e3a-bf8e-19a7c1a06d0b] [Similarity score:             0.763075] ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"Building Advanced RAG over Complex PDFs with Llama...\n",
      "> [Node aba30086-7f10-4266-9bff-908ffb633096] [Similarity score:             0.762987] query_by = CreatedBy(type=CreatedByType.AI, model_name=\"gpt-4\")\n",
      "reference_answer = \"Yes it is.\"\n",
      "r...\n",
      "> [Node 5b4425ba-54c9-450d-9643-7a0462a1a717] [Similarity score:             0.761947] Project ,  Tweet . We introduced a LlamaPack that enables the setup of a fully local RAG pipeline...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node ae7cd52b-79bc-4316-bd0c-a3efbff8facd] [Similarity score:             0.857712] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 4fe81f5b-f19e-437e-9b5b-f5acff5a45fb] [Similarity score:             0.786933] LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engin...\n",
      "> [Node 8d86ef6e-2dbe-498d-b827-d729eaef1f2d] [Similarity score:             0.784861] With LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions f...\n",
      "> [Node 3142f27b-deed-452a-ae54-f4a1f503d746] [Similarity score:             0.782067] Evaluation suite from `InformationRetrievalEvaluator` Conclusion We successfully finetuned an emb...\n",
      "> [Node c26efc62-6c0e-43b0-b167-ce5418e84c76] [Similarity score:             0.780554] Generation must be able to make good use of the retrieved documents to sufficiently answer the us...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node ae7cd52b-79bc-4316-bd0c-a3efbff8facd] [Similarity score:             0.857712] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 4fe81f5b-f19e-437e-9b5b-f5acff5a45fb] [Similarity score:             0.786933] LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engin...\n",
      "> [Node 8d86ef6e-2dbe-498d-b827-d729eaef1f2d] [Similarity score:             0.784861] With LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions f...\n",
      "> [Node 3142f27b-deed-452a-ae54-f4a1f503d746] [Similarity score:             0.782067] Evaluation suite from `InformationRetrievalEvaluator` Conclusion We successfully finetuned an emb...\n",
      "> [Node c26efc62-6c0e-43b0-b167-ce5418e84c76] [Similarity score:             0.780554] Generation must be able to make good use of the retrieved documents to sufficiently answer the us...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node d852c40d-593a-4a7a-859d-1f2fa4194e0f] [Similarity score:             0.807009] To illustrate how the llama-index-networks package can be used, we consider three made-up charact...\n",
      "> [Node 299a9760-547c-478d-a933-fa95d754c5d5] [Similarity score:             0.805983] Internal networks: another potential use case In addition to powering RAG marketplaces, we forese...\n",
      "> [Node e30c31a1-dda1-44bd-85ea-94d06be220a1] [Similarity score:             0.799092] Querying a network of knowledge with llama-index-networks\n",
      "The main premise behind RAG is the inje...\n",
      "> [Node 013046e2-0124-47d4-831d-3a90c432b0d4] [Similarity score:             0.789012] ), the query is sent to all contributors. Their responses are stored as new  Nodes  and a respons...\n",
      "> [Node 61d30fae-9dcd-488f-b745-0cb8279f6b55] [Similarity score:             0.786035] Note, that the dotenv file  .env.contributor  contains settings for the service as well as any ne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node d852c40d-593a-4a7a-859d-1f2fa4194e0f] [Similarity score:             0.807009] To illustrate how the llama-index-networks package can be used, we consider three made-up charact...\n",
      "> [Node 299a9760-547c-478d-a933-fa95d754c5d5] [Similarity score:             0.805983] Internal networks: another potential use case In addition to powering RAG marketplaces, we forese...\n",
      "> [Node e30c31a1-dda1-44bd-85ea-94d06be220a1] [Similarity score:             0.799092] Querying a network of knowledge with llama-index-networks\n",
      "The main premise behind RAG is the inje...\n",
      "> [Node 013046e2-0124-47d4-831d-3a90c432b0d4] [Similarity score:             0.789012] ), the query is sent to all contributors. Their responses are stored as new  Nodes  and a respons...\n",
      "> [Node 61d30fae-9dcd-488f-b745-0cb8279f6b55] [Similarity score:             0.786035] Note, that the dotenv file  .env.contributor  contains settings for the service as well as any ne...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node ae7cd52b-79bc-4316-bd0c-a3efbff8facd] [Similarity score:             0.884632] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 4fe81f5b-f19e-437e-9b5b-f5acff5a45fb] [Similarity score:             0.821731] LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engin...\n",
      "> [Node e9ca18c3-6c8b-4d00-9491-fdb5520e981f] [Similarity score:             0.819564] A Cheat Sheet and Some Recipes For Building Advanced RAG\n",
      "It‚Äôs the start of a new year and perhaps...\n",
      "> [Node 71d13fb7-beef-4f11-aebc-807945ac20a2] [Similarity score:             0.81418] Build ParamTuner object \n",
      "param_dict = { \"chunk_size\" : [ 256 ,  512 ,  1024 ]}  # params/values t...\n",
      "> [Node 1cbb3b50-03fc-46fa-9b4c-672ae2fac2e5] [Similarity score:             0.81238] LlamaIndex Generator-Enhanced Retrieval Recipe  ( notebook guide ) : from  llama_index.llms  impo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node ae7cd52b-79bc-4316-bd0c-a3efbff8facd] [Similarity score:             0.884632] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 4fe81f5b-f19e-437e-9b5b-f5acff5a45fb] [Similarity score:             0.821731] LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engin...\n",
      "> [Node e9ca18c3-6c8b-4d00-9491-fdb5520e981f] [Similarity score:             0.819564] A Cheat Sheet and Some Recipes For Building Advanced RAG\n",
      "It‚Äôs the start of a new year and perhaps...\n",
      "> [Node 71d13fb7-beef-4f11-aebc-807945ac20a2] [Similarity score:             0.81418] Build ParamTuner object \n",
      "param_dict = { \"chunk_size\" : [ 256 ,  512 ,  1024 ]}  # params/values t...\n",
      "> [Node 1cbb3b50-03fc-46fa-9b4c-672ae2fac2e5] [Similarity score:             0.81238] LlamaIndex Generator-Enhanced Retrieval Recipe  ( notebook guide ) : from  llama_index.llms  impo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 07e353a4-d8d7-48b7-988d-d091fcfe7a0b] [Similarity score:             0.804674] LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaIndex Community! ü¶ô Welcome to another exciting weekly...\n",
      "> [Node 848f7542-9d0f-4ae8-9748-a68d35aea13f] [Similarity score:             0.804029] LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ ...\n",
      "> [Node ebeead3d-3bf3-4d34-8d5a-3414ce1ec656] [Similarity score:             0.800468] Docs . ‚ú® Feature Releases and Enhancements: We launched a partnership with Google Gemini, offerin...\n",
      "> [Node 4c99e07f-5c04-483e-993b-5c263aa4aa4f] [Similarity score:             0.798163] LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Llama Followers ü¶ô, We‚Äôre excited to bring you another...\n",
      "> [Node 585dc161-b78b-48d2-be01-4285178e3a1d] [Similarity score:             0.791984] The Ultra variants (which are not yet publicly available) outperform GPT-4 on benchmarks from MML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 07e353a4-d8d7-48b7-988d-d091fcfe7a0b] [Similarity score:             0.804674] LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaIndex Community! ü¶ô Welcome to another exciting weekly...\n",
      "> [Node 848f7542-9d0f-4ae8-9748-a68d35aea13f] [Similarity score:             0.804029] LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ ...\n",
      "> [Node ebeead3d-3bf3-4d34-8d5a-3414ce1ec656] [Similarity score:             0.800468] Docs . ‚ú® Feature Releases and Enhancements: We launched a partnership with Google Gemini, offerin...\n",
      "> [Node 4c99e07f-5c04-483e-993b-5c263aa4aa4f] [Similarity score:             0.798163] LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Llama Followers ü¶ô, We‚Äôre excited to bring you another...\n",
      "> [Node 585dc161-b78b-48d2-be01-4285178e3a1d] [Similarity score:             0.791984] The Ultra variants (which are not yet publicly available) outperform GPT-4 on benchmarks from MML...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node ea376d9c-46a4-41da-b6fc-78a80e20b2ac] [Similarity score:             0.806621] This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, ...\n",
      "> [Node 2e3e2ddc-f871-4aea-993a-25161e79bcef] [Similarity score:             0.795336] LlamaIndex v0.10\n",
      "Today we‚Äôre excited to launch LlamaIndex v0.10.0. It is by far the biggest updat...\n",
      "> [Node 403117eb-3ca5-4b6e-8cb6-8861e0df1ed7] [Similarity score:             0.79467] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> [Node 52c26895-18c0-4a59-b6da-eb33fd80ba20] [Similarity score:             0.793389] LlamaIndex‚Äôs founder, Jerry Liu, says: Our community is everything at LlamaIndex. We love seeing ...\n",
      "> [Node d1109c35-3543-4ec2-a911-318afc5d2c58] [Similarity score:             0.781436] LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year ago, Jerry pushed his  first commit  to GPT Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node ea376d9c-46a4-41da-b6fc-78a80e20b2ac] [Similarity score:             0.806621] This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, ...\n",
      "> [Node 2e3e2ddc-f871-4aea-993a-25161e79bcef] [Similarity score:             0.795336] LlamaIndex v0.10\n",
      "Today we‚Äôre excited to launch LlamaIndex v0.10.0. It is by far the biggest updat...\n",
      "> [Node 403117eb-3ca5-4b6e-8cb6-8861e0df1ed7] [Similarity score:             0.79467] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> [Node 52c26895-18c0-4a59-b6da-eb33fd80ba20] [Similarity score:             0.793389] LlamaIndex‚Äôs founder, Jerry Liu, says: Our community is everything at LlamaIndex. We love seeing ...\n",
      "> [Node d1109c35-3543-4ec2-a911-318afc5d2c58] [Similarity score:             0.781436] LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year ago, Jerry pushed his  first commit  to GPT Index...\n",
      "\n",
      "Batch processing of predictions:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                                                                              | 1/8 [00:06<00:48,  6.88s/it]\u001b[A\n",
      "Batch processing of predictions:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                     | 2/8 [00:07<00:18,  3.15s/it]\u001b[A\n",
      "Batch processing of predictions:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                            | 3/8 [00:08<00:10,  2.01s/it]\u001b[A\n",
      "Batch processing of predictions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 4/8 [00:08<00:05,  1.48s/it]\u001b[A\n",
      "Batch processing of predictions:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                          | 5/8 [00:08<00:03,  1.01s/it]\u001b[A\n",
      "Batch processing of predictions:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                 | 6/8 [00:09<00:01,  1.38it/s]\u001b[A\n",
      "Batch processing of predictions:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 7/8 [00:09<00:00,  1.82it/s]\u001b[A\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.20s/it]\u001b[A\n",
      "\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 53b1e018-20fa-4c9b-a750-700ea14e9950] [Similarity score:             0.819326] In the past each developer would spend time re-indexing/experimenting with the same sources of da...\n",
      "> [Node 5ab8ae8a-86a1-4ac5-9bca-03c1294ebbf5] [Similarity score:             0.810833] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> [Node e0595867-ce41-44b5-b6b7-02ec18abdd54] [Similarity score:             0.803383] Try it yourself We‚Äôve opened up an official waitlist for LlamaCloud. Here's how you can get invol...\n",
      "> [Node 3db2fb35-442b-4a1d-a170-d66b9a6fbf56] [Similarity score:             0.790192] Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to ...\n",
      "> [Node fb271007-3770-4c0d-9b62-a82fdbc7063a] [Similarity score:             0.788645] LlamaCloud - Built for Enterprise LLM App Builders\n",
      "RAG is only as Good as your Data Building prod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 53b1e018-20fa-4c9b-a750-700ea14e9950] [Similarity score:             0.819326] In the past each developer would spend time re-indexing/experimenting with the same sources of da...\n",
      "> [Node 5ab8ae8a-86a1-4ac5-9bca-03c1294ebbf5] [Similarity score:             0.810833] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> [Node e0595867-ce41-44b5-b6b7-02ec18abdd54] [Similarity score:             0.803383] Try it yourself We‚Äôve opened up an official waitlist for LlamaCloud. Here's how you can get invol...\n",
      "> [Node 3db2fb35-442b-4a1d-a170-d66b9a6fbf56] [Similarity score:             0.790192] Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to ...\n",
      "> [Node fb271007-3770-4c0d-9b62-a82fdbc7063a] [Similarity score:             0.788645] LlamaCloud - Built for Enterprise LLM App Builders\n",
      "RAG is only as Good as your Data Building prod...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.866877] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node a0caee70-dc5d-4ca2-b434-e54632711c06] [Similarity score:             0.854334] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node ba4a8a14-2d17-4544-a666-32413083e7e5] [Similarity score:             0.818307] from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n",
      "\n",
      " # Initialize Huggingface-b...\n",
      "> [Node fb68a165-9047-4ca2-97d9-18d37a9e0fab] [Similarity score:             0.801813] ‚ÄúWhat did the author do growing up?‚Äù In this example, we retrieved two chunks from the index both...\n",
      "> [Node 65045de8-baeb-4093-8356-6a1719fa2a98] [Similarity score:             0.789648] Tweet The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.866877] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node a0caee70-dc5d-4ca2-b434-e54632711c06] [Similarity score:             0.854334] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node ba4a8a14-2d17-4544-a666-32413083e7e5] [Similarity score:             0.818307] from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n",
      "\n",
      " # Initialize Huggingface-b...\n",
      "> [Node fb68a165-9047-4ca2-97d9-18d37a9e0fab] [Similarity score:             0.801813] ‚ÄúWhat did the author do growing up?‚Äù In this example, we retrieved two chunks from the index both...\n",
      "> [Node 65045de8-baeb-4093-8356-6a1719fa2a98] [Similarity score:             0.789648] Tweet The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 1fc463fd-5baf-4e64-8d43-c6a7fee65bb7] [Similarity score:             0.7637] Before LlamaCloud, building even a simple application took forever because we needed to write our...\n",
      "> [Node e0595867-ce41-44b5-b6b7-02ec18abdd54] [Similarity score:             0.741164] Try it yourself We‚Äôve opened up an official waitlist for LlamaCloud. Here's how you can get invol...\n",
      "> [Node fb271007-3770-4c0d-9b62-a82fdbc7063a] [Similarity score:             0.736787] LlamaCloud - Built for Enterprise LLM App Builders\n",
      "RAG is only as Good as your Data Building prod...\n",
      "> [Node df64a40b-00c7-4a45-a590-31128013bc52] [Similarity score:             0.736685] Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud\n",
      "The Chall...\n",
      "> [Node 5ab8ae8a-86a1-4ac5-9bca-03c1294ebbf5] [Similarity score:             0.734541] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 1fc463fd-5baf-4e64-8d43-c6a7fee65bb7] [Similarity score:             0.7637] Before LlamaCloud, building even a simple application took forever because we needed to write our...\n",
      "> [Node e0595867-ce41-44b5-b6b7-02ec18abdd54] [Similarity score:             0.741164] Try it yourself We‚Äôve opened up an official waitlist for LlamaCloud. Here's how you can get invol...\n",
      "> [Node fb271007-3770-4c0d-9b62-a82fdbc7063a] [Similarity score:             0.736787] LlamaCloud - Built for Enterprise LLM App Builders\n",
      "RAG is only as Good as your Data Building prod...\n",
      "> [Node df64a40b-00c7-4a45-a590-31128013bc52] [Similarity score:             0.736685] Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud\n",
      "The Chall...\n",
      "> [Node 5ab8ae8a-86a1-4ac5-9bca-03c1294ebbf5] [Similarity score:             0.734541] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node a0caee70-dc5d-4ca2-b434-e54632711c06] [Similarity score:             0.845851] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 3ad42e4f-17b2-4d0d-8ba2-297acbc1adf0] [Similarity score:             0.8416] This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, u...\n",
      "> [Node 1f9a436c-5d0b-47d9-8e77-f9781b526dbe] [Similarity score:             0.827522] Data Querying:  Data retrieval, response synthesis, multi-step interactions over your data. Llama...\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.825274] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node 21113386-7ad2-4477-91e6-0e6e7dac8a72] [Similarity score:             0.823731] This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node a0caee70-dc5d-4ca2-b434-e54632711c06] [Similarity score:             0.845851] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 3ad42e4f-17b2-4d0d-8ba2-297acbc1adf0] [Similarity score:             0.8416] This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, u...\n",
      "> [Node 1f9a436c-5d0b-47d9-8e77-f9781b526dbe] [Similarity score:             0.827522] Data Querying:  Data retrieval, response synthesis, multi-step interactions over your data. Llama...\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.825274] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node 21113386-7ad2-4477-91e6-0e6e7dac8a72] [Similarity score:             0.823731] This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node a0ede49d-9687-4bfe-82f7-2a5ec12de9e8] [Similarity score:             0.783462] Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Str...\n",
      "> [Node 05d961cf-f2d9-4130-87e1-ce7b650a5349] [Similarity score:             0.776914] StreamlitChatPack Class The  StreamlitChatPack  class extends the base LlamaPack, setting up the ...\n",
      "> [Node ef76a35e-4376-4353-b0ba-8ded49a3ce67] [Similarity score:             0.775841] This powerful tool translates natural language queries into SQL, handles the execution of these q...\n",
      "> [Node 205da59f-c549-47ea-8881-75c24615dea3] [Similarity score:             0.771985] This is where the app sets up the engine responsible for converting natural language queries into...\n",
      "> [Node a3ed95d9-64f6-4377-9b84-397a15cf7182] [Similarity score:             0.76646] # Sidebar for database schema viewer \n",
      "st.sidebar.markdown( \"## Database Schema Viewer\" )\n",
      "\n",
      " # Crea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node a0ede49d-9687-4bfe-82f7-2a5ec12de9e8] [Similarity score:             0.783462] Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Str...\n",
      "> [Node 05d961cf-f2d9-4130-87e1-ce7b650a5349] [Similarity score:             0.776914] StreamlitChatPack Class The  StreamlitChatPack  class extends the base LlamaPack, setting up the ...\n",
      "> [Node ef76a35e-4376-4353-b0ba-8ded49a3ce67] [Similarity score:             0.775841] This powerful tool translates natural language queries into SQL, handles the execution of these q...\n",
      "> [Node 205da59f-c549-47ea-8881-75c24615dea3] [Similarity score:             0.771985] This is where the app sets up the engine responsible for converting natural language queries into...\n",
      "> [Node a3ed95d9-64f6-4377-9b84-397a15cf7182] [Similarity score:             0.76646] # Sidebar for database schema viewer \n",
      "st.sidebar.markdown( \"## Database Schema Viewer\" )\n",
      "\n",
      " # Crea...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node a0caee70-dc5d-4ca2-b434-e54632711c06] [Similarity score:             0.815245] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.807521] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node ba4a8a14-2d17-4544-a666-32413083e7e5] [Similarity score:             0.802582] from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n",
      "\n",
      " # Initialize Huggingface-b...\n",
      "> [Node fb68a165-9047-4ca2-97d9-18d37a9e0fab] [Similarity score:             0.792197] ‚ÄúWhat did the author do growing up?‚Äù In this example, we retrieved two chunks from the index both...\n",
      "> [Node 65045de8-baeb-4093-8356-6a1719fa2a98] [Similarity score:             0.739515] Tweet The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node a0caee70-dc5d-4ca2-b434-e54632711c06] [Similarity score:             0.815245] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.807521] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node ba4a8a14-2d17-4544-a666-32413083e7e5] [Similarity score:             0.802582] from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n",
      "\n",
      " # Initialize Huggingface-b...\n",
      "> [Node fb68a165-9047-4ca2-97d9-18d37a9e0fab] [Similarity score:             0.792197] ‚ÄúWhat did the author do growing up?‚Äù In this example, we retrieved two chunks from the index both...\n",
      "> [Node 65045de8-baeb-4093-8356-6a1719fa2a98] [Similarity score:             0.739515] Tweet The TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.750464] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node ba4a8a14-2d17-4544-a666-32413083e7e5] [Similarity score:             0.735787] from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n",
      "\n",
      " # Initialize Huggingface-b...\n",
      "> [Node d8d3d068-6ea6-421f-acd6-64de95720fdb] [Similarity score:             0.70746] You get a clear indication of which test cases ‚Äúpass‚Äù, given your evaluation criteria Testing in ...\n",
      "> [Node 9a7e5f61-4092-4273-aa05-da6eb83f0120] [Similarity score:             0.702739] 3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] ...\n",
      "> [Node b6804925-556f-4309-a802-3685875ed1ec] [Similarity score:             0.691693] ### Feedback :  \"\" \"\n",
      "\n",
      "prometheus_relevancy_refine_prompt_template = \" \"\" ### Task   Description :...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 35309d0a-75de-4ad1-998d-d8dff92663ea] [Similarity score:             0.750464] He also studied philosophy in college, but switched to AI after becoming bored with it. He then t...\n",
      "> [Node ba4a8a14-2d17-4544-a666-32413083e7e5] [Similarity score:             0.735787] from  trulens_eval  import  TruLlama, Tru, Query, Feedback, feedback\n",
      "\n",
      " # Initialize Huggingface-b...\n",
      "> [Node d8d3d068-6ea6-421f-acd6-64de95720fdb] [Similarity score:             0.70746] You get a clear indication of which test cases ‚Äúpass‚Äù, given your evaluation criteria Testing in ...\n",
      "> [Node 9a7e5f61-4092-4273-aa05-da6eb83f0120] [Similarity score:             0.702739] 3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] ...\n",
      "> [Node b6804925-556f-4309-a802-3685875ed1ec] [Similarity score:             0.691693] ### Feedback :  \"\" \"\n",
      "\n",
      "prometheus_relevancy_refine_prompt_template = \" \"\" ### Task   Description :...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node bd6f8b5c-0048-47cb-b2e5-4cf2abfb1f1e] [Similarity score:             0.817297] Being able to persist this data enables processing the data once and then being able to query it ...\n",
      "> [Node c8234012-9a28-4e0a-bf47-ea4700e16b19] [Similarity score:             0.805079] Also check out the reference notebook below! Reading data from MongoDB:  link Various Indexes in ...\n",
      "> [Node 43c096ab-fefa-4387-bec0-5e22187ad35b] [Similarity score:             0.804225] Example question: Who is the most recent UK prime minister? There are 2 main paradigms currently ...\n",
      "> [Node 739dd011-4f54-4463-a566-10e09a88dbb1] [Similarity score:             0.803274] Retrieving Privacy-Safe Documents Over A Network\n",
      "In a  recent blog post , we introduced our  llam...\n",
      "> [Node e1a1f644-8bc5-42bb-b2bd-8e5bd86a0867] [Similarity score:             0.803054] Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\n",
      "Co-authors: Prakul Agarwal ‚Äî ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node bd6f8b5c-0048-47cb-b2e5-4cf2abfb1f1e] [Similarity score:             0.817297] Being able to persist this data enables processing the data once and then being able to query it ...\n",
      "> [Node c8234012-9a28-4e0a-bf47-ea4700e16b19] [Similarity score:             0.805079] Also check out the reference notebook below! Reading data from MongoDB:  link Various Indexes in ...\n",
      "> [Node 43c096ab-fefa-4387-bec0-5e22187ad35b] [Similarity score:             0.804225] Example question: Who is the most recent UK prime minister? There are 2 main paradigms currently ...\n",
      "> [Node 739dd011-4f54-4463-a566-10e09a88dbb1] [Similarity score:             0.803274] Retrieving Privacy-Safe Documents Over A Network\n",
      "In a  recent blog post , we introduced our  llam...\n",
      "> [Node e1a1f644-8bc5-42bb-b2bd-8e5bd86a0867] [Similarity score:             0.803054] Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\n",
      "Co-authors: Prakul Agarwal ‚Äî ...\n",
      "\n",
      "Batch processing of predictions:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                                                                              | 1/8 [00:06<00:45,  6.50s/it]\u001b[A\n",
      "Batch processing of predictions:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                     | 2/8 [00:06<00:17,  2.88s/it]\u001b[A\n",
      "Batch processing of predictions:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                            | 3/8 [00:07<00:08,  1.65s/it]\u001b[A\n",
      "Batch processing of predictions:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                          | 5/8 [00:07<00:02,  1.22it/s]\u001b[A\n",
      "Batch processing of predictions:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                 | 6/8 [00:07<00:01,  1.39it/s]\u001b[A\n",
      "Batch processing of predictions:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 7/8 [00:08<00:00,  1.80it/s]\u001b[A\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  1.26s/it]\u001b[A\n",
      "\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 19c3c775-511c-4794-bc6e-c58d061ab92b] [Similarity score:             0.839761] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 5a9f511a-cf5e-47e7-94ec-b30ab3f88ba6] [Similarity score:             0.807441] First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powere...\n",
      "> [Node 8a9ae6b8-3ebe-412c-95e7-81389137211b] [Similarity score:             0.804634] tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
      "\n",
      " # create our agents \n",
      "worker1 = Functi...\n",
      "> [Node 0e96da47-6d49-4889-940c-bab9bc1305d8] [Similarity score:             0.798559] # Set up the launcher for local testing \n",
      " from  llama_agents  import  LocalLauncher\n",
      "\n",
      "launcher = L...\n",
      "> [Node 2e56ccb1-388d-444a-8a10-4c54555904c4] [Similarity score:             0.795586] Check out  the repo  to learn more, especially our library of  examples . We're excited to see wh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 19c3c775-511c-4794-bc6e-c58d061ab92b] [Similarity score:             0.839761] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 5a9f511a-cf5e-47e7-94ec-b30ab3f88ba6] [Similarity score:             0.807441] First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powere...\n",
      "> [Node 8a9ae6b8-3ebe-412c-95e7-81389137211b] [Similarity score:             0.804634] tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
      "\n",
      " # create our agents \n",
      "worker1 = Functi...\n",
      "> [Node 0e96da47-6d49-4889-940c-bab9bc1305d8] [Similarity score:             0.798559] # Set up the launcher for local testing \n",
      " from  llama_agents  import  LocalLauncher\n",
      "\n",
      "launcher = L...\n",
      "> [Node 2e56ccb1-388d-444a-8a10-4c54555904c4] [Similarity score:             0.795586] Check out  the repo  to learn more, especially our library of  examples . We're excited to see wh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node b63739f3-c42a-46e5-870c-b7c42412ea4e] [Similarity score:             0.789533] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.788115] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node 7829c363-51b9-4c2e-879d-e88ac40511be] [Similarity score:             0.787364] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.779287] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node f1b2a2b8-4296-42b6-9b65-5790705632d0] [Similarity score:             0.767192] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node b63739f3-c42a-46e5-870c-b7c42412ea4e] [Similarity score:             0.789533] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.788115] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node 7829c363-51b9-4c2e-879d-e88ac40511be] [Similarity score:             0.787364] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.779287] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node f1b2a2b8-4296-42b6-9b65-5790705632d0] [Similarity score:             0.767192] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node b63739f3-c42a-46e5-870c-b7c42412ea4e] [Similarity score:             0.760125] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> [Node 7829c363-51b9-4c2e-879d-e88ac40511be] [Similarity score:             0.752583] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.752451] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node f1b2a2b8-4296-42b6-9b65-5790705632d0] [Similarity score:             0.743849] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.73401] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node b63739f3-c42a-46e5-870c-b7c42412ea4e] [Similarity score:             0.760125] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> [Node 7829c363-51b9-4c2e-879d-e88ac40511be] [Similarity score:             0.752583] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.752451] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node f1b2a2b8-4296-42b6-9b65-5790705632d0] [Similarity score:             0.743849] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.73401] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 1c80265c-1dc1-4341-8cd2-58b601697567] [Similarity score:             0.736285] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 8197c80e-c6b5-44ce-ba6a-f57ed4e0e176] [Similarity score:             0.715305] Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controlla...\n",
      "> [Node 28b1f455-a238-49f0-bb33-b631961be79b] [Similarity score:             0.714345] ‚≠êÔ∏è Follow me on  LinkedIn  for updates on Large Language Models ‚≠êÔ∏è\n",
      "> [Node 0ff8b2cd-b664-438f-8c77-ce775b0fe9eb] [Similarity score:             0.70452] The system prompt for the meta-agent /top-agent: \n",
      "  You are an agent designed to answer queries a...\n",
      "> [Node 66090fce-295b-4481-b403-c75f19479a1d] [Similarity score:             0.704104] ========================\n",
      " \n",
      " \n",
      "   Observation: LlamaIndex offers data connectors that enable you to...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 1c80265c-1dc1-4341-8cd2-58b601697567] [Similarity score:             0.736285] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 8197c80e-c6b5-44ce-ba6a-f57ed4e0e176] [Similarity score:             0.715305] Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controlla...\n",
      "> [Node 28b1f455-a238-49f0-bb33-b631961be79b] [Similarity score:             0.714345] ‚≠êÔ∏è Follow me on  LinkedIn  for updates on Large Language Models ‚≠êÔ∏è\n",
      "> [Node 0ff8b2cd-b664-438f-8c77-ce775b0fe9eb] [Similarity score:             0.70452] The system prompt for the meta-agent /top-agent: \n",
      "  You are an agent designed to answer queries a...\n",
      "> [Node 66090fce-295b-4481-b403-c75f19479a1d] [Similarity score:             0.704104] ========================\n",
      " \n",
      " \n",
      "   Observation: LlamaIndex offers data connectors that enable you to...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node f6b433d2-e7ec-480b-babe-62d5b4af60c4] [Similarity score:             0.886135] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node a39e8e8e-dea9-4101-8825-7156333e46ad] [Similarity score:             0.759657] LlamaIndex and Transformers Agents\n",
      "Summary Agents are a popular use-case for Large Language Model...\n",
      "> [Node efdab717-aadc-459e-b91c-4c87f8a0ffa6] [Similarity score:             0.755879] It serves open-source models locally, handles embeddings, and allows you to train or fine-tune mo...\n",
      "> [Node e06225c3-aac0-43df-af70-bf7d1a6867a8] [Similarity score:             0.755539] For those familiar with the whirlwind nature of hackathons, you‚Äôll understand when I say that tim...\n",
      "> [Node 3e1afde4-2ed2-45ac-8c44-1618f61bba16] [Similarity score:             0.755135] Tutorials from the LlamaIndex Team. Sourabh   tutorial  on SEC Insights, End-to-End Guide on  sec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node f6b433d2-e7ec-480b-babe-62d5b4af60c4] [Similarity score:             0.886135] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node a39e8e8e-dea9-4101-8825-7156333e46ad] [Similarity score:             0.759657] LlamaIndex and Transformers Agents\n",
      "Summary Agents are a popular use-case for Large Language Model...\n",
      "> [Node efdab717-aadc-459e-b91c-4c87f8a0ffa6] [Similarity score:             0.755879] It serves open-source models locally, handles embeddings, and allows you to train or fine-tune mo...\n",
      "> [Node e06225c3-aac0-43df-af70-bf7d1a6867a8] [Similarity score:             0.755539] For those familiar with the whirlwind nature of hackathons, you‚Äôll understand when I say that tim...\n",
      "> [Node 3e1afde4-2ed2-45ac-8c44-1618f61bba16] [Similarity score:             0.755135] Tutorials from the LlamaIndex Team. Sourabh   tutorial  on SEC Insights, End-to-End Guide on  sec...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 1c80265c-1dc1-4341-8cd2-58b601697567] [Similarity score:             0.740211] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 8197c80e-c6b5-44ce-ba6a-f57ed4e0e176] [Similarity score:             0.717422] Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controlla...\n",
      "> [Node 72445f17-e72c-475d-adbe-325faefcb6be] [Similarity score:             0.712402] LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIndex Family! ü¶ô Welcome to another thrilling weekly ...\n",
      "> [Node 0ff8b2cd-b664-438f-8c77-ce775b0fe9eb] [Similarity score:             0.710181] The system prompt for the meta-agent /top-agent: \n",
      "  You are an agent designed to answer queries a...\n",
      "> [Node f93922a9-6978-49c9-80aa-5240e196fddf] [Similarity score:             0.699791] RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 1c80265c-1dc1-4341-8cd2-58b601697567] [Similarity score:             0.740211] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 8197c80e-c6b5-44ce-ba6a-f57ed4e0e176] [Similarity score:             0.717422] Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controlla...\n",
      "> [Node 72445f17-e72c-475d-adbe-325faefcb6be] [Similarity score:             0.712402] LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIndex Family! ü¶ô Welcome to another thrilling weekly ...\n",
      "> [Node 0ff8b2cd-b664-438f-8c77-ce775b0fe9eb] [Similarity score:             0.710181] The system prompt for the meta-agent /top-agent: \n",
      "  You are an agent designed to answer queries a...\n",
      "> [Node f93922a9-6978-49c9-80aa-5240e196fddf] [Similarity score:             0.699791] RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 1c80265c-1dc1-4341-8cd2-58b601697567] [Similarity score:             0.880629] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 66090fce-295b-4481-b403-c75f19479a1d] [Similarity score:             0.85685] ========================\n",
      " \n",
      " \n",
      "   Observation: LlamaIndex offers data connectors that enable you to...\n",
      "> [Node 0ff8b2cd-b664-438f-8c77-ce775b0fe9eb] [Similarity score:             0.839091] The system prompt for the meta-agent /top-agent: \n",
      "  You are an agent designed to answer queries a...\n",
      "> [Node 28b1f455-a238-49f0-bb33-b631961be79b] [Similarity score:             0.83901] ‚≠êÔ∏è Follow me on  LinkedIn  for updates on Large Language Models ‚≠êÔ∏è\n",
      "> [Node 9d08a147-72b4-4212-985d-af1d8f33f9b1] [Similarity score:             0.833076] You can see both of the agents running, and at the bottom you can inject a task like the query ‚ÄúW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 1c80265c-1dc1-4341-8cd2-58b601697567] [Similarity score:             0.880629] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 66090fce-295b-4481-b403-c75f19479a1d] [Similarity score:             0.85685] ========================\n",
      " \n",
      " \n",
      "   Observation: LlamaIndex offers data connectors that enable you to...\n",
      "> [Node 0ff8b2cd-b664-438f-8c77-ce775b0fe9eb] [Similarity score:             0.839091] The system prompt for the meta-agent /top-agent: \n",
      "  You are an agent designed to answer queries a...\n",
      "> [Node 28b1f455-a238-49f0-bb33-b631961be79b] [Similarity score:             0.83901] ‚≠êÔ∏è Follow me on  LinkedIn  for updates on Large Language Models ‚≠êÔ∏è\n",
      "> [Node 9d08a147-72b4-4212-985d-af1d8f33f9b1] [Similarity score:             0.833076] You can see both of the agents running, and at the bottom you can inject a task like the query ‚ÄúW...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node f6b433d2-e7ec-480b-babe-62d5b4af60c4] [Similarity score:             0.88898] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node efdab717-aadc-459e-b91c-4c87f8a0ffa6] [Similarity score:             0.769473] It serves open-source models locally, handles embeddings, and allows you to train or fine-tune mo...\n",
      "> [Node a39e8e8e-dea9-4101-8825-7156333e46ad] [Similarity score:             0.765849] LlamaIndex and Transformers Agents\n",
      "Summary Agents are a popular use-case for Large Language Model...\n",
      "> [Node e06225c3-aac0-43df-af70-bf7d1a6867a8] [Similarity score:             0.763805] For those familiar with the whirlwind nature of hackathons, you‚Äôll understand when I say that tim...\n",
      "> [Node 191c3822-14f9-4985-881e-64980a762a67] [Similarity score:             0.762387] LlamaIndex news special edition: OpenAI developer day!\n",
      "Hello Llama fans! Yesterday was a big day ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node f6b433d2-e7ec-480b-babe-62d5b4af60c4] [Similarity score:             0.88898] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node efdab717-aadc-459e-b91c-4c87f8a0ffa6] [Similarity score:             0.769473] It serves open-source models locally, handles embeddings, and allows you to train or fine-tune mo...\n",
      "> [Node a39e8e8e-dea9-4101-8825-7156333e46ad] [Similarity score:             0.765849] LlamaIndex and Transformers Agents\n",
      "Summary Agents are a popular use-case for Large Language Model...\n",
      "> [Node e06225c3-aac0-43df-af70-bf7d1a6867a8] [Similarity score:             0.763805] For those familiar with the whirlwind nature of hackathons, you‚Äôll understand when I say that tim...\n",
      "> [Node 191c3822-14f9-4985-881e-64980a762a67] [Similarity score:             0.762387] LlamaIndex news special edition: OpenAI developer day!\n",
      "Hello Llama fans! Yesterday was a big day ...\n",
      "\n",
      "Batch processing of predictions:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                                                                              | 1/8 [00:06<00:48,  6.91s/it]\u001b[A\n",
      "Batch processing of predictions:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                            | 3/8 [00:07<00:09,  1.84s/it]\u001b[A\n",
      "Batch processing of predictions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 4/8 [00:07<00:05,  1.29s/it]\u001b[A\n",
      "Batch processing of predictions:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                          | 5/8 [00:07<00:02,  1.08it/s]\u001b[A\n",
      "Batch processing of predictions:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 7/8 [00:07<00:00,  1.94it/s]\u001b[A\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.14s/it]\u001b[A\n",
      "\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/6 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.760899] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 9b2f6ad8-9a01-403d-b7b8-aac92eec99e0] [Similarity score:             0.750964] There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_ll...\n",
      "> [Node bc8748d2-5c7d-40c7-8119-677e9c717429] [Similarity score:             0.708871] We then create a Document object for each SEC filing. You can access Unstructured data loaders on...\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.704684] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node 6d99bffe-368e-4ca6-b12c-c8df384cc29c] [Similarity score:             0.699275] LlamaIndex Newsletter 2024-03-12\n",
      "Salutations, LlamaIndex fans! ü¶ô It's been another thrilling week...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.760899] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 9b2f6ad8-9a01-403d-b7b8-aac92eec99e0] [Similarity score:             0.750964] There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_ll...\n",
      "> [Node bc8748d2-5c7d-40c7-8119-677e9c717429] [Similarity score:             0.708871] We then create a Document object for each SEC filing. You can access Unstructured data loaders on...\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.704684] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node 6d99bffe-368e-4ca6-b12c-c8df384cc29c] [Similarity score:             0.699275] LlamaIndex Newsletter 2024-03-12\n",
      "Salutations, LlamaIndex fans! ü¶ô It's been another thrilling week...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 8197c80e-c6b5-44ce-ba6a-f57ed4e0e176] [Similarity score:             0.797019] Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controlla...\n",
      "> [Node 5b4425ba-54c9-450d-9643-7a0462a1a717] [Similarity score:             0.789494] Project ,  Tweet . We introduced a LlamaPack that enables the setup of a fully local RAG pipeline...\n",
      "> [Node efdab717-aadc-459e-b91c-4c87f8a0ffa6] [Similarity score:             0.78023] It serves open-source models locally, handles embeddings, and allows you to train or fine-tune mo...\n",
      "> [Node ab6f8f0a-149f-49ff-b42d-a228e2b7f40d] [Similarity score:             0.780006] Building An Intelligent Query-Response System with LlamaIndex and OpenLLM\n",
      "Over the past year, Lar...\n",
      "> [Node 90d5a874-012a-4b5c-b555-0ea38ce920be] [Similarity score:             0.774955] LlamaIndex Newsletter 2024-06-04\n",
      "Hello, LlamaIndex Family! ü¶ô We're thrilled to connect with you a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 8197c80e-c6b5-44ce-ba6a-f57ed4e0e176] [Similarity score:             0.797019] Code ,  Tweet . We have introduced  return_direct  feature in tools that enhances agent controlla...\n",
      "> [Node 5b4425ba-54c9-450d-9643-7a0462a1a717] [Similarity score:             0.789494] Project ,  Tweet . We introduced a LlamaPack that enables the setup of a fully local RAG pipeline...\n",
      "> [Node efdab717-aadc-459e-b91c-4c87f8a0ffa6] [Similarity score:             0.78023] It serves open-source models locally, handles embeddings, and allows you to train or fine-tune mo...\n",
      "> [Node ab6f8f0a-149f-49ff-b42d-a228e2b7f40d] [Similarity score:             0.780006] Building An Intelligent Query-Response System with LlamaIndex and OpenLLM\n",
      "Over the past year, Lar...\n",
      "> [Node 90d5a874-012a-4b5c-b555-0ea38ce920be] [Similarity score:             0.774955] LlamaIndex Newsletter 2024-06-04\n",
      "Hello, LlamaIndex Family! ü¶ô We're thrilled to connect with you a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 65a3b009-402c-4b66-a490-a105c186e814] [Similarity score:             0.743238] Ravi Theja   tutorial  on showcasing RAG with LlamaIndex on 15 Indian languages using Navarasa-2....\n",
      "> [Node 09cb975c-e82f-492e-82f1-8200ab9eb8a8] [Similarity score:             0.739994] Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a...\n",
      "> [Node d1d1d1fe-88fd-4f19-ab0d-015613bb453f] [Similarity score:             0.73708] Docs ,  Tweet . üëÄ Community Demos : MemoryCache: Mozilla‚Äôs new experimental project that curates ...\n",
      "> [Node ed311b55-d478-42ec-82ea-7abe5b5b0b06] [Similarity score:             0.732988] Lakshmi Narayana  tutorial  on Agentic RAG with LlamaIndex to enhance generative AI applications ...\n",
      "> [Node f4ef7315-d646-4ee8-8b48-d87f954d01a6] [Similarity score:             0.730928] LlamaIndex Newsletter 2024‚Äì01‚Äì23\n",
      "Hello LlamaIndex Explorers ü¶ô, Another exciting week at LlamaInde...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 65a3b009-402c-4b66-a490-a105c186e814] [Similarity score:             0.743238] Ravi Theja   tutorial  on showcasing RAG with LlamaIndex on 15 Indian languages using Navarasa-2....\n",
      "> [Node 09cb975c-e82f-492e-82f1-8200ab9eb8a8] [Similarity score:             0.739994] Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a...\n",
      "> [Node d1d1d1fe-88fd-4f19-ab0d-015613bb453f] [Similarity score:             0.73708] Docs ,  Tweet . üëÄ Community Demos : MemoryCache: Mozilla‚Äôs new experimental project that curates ...\n",
      "> [Node ed311b55-d478-42ec-82ea-7abe5b5b0b06] [Similarity score:             0.732988] Lakshmi Narayana  tutorial  on Agentic RAG with LlamaIndex to enhance generative AI applications ...\n",
      "> [Node f4ef7315-d646-4ee8-8b48-d87f954d01a6] [Similarity score:             0.730928] LlamaIndex Newsletter 2024‚Äì01‚Äì23\n",
      "Hello LlamaIndex Explorers ü¶ô, Another exciting week at LlamaInde...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 54e11644-4e1e-46e1-8a09-088f9ef29c81] [Similarity score:             0.812297] LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fans ü¶ô! Welcome back to our newsletter covering new ...\n",
      "> [Node ebeead3d-3bf3-4d34-8d5a-3414ce1ec656] [Similarity score:             0.812287] Docs . ‚ú® Feature Releases and Enhancements: We launched a partnership with Google Gemini, offerin...\n",
      "> [Node 09cb975c-e82f-492e-82f1-8200ab9eb8a8] [Similarity score:             0.809868] Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a...\n",
      "> [Node 13277cc5-5ff9-4c03-808c-18275f7851f3] [Similarity score:             0.806972] Register for free! ‚ú® Feature Releases and Enhancements: We introduced the LlamaIndex 0.9 version ...\n",
      "> [Node ea376d9c-46a4-41da-b6fc-78a80e20b2ac] [Similarity score:             0.806964] This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 54e11644-4e1e-46e1-8a09-088f9ef29c81] [Similarity score:             0.812297] LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fans ü¶ô! Welcome back to our newsletter covering new ...\n",
      "> [Node ebeead3d-3bf3-4d34-8d5a-3414ce1ec656] [Similarity score:             0.812287] Docs . ‚ú® Feature Releases and Enhancements: We launched a partnership with Google Gemini, offerin...\n",
      "> [Node 09cb975c-e82f-492e-82f1-8200ab9eb8a8] [Similarity score:             0.809868] Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a...\n",
      "> [Node 13277cc5-5ff9-4c03-808c-18275f7851f3] [Similarity score:             0.806972] Register for free! ‚ú® Feature Releases and Enhancements: We introduced the LlamaIndex 0.9 version ...\n",
      "> [Node ea376d9c-46a4-41da-b6fc-78a80e20b2ac] [Similarity score:             0.806964] This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.818749] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node b63739f3-c42a-46e5-870c-b7c42412ea4e] [Similarity score:             0.783143] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.780944] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 7829c363-51b9-4c2e-879d-e88ac40511be] [Similarity score:             0.744008] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node f1b2a2b8-4296-42b6-9b65-5790705632d0] [Similarity score:             0.74286] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 5165720d-0448-4bf9-93f5-67b87c81ad34] [Similarity score:             0.818749] We‚Äôll walk through a simple  Llama Pack  that gives the user a RAG pipeline setup with Voyage AI ...\n",
      "> [Node b63739f3-c42a-46e5-870c-b7c42412ea4e] [Similarity score:             0.783143] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> [Node 87d250f6-94cc-4995-8b66-10e8f341abd0] [Similarity score:             0.780944] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 7829c363-51b9-4c2e-879d-e88ac40511be] [Similarity score:             0.744008] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node f1b2a2b8-4296-42b6-9b65-5790705632d0] [Similarity score:             0.74286] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 2cdcfcec-59bb-42f5-a719-84ba0717096b] [Similarity score:             0.814157] Now, you can easily process complex documents like PDFs, PPTX, and Markdown files, ensuring clean...\n",
      "> [Node 09cb975c-e82f-492e-82f1-8200ab9eb8a8] [Similarity score:             0.806569] Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a...\n",
      "> [Node a8e236c0-25db-4dab-b602-eeedc16426f7] [Similarity score:             0.80018] Guide  to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic ...\n",
      "> [Node 0cbcc61e-85c6-4411-95a3-71a58d28aa34] [Similarity score:             0.799392] ‚úçÔ∏è Tutorials: Akash Mathur ‚Äôs  tutorial  on Data Management in LlamaIndex: Featuring LlamaCloud a...\n",
      "> [Node 300ca4c1-82bf-4385-82a3-3b7003c12c29] [Similarity score:             0.798582] Cookbook ,  Tweet . üí°¬†Demos: Automating Code Reviews,  project  by  Composio  with LlamaIndex aut...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 2cdcfcec-59bb-42f5-a719-84ba0717096b] [Similarity score:             0.814157] Now, you can easily process complex documents like PDFs, PPTX, and Markdown files, ensuring clean...\n",
      "> [Node 09cb975c-e82f-492e-82f1-8200ab9eb8a8] [Similarity score:             0.806569] Tweet ,  Docs . SQLRetriever:  We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a...\n",
      "> [Node a8e236c0-25db-4dab-b602-eeedc16426f7] [Similarity score:             0.80018] Guide  to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic ...\n",
      "> [Node 0cbcc61e-85c6-4411-95a3-71a58d28aa34] [Similarity score:             0.799392] ‚úçÔ∏è Tutorials: Akash Mathur ‚Äôs  tutorial  on Data Management in LlamaIndex: Featuring LlamaCloud a...\n",
      "> [Node 300ca4c1-82bf-4385-82a3-3b7003c12c29] [Similarity score:             0.798582] Cookbook ,  Tweet . üí°¬†Demos: Automating Code Reviews,  project  by  Composio  with LlamaIndex aut...\n",
      "\n",
      "Batch processing of predictions:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                                                                     | 1/6 [00:06<00:30,  6.19s/it]\u001b[A\n",
      "Batch processing of predictions:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                                    | 2/6 [00:06<00:11,  2.92s/it]\u001b[A\n",
      "Batch processing of predictions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 3/6 [00:07<00:05,  1.89s/it]\u001b[A\n",
      "Batch processing of predictions:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                  | 4/6 [00:07<00:02,  1.28s/it]\u001b[A\n",
      "Batch processing of predictions:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 5/6 [00:08<00:00,  1.13it/s]\u001b[A\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:09<00:00,  1.62s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_prediction_dataset = await synthetic_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e8f79a8bfe442392be769c8bf4c5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Querying a network of knowledge with llama-inde...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To illustrate how the llama-index-networks pack...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To illustrate how the llama-index-networks pack...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Querying a network of knowledge with llama-inde...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: To illustrate how the llama-index-networks pack...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: To illustrate how the llama-index-networks pack...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Launching the first GenAI-native document parsi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Launching the first GenAI-native document parsi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Launching the first GenAI-native document parsi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Launching the first GenAI-native document parsi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluation suite from `InformationRetrievalEval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluation suite from `InformationRetrievalEval...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Evaluation suite from `InformationRetrievalEval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Evaluation suite from `InformationRetrievalEval...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex‚Äôs founder, Jerry Liu, says: Our comm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex‚Äôs founder, Jerry Liu, says: Our comm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex‚Äôs founder, Jerry Liu, says: Our comm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex‚Äôs founder, Jerry Liu, says: Our comm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . RA-DIT:  We drew inspiration fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . RA-DIT:  We drew inspiration fr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex update 2023‚Äì10‚Äì10\n",
      "Here‚Äôs our weekly ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs ,  Tweet . RA-DIT:  We drew inspiration fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs ,  Tweet . RA-DIT:  We drew inspiration fr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Long-Context Embedding Models:  Explore models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Long-Context Embedding Models:  Explore models ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Long-Context Embedding Models:  Explore models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Long-Context Embedding Models:  Explore models ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ‚úçÔ∏è Tutorials: Kxsystems  advanced workshop on \"...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Example question: Who is the most recent UK pri...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Example question: Who is the most recent UK pri...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build a ChatGPT with your Private Data using Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Example question: Who is the most recent UK pri...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Example question: Who is the most recent UK pri...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build a ChatGPT with your Private Data using Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Transforming Natural Language to SQL and Insigh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This powerful tool translates natural language ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This powerful tool translates natural language ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Transforming Natural Language to SQL and Insigh...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This powerful tool translates natural language ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This powerful tool translates natural language ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Before LlamaCloud, building even a simple appli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Before LlamaCloud, building even a simple appli...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Before LlamaCloud, building even a simple appli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Before LlamaCloud, building even a simple appli...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the past each developer would spend time re-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the past each developer would spend time re-...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the past each developer would spend time re-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the past each developer would spend time re-...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This approach overcomes the limitations of fine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This approach overcomes the limitations of fine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This approach overcomes the limitations of fine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This approach overcomes the limitations of fine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: He also studied philosophy in college, but swit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: He also studied philosophy in college, but swit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: He also studied philosophy in college, but swit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: He also studied philosophy in college, but swit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You get a clear indication of which test cases ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You get a clear indication of which test cases ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  trulens_eval  import  TruLlama, Tru, Quer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  trulens_eval  import  TruLlama, Tru, Quer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: You get a clear indication of which test cases ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: You get a clear indication of which test cases ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: from  trulens_eval  import  TruLlama, Tru, Quer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: from  trulens_eval  import  TruLlama, Tru, Quer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet The TruLens team has introduced tracing f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet The TruLens team has introduced tracing f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet The TruLens team has introduced tracing f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet The TruLens team has introduced tracing f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The system prompt for the meta-agent /top-agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The system prompt for the meta-agent /top-agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: The system prompt for the meta-agent /top-agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: The system prompt for the meta-agent /top-agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: ========================\n",
      " \n",
      " \n",
      "   Observation: Ll...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Set up the launcher for local testing \n",
      " from ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Set up the launcher for local testing \n",
      " from ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: # Set up the launcher for local testing \n",
      " from ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: # Set up the launcher for local testing \n",
      " from ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: They can be downloaded either through our  llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: They can be downloaded either through our  llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: They can be downloaded either through our  llam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: They can be downloaded either through our  llam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: We‚Äôll walk through a simple  Llama Pack  that g...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Docs . ‚ú® Feature Releases and Enhancements: We ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Code ,  Tweet . We have introduced  return_dire...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Code ,  Tweet . We have introduced  return_dire...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Code ,  Tweet . We have introduced  return_dire...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Code ,  Tweet . We have introduced  return_dire...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: It serves open-source models locally, handles e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Now, you can easily process complex documents l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Now, you can easily process complex documents l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  Docs . SQLRetriever:  We introduce SQL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  Docs . SQLRetriever:  We introduce SQL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Now, you can easily process complex documents l...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Now, you can easily process complex documents l...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Tweet ,  Docs . SQLRetriever:  We introduce SQL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Tweet ,  Docs . SQLRetriever:  We introduce SQL...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Ravi Theja   tutorial  on showcasing RAG with L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Ravi Theja   tutorial  on showcasing RAG with L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Lakshmi Narayana  tutorial  on Agentic RAG with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Lakshmi Narayana  tutorial  on Agentic RAG with...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Ravi Theja   tutorial  on showcasing RAG with L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Ravi Theja   tutorial  on showcasing RAG with L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Lakshmi Narayana  tutorial  on Agentic RAG with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Lakshmi Narayana  tutorial  on Agentic RAG with...\n"
     ]
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    synthetic_response_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.020833\n",
       "mean_relevancy_score     0.966667\n",
       "mean_faithfulness_score  0.933333"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the new feature released by LlamaInde...</td>\n",
       "      <td>\\nThe new feature released by LlamaIndex, llam...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Querying a network of knowledge with llama-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discuss the advancements made by LlamaIndex in...</td>\n",
       "      <td>\\nLlamaIndex has made significant advancements...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2024-03-19\\nGreetings, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain the three main sections of the OpenAI ...</td>\n",
       "      <td>\\nThe OpenAI Cookbook for evaluating RAG syste...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the OpenAI Cookbook suggest evaluatin...</td>\n",
       "      <td>\\nThe OpenAI Cookbook suggests evaluating the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has LlamaIndex evolved over the past year ...</td>\n",
       "      <td>\\nLlamaIndex's community has grown significant...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex has evolved into a broad toolkit c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can you explain the significance of the Retrie...</td>\n",
       "      <td>\\nThe significance of Retrieval-Augmented Gene...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex update 2023‚Äì10‚Äì10\\nHere‚Äôs our week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the partnership with Google Gemini be...</td>\n",
       "      <td>\\nThe partnership with Google Gemini offers da...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Docs . ‚ú® Feature Releases and Enhancements: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Describe the Multi-Doc SEC 10Q Dataset launche...</td>\n",
       "      <td>\\nThe Multi-Doc SEC 10Q Dataset, launched by T...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Long-Context Embedding Models:  Explore model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>\\nThe MemoryCache project by Mozilla is not me...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Example question: Who is the most recent UK p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Discuss the significance of integrating Na2SQL...</td>\n",
       "      <td>\\nThe integration of Na2SQL with Llama Index i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Transforming Natural Language to SQL and Insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does LlamaCloud help in reducing productio...</td>\n",
       "      <td>\\nLlamaCloud helps in reducing production deve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The latest updates to LlamaCloud\\nTo build a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>\\nThe latest updates to LlamaCloud have introd...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The latest updates to LlamaCloud\\nTo build a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does LlamaIndex help in building LLM apps ...</td>\n",
       "      <td>\\nLlamaIndex is a popular open-source framewor...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Evaluate LLM Apps with LlamaIndex a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Can you explain the process of wrapping a Llam...</td>\n",
       "      <td>\\nYou can wrap a LlamaIndex app with TruLens b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[He also studied philosophy in college, but sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the feedback function in TruLlama hel...</td>\n",
       "      <td>\\nThe feedback function in TruLlama helps impr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[You get a clear indication of which test case...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the significance of tracking a...</td>\n",
       "      <td>\\nTracing and evaluating app versions using Tr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Evaluate LLM Apps with LlamaIndex a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explain the basic structure of LlamaIndex's Ag...</td>\n",
       "      <td>\\nThe basic structure of LlamaIndex's Agentic ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How does the meta-agent or top-level agent in ...</td>\n",
       "      <td>\\nThe meta-agent or top-level agent in the Age...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Explain the concept of Agentic RAG as describe...</td>\n",
       "      <td>\\nAgentic RAG is a concept that incorporates a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does the implementation by LlamaIndex demo...</td>\n",
       "      <td>\\nThe implementation by LlamaIndex demonstrate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the integration of NVIDIA NIM with Ll...</td>\n",
       "      <td>\\nThe integration of NVIDIA NIM with LlamaInde...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What benefits do developers gain from using NV...</td>\n",
       "      <td>\\nBy integrating NVIDIA NIM with LlamaIndex, d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How does Llama Packs aim to simplify the proce...</td>\n",
       "      <td>\\nLlama Packs aim to simplify the process of b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we‚Äôre excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Can you explain the two ways in which Llama Pa...</td>\n",
       "      <td>\\nLlama Packs can be described and utilized by...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We‚Äôll walk through a simple  Llama Pack  that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Explain the process of downloading and initial...</td>\n",
       "      <td>\\nTo download and initialize a Llama Pack, suc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We‚Äôll walk through a simple  Llama Pack  that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How can a user customize a Llama Pack, such as...</td>\n",
       "      <td>\\nTo customize a Llama Pack, such as swapping ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let‚Äôs take a look at the downloaded pack in  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What are some of the key feature releases and ...</td>\n",
       "      <td>\\nThe LlamaIndex Newsletter highlights several...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Register for free! ‚ú® Feature Releases and Enh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Describe the MechGPT project by Professor Mark...</td>\n",
       "      <td>\\nProfessor Markus J. Buehler's project, \"Usin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Code ,  Tweet . We have introduced  return_di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What are some of the tutorials covered in the ...</td>\n",
       "      <td>\\nAccording to the context, some of the tutori...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, you can easily process complex documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Who are the speakers featured in the webinars ...</td>\n",
       "      <td>\\nThe speakers featured in the webinars mentio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Ravi Theja   tutorial  on showcasing RAG with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the new feature released by LlamaInde...   \n",
       "1   Discuss the advancements made by LlamaIndex in...   \n",
       "2   Explain the three main sections of the OpenAI ...   \n",
       "3   How does the OpenAI Cookbook suggest evaluatin...   \n",
       "4   How has LlamaIndex evolved over the past year ...   \n",
       "5   Can you explain the significance of the Retrie...   \n",
       "6   How does the partnership with Google Gemini be...   \n",
       "7   Describe the Multi-Doc SEC 10Q Dataset launche...   \n",
       "8   How does the MemoryCache project by Mozilla ut...   \n",
       "9   Discuss the significance of integrating Na2SQL...   \n",
       "10  How does LlamaCloud help in reducing productio...   \n",
       "11  What new features have been introduced in Llam...   \n",
       "12  How does LlamaIndex help in building LLM apps ...   \n",
       "13  Can you explain the process of wrapping a Llam...   \n",
       "14  How does the feedback function in TruLlama hel...   \n",
       "15  Can you explain the significance of tracking a...   \n",
       "16  Explain the basic structure of LlamaIndex's Ag...   \n",
       "17  How does the meta-agent or top-level agent in ...   \n",
       "18  Explain the concept of Agentic RAG as describe...   \n",
       "19  How does the implementation by LlamaIndex demo...   \n",
       "20  How does the integration of NVIDIA NIM with Ll...   \n",
       "21  What benefits do developers gain from using NV...   \n",
       "22  How does Llama Packs aim to simplify the proce...   \n",
       "23  Can you explain the two ways in which Llama Pa...   \n",
       "24  Explain the process of downloading and initial...   \n",
       "25  How can a user customize a Llama Pack, such as...   \n",
       "26  What are some of the key feature releases and ...   \n",
       "27  Describe the MechGPT project by Professor Mark...   \n",
       "28  What are some of the tutorials covered in the ...   \n",
       "29  Who are the speakers featured in the webinars ...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   \\nThe new feature released by LlamaIndex, llam...              1.0   \n",
       "1   \\nLlamaIndex has made significant advancements...              1.0   \n",
       "2   \\nThe OpenAI Cookbook for evaluating RAG syste...              1.0   \n",
       "3   \\nThe OpenAI Cookbook suggests evaluating the ...              1.0   \n",
       "4   \\nLlamaIndex's community has grown significant...              1.0   \n",
       "5   \\nThe significance of Retrieval-Augmented Gene...              1.0   \n",
       "6   \\nThe partnership with Google Gemini offers da...              1.0   \n",
       "7   \\nThe Multi-Doc SEC 10Q Dataset, launched by T...              1.0   \n",
       "8   \\nThe MemoryCache project by Mozilla is not me...              0.0   \n",
       "9   \\nThe integration of Na2SQL with Llama Index i...              1.0   \n",
       "10  \\nLlamaCloud helps in reducing production deve...              1.0   \n",
       "11  \\nThe latest updates to LlamaCloud have introd...              1.0   \n",
       "12  \\nLlamaIndex is a popular open-source framewor...              1.0   \n",
       "13  \\nYou can wrap a LlamaIndex app with TruLens b...              1.0   \n",
       "14  \\nThe feedback function in TruLlama helps impr...              1.0   \n",
       "15  \\nTracing and evaluating app versions using Tr...              1.0   \n",
       "16  \\nThe basic structure of LlamaIndex's Agentic ...              1.0   \n",
       "17  \\nThe meta-agent or top-level agent in the Age...              1.0   \n",
       "18  \\nAgentic RAG is a concept that incorporates a...              1.0   \n",
       "19  \\nThe implementation by LlamaIndex demonstrate...              1.0   \n",
       "20  \\nThe integration of NVIDIA NIM with LlamaInde...              1.0   \n",
       "21  \\nBy integrating NVIDIA NIM with LlamaIndex, d...              1.0   \n",
       "22  \\nLlama Packs aim to simplify the process of b...              1.0   \n",
       "23  \\nLlama Packs can be described and utilized by...              1.0   \n",
       "24  \\nTo download and initialize a Llama Pack, suc...              1.0   \n",
       "25  \\nTo customize a Llama Pack, such as swapping ...              1.0   \n",
       "26  \\nThe LlamaIndex Newsletter highlights several...              1.0   \n",
       "27  \\nProfessor Markus J. Buehler's project, \"Usin...              1.0   \n",
       "28  \\nAccording to the context, some of the tutori...              1.0   \n",
       "29  \\nThe speakers featured in the webinars mentio...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 4.0                 1.0   \n",
       "1                 4.0                 1.0   \n",
       "2                 NaN                 1.0   \n",
       "3                 NaN                 1.0   \n",
       "4                 NaN                 1.0   \n",
       "5                 4.0                 1.0   \n",
       "6                 4.5                 1.0   \n",
       "7                 4.5                 0.0   \n",
       "8                 2.0                 0.0   \n",
       "9                 NaN                 1.0   \n",
       "10                3.5                 1.0   \n",
       "11                4.5                 1.0   \n",
       "12                4.5                 1.0   \n",
       "13                4.5                 1.0   \n",
       "14                4.0                 1.0   \n",
       "15                3.5                 1.0   \n",
       "16                4.5                 1.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.0                 1.0   \n",
       "19                4.0                 1.0   \n",
       "20                NaN                 1.0   \n",
       "21                4.5                 1.0   \n",
       "22                4.5                 1.0   \n",
       "23                4.0                 1.0   \n",
       "24                4.5                 1.0   \n",
       "25                NaN                 1.0   \n",
       "26                4.0                 1.0   \n",
       "27                4.5                 1.0   \n",
       "28                4.0                 1.0   \n",
       "29                2.0                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [Querying a network of knowledge with llama-in...  \n",
       "1   [LlamaIndex Newsletter 2024-03-19\\nGreetings, ...  \n",
       "2   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "3   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "4   [LlamaIndex has evolved into a broad toolkit c...  \n",
       "5   [LlamaIndex update 2023‚Äì10‚Äì10\\nHere‚Äôs our week...  \n",
       "6   [Docs . ‚ú® Feature Releases and Enhancements: W...  \n",
       "7   [Long-Context Embedding Models:  Explore model...  \n",
       "8   [Example question: Who is the most recent UK p...  \n",
       "9   [Transforming Natural Language to SQL and Insi...  \n",
       "10  [The latest updates to LlamaCloud\\nTo build a ...  \n",
       "11  [The latest updates to LlamaCloud\\nTo build a ...  \n",
       "12  [Build and Evaluate LLM Apps with LlamaIndex a...  \n",
       "13  [He also studied philosophy in college, but sw...  \n",
       "14  [You get a clear indication of which test case...  \n",
       "15  [Build and Evaluate LLM Apps with LlamaIndex a...  \n",
       "16  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "17  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "18  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "19  [Introducing llama-agents: A Powerful Framewor...  \n",
       "20  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "21  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "22  [Introducing Llama Packs\\nToday we‚Äôre excited ...  \n",
       "23  [We‚Äôll walk through a simple  Llama Pack  that...  \n",
       "24  [We‚Äôll walk through a simple  Llama Pack  that...  \n",
       "25  [Let‚Äôs take a look at the downloaded pack in  ...  \n",
       "26  [Register for free! ‚ú® Feature Releases and Enh...  \n",
       "27  [Code ,  Tweet . We have introduced  return_di...  \n",
       "28  [Now, you can easily process complex documents...  \n",
       "29  [Ravi Theja   tutorial  on showcasing RAG with...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2876bc3c-251b-4686-9321-6a7a0c081019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node ae7cd52b-79bc-4316-bd0c-a3efbff8facd] [Similarity score:             0.828923] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 4fe81f5b-f19e-437e-9b5b-f5acff5a45fb] [Similarity score:             0.798588] LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engin...\n",
      "> [Node b9a5811a-a550-4ace-846e-449577ac58dc] [Similarity score:             0.795742] Faithfulness Evaluator  ‚Äî It is useful for measuring if the response was hallucinated and measure...\n",
      "> [Node 787a2ca5-23f5-422a-a9d8-227e650f7986] [Similarity score:             0.793991] total_response_time =  0 \n",
      "    total_faithfulness =  0 \n",
      "    total_relevancy =  0 \n",
      "\n",
      "     # create v...\n",
      "> [Node 142c9060-f370-4af1-b499-6fac0cdfd82b] [Similarity score:             0.79393] It's vital to undergo thorough testing with various sizes to find a configuration that suits the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node ae7cd52b-79bc-4316-bd0c-a3efbff8facd] [Similarity score:             0.828923] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 4fe81f5b-f19e-437e-9b5b-f5acff5a45fb] [Similarity score:             0.798588] LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engin...\n",
      "> [Node b9a5811a-a550-4ace-846e-449577ac58dc] [Similarity score:             0.795742] Faithfulness Evaluator  ‚Äî It is useful for measuring if the response was hallucinated and measure...\n",
      "> [Node 787a2ca5-23f5-422a-a9d8-227e650f7986] [Similarity score:             0.793991] total_response_time =  0 \n",
      "    total_faithfulness =  0 \n",
      "    total_relevancy =  0 \n",
      "\n",
      "     # create v...\n",
      "> [Node 142c9060-f370-4af1-b499-6fac0cdfd82b] [Similarity score:             0.79393] It's vital to undergo thorough testing with various sizes to find a configuration that suits the ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 19c3c775-511c-4794-bc6e-c58d061ab92b] [Similarity score:             0.795308] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 5a9f511a-cf5e-47e7-94ec-b30ab3f88ba6] [Similarity score:             0.782661] First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powere...\n",
      "> [Node 8a9ae6b8-3ebe-412c-95e7-81389137211b] [Similarity score:             0.780028] tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
      "\n",
      " # create our agents \n",
      "worker1 = Functi...\n",
      "> [Node dacc1bbf-45e9-4a02-a50b-3dd1edafbaf0] [Similarity score:             0.771633] His name is Peter.\" )]\n",
      "index = VectorStoreIndex.from_documents(docs)\n",
      "\n",
      " # Define a query rewrite a...\n",
      "> [Node 0e96da47-6d49-4889-940c-bab9bc1305d8] [Similarity score:             0.771463] # Set up the launcher for local testing \n",
      " from  llama_agents  import  LocalLauncher\n",
      "\n",
      "launcher = L...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 19c3c775-511c-4794-bc6e-c58d061ab92b] [Similarity score:             0.795308] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 5a9f511a-cf5e-47e7-94ec-b30ab3f88ba6] [Similarity score:             0.782661] First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powere...\n",
      "> [Node 8a9ae6b8-3ebe-412c-95e7-81389137211b] [Similarity score:             0.780028] tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
      "\n",
      " # create our agents \n",
      "worker1 = Functi...\n",
      "> [Node dacc1bbf-45e9-4a02-a50b-3dd1edafbaf0] [Similarity score:             0.771633] His name is Peter.\" )]\n",
      "index = VectorStoreIndex.from_documents(docs)\n",
      "\n",
      " # Define a query rewrite a...\n",
      "> [Node 0e96da47-6d49-4889-940c-bab9bc1305d8] [Similarity score:             0.771463] # Set up the launcher for local testing \n",
      " from  llama_agents  import  LocalLauncher\n",
      "\n",
      "launcher = L...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 5 nodes:\n",
      "> [Node 6f5711d0-6264-4318-8460-e140798f8fa9] [Similarity score:             0.740395] Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the...\n",
      "> [Node 253605a1-88d3-4cba-a980-2e6acc42d01e] [Similarity score:             0.740139] Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Ji...\n",
      "> [Node 39cb5226-2608-46f9-886e-42189f013b3e] [Similarity score:             0.733596] bge-large : Experiences significant improvement with rerankers, with the best results from  Coher...\n",
      "> [Node bf17f2c7-8703-460b-a31a-fc002354ad6f] [Similarity score:             0.732614] Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining sea...\n",
      "> [Node aa6d1140-3604-4eca-ad44-3575b13e71f2] [Similarity score:             0.729997] Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "         return  self._retrieve(query_bundle)\n",
      "\n",
      "     async ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 5 nodes:\n",
      "> [Node 6f5711d0-6264-4318-8460-e140798f8fa9] [Similarity score:             0.740395] Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the...\n",
      "> [Node 253605a1-88d3-4cba-a980-2e6acc42d01e] [Similarity score:             0.740139] Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Ji...\n",
      "> [Node 39cb5226-2608-46f9-886e-42189f013b3e] [Similarity score:             0.733596] bge-large : Experiences significant improvement with rerankers, with the best results from  Coher...\n",
      "> [Node bf17f2c7-8703-460b-a31a-fc002354ad6f] [Similarity score:             0.732614] Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining sea...\n",
      "> [Node aa6d1140-3604-4eca-ad44-3575b13e71f2] [Similarity score:             0.729997] Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "         return  self._retrieve(query_bundle)\n",
      "\n",
      "     async ...\n",
      "\n",
      "Batch processing of predictions:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                                    | 1/3 [00:03<00:07,  3.72s/it]\u001b[A\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.62s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "curated_response_eval_prediction_dataset = await curated_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a9ccaa3cfa40efa62cbd3140613db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: tool = FunctionTool.from_defaults(fn=get_the_se...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: tool = FunctionTool.from_defaults(fn=get_the_se...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Introducing llama-agents: A Powerful Framework ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: tool = FunctionTool.from_defaults(fn=get_the_se...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: tool = FunctionTool.from_defaults(fn=get_the_se...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Faithfulness Evaluator  ‚Äî It is useful for meas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Faithfulness Evaluator  ‚Äî It is useful for meas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Faithfulness Evaluator  ‚Äî It is useful for meas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Faithfulness Evaluator  ‚Äî It is useful for meas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: bge-large : Experiences significant improvement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: bge-large : Experiences significant improvement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Necessity of Rerankers : The data clearly indic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Necessity of Rerankers : The data clearly indic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: bge-large : Experiences significant improvement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: bge-large : Experiences significant improvement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Necessity of Rerankers : The data clearly indic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Necessity of Rerankers : The data clearly indic...\n"
     ]
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    curated_response_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score        5.0\n",
       "mean_relevancy_score          1.0\n",
       "mean_faithfulness_score       1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>\\nDistributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via Standardized API Interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of Deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\n\\nSources:\\n- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # create our agents \\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\\nagent1 = worker1.as_agent()\\nagent2 = worker2.as_agent() We turn those agents into services: agent_server_1 = AgentService(\\n    agent=agent1,\\n    message_queue=message_queue,\\n    description= \"Useful for getting the secret fact.\" ,\\n    service_name= \"secret_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8003 \\n)\\nagent_server_2 = AgentService(\\n    agent=agent2,\\n    message_queue=message_queue,\\n    description= \"Useful for getting random dumb facts.\" ,\\n    service_name= \"dumb_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8004 \\n) And finally we launch each service as an independent agent. Here we‚Äôre doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently: from  llama_agents  import  ServerLauncher, CallableMessageConsumer\\n\\n # Additional human consumer \\n def   handle_result ( message ) -&gt;  None :\\n     print ( f\"Got result:\" , message.data)\\n\\n\\n # the final result is published to a \"human\" consumer \\n # so we define one to handle it! \\nhuman_consumer = CallableMessageConsumer(\\n    handler=handle_result, message_type= \"human\" \\n)\\n\\n # Define Launcher \\nlauncher = ServerLauncher(\\n    [agent_server_1, agent_server_2],\\n    control_plane,\\n    message_queue,\\n    additional_consumers=[human_consumer]\\n)\\n\\nlauncher.launch_servers() Real-time monitoring One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this: llama-agents monitor --control-plane-url http://127.0.0.1:8000 Once launched, you get an intuitive, point-and-click terminal application.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>\\nThe two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\\n\\n\\nSources:\\n[OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)- [Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, Faithfulness Evaluator  ‚Äî It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. Relevancy Evaluator  ‚Äî It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. # We will use GPT-4 for evaluating the responses\\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\\n\\n# Define service context for GPT-4 for evaluation\\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\\n\\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4) Response Evaluation For A Chunk Size We evaluate each chunk_size based on 3 metrics. Average Response Time. Average Faithfulness. Average Relevancy. Here‚Äôs a function,  evaluate_response_time_and_accuracy , that does just that which has: VectorIndex Creation. Building the Query Engine. Metrics Calculation. # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size \\n # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it. \\n def   evaluate_response_time_and_accuracy ( chunk_size, eval_questions ):\\n     \"\"\"\\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\\n    \\n    Parameters:\\n    chunk_size (int): The size of data chunks being processed.\\n    \\n    Returns:\\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\\n    \"\"\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>\\nThe two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\\n\\n\\nSources:\\n- [Boosting RAG: Picking the Best Embedding &amp; Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results., Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 query  \\\n",
       "0                                                                                                               What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                         What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         answer  \\\n",
       "0  \\nDistributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via Standardized API Interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of Deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\n\\nSources:\\n- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\\n\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\nThe two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\\n\\n\\nSources:\\n[OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)- [Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\\n\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\nThe two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\\n\\n\\nSources:\\n- [Boosting RAG: Picking the Best Embedding & Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\\n\\n   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \\\n",
       "0              1.0                NaN                 1.0   \n",
       "1              1.0                5.0                 1.0   \n",
       "2              1.0                5.0                 1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            contexts  \n",
       "0  [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # create our agents \\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\\nagent1 = worker1.as_agent()\\nagent2 = worker2.as_agent() We turn those agents into services: agent_server_1 = AgentService(\\n    agent=agent1,\\n    message_queue=message_queue,\\n    description= \"Useful for getting the secret fact.\" ,\\n    service_name= \"secret_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8003 \\n)\\nagent_server_2 = AgentService(\\n    agent=agent2,\\n    message_queue=message_queue,\\n    description= \"Useful for getting random dumb facts.\" ,\\n    service_name= \"dumb_fact_agent\" ,\\n    host= \"localhost\" ,\\n    port= 8004 \\n) And finally we launch each service as an independent agent. Here we‚Äôre doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently: from  llama_agents  import  ServerLauncher, CallableMessageConsumer\\n\\n # Additional human consumer \\n def   handle_result ( message ) ->  None :\\n     print ( f\"Got result:\" , message.data)\\n\\n\\n # the final result is published to a \"human\" consumer \\n # so we define one to handle it! \\nhuman_consumer = CallableMessageConsumer(\\n    handler=handle_result, message_type= \"human\" \\n)\\n\\n # Define Launcher \\nlauncher = ServerLauncher(\\n    [agent_server_1, agent_server_2],\\n    control_plane,\\n    message_queue,\\n    additional_consumers=[human_consumer]\\n)\\n\\nlauncher.launch_servers() Real-time monitoring One of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this: llama-agents monitor --control-plane-url http://127.0.0.1:8000 Once launched, you get an intuitive, point-and-click terminal application.]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, Faithfulness Evaluator  ‚Äî It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes. Relevancy Evaluator  ‚Äî It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query. # We will use GPT-4 for evaluating the responses\\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\\n\\n# Define service context for GPT-4 for evaluation\\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\\n\\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4) Response Evaluation For A Chunk Size We evaluate each chunk_size based on 3 metrics. Average Response Time. Average Faithfulness. Average Relevancy. Here‚Äôs a function,  evaluate_response_time_and_accuracy , that does just that which has: VectorIndex Creation. Building the Query Engine. Metrics Calculation. # Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size \\n # We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it. \\n def   evaluate_response_time_and_accuracy ( chunk_size, eval_questions ):\\n     \"\"\"\\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\\n    \\n    Parameters:\\n    chunk_size (int): The size of data chunks being processed.\\n    \\n    Returns:\\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\\n    \"\"\"]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                              [bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results., Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "113a264d-1cea-4e41-82ac-22135f12f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results.\n",
      "----------\n",
      "Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for context in curated_deep_eval_df.iloc[2]['contexts']:\n",
    "    print(context)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
