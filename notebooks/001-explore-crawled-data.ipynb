{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f554fb-79c8-4201-a0c9-e7464118c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c852ac68-9eb6-46f7-b33c-a1a12f8af228",
   "metadata": {},
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True\n",
    "DEBUG = False\n",
    "OBSERVABILITY = True\n",
    "LOG_TO_MLFLOW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ae48fa-a9cb-4f32-a91e-6a63c9b53506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721817562.073921  346170 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "if OBSERVABILITY:\n",
    "    import phoenix as px\n",
    "    px.launch_app()\n",
    "    import llama_index.core\n",
    "    llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"exp_003_source_ref\"\n",
    "if LOG_TO_MLFLOW:\n",
    "    RUN_DESCRIPTION = \"\"\"\n",
    "# Making the RAG outputs referenced sources\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_002\n",
    "- Update source data to have URLs as well\n",
    "\"\"\"\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc26a2-7df5-4963-855d-92ec99c958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CACHE_DP = f'data/001/{RUN_NAME}'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"NOTEBOOK_CACHE_DP\", NOTEBOOK_CACHE_DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs-v2.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'content': \"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\",\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['AI', 'Evaluation', 'Rag'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       "  'content': \"Greetings, LlamaIndex community! ü¶ô Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more. ü§©\\xa0 The highlights: DeepLearningAI Course:  JavaScript RAG Web Apps with\\xa0LlamaIndex collaborative course with DeepLearningAI.  Course ,  Tweet . RAFTDatasetPack LlamaPack : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents.  LlamaPack ,  Tweet . Memory Efficiency with Cohere Embeddings:  Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations.  Notebook ,  Tweet . Python Docs Makeover:  Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details.  API Ref ,  Tweet ,  Docs ‚ú® Feature Releases and Enhancements: We introduced RAFT - Retrieval Augmented Fine Tuning, a method from  Tianjun Zhang \\xa0and  Shishir Patil \\xa0to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation.  LlamaPack ,  Tweet . We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.  Course ,  Tweet . We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG.  Notebook ,  Tweet We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation.  API Ref ,  Tweet ,  Docs üé•\\xa0Demos: RestAI , a project by  Pedro Dias  is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI. Ragdoll  and  Ragdoll Studio  by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI. üó∫Ô∏è Guides: Guide  to Designing RAG Systems by  Micha≈Ç Oleszak  for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation. ‚úçÔ∏è Tutorials: Sujit Patil   tutorial  on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment. Florian June's  tutorial  on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch. Laurie's  video tutorial  on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries. Akriti‚Äôs   tutorial  on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant. Marco Bertelli's  tutorial  on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience. Sudarshan Koirala‚Äôs   tutorial  on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking. üé•\\xa0 Webinars: Register for a webinar  with  Tianjun Zhang \\xa0and  Shishir Patil \\xa0on how to do retrieval-augmented fine-tuning (RAFT). Webinar  with  Daniel  on  CodeGPT  - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components. Vectara‚Äôs   Panel Discussion  on 'Why RAG will Never Die?‚Äô.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Apr 2, 2024',\n",
       "  'tags': ['LLM'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-03-19',\n",
       "  'content': \"Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting weekly update from the world of LlamaVerse! We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our  launch post  for all the details! ü§©\\xa0 The highlights: New observability with Instrumentation:  Enhanced developer workflow with a new Instrumentation module for improved observability.  Docs ,  Tweet . LlamaParse accepts natural language parsing instructions : Easily extract math snippets from PDFs into LaTeX with LlamaParse.  Blogpost ,  Tweet . Financial Data Parsing:  Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis.  Notebook ,  Tweet . ‚ú® Feature Releases and Enhancements: We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application.  Docs ,  Tweet . We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline.  Blogpost ,  Tweet . We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data.  Notebook ,  Tweet . We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code.  Blogpost ,  Docs ,  Tweet . Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts.  LlamaPack ,  Tweet üé•\\xa0Demos: Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency.  Blogpost ,  Code ,  Tweet . üó∫Ô∏è Guides: Guide  to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique. ‚úçÔ∏è Tutorials: Thomas Reid ‚Äôs  tutorial  on using LlamaParse can help properly extract text from a Tesla quarterly filings. Sudarshan Koirala   video tutorial  on RAG with LlamaParse, Qdrant, and Groq. Kyosuke Morita  tutorial  showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex. Cobus Greyling   tutorial  on Agentic RAG: Context-Augmented OpenAI Agents. Roey Ben Chaim ‚Äôs  tutorial  on PII Detector: hacking privacy in RAG. üé•\\xa0 Webinars: Webinar  with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT üìÖ\\xa0Events: We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['LlamaParse', 'AI', 'LLM', 'Newsletter'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19'},\n",
       " {'title': 'One-click Open Source RAG Observability with Langfuse',\n",
       "  'content': 'This is a guest post from the team at Langfuse There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once. Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your  production  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point? Thus, we started working on  Langfuse.com  ( GitHub ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either  self-host  Langfuse or use the  cloud instance  maintained by us. We are thrilled to announce our new integration with LlamaIndex today. This feature was  highly requested  by our community and aligns with our project\\'s focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase! The challenge We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine. from  llama_index.core  import  SimpleDirectoryReader\\n from  llama_index.core  import  VectorStoreIndex\\n\\ndocuments = SimpleDirectoryReader( \"./data\" ).load_data()\\n\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nchat_engine = index.as_chat_engine()\\n\\n print (chat_engine.chat( \"What problems can I solve with RAG?\" ))\\n print (chat_engine.chat( \"How do I optimize my RAG application?\" )) In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine. This is awesome to get started, but we quickly run into questions like: ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù ‚ÄúHow is chat memory managed?‚Äù ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù One-click OSS observability to the rescue We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager. Preparation Install the community package (pip install llama-index-callbacks-langfuse) Copy/paste the environment variables from the Langfuse project settings to your Python project: \\'LANGFUSE_SECRET_KEY\\', \\'LANGFUSE_PUBLIC_KEY\\' and \\'LANGFUSE_HOST\\' Now, you only need to set the global langfuse handler: from  llama_index.core  import  set_global_handler\\n\\nset_global_handler( \"langfuse\" ) And voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns. Group multiple chat threads into a session Working with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is  session tracking  for conversational applications to see the traces in context of a full message thread. To activate it, just add an id that identifies the session as a trace param before calling the chat_engine. from  llama_index.core  import  global_handler\\n\\nglobal_handler.set_trace_params(\\n  session_id= \"your-session-id\" \\n)\\n\\nchat_engine.chat( \"What did he do growing up?\" )\\nchat_engine.chat( \"What did he do at USC?\" )\\nchat_engine.chat( \"How old is he?\" ) Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing: Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces. Trace more complex applications and use other Langfuse features for prompt management and evaluation This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable. We have also built additional features to version control and collaborate on prompts (langfuse  prompt management ), track  experiments , and  evaluate  production traces. For RAG specifically, we collaborated with the RAGAS team and it‚Äôs easy to run their popular eval suite on traces captured with Langfuse (see  cookbook ). Get started The easiest way to get started is to follow the  cookbook  and check out the  docs . Feedback? Ping us We‚Äôd love to hear any feedback. Come join us on our  community discord  or add your thoughts to this  GitHub thread .',\n",
       "  'author': 'Langfuse',\n",
       "  'date': 'Mar 18, 2024',\n",
       "  'tags': ['LLM', 'Observability'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse'},\n",
       " {'title': 'Retrieving Privacy-Safe Documents Over A Network',\n",
       "  'content': 'In a  recent blog post , we introduced our  llama-index-networks  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores‚Äîthat one may not otherwise have access to‚Äîmeans more accurate responses to an even wider breadth of queries. A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we‚Äôll be relying on some recent developments in the area of Privacy-Enhancing Techniques. The story of Alex, Bob and Beth continues To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply. We showed then how such data a collaboration could be permitted through  llama-index-networks  by taking the following steps: Bob and Beth both build their respective QueryEngine‚Äôs (RAG in llama-index lingo) Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets. Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Sidebar: differential privacy primer In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual‚Äôs inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm. By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion). A light introduction to differential privacy. A light introduction to differential privacy. Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth‚Äôs data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled ‚ÄúPrivacy-preserving in-context learning with differentially private few-shot generation‚Äù by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network! There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration. Bob and Beth create privacy-safe synthetic copies of their private datasets Bob and Beth both build their respective QueryEngine‚Äôs over their synthetic datasets Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs Creating differentially private synthetic copies of a private dataset Fortunately, for step 0., we can make use of the  DiffPrivateSimpleDataset  pack. from  llama_index.core.llama_datasets.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n\\n # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,   # OpenAI only allows for top 5 next token \\n)                     # as opposed to entire vocabulary \\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nbeth_private_dataset: LabelledSimpleDataset = ...  # a dataset that contains \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # examples with two attributes \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # `text` and `reference_label` \\n\\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,     # params for preparing required prompts \\n    simple_dataset=simple_dataset,   # to generate the synthetic examples  \\n)\\n\\nbeth_synthetic_dataset =  await  beth_synthetic_generator.arun(\\n\\t\\tsize= 3 ,   # number of synthetic observations to create \\n\\t\\tsigma= 0.5    # param that determines the level of privacy \\n) With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It‚Äôs worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.) Example: Symptom2Disease In this section of the blog post, we go over an actual example application of the privacy-safe networks over the  Symptom2Disease  dataset. This dataset consists of 1,200 examples each containing a ‚Äúsymptoms‚Äù description as well as the associated ‚Äúdisease‚Äù label ‚Äî the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network. Generate privacy-safe synthetic observations of Symptom2Disease We use the training subset and apply the  DiffPrivateSimpleDatasetPack  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a  LabelledSimpleDataset  object. import  pandas  as  pd\\n from  sklearn.model_selection  import  train_test_split\\n from  llama_index.core.llama_dataset.simple  import  (\\n    LabelledSimpleDataExample,\\n    LabelledSimpleDataset,\\n)\\n from  llama_index.core.llama_dataset.base  import  CreatedBy, CreatedByType\\n\\n # load the Symptom2Disease.csv file \\ndf = pd.read_csv( \"Symptom2Disease.csv\" )\\ntrain, test = train_test_split(df, test_size= 0.2 )\\n\\n # create a LabelledSimpleDataset (which is what the pack works with) \\nexamples = []\\n for  index, row  in  df.iterrows():\\n    example = LabelledSimpleDataExample(\\n        reference_label=row[ \"label\" ],\\n        text=row[ \"text\" ],\\n        text_by=CreatedBy( type =CreatedByType.HUMAN),\\n    )\\n    examples.append(example)\\n\\nsimple_dataset = LabelledSimpleDataset(examples=examples) Now we can use the llama-pack to create our synthetic observations. import  llama_index.core.instrumentation  as  instrument\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n from  .event_handler  import  DiffPrivacyEventHandler\\n import  asyncio\\n import  os\\n\\nNUM_SPLITS =  3 \\nT_MAX =  150 \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,\\n)\\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nprompt_bundle = PromptBundle(\\n    instruction=(\\n         \"You are a patient experiencing symptoms of a specific disease. \" \\n         \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\\\n\" \\n         \"Start your answer directly after \\'Symptoms: \\'. Begin your answer with [RESULT].\\\\n\" \\n    ),\\n    label_heading= \"Disease\" ,\\n    text_heading= \"Symptoms\" ,\\n)\\n\\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,\\n    simple_dataset=simple_dataset,\\n)\\n\\nsynthetic_dataset =  await  dp_simple_dataset_pack.arun(\\n    sizes= 3 ,\\n    t_max=T_MAX,\\n    sigma= 1.5 ,\\n    num_splits=NUM_SPLITS,\\n    num_samples_per_split= 8 ,   # number of private observations to create a \\n)                              # synthetic obsevation \\nsynthetic_dataset.save_json( \"synthetic_dataset.json\" ) Create a network with two contributors Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well. Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we‚Äôre using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user). Step 1:  Contributor‚Äôs build their Retriever over their synthetic datasets. import  os\\n from  llama_index.core  import  VectorStoreIndex\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.core.schema  import  TextNode\\n\\n\\n # load the synthetic dataset \\nsynthetic_dataset = LabelledSimpleDataset.from_json(\\n     \"./data/contributor1_synthetic_dataset.json\" \\n)\\n\\n\\nnodes = [\\n    TextNode(text=el.text, metadata={ \"reference_label\" : el.reference_label})\\n     for  el  in  synthetic_dataset[:]\\n]\\n\\nindex = VectorStoreIndex(nodes=nodes)\\nsimilarity_top_k =  int (os.environ.get( \"SIMILARITY_TOP_K\" ))\\nretriever = index.as_retriever(similarity_top_k=similarity_top_k) Step 2:  Contributor‚Äôs expose their Retrievers behind a ContributorRetrieverService from  llama_index.networks.contributor.retriever.service  import  (\\n    ContributorRetrieverService,\\n    ContributorRetrieverServiceSettings,\\n)\\n\\nsettings = ContributorRetrieverServiceSettings()  # loads from .env file \\nservice = ContributorRetrieverService(config=settings, retriever=retriever)\\napp = service.app Step 3:  Define the NetworkRetriever that connects to the ContributorRetrieverServices from  llama_index.networks.network.retriever  import  NetworkRetriever\\n from  llama_index.networks.contributor.retriever  import  ContributorRetrieverClient\\n from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\\n\\n # ContributorRetrieverClient\\'s connect to the ContributorRetrieverService \\ncontributors = [\\n    ContributorRetrieverClient.from_config_file(\\n        env_file= f\"./client-env-files/.env.contributor_ {ix} .client\" \\n    )\\n     for  ix  in   range ( 1 ,  3 )\\n]\\nreranker = CohereRerank(top_n= 5 )\\nnetwork_retriever = NetworkRetriever(\\n    contributors=contributors, node_postprocessors=[reranker]\\n) With the  NetworkRetriever  established, we can retrieve synthetic observations from the two contributors data against a query. related_records = network_retriever.aretrieve( \"Vomitting and nausea\" )\\n print (related_records)  # contain symptoms/disease records that are similar to \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  # to the queried symptoms. Evaluating the  NetworkRetriever To evaluate the efficacy of the  NetworkRetriever  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. hit rate:  a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. mean reciprocal rank:  similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. In addition to evaluating the  NetworkRetriever  we consider the two baselines that represent Retrieving only over the individual Contributor‚Äôs synthetic datasets. Retriever evaluations, with sigma equal to 1.5. In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever‚Äôs in the test set. This shouldn‚Äôt be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor‚Äôs synthetic observations‚Äîthis is the point after all of a network! Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated). Finally, this level of privacy can be controlled via the noise parameter  sigma . In the example above we used a  sigma  of 1.5, which for this dataset amounts to an  epsilon  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are  generally considered to be quite private .) Below, we share the evaluations that result from using a  sigma  of 0.5, which amounts to an  epsilon  of 15.9‚Äîhigher values of  epsilon  or privacy-loss means less privacy. # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon \\nepsilon = dp_simple_dataset_pack.sigma_to_eps(\\n\\t\\tsigma= 0.5 ,\\n\\t\\tmechanism= \"gaussian\" ,\\n\\t\\tsize= 3 * 24 ,\\n\\t\\tmax_token_cnt= 150    # number of max tokens to generate per synthetic example \\n) Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5. So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening. # synthetic example epsilon = 1.3 \\n{\\n     \"reference_label\" :  \"Psoriasis\" ,\\n     \"text\" :  \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\" ,\\n     \"text_by\" : {\\n         \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n         \"type\" :  \"ai\" \\n    }\\n},\\n\\n # synthetic example epsilon = 15.9 \\n{\\n   \"reference_label\" :  \"Migraine\" ,\\n   \"text\" :  \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\" ,\\n   \"text_by\" : {\\n     \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n     \"type\" :  \"ai\" \\n  }\\n}, We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM. In summary We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. Learn more! To delve deeper into the materials of this blog post, we share a few links below: Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( link ) Demo notebooks for the  DiffPrivateSimpleDataset  ( link ) The source code for creating the synthetic Symptom2Disease observations using the  DiffPrivateSimpleDataset  ( link )',\n",
       "  'author': 'Andrei',\n",
       "  'date': 'Mar 20, 2024',\n",
       "  'tags': ['Privacy', 'llama-index-networks'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:16.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_input_data\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags']),\n",
    "        'url': record['url']\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='37da9fc8-b3d7-4998-b12e-d9f565ae0f54', embedding=None, metadata={'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\\nThis is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       " 'author': 'LlamaIndex',\n",
       " 'date': 'Apr 2, 2024',\n",
       " 'tags': 'LLM',\n",
       " 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ee5bd7-3f29-4b0e-b2e3-19d69ed7adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_documents\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_OPTION = 'openai'\n",
    "# LLM_OPTION = 'ollama'\n",
    "LLM_OPTION = 'togetherai'\n",
    "\n",
    "# LLM_MODEL_NAME = 'llama3'\n",
    "# LLM_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "LLM_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct-Lite'\n",
    "\n",
    "# EMBED_OPTION = 'openai'\n",
    "# EMBED_OPTION = 'togetherai'\n",
    "# EMBED_OPTION = 'ollama'\n",
    "EMBED_OPTION = 'huggingface'\n",
    "\n",
    "# EMBED_MODEL_NAME = 'llama3'\n",
    "# EMBED_MODEL_NAME = 'togethercomputer/m2-bert-80M-2k-retrieval'\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)\n",
    "    mlflow.log_param(\"LLM_MODEL_NAME\", LLM_MODEL_NAME)\n",
    "    mlflow.log_param(\"EMBED_OPTION\", EMBED_OPTION)\n",
    "    mlflow.log_param(\"EMBED_MODEL_NAME\", EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831156c6-08f7-4ed6-9063-6c7d7f00b312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:25.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mLLM:\n",
      "TogetherLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x70281f5e0550>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x702830060d60>, completion_to_prompt=<function default_completion_to_prompt at 0x7028300c7100>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Meta-Llama-3-8B-Instruct-Lite', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='3cf613093b6eb9b479c341126dc8d3761c67f9340d0a4a8e1fdc62ed41b58126', api_base='https://api.together.xyz/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=False, tokenizer=None)\u001b[0m\n",
      "\u001b[32m2024-07-24 17:40:25.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEmbed model:\n",
      "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x702806181590>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# LLM options\n",
    "if LLM_OPTION == 'ollama':\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    llm = Ollama(base_url=base_url, model=LLM_MODEL_NAME, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "elif LLM_OPTION == 'openai':\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    llm = OpenAI(model=LLM_MODEL_NAME)\n",
    "elif LLM_OPTION == 'togetherai':\n",
    "    from llama_index.llms.together import TogetherLLM\n",
    "    llm = TogetherLLM(model=LLM_MODEL_NAME)\n",
    "\n",
    "# Embed options\n",
    "if EMBED_OPTION == 'huggingface':\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME\n",
    "    )\n",
    "elif EMBED_OPTION == 'openai':\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embed_model = OpenAIEmbedding()\n",
    "elif EMBED_OPTION == 'togetherai':\n",
    "    from llama_index.embeddings.together import TogetherEmbedding\n",
    "    embed_model = TogetherEmbedding(EMBED_MODEL_NAME)\n",
    "elif EMBED_OPTION == 'ollama':\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    embed_model = OllamaEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "\n",
    "logger.info(f\"LLM:\\n{repr(llm)}\")\n",
    "logger.info(f\"Embed model:\\n{repr(embed_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db5ed88-2448-4308-97e0-73bc9451c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:26.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1membed_model_dim=1024\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embed_model_dim = len(embed_model.get_text_embedding('sample text to find embedding dimensions'))\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "logger.info(f\"{embed_model_dim=}\")\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"embedding_model_dim\", embed_model_dim)\n",
    "    mlflow.log_param(\"LLM_MODEL\", repr(llm))\n",
    "    mlflow.log_param(\"EMBEDDING_MODEL\", repr(embed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a05d2-4b9e-4295-9dc5-e1b3bdf22be0",
   "metadata": {},
   "source": [
    "## Qdrant as VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be2dff6-23da-448b-866b-d791bb54812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08b83e18-6391-408c-93d9-dcf225813302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:27.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1msubstitute_punctuation(collection_raw_name)='huggingface__BAAI_bge_large_en_v1_5__exp_003_source_ref'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    # Create a translation table that maps each punctuation character to an underscore\n",
    "    translator = str.maketrans(string.punctuation, '_' * len(string.punctuation))\n",
    "    # Translate the text using the translation table\n",
    "    return text.translate(translator)\n",
    "\n",
    "collection_raw_name = f\"{EMBED_OPTION}__{EMBED_MODEL_NAME}__{RUN_NAME}\"\n",
    "logger.info(f\"{substitute_punctuation(collection_raw_name)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da75b19f-ef70-4ca7-9564-0bf663466dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "COLLECTION = substitute_punctuation(collection_raw_name)\n",
    "\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "# NODES_PERSIST_FP = 'data/001/exp_001_qdrant_togetherai_llama3/nodes.pkl'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(f\"COLLECTION\", COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec5d7b9-7ad3-427f-a423-0fee1f7f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:27.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mDeleting existing Qdrant collection...\u001b[0m\n",
      "\u001b[32m2024-07-24 17:40:27.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mDeleting persisted nodes object at data/001/exp_003_source_ref/nodes.pkl...\u001b[0m\n",
      "\u001b[32m2024-07-24 17:40:27.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mCreating new Qdrant collection...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [llama_index.vector_stores.qdrant.base] Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "collection_exists = qdrantdb.collection_exists(COLLECTION)\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing Qdrant collection...\")\n",
    "        qdrantdb.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "    logger.info(f\"Creating new Qdrant collection...\")\n",
    "    qdrantdb.create_collection(\n",
    "        COLLECTION,\n",
    "        vectors_config=VectorParams(size=embed_model_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Use existing Qdrant collection\")\n",
    "db_collection = qdrantdb.get_collection(COLLECTION)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=COLLECTION,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6e0c6f0-92a6-430b-a509-682e5884b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e60fed4-7ea5-463c-bd19-d5b6da2cb04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:27.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mCreating new DB index...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Supercharge your LlamaIndex RAG Pipeline with U...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Supercharge your LlamaIndex RAG Pipeline with U...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: In the following sections, we will illustrate h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: In the following sections, we will illustrate h...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Context Relevance : However informative the doc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Context Relevance : However informative the doc...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: RAG pipelines retrieve documents based on seman...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: RAG pipelines retrieve documents based on seman...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Then, we moved to advanced RAG concepts like th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Then, we moved to advanced RAG concepts like th...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-04-02\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: LlamaIndex Newsletter 2024-04-02\n",
      "Greetings, Lla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: This addresses the high memory usage and costs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: This addresses the high memory usage and costs ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Webinar  with  Daniel  on  CodeGPT  - a platfor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.node_parser.node_utils] > Adding chunk: Webinar  with  Daniel  on  CodeGPT  - a platfor...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.13it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.62it/s]\n",
      "\u001b[32m2024-07-24 17:40:36.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mIndexing 2 into VectorStoreIndex took 9s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = None\n",
    "    # Currently setting num_workers leads to error `AttributeError: 'HuggingFaceEmbedding' object has no attribute '_model'`\n",
    "    # num_workers = os.cpu_count() - 1\n",
    "    # logger.info(f\"Running Ingestion Pipeline with {num_workers=}...\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()\n",
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae9a042-75d5-4b9a-8dec-42f75c86c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:40:36.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexed 8 nodes into Vector Store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexed {len(nodes)} nodes into Vector Store\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_nodes\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f72b3d52-7be9-49e1-bf4e-4cd586635d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5a5fb8a-636a-463c-a938-1864cff81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "COLLECTION = 'togetherai'\n",
    "NOTEBOOK_CACHE_DP = 'data/001/togetherai'\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4a1d0d1-142d-4e7a-b626-ef154ba08e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:16:44.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mUse existing ChromaDB collection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = chromadb.PersistentClient(path=f\"{NOTEBOOK_CACHE_DP}/chroma_db\")\n",
    "collection_exists = COLLECTION in [c.name for c in db.list_collections()]\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    logger.info(f\"Creating new ChromaDB collection...\")\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing ChromaDB collection...\")\n",
    "        db.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "else:\n",
    "    logger.info(f\"Use existing ChromaDB collection\")\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ecdec51-2720-4628-a43f-caaa98310630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baa0a955-fbdf-4029-8e41-2efdea6ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0f36679-603e-40d6-862b-5d9313b3c1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 17:16:44.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mLoading index from existing ChromaDB...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if chroma_collection.count() > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing ChromaDB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new ChromaDB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embedding,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "    \n",
    "    # Need to use await and arun here to run the pipeline else error\n",
    "    # Ref: https://docs.llamaindex.ai/en/stable/examples/ingestion/async_ingestion_pipeline/\n",
    "    # Ref: https://github.com/run-llama/llama_index/issues/13904#issuecomment-2145561710\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a338d-469e-4461-9511-553dd6c1f766",
   "metadata": {},
   "source": [
    "#### Inspect nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f68dde-bd70-49b2-a62b-40a527fc44ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# embeddings are excluded by default for performance, if need then explicitly ask for it in `include`\n",
    "# chroma_collection.get(include=['embeddings'])\n",
    "chroma_collection.get()['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40da3529-cf6a-41ce-853e-1526faf2e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.append_reference.custom_query_engine import ManualAppendReferenceQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "# APPEND_REF_MODE = 'response_synthesizer'\n",
    "APPEND_REF_MODE = 'query_engine'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "if APPEND_REF_MODE == 'response_synthesizer':\n",
    "    response_synthesizer = ManualAppendReferenceSynthesizer(verbose=0)\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "elif APPEND_REF_MODE == 'query_engine':\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = ManualAppendReferenceQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "else:\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63bb5789-dde7-465c-86c2-94aadbf5b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 38acfa71-0609-44bb-8c73-cd689be9e3bf] [Similarity score:             0.648007] Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\n",
      "This is a guest post from Uptra...\n",
      "> [Node 98a756b6-fc8e-4e1f-8c38-4932ceb36171] [Similarity score:             0.6267] In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [llama_index.core.indices.utils] > Top 2 nodes:\n",
      "> [Node 38acfa71-0609-44bb-8c73-cd689be9e3bf] [Similarity score:             0.648007] Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\n",
      "This is a guest post from Uptra...\n",
      "> [Node 98a756b6-fc8e-4e1f-8c38-4932ceb36171] [Similarity score:             0.6267] In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** UpTrain Evaluations is an open-source LLM evaluation framework that evaluates your RAG pipelines and experiment with different configurations to fine-tune your LLM-based applications for optimal performance.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations](https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 38acfa71-0609-44bb-8c73-cd689be9e3bf<br>**Similarity:** 0.6480074<br>**Text:** Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\n",
       "This is a guest post from Uptra...<br>**Metadata:** {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations', 'document_title': 'Based on the candidate titles and content, I would suggest the following comprehensive title:\\n\\n\"Optimizing Language Model Performance through Advanced RAG Techniques and Integrating with UpTrain: Evaluating Context Relevance, Factual Accuracy, and Response Completeness for Effective Query Answering and Retrieval\"\\n\\nThis title captures the main themes and entities mentioned in the context, including:\\n\\n* Optimizing language model performance\\n* Advanced RAG techniques (such as reranking and contextual evaluation)\\n* Integrating with UpTrain\\n* Evaluating context relevance, factual accuracy, and response completeness\\n* Query answering and retrieval\\n* Language models (LLMs)\\n* Context conciseness\\n\\nThis title provides a clear and concise summary of the document\\'s content, making it easy to understand the main topics and themes discussed.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 98a756b6-fc8e-4e1f-8c38-4932ceb36171<br>**Similarity:** 0.6267004<br>**Text:** In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex...<br>**Metadata:** {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations', 'document_title': 'Based on the candidate titles and content, I would suggest the following comprehensive title:\\n\\n\"Optimizing Language Model Performance through Advanced RAG Techniques and Integrating with UpTrain: Evaluating Context Relevance, Factual Accuracy, and Response Completeness for Effective Query Answering and Retrieval\"\\n\\nThis title captures the main themes and entities mentioned in the context, including:\\n\\n* Optimizing language model performance\\n* Advanced RAG techniques (such as reranking and contextual evaluation)\\n* Integrating with UpTrain\\n* Evaluating context relevance, factual accuracy, and response completeness\\n* Query answering and retrieval\\n* Language models (LLMs)\\n* Context conciseness\\n\\nThis title provides a clear and concise summary of the document\\'s content, making it easy to understand the main topics and themes discussed.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'38acfa71-0609-44bb-8c73-cd689be9e3bf': {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': 'AI, Evaluation, Rag',\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations',\n",
       "  'document_title': 'Based on the candidate titles and content, I would suggest the following comprehensive title:\\n\\n\"Optimizing Language Model Performance through Advanced RAG Techniques and Integrating with UpTrain: Evaluating Context Relevance, Factual Accuracy, and Response Completeness for Effective Query Answering and Retrieval\"\\n\\nThis title captures the main themes and entities mentioned in the context, including:\\n\\n* Optimizing language model performance\\n* Advanced RAG techniques (such as reranking and contextual evaluation)\\n* Integrating with UpTrain\\n* Evaluating context relevance, factual accuracy, and response completeness\\n* Query answering and retrieval\\n* Language models (LLMs)\\n* Context conciseness\\n\\nThis title provides a clear and concise summary of the document\\'s content, making it easy to understand the main topics and themes discussed.'},\n",
       " '98a756b6-fc8e-4e1f-8c38-4932ceb36171': {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': 'AI, Evaluation, Rag',\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations',\n",
       "  'document_title': 'Based on the candidate titles and content, I would suggest the following comprehensive title:\\n\\n\"Optimizing Language Model Performance through Advanced RAG Techniques and Integrating with UpTrain: Evaluating Context Relevance, Factual Accuracy, and Response Completeness for Effective Query Answering and Retrieval\"\\n\\nThis title captures the main themes and entities mentioned in the context, including:\\n\\n* Optimizing language model performance\\n* Advanced RAG techniques (such as reranking and contextual evaluation)\\n* Integrating with UpTrain\\n* Evaluating context relevance, factual accuracy, and response completeness\\n* Query answering and retrieval\\n* Language models (LLMs)\\n* Context conciseness\\n\\nThis title provides a clear and concise summary of the document\\'s content, making it easy to understand the main topics and themes discussed.'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is UptrainEvaluations?\"\n",
    "response = query_engine.query(question)\n",
    "display_response(response, show_source=True, show_metadata=True, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_RETRIEVAL_EVAL_DATASET = True\n",
    "# Currently can not reuse retrieval_eval_dataset because the retrieval evaluation is based on ids\n",
    "# RETRIEVAL_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = min(len(nodes), RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "RETRIEVAL_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RETRIEVAL_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:05:08.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mSampling 10 nodes for retrieval evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes\n",
    "else:\n",
    "    logger.info(f\"Loading retrieval_eval_nodes from {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    with open(RETRIEVAL_EVAL_DATASET_FP, 'r') as f:\n",
    "        retrieval_eval_nodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:05:09.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                     | 7/10 [00:11<00:04,  1.45s/it]/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/llama_index/core/llama_dataset/legacy/embedding.py:99: UserWarning: Fewer questions generated (1) than requested (2).\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:15<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    retrieval_eval_llm = OpenAI(model=RETRIEVAL_EVAL_LLM_MODEL, **RETRIEVAL_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes, llm=retrieval_eval_llm, num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef47844c-5787-458e-9860-4c1f6a0f1935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a0308b12-3a62-4763-b3f5-0497f22d16d2': 'How do complex and unconstrained agent interaction techniques, such as ReAct, differ from simple and constrained agent interaction mechanisms in terms of their approach to handling data queries?',\n",
       " 'c81bd84e-9978-4da9-bebc-37607bd94bab': 'How can agents, specifically those integrated with LlamaIndex query engines, assist in performing complex user queries across multiple data sources and synthesizing insights for users?',\n",
       " 'b8611759-7643-48ff-a926-1d144def53e3': 'How does LlamaIndex simplify the evaluation process for LLM and RAG apps, and what are the four key metrics it assesses these apps on?',\n",
       " 'eb369767-7e81-4a07-a3de-29b0826d9b88': 'Describe the integration of LlamaIndex with various tools and frameworks mentioned in the document, and explain how these integrations enhance the functionality and capabilities of LlamaIndex.',\n",
       " 'e37ae996-fd00-4277-b695-4362d547f5bc': \"How does the integration of videos and code snippets enhance the viewer's understanding of the code in the context of Knowledge Transfer (KT) videos?\",\n",
       " '746238d5-cc3a-49db-99d8-91335f2c0988': 'Discuss the potential impact of LlamaIndex in revolutionizing the creation of Knowledge Transfer (KT) Videos for code bases, and propose a possible future platform similar to KodeTube(KT) for cataloging organizational codebases through explanatory videos.',\n",
       " '36b28ebc-4725-4785-8756-ed58d602298e': \"How can you visualize and monitor performance changes in a system using Tonic Validate's API?\",\n",
       " '7cad7c60-f636-4c6b-b95d-1c190916250a': 'Why is it important to set up environment variables for the API key and project ID when using Tonic Validate for scoring runs in a system?',\n",
       " 'd8679392-2a27-40c2-bdfd-79c473eca5f9': 'How does the function \"messages_to_prompt\" in the provided code snippet handle different types of messages based on their roles (system, user, assistant)? Provide an example of how the prompt is constructed for each message role.',\n",
       " 'a74fd1a2-11f4-42e8-b6c7-aa446dd8c311': 'What is the purpose of the \"generate_kwargs\" parameter in the \"HuggingFaceLLM\" initialization? How does it affect the generation of responses by the language model?',\n",
       " '97d2fda0-1aec-4c80-ad70-b337f83437ed': 'How do Deng et al. (2023) propose to address the concerns related to chatbot-oriented LLMs, and what is the significance of their taxonomic framework in tackling these issues?',\n",
       " '33d4fb8f-3423-476b-8e74-ed31c68ac350': 'Discuss the specific challenges identified in tuned LLMs as highlighted by studies conducted by Ganguli et al. (2022) and Zhuo et al. (2023), particularly focusing on the successful attack types and their effects on the generation of harmful content.',\n",
       " '1e1d2d7e-d3b6-4d1f-9775-8d2ca04eafe3': 'How does the introduction of multi-document agents in the RAG system enhance advanced QA beyond typical top-k RAG?',\n",
       " 'da6362f5-a399-4abd-9373-36a4e84183b8': \"Explain the significance of using OpenAI's latest function calling fine-tuning for structured data extraction and optimization of gpt-3.5-turbo in the RAG system.\",\n",
       " '0a24ee07-2cbf-4fa8-9bc7-d5796e6d9d73': 'In this context, the process involves generating synthetic positive pairs of (query, relevant documents) using an LLM to create hypothetical questions based on a given piece of context. This eliminates the need for human labellers and allows for scalable generation of training data. The example prompt provided demonstrates how questions can be generated for an upcoming quiz/examination based on the context information provided.',\n",
       " 'e93e57a3-551f-4af7-ae4c-56718e14058e': 'How does the RAGs app allow users to create and customize their own RAG pipeline without needing to code?',\n",
       " 'bba7c980-0fbf-48ba-a950-14fdd4457dcc': 'What are the components required to build a RAG pipeline according to the instructions on the Home Page section of the app?',\n",
       " '0927dab5-f719-43f5-bfa7-7c6e008df5ab': 'How does the use of LlamaIndex and Ray contribute to the efficiency and speed of querying data across disparate sources in the application described in the context?',\n",
       " 'c02ebb51-382f-454c-a27d-13c58561d833': 'What are the next steps recommended for individuals interested in learning more about the LlamaIndex data framework and building scalable LLM applications using Ray, as mentioned in the context information provided?'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71cd5482-b261-42a0-b822-af1f1303bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 87ffd15d-0c04-49c1-b087-a33eb2c0d444] [Similarity score:             0.809213] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> [Node d5280cda-0a90-4982-a02d-59a3eaaec876] [Similarity score:             0.778968] Dumber LLM Agents Need More Constraints and Better Tools\n",
      "Summary In this article, we compare how ...\n",
      "> Top 2 nodes:\n",
      "> [Node 8c640df7-e039-4425-8878-f70986cc643b] [Similarity score:             0.835663] As a result some of our existing query capabilities contain ‚Äúagent-like‚Äù components: we have quer...\n",
      "> [Node 109f25a3-1a96-4981-89c1-9481978b5d9f] [Similarity score:             0.831301] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> Top 2 nodes:\n",
      "> [Node acccfcb5-6e27-4c14-afc6-6328d0538c84] [Similarity score:             0.788382] The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your...\n",
      "> [Node d78e374d-ce59-4d24-96bb-8a0f1df40e83] [Similarity score:             0.787607] As is common in modern software development practices, you will likely continue to fix bugs, make...\n",
      "> Top 2 nodes:\n",
      "> [Node 17eece8c-69b3-4509-a2ba-02295d950635] [Similarity score:             0.798004] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> [Node 8e9df9f8-99ae-4257-b46b-9ff5c7d55d21] [Similarity score:             0.78273] Integrations with External Platforms Integration with PortkeyAI : LlamaIndex integrates with Port...\n",
      "> Top 2 nodes:\n",
      "> [Node 987c2eb3-6be2-4661-a3b2-d4620e5bc0f3] [Similarity score:             0.779961] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node 5d5206b6-9c2e-4c67-b1a6-6d81921ac381] [Similarity score:             0.747356] However, a challenge arises. While\n",
      "   accumulate  provides in-depth insights into each block, it ...\n",
      "> Top 2 nodes:\n",
      "> [Node 987c2eb3-6be2-4661-a3b2-d4620e5bc0f3] [Similarity score:             0.88654] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node fbe625ab-742c-4838-9a1f-dd4aab3d20ae] [Similarity score:             0.869195] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> Top 2 nodes:\n",
      "> [Node 26c86383-a540-4466-bc17-5e1a4f2c0aa1] [Similarity score:             0.739589] def   test_llama_index ():\n",
      "    questions, reference_answers = get_q_and_a()\n",
      "    llm_answers, cont...\n",
      "> [Node fe68b230-5b05-4882-8fcd-cdf7e49c2902] [Similarity score:             0.726304] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node 06b5cc3f-09b7-4856-96ba-2a0c3d6ad86d] [Similarity score:             0.721088] In this file, we will include the following code configuration that defines the integration test ...\n",
      "> [Node fe68b230-5b05-4882-8fcd-cdf7e49c2902] [Similarity score:             0.720422] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node a08bb2c8-ae48-4ed0-b8a7-d99e682e1456] [Similarity score:             0.701378] query ( \"Draw me a picture of a happy dog\" ) Snag #1 One main drawback of Transformers Agents cur...\n",
      "> [Node 373e75bc-89c8-42c3-b15a-605c2b21da86] [Similarity score:             0.700783] LlamaIndex and Transformers Agents\n",
      "Summary Agents are a popular use-case for Large Language Model...\n",
      "> Top 2 nodes:\n",
      "> [Node c4db3553-e353-4cd6-b9d9-5687e4202b98] [Similarity score:             0.670161] This triggers the outer loop to run the continuation agent, to see if there's anything else to do...\n",
      "> [Node 59b73a3c-ed08-4bd6-acf4-005eb80fa082] [Similarity score:             0.654938] At the core of that is a very simple block that simply asks the orchestration agent who should sp...\n",
      "> Top 2 nodes:\n",
      "> [Node 3cc02bcf-4971-4898-9d19-acb969d126e0] [Similarity score:             0.709608] COVID -19  pandemic: The ongoing pandemic remains a constant risk factor across all quarters, wit...\n",
      "> [Node d1dfc5fb-68e9-4cc8-8a22-f267576e4fa0] [Similarity score:             0.707448] ‚àóEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com ‚Ä†Second author Contribu...\n",
      "> Top 2 nodes:\n",
      "> [Node 6558c106-3481-4b2c-86c7-70885c8a5149] [Similarity score:             0.781431] Now, the LLM Predictor class is mostly a lightweight wrapper on top of the  LLM  abstraction that...\n",
      "> [Node 11593ed3-8fb2-4296-a864-7d4e7e8f8d8b] [Similarity score:             0.777265] We also offer an extensive array of integrations with other storage providers and downstream appl...\n",
      "> Top 2 nodes:\n",
      "> [Node 0e838eff-044e-433f-81be-76720167a664] [Similarity score:             0.760642] Tweet . We integrated OpenAI‚Äôs parallel function calling for efficient extraction of structured d...\n",
      "> [Node b06068bd-ae32-4522-8830-afa9f9e1268a] [Similarity score:             0.753647] # We will use GPT-4 for evaluating the responses\n",
      "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
      "\n",
      "# D...\n",
      "> Top 2 nodes:\n",
      "> [Node 632ee572-e23d-4287-bf35-4b9bd63095d6] [Similarity score:             0.754962] This optimizes for reduced latency and costs, and effectively halts the agent after crucial actio...\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.74772] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node 15fe877d-b087-47f8-b02a-7f88a5c47204] [Similarity score:             0.798072] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node 3a18e55d-786c-4df3-9f06-77439fc28691] [Similarity score:             0.761162] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node 2fa0f7b7-4e7d-4702-af2d-5d70935413fb] [Similarity score:             0.799559] These (question, chunk) pairs are then used as positive examples as training signals for the mode...\n",
      "> [Node e151fbe9-88f7-4b8f-8b2c-823f7f3754cc] [Similarity score:             0.763158] NO ‚Äî Response and Source Nodes (Context) are not matching. from  llama_index.evaluation  import  ...\n",
      "> Top 2 nodes:\n",
      "> [Node 15fe877d-b087-47f8-b02a-7f88a5c47204] [Similarity score:             0.712357] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node 3a18e55d-786c-4df3-9f06-77439fc28691] [Similarity score:             0.710272] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node 3355b515-6a73-4e5d-82d6-95ed69d8d8b9] [Similarity score:             0.857274] More specifically, we showcase a very relevant use case ‚Äî highlighting Ray features that are pres...\n",
      "> [Node 71959895-40e7-413a-985d-9421e3240088] [Similarity score:             0.856767] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> Top 2 nodes:\n",
      "> [Node 71959895-40e7-413a-985d-9421e3240088] [Similarity score:             0.854342] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> [Node 37967411-7251-4954-9ba9-9d6dd7819a6f] [Similarity score:             0.849632] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_2_retrieval_eval</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.266618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  top_2_retrieval_eval  0.473684  0.421053   0.236842  0.473684  0.421053   \n",
       "\n",
       "       ndcg  \n",
       "0  0.266618  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_rag_dataset(response_eval_dataset, response_eval_prediction_dataset, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "        evals[\"contexts\"].append(prediction.contexts)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "        \"contexts\": evals['contexts'],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "        pd.Series(evals['contexts'], name='contexts')\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_SYNTHETIC_EVAL_DATASET = False\n",
    "# RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = min(len(documents), RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:10:58.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:10:59.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading existing synthetic response eval dataset at data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # Use good model to generate the eval dataset\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=response_eval_llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        show_progress=True,\n",
    "        workers=(os.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "940feda2-1aad-427c-895f-2d4cdf43918f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node d14146be-9c16-4437-9597-a6460aa0b37a] [Similarity score:             0.809745] Launching the first GenAI-native document parsing platform\n",
      "Our mission at LlamaIndex is to connec...\n",
      "> [Node 9b3bd895-09d8-4f85-b5b2-6c6ad4dd27d5] [Similarity score:             0.803162] LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting...\n",
      "> Top 2 nodes:\n",
      "> [Node cd0c5df2-0cd7-4ca9-802e-1d1d706bf164] [Similarity score:             0.876471] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 78354e9f-fbd8-4ca1-bf15-a75b17992baf] [Similarity score:             0.79349] Setup Weaviate Client url = 'cluster URL'\n",
      "api_key = 'your api key'\n",
      "\n",
      "client = get_weaviate_client(...\n",
      "> Top 2 nodes:\n",
      "> [Node acf53e3e-a1b0-4341-ae71-2f1e2f048930] [Similarity score:             0.775634] Tweet . We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mi...\n",
      "> [Node 8b5e6871-542f-4964-855c-20bdd1868eac] [Similarity score:             0.767926] NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkutx FinSight by Vishwas Gowda:  https://buff.ly/3PzO...\n",
      "> Top 2 nodes:\n",
      "> [Node cd0c5df2-0cd7-4ca9-802e-1d1d706bf164] [Similarity score:             0.861998] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node c820d085-0a73-44e5-baaf-9d61ea764585] [Similarity score:             0.81176] Below, we list a select few of the evaluation notebook guides. Answer Relevancy and Context Relev...\n",
      "> Top 2 nodes:\n",
      "> [Node 479d31af-0811-447b-9518-56a0ac5ad2e3] [Similarity score:             0.805334] LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year ago, Jerry pushed his  first commit  to GPT Index...\n",
      "> [Node 17eece8c-69b3-4509-a2ba-02295d950635] [Similarity score:             0.797438] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> Top 2 nodes:\n",
      "> [Node 304e47e2-aba0-4928-943e-aa60754e21c2] [Similarity score:             0.860081] MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB\n",
      "The widespread consumption...\n",
      "> [Node 3d065fa5-3dec-4619-9a1e-6fbf9cdbf841] [Similarity score:             0.845695] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node fda5a801-53b1-4d79-84f9-211c3e6d4304] [Similarity score:             0.815758] LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ ...\n",
      "> [Node d06a5ca7-1975-49ad-9cb3-71183680e684] [Similarity score:             0.798446] LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaIndex Community! ü¶ô Welcome to another exciting weekly...\n",
      "> Top 2 nodes:\n",
      "> [Node d1296c8b-a75b-4358-8138-a7d76f84944b] [Similarity score:             0.811921] Querying a network of knowledge with llama-index-networks\n",
      "The main premise behind RAG is the inje...\n",
      "> [Node 0ebff8b1-888a-4bb2-a3c6-b1579888ca00] [Similarity score:             0.792621] Alex has heard about these insightful documents that both Bob and Beth have and would like to be ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.55it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 42e4eb0c-3488-4599-918b-35e8798f73d5] [Similarity score:             0.842008] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> [Node 36455c92-7685-4da6-a81f-ec026ec4dc88] [Similarity score:             0.830373] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> Top 2 nodes:\n",
      "> [Node 378632bf-a7ea-40b5-a254-d8c2586e25d0] [Similarity score:             0.795793] Retrieving Privacy-Safe Documents Over A Network\n",
      "In a  recent blog post , we introduced our  llam...\n",
      "> [Node a9521ec4-62c9-4c8c-9798-6ed7b67e579e] [Similarity score:             0.787769] There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation...\n",
      "> Top 2 nodes:\n",
      "> [Node 36455c92-7685-4da6-a81f-ec026ec4dc88] [Similarity score:             0.837976] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 8abfc95b-765e-46ec-9a98-590a893956cf] [Similarity score:             0.837232] Old from  llama_index  import  (\n",
      "    VectorStoreIndex,\n",
      "    ResponseSynthesizer,\n",
      ")\n",
      " from  llama_in...\n",
      "> Top 2 nodes:\n",
      "> [Node 841e5f35-d7f3-4ff8-a866-ba8f3f42e6f7] [Similarity score:             0.823183] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node c105d699-ecbe-4005-a43e-4281fb852b48] [Similarity score:             0.807417] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 42e4eb0c-3488-4599-918b-35e8798f73d5] [Similarity score:             0.760965] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> [Node 1c1f34eb-2951-489e-8544-dcc059800408] [Similarity score:             0.749447] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> Top 2 nodes:\n",
      "> [Node 556e79ab-d296-41ab-b512-0d2f58f27f14] [Similarity score:             0.763005] Doing that stuff with LlamaCloud and LlamaParse is remarkably simpler. This in turn has really he...\n",
      "> [Node c105d699-ecbe-4005-a43e-4281fb852b48] [Similarity score:             0.746297] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 1c1f34eb-2951-489e-8544-dcc059800408] [Similarity score:             0.824055] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> [Node 36455c92-7685-4da6-a81f-ec026ec4dc88] [Similarity score:             0.815926] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> Top 2 nodes:\n",
      "> [Node 134fe094-4f11-4ff2-9142-520682a71782] [Similarity score:             0.799157] Its integration ensures a smooth transition from user inputs to database insights, culminating in...\n",
      "> [Node a72f5137-7d1c-4657-9b3a-f8d782ce7ca9] [Similarity score:             0.794769] Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02<00:00,  2.69it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 6e3de820-cdfc-40ab-b64f-1d2c00382be3] [Similarity score:             0.904004] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node ed199b1b-2dca-48c8-9cfc-340fdca3fa7e] [Similarity score:             0.775254] (For me, I usually overestimate what I can achieve by 3‚Äì10x!) With this in mind, I tried to set a...\n",
      "> Top 2 nodes:\n",
      "> [Node 57707274-0eb2-4175-b98d-e3fb5034ad42] [Similarity score:             0.826259] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 75e7cc55-23e3-4223-aecb-2ab4722b5cd1] [Similarity score:             0.814519] Codebase . üó∫Ô∏è Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node 6e3de820-cdfc-40ab-b64f-1d2c00382be3] [Similarity score:             0.906435] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node e2d16ded-69db-4d4a-9f3d-b5c65a8b8477] [Similarity score:             0.783043] ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline ...\n",
      "> Top 2 nodes:\n",
      "> [Node 0316bdda-2c85-407b-b333-000c2444e251] [Similarity score:             0.794386] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 55c75197-bf41-493e-a253-5ec478e72a06] [Similarity score:             0.792443] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 9c3c536a-86d0-4186-9473-d4de1c3cb6eb] [Similarity score:             0.731607] Reranking involves using a semantic search model (specially tuned for the reranking task) that br...\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.731154] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.735694] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 75e7cc55-23e3-4223-aecb-2ab4722b5cd1] [Similarity score:             0.725484] Codebase . üó∫Ô∏è Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.890947] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 218282c6-56f3-404d-8100-55da8947c88e] [Similarity score:             0.875135] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> Top 2 nodes:\n",
      "> [Node 0316bdda-2c85-407b-b333-000c2444e251] [Similarity score:             0.764335] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 55c75197-bf41-493e-a253-5ec478e72a06] [Similarity score:             0.762903] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.20it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 7d3fda45-2028-4a1d-b8b2-4c47adab8ee5] [Similarity score:             0.807019] Register for free! ‚ú® Feature Releases and Enhancements: We introduced the LlamaIndex 0.9 version ...\n",
      "> [Node cec1e71e-330c-4eb5-ade9-9eeb1ab95ca6] [Similarity score:             0.798853] LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fam ü¶ô What an amazing week we‚Äôve had! We‚Äôre excited ...\n",
      "> Top 2 nodes:\n",
      "> [Node e40b7bb4-d699-4e0f-aab6-0f386ed54edb] [Similarity score:             0.815946] Mervin Praison‚Äôs   tutorial  on using llama-agents, detailing the framework‚Äôs purpose, a step-by-...\n",
      "> [Node 3aaf1433-c6ca-49cc-98fe-c117932a836b] [Similarity score:             0.815772] Guide  to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic ...\n",
      "> Top 2 nodes:\n",
      "> [Node d1f5a699-c778-44fe-b1f6-a8c0974f4000] [Similarity score:             0.751528] We now accommodate custom models that align with the OpenAI-compatible API. üé• Webinars: Wenqi Gla...\n",
      "> [Node 4214f9a7-9bca-4e81-b8f8-065ca99128b5] [Similarity score:             0.739651] Webinar  with Omar Khattab and Thomas Joshi on DSPy ‚Äî a framework for LLMs that emphasizes progra...\n",
      "> Top 2 nodes:\n",
      "> [Node f11f2733-8e93-4cd8-8976-7c16ce2b0625] [Similarity score:             0.81206] Voyage AI Pack. Every Pack has a detailed README on how to use / modules. First, we download and ...\n",
      "> [Node 55c75197-bf41-493e-a253-5ec478e72a06] [Similarity score:             0.786931] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 632ee572-e23d-4287-bf35-4b9bd63095d6] [Similarity score:             0.794774] This optimizes for reduced latency and costs, and effectively halts the agent after crucial actio...\n",
      "> [Node 0802f619-a16f-48fe-b896-fe3eb958fbe6] [Similarity score:             0.785191] LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIndex Family! ü¶ô Welcome to another thrilling weekly ...\n",
      "> Top 2 nodes:\n",
      "> [Node c8b2de1f-384c-4a2c-ad55-0e3f8849946b] [Similarity score:             0.770305] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 3a4c7e52-6535-4680-9fb9-74fb7eb95fae] [Similarity score:             0.729414] There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:05<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_prediction_dataset = await synthetic_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ddbfbee8144309bf28f87c32057b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: Alex has heard about these insightful documents...\n",
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: Alex has heard about these insightful documents...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Setup Weaviate Client url = 'cluster URL'\n",
      "api_k...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Setup Weaviate Client url = 'cluster URL'\n",
      "api_k...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n",
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n",
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n",
      "> Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n",
      "> Adding chunk: There are 2 main paradigms currently for extend...\n",
      "> Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n",
      "> Adding chunk: There are 2 main paradigms currently for extend...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Doing that stuff with LlamaCloud and LlamaParse...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Doing that stuff with LlamaCloud and LlamaParse...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Old from  llama_index  import  (\n",
      "    VectorStor...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Old from  llama_index  import  (\n",
      "    VectorStor...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: (For me, I usually overestimate what I can achi...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: (For me, I usually overestimate what I can achi...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Register for free! ‚ú® Feature Releases and Enhan...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: This optimizes for reduced latency and costs, a...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: This optimizes for reduced latency and costs, a...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: Mervin Praison‚Äôs   tutorial  on using llama-age...\n",
      "> Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n",
      "> Adding chunk: Mervin Praison‚Äôs   tutorial  on using llama-age...\n",
      "> Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n",
      "> Adding chunk: We now accommodate custom models that align wit...\n",
      "> Adding chunk: Webinar  with Omar Khattab and Thomas Joshi on ...\n",
      "> Adding chunk: We now accommodate custom models that align wit...\n",
      "> Adding chunk: Webinar  with Omar Khattab and Thomas Joshi on ...\n"
     ]
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    synthetic_response_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.208333\n",
       "mean_relevancy_score     0.966667\n",
       "mean_faithfulness_score  1.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the new feature released by LlamaInde...</td>\n",
       "      <td>The new feature, llama-index-networks, enables...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Querying a network of knowledge with llama-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discuss the advancements made by LlamaIndex in...</td>\n",
       "      <td>LlamaIndex has made significant advancements i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Launching the first GenAI-native document par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain the three main sections of the OpenAI ...</td>\n",
       "      <td>The OpenAI Cookbook for evaluating RAG systems...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the OpenAI Cookbook suggest evaluatin...</td>\n",
       "      <td>The OpenAI Cookbook suggests evaluating the pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has LlamaIndex evolved over the past year ...</td>\n",
       "      <td>Over the past year, LlamaIndex has experienced...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex turns 1!\\nIt‚Äôs our birthday! One y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can you explain the significance of the Retrie...</td>\n",
       "      <td>RAG technology plays a crucial role in the dev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[MultiModal RAG for Advanced Video Processing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the partnership with Google Gemini be...</td>\n",
       "      <td>The partnership with Google Gemini benefits Ll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex + Gemini\\n(co-authored by Jerry Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Describe the Multi-Doc SEC 10Q Dataset launche...</td>\n",
       "      <td>The Multi-Doc SEC 10Q Dataset, launched by Taq...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tweet . We introduced day-0 integrations with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>The MemoryCache project by Mozilla utilizes Pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Retrieving Privacy-Safe Documents Over A Netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Discuss the significance of integrating Na2SQL...</td>\n",
       "      <td>The integration of Na2SQL with Llama Index is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Its integration ensures a smooth transition f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does LlamaCloud help in reducing productio...</td>\n",
       "      <td>According to the latest updates, LlamaCloud he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Doing that stuff with LlamaCloud and LlamaPar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>LlamaCloud has introduced several new features...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This feature enables transparency, re-use, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does LlamaIndex help in building LLM apps ...</td>\n",
       "      <td>LlamaIndex is a popular open-source framework ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Evaluate LLM Apps with LlamaIndex a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Can you explain the process of wrapping a Llam...</td>\n",
       "      <td>You can wrap a LlamaIndex app with TruLens by ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Wrap A LlamaIndex App with TruLens With TruLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the feedback function in TruLlama hel...</td>\n",
       "      <td>The feedback function in TruLlama helps improv...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Wrap A LlamaIndex App with TruLens With TruLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the significance of tracking a...</td>\n",
       "      <td>Tracking and evaluating app versions using Tru...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[min )\\n\\n\\nfeedbacks = [f_lang_match, f_qa_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explain the basic structure of LlamaIndex's Ag...</td>\n",
       "      <td>The basic structure of LlamaIndex's Agentic RA...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How does the meta-agent or top-level agent in ...</td>\n",
       "      <td>The meta-agent or top-level agent in the Agent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Reranking involves using a semantic search mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Explain the concept of Agentic RAG as describe...</td>\n",
       "      <td>Agentic RAG is a concept that integrates agent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does the implementation by LlamaIndex demo...</td>\n",
       "      <td>The implementation by LlamaIndex demonstrates ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the integration of NVIDIA NIM with Ll...</td>\n",
       "      <td>The integration of NVIDIA NIM with LlamaIndex ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What benefits do developers gain from using NV...</td>\n",
       "      <td>By integrating NVIDIA NIM with LlamaIndex, dev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How does Llama Packs aim to simplify the proce...</td>\n",
       "      <td>Llama Packs aim to simplify the process of bui...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we‚Äôre excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Can you explain the two ways in which Llama Pa...</td>\n",
       "      <td>Llama Packs can be described and utilized by u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we‚Äôre excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Explain the process of downloading and initial...</td>\n",
       "      <td>To download and initialize a Llama Pack, you c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Voyage AI Pack. Every Pack has a detailed REA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How can a user customize a Llama Pack, such as...</td>\n",
       "      <td>To customize a Llama Pack, such as swapping ou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let‚Äôs take a look at the downloaded pack in  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What are some of the key feature releases and ...</td>\n",
       "      <td>The LlamaIndex Newsletter highlights several k...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Register for free! ‚ú® Feature Releases and Enh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Describe the MechGPT project by Professor Mark...</td>\n",
       "      <td>Professor Markus J. Buehler's project, \"Using ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This optimizes for reduced latency and costs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What are some of the tutorials covered in the ...</td>\n",
       "      <td>Some of the tutorials covered in the LlamaInde...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Mervin Praison‚Äôs   tutorial  on using llama-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Who are the speakers featured in the webinars ...</td>\n",
       "      <td>Wenqi Glantz, Omar Khattab, and Thomas Joshi a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We now accommodate custom models that align w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the new feature released by LlamaInde...   \n",
       "1   Discuss the advancements made by LlamaIndex in...   \n",
       "2   Explain the three main sections of the OpenAI ...   \n",
       "3   How does the OpenAI Cookbook suggest evaluatin...   \n",
       "4   How has LlamaIndex evolved over the past year ...   \n",
       "5   Can you explain the significance of the Retrie...   \n",
       "6   How does the partnership with Google Gemini be...   \n",
       "7   Describe the Multi-Doc SEC 10Q Dataset launche...   \n",
       "8   How does the MemoryCache project by Mozilla ut...   \n",
       "9   Discuss the significance of integrating Na2SQL...   \n",
       "10  How does LlamaCloud help in reducing productio...   \n",
       "11  What new features have been introduced in Llam...   \n",
       "12  How does LlamaIndex help in building LLM apps ...   \n",
       "13  Can you explain the process of wrapping a Llam...   \n",
       "14  How does the feedback function in TruLlama hel...   \n",
       "15  Can you explain the significance of tracking a...   \n",
       "16  Explain the basic structure of LlamaIndex's Ag...   \n",
       "17  How does the meta-agent or top-level agent in ...   \n",
       "18  Explain the concept of Agentic RAG as describe...   \n",
       "19  How does the implementation by LlamaIndex demo...   \n",
       "20  How does the integration of NVIDIA NIM with Ll...   \n",
       "21  What benefits do developers gain from using NV...   \n",
       "22  How does Llama Packs aim to simplify the proce...   \n",
       "23  Can you explain the two ways in which Llama Pa...   \n",
       "24  Explain the process of downloading and initial...   \n",
       "25  How can a user customize a Llama Pack, such as...   \n",
       "26  What are some of the key feature releases and ...   \n",
       "27  Describe the MechGPT project by Professor Mark...   \n",
       "28  What are some of the tutorials covered in the ...   \n",
       "29  Who are the speakers featured in the webinars ...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   The new feature, llama-index-networks, enables...              1.0   \n",
       "1   LlamaIndex has made significant advancements i...              1.0   \n",
       "2   The OpenAI Cookbook for evaluating RAG systems...              1.0   \n",
       "3   The OpenAI Cookbook suggests evaluating the pe...              1.0   \n",
       "4   Over the past year, LlamaIndex has experienced...              1.0   \n",
       "5   RAG technology plays a crucial role in the dev...              1.0   \n",
       "6   The partnership with Google Gemini benefits Ll...              1.0   \n",
       "7   The Multi-Doc SEC 10Q Dataset, launched by Taq...              1.0   \n",
       "8   The MemoryCache project by Mozilla utilizes Pr...              0.0   \n",
       "9   The integration of Na2SQL with Llama Index is ...              1.0   \n",
       "10  According to the latest updates, LlamaCloud he...              1.0   \n",
       "11  LlamaCloud has introduced several new features...              1.0   \n",
       "12  LlamaIndex is a popular open-source framework ...              1.0   \n",
       "13  You can wrap a LlamaIndex app with TruLens by ...              1.0   \n",
       "14  The feedback function in TruLlama helps improv...              1.0   \n",
       "15  Tracking and evaluating app versions using Tru...              1.0   \n",
       "16  The basic structure of LlamaIndex's Agentic RA...              1.0   \n",
       "17  The meta-agent or top-level agent in the Agent...              1.0   \n",
       "18  Agentic RAG is a concept that integrates agent...              1.0   \n",
       "19  The implementation by LlamaIndex demonstrates ...              1.0   \n",
       "20  The integration of NVIDIA NIM with LlamaIndex ...              1.0   \n",
       "21  By integrating NVIDIA NIM with LlamaIndex, dev...              1.0   \n",
       "22  Llama Packs aim to simplify the process of bui...              1.0   \n",
       "23  Llama Packs can be described and utilized by u...              1.0   \n",
       "24  To download and initialize a Llama Pack, you c...              1.0   \n",
       "25  To customize a Llama Pack, such as swapping ou...              1.0   \n",
       "26  The LlamaIndex Newsletter highlights several k...              1.0   \n",
       "27  Professor Markus J. Buehler's project, \"Using ...              1.0   \n",
       "28  Some of the tutorials covered in the LlamaInde...              1.0   \n",
       "29  Wenqi Glantz, Omar Khattab, and Thomas Joshi a...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 4.0                 1.0   \n",
       "1                 3.5                 1.0   \n",
       "2                 NaN                 1.0   \n",
       "3                 4.5                 1.0   \n",
       "4                 NaN                 1.0   \n",
       "5                 4.0                 1.0   \n",
       "6                 5.0                 1.0   \n",
       "7                 4.5                 1.0   \n",
       "8                 NaN                 1.0   \n",
       "9                 4.5                 1.0   \n",
       "10                NaN                 1.0   \n",
       "11                4.5                 1.0   \n",
       "12                4.0                 1.0   \n",
       "13                4.5                 1.0   \n",
       "14                4.5                 1.0   \n",
       "15                NaN                 1.0   \n",
       "16                4.5                 1.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.0                 1.0   \n",
       "19                4.0                 1.0   \n",
       "20                4.5                 1.0   \n",
       "21                4.5                 1.0   \n",
       "22                4.5                 1.0   \n",
       "23                4.5                 1.0   \n",
       "24                4.5                 1.0   \n",
       "25                NaN                 1.0   \n",
       "26                4.0                 1.0   \n",
       "27                4.0                 1.0   \n",
       "28                4.0                 1.0   \n",
       "29                2.0                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [Querying a network of knowledge with llama-in...  \n",
       "1   [Launching the first GenAI-native document par...  \n",
       "2   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "3   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "4   [LlamaIndex turns 1!\\nIt‚Äôs our birthday! One y...  \n",
       "5   [MultiModal RAG for Advanced Video Processing ...  \n",
       "6   [LlamaIndex + Gemini\\n(co-authored by Jerry Li...  \n",
       "7   [Tweet . We introduced day-0 integrations with...  \n",
       "8   [Retrieving Privacy-Safe Documents Over A Netw...  \n",
       "9   [Its integration ensures a smooth transition f...  \n",
       "10  [Doing that stuff with LlamaCloud and LlamaPar...  \n",
       "11  [This feature enables transparency, re-use, an...  \n",
       "12  [Build and Evaluate LLM Apps with LlamaIndex a...  \n",
       "13  [Wrap A LlamaIndex App with TruLens With TruLe...  \n",
       "14  [Wrap A LlamaIndex App with TruLens With TruLe...  \n",
       "15  [min )\\n\\n\\nfeedbacks = [f_lang_match, f_qa_re...  \n",
       "16  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "17  [Reranking involves using a semantic search mo...  \n",
       "18  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "19  [Introducing llama-agents: A Powerful Framewor...  \n",
       "20  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "21  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "22  [Introducing Llama Packs\\nToday we‚Äôre excited ...  \n",
       "23  [Introducing Llama Packs\\nToday we‚Äôre excited ...  \n",
       "24  [Voyage AI Pack. Every Pack has a detailed REA...  \n",
       "25  [Let‚Äôs take a look at the downloaded pack in  ...  \n",
       "26  [Register for free! ‚ú® Feature Releases and Enh...  \n",
       "27  [This optimizes for reduced latency and costs,...  \n",
       "28  [Mervin Praison‚Äôs   tutorial  on using llama-a...  \n",
       "29  [We now accommodate custom models that align w...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2876bc3c-251b-4686-9321-6a7a0c081019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node cd0c5df2-0cd7-4ca9-802e-1d1d706bf164] [Similarity score:             0.822134] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 597f4385-0f4e-4bdf-b17c-9deb5b274d9e] [Similarity score:             0.793708] chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\n",
      "\n",
      " for  chunk_size  in  chunk_sizes:\n",
      "  avg_res...\n",
      "> Top 2 nodes:\n",
      "> [Node 2c85936f-3734-4425-8d17-12fe2a509549] [Similarity score:             0.739013] bge-large : Experiences significant improvement with rerankers, with the best results from  Coher...\n",
      "> [Node dc28df72-ed41-41f5-b9d6-05a284d8a2bc] [Similarity score:             0.736419] Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Ji...\n",
      "> Top 2 nodes:\n",
      "> [Node 57707274-0eb2-4175-b98d-e3fb5034ad42] [Similarity score:             0.791887] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node dbeeb5bf-783c-4124-b8c0-7a4d19e93ea4] [Similarity score:             0.775856] import  dotenv\n",
      "dotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \n",
      " from  llama_index.core  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "curated_response_eval_prediction_dataset = await curated_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbe9647b2a1410eb71d46e73bde1eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: import  dotenv\n",
      "dotenv.load_dotenv()  # our .env...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: import  dotenv\n",
      "dotenv.load_dotenv()  # our .env...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n",
      "> Adding chunk: bge-large : Experiences significant improvement...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n",
      "> Adding chunk: bge-large : Experiences significant improvement...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n"
     ]
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    curated_response_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score        4.5\n",
       "mean_relevancy_score          1.0\n",
       "mean_faithfulness_score       1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>Distributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via standardized API interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\\n\\nDefine agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\nScalability and resource management: Use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., import  dotenv\\ndotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \\n from  llama_index.core  import  VectorStoreIndex, Document\\n from  llama_index.core.agent  import  FnAgentWorker\\n from  llama_index.core  import  PromptTemplate\\n from  llama_index.core.query_pipeline  import  QueryPipeline\\n from  llama_index.core.query_engine  import  RetrieverQueryEngine\\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    PipelineOrchestrator,\\n    ServiceComponent,\\n)\\n from  llama_agents.launchers  import  LocalLauncher\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # change logging level to enable or disable more verbose logging \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Load and index your document \\ndocs = [Document(text= \"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\" )]\\nindex = VectorStoreIndex.from_documents(docs)\\n\\n # Define a query rewrite agent \\nHYDE_PROMPT_STR = (\\n     \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" \\n)\\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\\n\\n def   run_hyde_fn ( state ):\\n    prompt_tmpl, llm, input_str = (\\n        state[ \"prompt_tmpl\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ]. input ,\\n    )\\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\\n    output = qp.run(query_str=input_str)\\n    state[ \"__output__\" ] =  str (output)\\n     return  state,  True \\n\\nhyde_agent = FnAgentWorker(\\n    fn=run_hyde_fn,\\n    initial_state={ \"prompt_tmpl\" : HYDE_PROMPT_TMPL,  \"llm\" : OpenAI()}\\n).as_agent()\\n\\n # Define a RAG agent \\n def   run_rag_fn ( state ):\\n    retriever, llm, input_str = (\\n        state[ \"retriever\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\\n\\n for  chunk_size  in  chunk_sizes:\\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\\n   print ( f\"Chunk size  {chunk_size}  - Average Response time:  {avg_response_time: .2 f} s, Average Faithfulness:  {avg_faithfulness: .2 f} , Average Relevancy:  {avg_relevancy: .2 f} \" ) Bringing It All Together Let‚Äôs compile the processes: import  nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n from  llama_index  import  (\\n    SimpleDirectoryReader,\\n    VectorStoreIndex,\\n    ServiceContext,\\n)\\n from  llama_index.evaluation  import  (\\n    DatasetGenerator,\\n    FaithfulnessEvaluator,\\n    RelevancyEvaluator\\n)\\n from  llama_index.llms  import  OpenAI\\n\\n import  openai\\n import  time\\n\\nopenai.api_key =  'OPENAI-API-KEY' \\n\\n # Download Data \\n!mkdir -p  'data/10k/' \\n!wget  'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'  -O  'data/10k/uber_2021.pdf' \\n\\n # Load Data \\nreader = SimpleDirectoryReader( \"./data/10k/\" )\\ndocuments = reader.load_data()\\n\\n # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages. \\neval_documents = documents[: 20 ]\\ndata_generator = DatasetGenerator.from_documents()\\neval_questions = data_generator.generate_questions_from_nodes(num =  20 )\\n\\n # We will use GPT-4 for evaluating the responses \\ngpt4 = OpenAI(temperature= 0 , model= \"gpt-4\" )\\n\\n # Define service context for GPT-4 for evaluation \\nservice_context_gpt4 = ServiceContext.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results., Boosting RAG: Picking the Best Embedding &amp; Reranker models\\nUPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we‚Äôll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let‚Äôs first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let‚Äôs delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 query  \\\n",
       "0                                                                                                               What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                         What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             answer  \\\n",
       "0  Distributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via standardized API interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\\n\\nDefine agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\nScalability and resource management: Use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \\\n",
       "0              1.0                NaN                 1.0   \n",
       "1              1.0                4.5                 1.0   \n",
       "2              1.0                NaN                 1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               contexts  \n",
       "0                              [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., import  dotenv\\ndotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \\n from  llama_index.core  import  VectorStoreIndex, Document\\n from  llama_index.core.agent  import  FnAgentWorker\\n from  llama_index.core  import  PromptTemplate\\n from  llama_index.core.query_pipeline  import  QueryPipeline\\n from  llama_index.core.query_engine  import  RetrieverQueryEngine\\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    PipelineOrchestrator,\\n    ServiceComponent,\\n)\\n from  llama_agents.launchers  import  LocalLauncher\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # change logging level to enable or disable more verbose logging \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Load and index your document \\ndocs = [Document(text= \"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\" )]\\nindex = VectorStoreIndex.from_documents(docs)\\n\\n # Define a query rewrite agent \\nHYDE_PROMPT_STR = (\\n     \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" \\n)\\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\\n\\n def   run_hyde_fn ( state ):\\n    prompt_tmpl, llm, input_str = (\\n        state[ \"prompt_tmpl\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ]. input ,\\n    )\\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\\n    output = qp.run(query_str=input_str)\\n    state[ \"__output__\" ] =  str (output)\\n     return  state,  True \\n\\nhyde_agent = FnAgentWorker(\\n    fn=run_hyde_fn,\\n    initial_state={ \"prompt_tmpl\" : HYDE_PROMPT_TMPL,  \"llm\" : OpenAI()}\\n).as_agent()\\n\\n # Define a RAG agent \\n def   run_rag_fn ( state ):\\n    retriever, llm, input_str = (\\n        state[ \"retriever\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ].]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\\n\\n for  chunk_size  in  chunk_sizes:\\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\\n   print ( f\"Chunk size  {chunk_size}  - Average Response time:  {avg_response_time: .2 f} s, Average Faithfulness:  {avg_faithfulness: .2 f} , Average Relevancy:  {avg_relevancy: .2 f} \" ) Bringing It All Together Let‚Äôs compile the processes: import  nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n from  llama_index  import  (\\n    SimpleDirectoryReader,\\n    VectorStoreIndex,\\n    ServiceContext,\\n)\\n from  llama_index.evaluation  import  (\\n    DatasetGenerator,\\n    FaithfulnessEvaluator,\\n    RelevancyEvaluator\\n)\\n from  llama_index.llms  import  OpenAI\\n\\n import  openai\\n import  time\\n\\nopenai.api_key =  'OPENAI-API-KEY' \\n\\n # Download Data \\n!mkdir -p  'data/10k/' \\n!wget  'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'  -O  'data/10k/uber_2021.pdf' \\n\\n # Load Data \\nreader = SimpleDirectoryReader( \"./data/10k/\" )\\ndocuments = reader.load_data()\\n\\n # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages. \\neval_documents = documents[: 20 ]\\ndata_generator = DatasetGenerator.from_documents()\\neval_questions = data_generator.generate_questions_from_nodes(num =  20 )\\n\\n # We will use GPT-4 for evaluating the responses \\ngpt4 = OpenAI(temperature= 0 , model= \"gpt-4\" )\\n\\n # Define service context for GPT-4 for evaluation \\nservice_context_gpt4 = ServiceContext.]  \n",
       "2  [bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results., Boosting RAG: Picking the Best Embedding & Reranker models\\nUPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we‚Äôll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let‚Äôs first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let‚Äôs delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "113a264d-1cea-4e41-82ac-22135f12f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results.\n",
      "----------\n",
      "Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we‚Äôll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let‚Äôs first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let‚Äôs delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for context in curated_deep_eval_df.iloc[2]['contexts']:\n",
    "    print(context)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
