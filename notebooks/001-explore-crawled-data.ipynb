{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "DEBUG = True\n",
    "LOG_TO_MLFLOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    RUN_NAME = \"exp_001_v3\"\n",
    "    RUN_DESCRIPTION = \"\"\"\n",
    "# Qdrant with TogetherAI Llama3 model\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_001_v2\n",
    "- Use OpenAI GPT-3.5-Turbo for generating question-context retrieval evaluation dataset\n",
    "\"\"\"\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafc26a2-7df5-4963-855d-92ec99c958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CACHE_DP = f'data/001/{RUN_NAME}'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"NOTEBOOK_CACHE_DP\", NOTEBOOK_CACHE_DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Automate online tasks with MultiOn and LlamaIndex',\n",
       "  'content': 'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.',\n",
       "  'author': 'MultiOn',\n",
       "  'date': 'May 23, 2024',\n",
       "  'tags': ['automation', 'Agents']},\n",
       " {'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       "  'content': 'We‚Äôre happy to announce the recent integration of LlamaIndex with PostgresML ‚Äî a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows. The problem with typical RAG workflows Typical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users. Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges: Increased dev time to master new technologies Complicated maintenance and scalability issues due to multiple points of failure Costly vendors required for multiple services The diagram above illustrates the complexity, showing how each component interacts across different services ‚Äî exacerbating these problems. Solution The PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows. By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call ‚Äî resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow. About PostgresML PostgresML [ github  ||  website  ||  docs ] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides. Key highlights: Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning PostgresML has a range of capabilities. In the following sections, we‚Äôll guide you through just one use case ‚Äì RAG ‚Äì and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app. How it works in LlamaIndex Let‚Äôs look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham‚Äôs essays. Step 1: Get Your Database Connection String If you haven‚Äôt already,  create your PostgresML account . You‚Äôll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Alternatively, you can pass the pgml_database_url argument when creating the index. Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed Index component: pip install llama_index llama-index-indices-managed-postgresml Then load in the data: mkdir  data\\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt Finally create the PostgresML Managed Index: from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\\n\\n\\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\\nindex = PostgresMLIndex.from_documents(\\n    documents, collection_name= \"llama-index-example\" \\n) Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see:  https://postgresml.org/docs/api/client-sdk/pipelines  Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 . Step 3: Querying Now that we have created our index we can use it for retrieval and querying: retriever = index.as_retriever()\\ndocs = retriever.retrieve( \"Was the author puzzled by the IBM 1401?\" )\\n for  doc  in  docs:\\n     print (doc) PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up. Using the PostgresML Index as a query_engine is just as easy: response = index.as_query_engine().query( \"Was the author puzzled by the IBM 1401?\" )\\n print (response) Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming: query_engine = index.as_query_engine(streaming= True )\\nresults = query_engine.query( \"Was the author puzzled by the IBM 1401?\" )\\n for  text  in  results.response_gen:\\n     print (text, end= \"\" , flush= True ) Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable. Key takeaways The PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend. To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro  guide  to setup your account, and the examples above with your own data.',\n",
       "  'author': 'PostgresML',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': ['Managed Indexes']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-06-04',\n",
       "  'content': \"Hello, LlamaIndex Family! ü¶ô We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need. ü§©\\xa0 The highlights: Elevating Knowledge Graphs:  The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities.  Docs ,  Tweet . Spreadsheet Insights with LlamaParse:  LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling.  Notebook ,  Tweet . Code Generation with Codestral:  Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages.  Docs ,  Tweet . ‚ú® Feature Releases and Enhancements: We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support.  Docs ,  Tweet . We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance.  Notebook ,  Tweet . We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages.  Docs ,  Tweet . We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript.  Blogpost ,  Tweet . We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex.  Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results. Guide  to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results. Guide  to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights. üñ•Ô∏è\\xa0Demos: Omakase RAG Orchestrator , a project developed by  Amir Mehr , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel. gmail-extractor , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision. ‚úçÔ∏è Tutorials: Sherlock Xu‚Äôs  tutorial  from BentoML on Serving A LlamaIndex RAG App as REST APIs. üìë\\xa0Papers: FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text. üìπ\\xa0Webinar: Webinar  with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents üìÖ\\xa0 Events: Join  Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Jun 4, 2024',\n",
       "  'tags': []},\n",
       " {'title': 'Batch inference with MyMagic AI and LlamaIndex',\n",
       "  'content': 'This is a guest post from MyMagic AI. MyMagic AI  allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for  batch  inference (also known as  offline  or  delayed  inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it\\'s integrated directly into LlamaIndex! Part 1: batch inference How It Works: 1. Setup : Organize Your Data in an AWS S3 or GCS Bucket: Create a folder using your user ID assigned to you upon registration. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. Purpose of the \\'Session\\' Folder: This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. Granting Access to MyMagic AI: To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the  MyMagic AI documentation . 2. Install : Install both MyMagic AI‚Äôs API integration and LlamaIndex library: pip install llama-index\\npip install llama-index-llms-mymagic 3. API Request:  The llamaIndex library is a wrapper around MyMagic AI‚Äôs API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details. import  asyncio\\n from  llama_index.llms.mymagic  import  MyMagicAI\\n\\nllm = MyMagicAI(\\n    api_key= \"user_...\" ,  # provided by MyMagic AI upon sign-up \\n    storage_provider= \"s3\" ,\\n    bucket_name= \"batch-bucket\" ,  # you may name anything \\n    session= \"my-session\" ,\\n    role_arn= \"arn:aws:iam::<your account id>:role/mymagic-role\" ,\\n    system_prompt= \"You are an AI assistant that helps to summarize the documents without essential loss of information\" ,  # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create \\n    region= \"eu-west-2\" ,\\n) We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests. resp = llm.complete(\\n    question= \"Summarise this in one sentence.\" ,\\n    model= \"mixtral8x7\" , \\n    max_tokens= 20 ,   # default is 10 \\n)\\n print (resp)\\n async   def   main ():\\n    aresp =  await  llm.acomplete(\\n        question= \"Summarize this in one sentence.\" ,\\n        model= \"llama7b\" ,\\n        max_tokens= 20 ,\\n    )\\n     print (aresp)\\n\\nasyncio.run(main()) This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI‚Äôs backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data). Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a ‚Äúbatch way‚Äù. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once. Use Cases While there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing. 1. Extraction Imagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones. 2. Classification For businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning. 3. Embedding Embedding text files for further machine learning applications is another powerful use case of MyMagic AI\\'s API. You will be ready for your vector db in a matter of days not weeks. 4. Training (Fine-tuning) Data Generation Imagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI‚Äôs API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5. 5. Transcription MyMagic AI‚Äôs API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket. Part 2: Integration with LlamaIndex‚Äôs RAG Pipeline The output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex\\'s RAG pipeline for effective data storage and retrieval. This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled: pip install llama-index-embeddings-huggingface\\ncurl -fsSL https://ollama.com/install.sh | sh\\nollama pull llama3 For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json: {\\n  \"id_review1\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\\n  },\\n  \"id_review2\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen\\'s control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson\\'s performance. It concludes by calling the film a great comedy to watch with friends.\"\\n  },\\n  \"id_review3\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell\\'s flat. Overall, it is a terrific and well-written piece.\"\\n  },\\n  \"id_review4\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"Petter Mattei\\'s \\'Love in the Time of Money\\' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler\\'s play on the same theme. Mattei\\'s work is appreciated, and viewers look forward to his future projects.\"\\n  },\\n  \"id_review5\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes the TV show \\'Oz\\', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn\\'t dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\\n  },\\n  \"token_count\": 3391\\n}\\n Now let‚Äôs embed and store this document and ask questions using LlamaIndex‚Äôs query engine. Bring in our dependencies: import  os\\n\\n from  llama_index.embeddings.huggingface  import  HuggingFaceEmbedding\\n from  llama_index.core.indices.vector_store  import  VectorStoreIndex\\n from  llama_index.core.settings  import  Settings\\n from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.llms.ollama  import  Ollama Configure the embedding model and Llama3 model embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-base-en-v1.5\" )\\nllm = Ollama(model= \"llama3\" , request_timeout= 300.0 ) Update settings for the indexing pipeline: Settings.llm = llm\\nSettings.embed_model = embed_model\\nSettings.chunk_size =  512   # This parameter defines the size of text chunks for embedding \\n\\ndocuments = SimpleDirectoryReader( \"reviews_1_5.json\" ).load_data()  #Modify path for your case Now create our index, our query engine and run a query: index = VectorStoreIndex.from_documents(documents, show_progress= True )\\n\\nquery_engine = index.as_query_engine(similarity_top_k= 3 )\\n\\nresponse = query_engine.query( \"What is the least favourite movie?\" )\\n print (response) Output: Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10. Now we know that the review 1 is the least favorite movie among these reviews. Next Steps This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.  Get started with MyMagic AI‚Äôs API  today!',\n",
       "  'author': 'MyMagic AI',\n",
       "  'date': 'May 22, 2024',\n",
       "  'tags': ['MyMagic AI', 'Batch inference']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-05-28',\n",
       "  'content': \"Greetings, LlamaIndex Family! ü¶ô Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week. ü§©\\xa0 The highlights: Secure Code Execution with AzureCodeInterpreterTool:  Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution. Build Automated Email Agents:  Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails. LlamaFS for Organized Files:  Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API. RAGApp's No-Code Chatbots:  Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible. ‚ú® Feature Releases and Enhancements: We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you‚Äôre ready to go.  Blogpost ,  Tweet . We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance.  Docs ,  Tweet . We have integrated the Vespa vector store, supporting hybrid search with BM25.  Docs ,  Tweet . We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities.  Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities. Guide  to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates. üñ•Ô∏è\\xa0Demos: LlamaFS, a project developed by  Alex Reibman  and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency.  Code ,  Tweet . RAGApp, a project developed by  Marcus Schiesser , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API.  Code ,  Tweet . ‚úçÔ∏è Tutorials: Phil Chirchir‚Äôs   tutorial  on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse. Pavan Kumar‚Äôs   tutorial  on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine‚Äôs capabilities. Jayita Bhattacharyya‚Äôs  tutorial  on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit. üìπ\\xa0Webinar: Webinar  with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:16.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=159\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_input_data\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags'])\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='8a1f0f99-45e2-4c9f-9c64-06e9ff23705e', embedding=None, metadata={'title': 'Automate online tasks with MultiOn and LlamaIndex', 'author': 'MultiOn', 'date': 'May 23, 2024', 'tags': 'automation, Agents'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Automate online tasks with MultiOn and LlamaIndex\\nIntroduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       " 'author': 'PostgresML',\n",
       " 'date': 'May 28, 2024',\n",
       " 'tags': 'Managed Indexes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ee5bd7-3f29-4b0e-b2e3-19d69ed7adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_documents\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_OPTION = 'openai'\n",
    "# LLM_OPTION = 'ollama'\n",
    "LLM_OPTION = 'togetherai'\n",
    "\n",
    "# LLM_MODEL_NAME = 'llama3'\n",
    "# LLM_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "LLM_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct-Lite'\n",
    "\n",
    "# EMBED_OPTION = 'openai'\n",
    "# EMBED_OPTION = 'togetherai'\n",
    "# EMBED_OPTION = 'ollama'\n",
    "EMBED_OPTION = 'huggingface'\n",
    "\n",
    "# EMBED_MODEL_NAME = 'llama3'\n",
    "# EMBED_MODEL_NAME = 'togethercomputer/m2-bert-80M-2k-retrieval'\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)\n",
    "    mlflow.log_param(\"LLM_MODEL_NAME\", LLM_MODEL_NAME)\n",
    "    mlflow.log_param(\"EMBED_OPTION\", EMBED_OPTION)\n",
    "    mlflow.log_param(\"EMBED_MODEL_NAME\", EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831156c6-08f7-4ed6-9063-6c7d7f00b312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:24.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mLLM:\n",
      "TogetherLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7ba24ab8f790>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7ba2baac9080>, completion_to_prompt=<function default_completion_to_prompt at 0x7ba2bab37420>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Meta-Llama-3-8B-Instruct-Lite', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='3cf613093b6eb9b479c341126dc8d3761c67f9340d0a4a8e1fdc62ed41b58126', api_base='https://api.together.xyz/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=False, tokenizer=None)\u001b[0m\n",
      "\u001b[32m2024-07-23 15:03:24.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEmbed model:\n",
      "HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7ba269ba7a50>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# LLM options\n",
    "if LLM_OPTION == 'ollama':\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    llm = Ollama(base_url=base_url, model=LLM_MODEL_NAME, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "elif LLM_OPTION == 'openai':\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    llm = OpenAI(model=LLM_MODEL_NAME)\n",
    "elif LLM_OPTION == 'togetherai':\n",
    "    from llama_index.llms.together import TogetherLLM\n",
    "    llm = TogetherLLM(model=LLM_MODEL_NAME)\n",
    "\n",
    "# Embed options\n",
    "if EMBED_OPTION == 'huggingface':\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME\n",
    "    )\n",
    "elif EMBED_OPTION == 'openai':\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embed_model = OpenAIEmbedding()\n",
    "elif EMBED_OPTION == 'togetherai':\n",
    "    from llama_index.embeddings.together import TogetherEmbedding\n",
    "    embed_model = TogetherEmbedding(EMBED_MODEL_NAME)\n",
    "elif EMBED_OPTION == 'ollama':\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    embed_model = OllamaEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "\n",
    "logger.info(f\"LLM:\\n{repr(llm)}\")\n",
    "logger.info(f\"Embed model:\\n{repr(embed_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0db5ed88-2448-4308-97e0-73bc9451c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_model_dim = len(embed_model.get_text_embedding('sample text to find embedding dimensions'))\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"embedding_model_dim\", embed_model_dim)\n",
    "    mlflow.log_param(\"LLM_MODEL\", repr(llm))\n",
    "    mlflow.log_param(\"EMBEDDING_MODEL\", repr(embed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a05d2-4b9e-4295-9dc5-e1b3bdf22be0",
   "metadata": {},
   "source": [
    "## Qdrant as VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0be2dff6-23da-448b-866b-d791bb54812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08b83e18-6391-408c-93d9-dcf225813302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:26.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1msubstitute_punctuation(collection_raw_name)='huggingface__BAAI_bge_small_en_v1_5'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    # Create a translation table that maps each punctuation character to an underscore\n",
    "    translator = str.maketrans(string.punctuation, '_' * len(string.punctuation))\n",
    "    # Translate the text using the translation table\n",
    "    return text.translate(translator)\n",
    "\n",
    "collection_raw_name = f\"{EMBED_OPTION}__{EMBED_MODEL_NAME}\"\n",
    "logger.info(f\"{substitute_punctuation(collection_raw_name)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da75b19f-ef70-4ca7-9564-0bf663466dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "COLLECTION = substitute_punctuation(collection_raw_name)\n",
    "\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "NODES_PERSIST_FP = 'data/001/exp_001_qdrant_togetherai_llama3/nodes.pkl'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(f\"COLLECTION\", COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec5d7b9-7ad3-427f-a423-0fee1f7f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:28.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUse existing Qdrant collection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "collection_exists = qdrantdb.collection_exists(COLLECTION)\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing Qdrant collection...\")\n",
    "        qdrantdb.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "    logger.info(f\"Creating new Qdrant collection...\")\n",
    "    qdrantdb.create_collection(\n",
    "        COLLECTION,\n",
    "        vectors_config=VectorParams(size=embed_model_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Use existing Qdrant collection\")\n",
    "db_collection = qdrantdb.get_collection(COLLECTION)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=COLLECTION,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e0c6f0-92a6-430b-a509-682e5884b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e60fed4-7ea5-463c-bd19-d5b6da2cb04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:29.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading index from existing DB...\u001b[0m\n",
      "\u001b[32m2024-07-23 15:03:30.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mIndexing 159 into VectorStoreIndex took 1s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = os.cpu_count() - 1\n",
    "    logger.info(f\"Running Ingestion Pipeline with {num_workers=}\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()\n",
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ae9a042-75d5-4b9a-8dec-42f75c86c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:32.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexed 808 nodes into Vector Store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexed {len(nodes)} nodes into Vector Store\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_nodes\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f72b3d52-7be9-49e1-bf4e-4cd586635d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5a5fb8a-636a-463c-a938-1864cff81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "COLLECTION = 'togetherai'\n",
    "NOTEBOOK_CACHE_DP = 'data/001/togetherai'\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4a1d0d1-142d-4e7a-b626-ef154ba08e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 12:09:50.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mUse existing ChromaDB collection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = chromadb.PersistentClient(path=f\"{NOTEBOOK_CACHE_DP}/chroma_db\")\n",
    "collection_exists = COLLECTION in [c.name for c in db.list_collections()]\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    logger.info(f\"Creating new ChromaDB collection...\")\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing ChromaDB collection...\")\n",
    "        db.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "else:\n",
    "    logger.info(f\"Use existing ChromaDB collection\")\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ecdec51-2720-4628-a43f-caaa98310630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baa0a955-fbdf-4029-8e41-2efdea6ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f36679-603e-40d6-862b-5d9313b3c1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 12:09:51.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mLoading index from existing ChromaDB...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if chroma_collection.count() > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing ChromaDB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new ChromaDB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embedding,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "    \n",
    "    # Need to use await and arun here to run the pipeline else error\n",
    "    # Ref: https://docs.llamaindex.ai/en/stable/examples/ingestion/async_ingestion_pipeline/\n",
    "    # Ref: https://github.com/run-llama/llama_index/issues/13904#issuecomment-2145561710\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a338d-469e-4461-9511-553dd6c1f766",
   "metadata": {},
   "source": [
    "#### Inspect nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f68dde-bd70-49b2-a62b-40a527fc44ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# embeddings are excluded by default for performance, if need then explicitly ask for it in `include`\n",
    "# chroma_collection.get(include=['embeddings'])\n",
    "chroma_collection.get()['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node f0812544-41a5-4374-9c28-4a89704c920f] [Similarity score:             0.708697] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> [Node dbc2d800-7fe5-4d49-81cf-a6f5f6d5b1ce] [Similarity score:             0.691203] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:39.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mMultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"What is MultiOn?\"\n",
    "response = query_engine.query(question)\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_RETRIEVAL_EVAL_DATASET = True\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = min(len(nodes), RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "RETRIEVAL_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RETRIEVAL_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:03:47.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mSampling 10 nodes for retrieval evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes\n",
    "else:\n",
    "    logger.info(f\"Loading retrieval_eval_nodes from {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    with open(RETRIEVAL_EVAL_DATASET_FP, 'r') as f:\n",
    "        retrieval_eval_nodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:04:47.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                     | 7/10 [00:09<00:03,  1.27s/it]/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/llama_index/core/llama_dataset/legacy/embedding.py:99: UserWarning: Fewer questions generated (1) than requested (2).\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:12<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    retrieval_eval_llm = OpenAI(model=RETRIEVAL_EVAL_LLM_MODEL, **RETRIEVAL_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes, llm=retrieval_eval_llm, num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef47844c-5787-458e-9860-4c1f6a0f1935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4ad13f0d-6096-4ca7-974e-471a728dffc6': 'How do complex and unconstrained agent interaction techniques, such as ReAct, differ from simple and constrained agent interaction mechanisms in terms of their approach to handling data queries?',\n",
       " 'b62f9886-c4fc-47fa-b754-4fcaa45ab95a': 'How can agents, specifically those integrated with LlamaIndex query engines, assist users in performing complex user queries across multiple data sources, and what are the potential benefits and drawbacks of utilizing these agents in data tasks?',\n",
       " 'b90b6c86-e498-447a-b177-d6a6aec2d3b5': 'How does LlamaIndex simplify the evaluation process for LLM and RAG apps, and what are the four key metrics it assesses these apps on?',\n",
       " 'd2970e24-d222-42ab-927b-99c095a95848': 'Describe the various integrations and enhancements made by LlamaIndex, such as its integration with AskMarvinAI and RunGPT by JinaAI. How do these integrations improve the functionality and usability of LlamaIndex for users?',\n",
       " '13857f83-40ae-4e6f-877a-a8cfccc6f205': 'How does the integration of videos and code snippets enhance the learning experience for viewers according to the document?',\n",
       " '2180e373-cbf6-4b7b-b50d-dde8edc0b43e': 'How does the use of the carbon and moviepy libraries contribute to the creation of Knowledge Transfer (KT) Videos for code bases as described in the document?',\n",
       " 'a76420d9-243e-42dd-820b-4b79cd2aaee4': \"How can you visualize and monitor performance changes in a system using Tonic Validate's API?\",\n",
       " '08efc2c2-44c9-4579-b0b0-71f76d9c6ac6': 'Why is it important to set up environment variables for the API key and project ID when sending metrics to the UI in the LlamaIndex system?',\n",
       " '90d05800-735b-428f-88d8-de30efe91e40': 'How does the function \"messages_to_prompt\" in the provided code snippet handle different types of messages based on their roles? Provide an example of how the function constructs the prompt for each message role.',\n",
       " '46db7de3-4b10-4916-9a12-d9a8da17a17e': 'What is the purpose of the \"generate_kwargs\" parameter in the HuggingFaceLLM initialization for the model? How does adjusting the \"temperature\", \"top_k\", and \"top_p\" values affect the model\\'s generation of responses?',\n",
       " '0f54d09e-d79c-47bb-8951-8ea7282abfc4': 'What are some of the concerns and challenges associated with chatbot-oriented LLMs, as discussed in the context information? Provide examples from the studies mentioned in the text.',\n",
       " '1998bac7-945f-4ad2-aa7a-898119a6c769': 'How do the Llama 2 models introduced in the study compare to existing open-source chat models and proprietary models in terms of competitiveness and competency? What limitations do these models still face compared to models like GPT-4?',\n",
       " 'c5e2bb2e-93f2-4578-8f2c-e52db80631b8': 'How does the introduction of multi-document agents in the RAG system enhance advanced QA beyond typical top-k RAG?',\n",
       " '19bf5760-aa47-4b08-8acc-ac213fc4e6be': \"Explain the significance of using OpenAI's latest function calling fine-tuning for structured data extraction in optimizing gpt-3.5-turbo for improved extraction in RAG.\",\n",
       " '60f70c59-8308-43ff-982d-fedde9946c04': 'In the provided context, the process involves generating synthetic dataset for training and evaluation by leveraging an LLM to generate hypothetical questions that are best answered by a given piece of context. This allows for the creation of synthetic positive pairs of (query, relevant documents) without the need for human labellers. The text chunks are processed using the SimpleNodeParser module in LlamaIndex, and for each chunk, hypothetical questions are generated using LLM. The example prompt provided is for setting up questions for an upcoming quiz/examination as a Teacher/Professor based on the context information provided.',\n",
       " 'd88eee8a-6c3d-4845-aa1a-e259139f8f18': 'How does the RAGs Streamlit app allow users to create and customize their own RAG pipeline without needing to code?',\n",
       " '6742b89a-0fc5-4a8c-9bf0-514d51eea864': 'What are the components required to set up a RAG pipeline using the RAGs app, as described on the Home Page section?',\n",
       " '43436969-0241-48a9-b33d-e58c479bf09c': 'How does the use of LlamaIndex and Ray contribute to the efficiency and speed of querying and synthesizing insights about Ray across multiple data sources?',\n",
       " '2712c2b9-54d2-49a4-89df-c87c1f33007f': 'What are the next steps recommended for individuals interested in learning more about LlamaIndex, Ray, and participating in related communities and events such as the Ray Summit 2023?'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71cd5482-b261-42a0-b822-af1f1303bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 83bb5ac9-f883-48dd-8efa-b4ef00a3d9f7] [Similarity score:             0.890583] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> [Node b3bbaa9c-83fa-42c7-a22e-e0d7d3f5afc0] [Similarity score:             0.887746] As a result some of our existing query capabilities contain ‚Äúagent-like‚Äù components: we have quer...\n",
      "> Top 2 nodes:\n",
      "> [Node 8cf60c44-e9e5-4448-a8c6-216130f7e88e] [Similarity score:             0.85269] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> [Node 7f1a0138-0baa-4746-88fc-3087b254559f] [Similarity score:             0.815742] Dumber LLM Agents Need More Constraints and Better Tools\n",
      "Summary In this article, we compare how ...\n",
      "> Top 2 nodes:\n",
      "> [Node 3b5ad0bd-36ff-41f3-8197-81d880eeeb19] [Similarity score:             0.823939] Next, we can add the code that queries LlamaIndex: def   get_responses ( questions ):\n",
      "    llm_ans...\n",
      "> [Node 91ffc125-c6be-490d-b626-3f84b64069f7] [Similarity score:             0.822518] Slides . Simon  conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Pro...\n",
      "> Top 2 nodes:\n",
      "> [Node 71f869b7-df66-4717-8d53-1330bfbd97e4] [Similarity score:             0.82097] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> [Node 80b13eac-a7e7-4ef4-a4ab-c3fd886e7959] [Similarity score:             0.81061] There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_ll...\n",
      "> Top 2 nodes:\n",
      "> [Node f9a514ee-8985-411e-8b5f-a664fb8258ef] [Similarity score:             0.842315] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node d1fec455-301c-4013-b060-10fd7d16168e] [Similarity score:             0.802503] However, a challenge arises. While\n",
      "   accumulate  provides in-depth insights into each block, it ...\n",
      "> Top 2 nodes:\n",
      "> [Node f9a514ee-8985-411e-8b5f-a664fb8258ef] [Similarity score:             0.756415] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node d1fec455-301c-4013-b060-10fd7d16168e] [Similarity score:             0.741472] However, a challenge arises. While\n",
      "   accumulate  provides in-depth insights into each block, it ...\n",
      "> Top 2 nodes:\n",
      "> [Node 05d3a7f2-71f2-4177-8b44-36f6e8eefb89] [Similarity score:             0.8137] Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex\n",
      "In this technical walk...\n",
      "> [Node 5480c42b-1f82-444a-8bdc-17a49db5b44a] [Similarity score:             0.806896] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node aad6363e-6f39-4dd9-a1e2-0d2aa7fb81ac] [Similarity score:             0.796662] def   test_llama_index ():\n",
      "    questions, reference_answers = get_q_and_a()\n",
      "    llm_answers, cont...\n",
      "> [Node 13bee1a0-0273-4f66-a91e-d0fcdc6745fb] [Similarity score:             0.771583] To install it, we need to make sure we have Node.JS installed and run the following command: npx ...\n",
      "> Top 2 nodes:\n",
      "> [Node 39ab9161-b7d3-47fe-955e-48761c0755bf] [Similarity score:             0.711772] markdown ( \"\" \"\n",
      "                    [Embedding Models Leaderboard](https://huggingface.co/spaces/...\n",
      "> [Node 4eebb458-cfb1-4842-b1ae-a6be28bceeb0] [Similarity score:             0.705494] You can choose from Google‚Äôs Gemini Pro, Cohere, OpenAI‚Äôs GPT 3.5 and GPT 4. def   select_llm ():...\n",
      "> Top 2 nodes:\n",
      "> [Node b9085781-a6dd-4b53-8d5f-192767c7d9c8] [Similarity score:             0.663737] However, using entire code bases for\n",
      "  explanations can overwhelm language models like LLMs, caus...\n",
      "> [Node 2d6a4449-7b33-4768-99e4-65e7b2735f40] [Similarity score:             0.660148] https://github.com/run-llama/ts-playground Main Differences from LlamaIndex Python All function n...\n",
      "> Top 2 nodes:\n",
      "> [Node fb94e64e-a9a0-42b7-930a-398f40544133] [Similarity score:             0.879507] Query:  Based on the abstract of ‚ÄúLlama 2: Open Foundation and Fine-Tuned Chat Models,‚Äù what are ...\n",
      "> [Node be5689fe-37b8-44a8-b0aa-0851c87362b4] [Similarity score:             0.856485] The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language...\n",
      "> Top 2 nodes:\n",
      "> [Node 924ed153-bb6e-4044-9431-205e36c96274] [Similarity score:             0.792329] Say that you want to build a chatbot Define the dataset (here it‚Äôs a web page, can also be a loca...\n",
      "> [Node 7ddb96ff-fd5e-4b34-a9f9-17e172ff32d3] [Similarity score:             0.788621] Now, the LLM Predictor class is mostly a lightweight wrapper on top of the  LLM  abstraction that...\n",
      "> Top 2 nodes:\n",
      "> [Node 0ddba32a-0deb-4636-bc64-a3d8697abadf] [Similarity score:             0.79507] from  llama_index.finetuning  import  EmbeddingAdapterFinetuneEngine\n",
      " from  llama_index.embedding...\n",
      "> [Node 1e75af1a-09fa-4c55-b064-b94e90256568] [Similarity score:             0.793221] Tweet . Fine-Tuning Guides: OpenAI Fine-Tuning:  LlamaIndex unveils a fresh guide on harnessing O...\n",
      "> Top 2 nodes:\n",
      "> [Node 390ed7c9-6024-4f86-bcca-5eba197a02d3] [Similarity score:             0.844839] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 019b26a2-e46a-4df2-909e-0e551e27f1a8] [Similarity score:             0.838938] Define objective function \n",
      " def   objective_function ( params_dict ):\n",
      "    chunk_size = params_dic...\n",
      "> Top 2 nodes:\n",
      "> [Node 6d9caeb3-1250-4b2c-946d-d25c2ceed141] [Similarity score:             0.821635] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node e560696d-0a2e-4f2e-954f-8a649ef0f468] [Similarity score:             0.797249] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node fe569503-26a6-4041-8cb2-585f28401978] [Similarity score:             0.842561] NO ‚Äî Response and Source Nodes (Context) are not matching. from  llama_index.evaluation  import  ...\n",
      "> [Node eb2e69f6-6d0d-4810-bda8-b272aa270dc9] [Similarity score:             0.833873] Building and Evaluating a QA System with LlamaIndex\n",
      "Introduction LlamaIndex (GPT Index)  offers a...\n",
      "> Top 2 nodes:\n",
      "> [Node 79099ab3-2991-4ea7-ba1f-634a74787621] [Similarity score:             0.858078] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n",
      "> [Node 7cc036bc-5ba0-40d4-af45-1dc98807dbd5] [Similarity score:             0.85223] Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n",
      "\n",
      "Response: \n",
      "The Ra...\n",
      "> Top 2 nodes:\n",
      "> [Node 6d9caeb3-1250-4b2c-946d-d25c2ceed141] [Similarity score:             0.784167] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node e560696d-0a2e-4f2e-954f-8a649ef0f468] [Similarity score:             0.738768] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node 609fdc49-3e77-49ce-9977-7b5cd9e53a97] [Similarity score:             0.733042] Mariboo‚Äôs  tutorial  on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetun...\n",
      "> [Node c5c05999-8917-4629-9618-6bde344c62fe] [Similarity score:             0.72949] LlamaIndex Update ‚Äî 07/11/2023\n",
      "Greetings once again, LlamaIndex community! Welcome back to our se...\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_2_retrieval_eval</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.225896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  top_2_retrieval_eval  0.368421  0.368421   0.184211  0.368421  0.368421   \n",
       "\n",
       "       ndcg  \n",
       "0  0.225896  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_rag_dataset(response_eval_dataset, response_eval_prediction_dataset, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "        evals[\"contexts\"].append(prediction.contexts)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "        \"contexts\": evals['contexts'],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "        pd.Series(evals['contexts'], name='contexts')\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_SYNTHETIC_EVAL_DATASET = True\n",
    "RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = min(len(documents), RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:06:27.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 15:06:49.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mCreating synthetic response eval dataset...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235d9cc3bb7a4434bbc1716f52aa4d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex Newsletter 2024-03-05\n",
      "Greetings, Lla...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n",
      "> Adding chunk: üëÄ Community Demos : MemoryCache: Mozilla‚Äôs new ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Finally, the third feedback function checks how...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: These connectors can work with\n",
      "  APIs, PDFs, SQ...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Special thanks to Logan Markewich and Andrei Fa...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Fr...\n",
      "> Adding chunk: Finally, we released a guide to craft a GPT Bui...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-05\n",
      "Greetings, Lla...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n",
      "> Adding chunk: üëÄ Community Demos : MemoryCache: Mozilla‚Äôs new ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Finally, the third feedback function checks how...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: These connectors can work with\n",
      "  APIs, PDFs, SQ...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Special thanks to Logan Markewich and Andrei Fa...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Fr...\n",
      "> Adding chunk: Finally, we released a guide to craft a GPT Bui...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:02<00:00,  7.49it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.03s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.25s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.32s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.75s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.09s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.83s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.64s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.37s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.33s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.40s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.65s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # Use good model to generate the eval dataset\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=response_eval_llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        show_progress=True,\n",
    "        workers=(os.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "940feda2-1aad-427c-895f-2d4cdf43918f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 89285fc8-241c-41b1-b377-1d49ceccbae1] [Similarity score:             0.797784] Tweet . We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mi...\n",
      "> [Node cae32e51-5fd7-437c-a89e-6fcc6af464b7] [Similarity score:             0.78116] Tweet AI Chatbot Starter (from the DataStax team), a web server powered by AstraDB and LlamaIndex...\n",
      "> Top 2 nodes:\n",
      "> [Node e009b8ae-616f-47c3-9cf9-20fa1c683ac3] [Similarity score:             0.899186] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 019b26a2-e46a-4df2-909e-0e551e27f1a8] [Similarity score:             0.842587] Define objective function \n",
      " def   objective_function ( params_dict ):\n",
      "    chunk_size = params_dic...\n",
      "> Top 2 nodes:\n",
      "> [Node d9029ced-46e4-4009-996e-410b91376c7c] [Similarity score:             0.840824] Introducing LlamaCloud and LlamaParse\n",
      "Today is a big day for the LlamaIndex ecosystem: we are ann...\n",
      "> [Node edf2df9d-dd78-410a-837d-2ccd4843022d] [Similarity score:             0.838668] Launching the first GenAI-native document parsing platform\n",
      "Our mission at LlamaIndex is to connec...\n",
      "> Top 2 nodes:\n",
      "> [Node e009b8ae-616f-47c3-9cf9-20fa1c683ac3] [Similarity score:             0.916044] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 0020f723-6a45-4396-9543-f414375e8936] [Similarity score:             0.851188] LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Community ü¶ô, We are excited to collaborate with Deep...\n",
      "> Top 2 nodes:\n",
      "> [Node 618ad8e0-394a-435e-90b3-188ec0f36be2] [Similarity score:             0.806151] LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fans ü¶ô! Welcome back to our newsletter covering new ...\n",
      "> [Node 1916ae8b-1e41-4707-ab0e-168034168e6e] [Similarity score:             0.802237] Notebook ,  Tweet We launched revamped Python docs with top-level example notebooks, improved sea...\n",
      "> Top 2 nodes:\n",
      "> [Node 1b7c3a38-ad97-4365-9dbf-81877e9f78ed] [Similarity score:             0.822528] It‚Äôs what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what...\n",
      "> [Node 99874c4e-123d-4d24-b677-f4d581b37bd7] [Similarity score:             0.80977] LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year ago, Jerry pushed his  first commit  to GPT Index...\n",
      "> Top 2 nodes:\n",
      "> [Node aad1331f-7826-4576-9a63-42cd1eda5725] [Similarity score:             0.79805] Linking the resources again below: Gemini (text-only) Guide Gemini (multi-modal) Guide Semantic R...\n",
      "> [Node d4da6225-c5ff-4c01-bbcf-125de0ec0e8d] [Similarity score:             0.79417] Their  multimodal demos  demonstrate joint image/text understanding from domains like scientific ...\n",
      "> Top 2 nodes:\n",
      "> [Node 0e7c915f-8712-4302-930f-662ea61aeba4] [Similarity score:             0.879293] MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB\n",
      "The widespread consumption...\n",
      "> [Node 91ffc125-c6be-490d-b626-3f84b64069f7] [Similarity score:             0.87856] Slides . Simon  conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 4/8 [00:07<00:07,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5161614868990574 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.', 'type': 'credit_limit', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7919424367226937 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.', 'type': 'credit_limit', 'param': None, 'code': None}}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:12<00:00,  1.52s/it]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 3a98052b-19dc-4e9a-9387-993ebe280653] [Similarity score:             0.843707] Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Str...\n",
      "> [Node f14ba7ba-1d1f-4d68-aafa-0b7e536b0744] [Similarity score:             0.835205] Its integration ensures a smooth transition from user inputs to database insights, culminating in...\n",
      "> Top 2 nodes:\n",
      "> [Node 886fdc63-e4b0-4e8a-89e3-6eb1cfa94361] [Similarity score:             0.86808] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node d7c67a6d-57d9-4ab2-bd82-e39294b964e7] [Similarity score:             0.850782] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node dbba146b-8249-4221-a2b6-0e5475bd9e17] [Similarity score:             0.839747] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> [Node e8b5aa20-2203-491e-92e3-d02a3e0c7050] [Similarity score:             0.813318] f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
      " # By default this will check lang...\n",
      "> Top 2 nodes:\n",
      "> [Node d399ae45-6a78-471d-aca2-8e68f08ecc0b] [Similarity score:             0.81307] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 3480df1a-0e04-4045-a787-45123719d7e5] [Similarity score:             0.811326] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> Top 2 nodes:\n",
      "> [Node 50981f60-d290-4f67-a5dc-3fe1da7759b7] [Similarity score:             0.78552] Query ‚ÄúHow does GPT4 do on the bar exam?‚Äù Response ‚ÄúGPT-4 performs well on the Uniform Bar Exam, ...\n",
      "> [Node 0b4f9aa9-7f6a-444d-89e0-bb5d82cd0cd6] [Similarity score:             0.782744] Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\n",
      "Co-authors: Prakul Agarwal ‚Äî ...\n",
      "> Top 2 nodes:\n",
      "> [Node 3480df1a-0e04-4045-a787-45123719d7e5] [Similarity score:             0.806689] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> [Node e8b5aa20-2203-491e-92e3-d02a3e0c7050] [Similarity score:             0.773636] f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
      " # By default this will check lang...\n",
      "> Top 2 nodes:\n",
      "> [Node d399ae45-6a78-471d-aca2-8e68f08ecc0b] [Similarity score:             0.83118] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node b0e28074-e1bb-4a0e-a2ff-525b647d0705] [Similarity score:             0.829416] LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development\n",
      "A few months ago, we lau...\n",
      "> Top 2 nodes:\n",
      "> [Node af5b4ffc-53f5-47d3-bf27-4b33d72cbf52] [Similarity score:             0.80502] LlamaCloud - Built for Enterprise LLM App Builders\n",
      "RAG is only as Good as your Data Building prod...\n",
      "> [Node d7c67a6d-57d9-4ab2-bd82-e39294b964e7] [Similarity score:             0.782283] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 4/8 [00:06<00:05,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.891192177247035 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.', 'type': 'credit_limit', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.07405918574574444 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.', 'type': 'credit_limit', 'param': None, 'code': None}}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  1.48s/it]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 390ed7c9-6024-4f86-bcca-5eba197a02d3] [Similarity score:             0.773554] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 924ed153-bb6e-4044-9431-205e36c96274] [Similarity score:             0.759326] Say that you want to build a chatbot Define the dataset (here it‚Äôs a web page, can also be a loca...\n",
      "> Top 2 nodes:\n",
      "> [Node 9929ccb4-3d3b-4e9d-80ac-dc86ff8df3c1] [Similarity score:             0.838954] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node b8de4586-9cc8-4278-9c60-ec62b9419239] [Similarity score:             0.838108] Codebase . üó∫Ô∏è Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node 390ed7c9-6024-4f86-bcca-5eba197a02d3] [Similarity score:             0.824891] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node c9fa91c1-1625-42a0-a8ee-24c70481fa19] [Similarity score:             0.801769] Notebook . üó∫Ô∏è Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pip...\n",
      "> Top 2 nodes:\n",
      "> [Node 839ad04f-5520-4423-ae2d-cf7911106a8a] [Similarity score:             0.805014] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 771cea2f-164f-43dc-9483-97c64608a538] [Similarity score:             0.775984] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 390ed7c9-6024-4f86-bcca-5eba197a02d3] [Similarity score:             0.895399] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node fa4d2de7-b6aa-4a80-b679-e792f357768e] [Similarity score:             0.873673] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> Top 2 nodes:\n",
      "> [Node f35731ac-d5e7-4c39-8b58-b5fb13421846] [Similarity score:             0.915111] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node 686d673e-878d-4fa7-af25-71488bdb124c] [Similarity score:             0.787671] ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline ...\n",
      "> Top 2 nodes:\n",
      "> [Node 839ad04f-5520-4423-ae2d-cf7911106a8a] [Similarity score:             0.860879] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 201b4f5f-38d0-4a42-bf6d-9eeae460dff9] [Similarity score:             0.831016] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> Top 2 nodes:\n",
      "> [Node f35731ac-d5e7-4c39-8b58-b5fb13421846] [Similarity score:             0.928648] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node 686d673e-878d-4fa7-af25-71488bdb124c] [Similarity score:             0.79337] ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 4/8 [00:05<00:05,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.2825351111462948 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.', 'type': 'credit_limit', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.32931889319086505 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.', 'type': 'credit_limit', 'param': None, 'code': None}}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  1.44s/it]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 20d04a54-70b8-427e-82e4-f08088283ed1] [Similarity score:             0.70927] Tonic Validate  tutorial on Implementing integration tests for LlamaIndex. Chia Jeng Yang   tutor...\n",
      "> [Node 7de978b1-b255-4247-b99f-2985fd3486d6] [Similarity score:             0.702937] Tweet ‚úçÔ∏è Tutorials: Build a best-in-class RAG application using Qdrant as a vector store, Jina AI...\n",
      "> Top 2 nodes:\n",
      "> [Node 201b4f5f-38d0-4a42-bf6d-9eeae460dff9] [Similarity score:             0.781271] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 80b13eac-a7e7-4ef4-a4ab-c3fd886e7959] [Similarity score:             0.764892] There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_ll...\n",
      "> Top 2 nodes:\n",
      "> [Node 20d04a54-70b8-427e-82e4-f08088283ed1] [Similarity score:             0.788622] Tonic Validate  tutorial on Implementing integration tests for LlamaIndex. Chia Jeng Yang   tutor...\n",
      "> [Node 9f15d4bf-e69d-40ca-aebd-193079e324f3] [Similarity score:             0.783551] LlamaIndex Newsletter 2024‚Äì01‚Äì30\n",
      "Hello LlamaIndex Adventurers ü¶ô, Welcome to another thrilling wee...\n",
      "> Top 2 nodes:\n",
      "> [Node 16850228-59b5-460a-9c3e-31cd62ea59f9] [Similarity score:             0.837439] LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fam ü¶ô What an amazing week we‚Äôve had! We‚Äôre excited ...\n",
      "> [Node 8d03a27d-fbd3-42dc-9f7e-d30baad17f4d] [Similarity score:             0.826975] LlamaIndex Newsletter 2023‚Äì10‚Äì31\n",
      "Greetings Llama Enthusiasts ü¶ô! Another week has zoomed past, and...\n",
      "> Top 2 nodes:\n",
      "> [Node 201b4f5f-38d0-4a42-bf6d-9eeae460dff9] [Similarity score:             0.83765] Let‚Äôs take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 771cea2f-164f-43dc-9483-97c64608a538] [Similarity score:             0.834866] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 1e75af1a-09fa-4c55-b064-b94e90256568] [Similarity score:             0.815387] Tweet . Fine-Tuning Guides: OpenAI Fine-Tuning:  LlamaIndex unveils a fresh guide on harnessing O...\n",
      "> [Node 6c5f6b8e-0c44-4866-87f8-b89100aba4ac] [Similarity score:             0.815079] LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama Fans! ü¶ô We hope you enjoyed our  OpenAI Dev Day s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:12<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_prediction_dataset = await synthetic_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bf544036e140829f6ce3a695910f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Notebook ,  Tweet We launched revamped Python d...\n",
      "> Adding chunk: LlamaIndex newsletter 2023‚Äì10‚Äì24\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Notebook ,  Tweet We launched revamped Python d...\n",
      "> Adding chunk: Introducing LlamaCloud and LlamaParse\n",
      "Today is ...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: Introducing LlamaCloud and LlamaParse\n",
      "Today is ...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Define objective function \n",
      " def   objective_fun...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Define objective function \n",
      " def   objective_fun...\n",
      "> Adding chunk: It‚Äôs what gets us up in the morning and keeps u...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n",
      "> Adding chunk: It‚Äôs what gets us up in the morning and keeps u...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It‚Äôs our birthday! One year...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: Slides . Simon  conducted a workshop on Buildin...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: Slides . Simon  conducted a workshop on Buildin...\n",
      "> Adding chunk: Linking the resources again below: Gemini (text...\n",
      "> Adding chunk: Their  multimodal demos  demonstrate joint imag...\n",
      "> Adding chunk: Linking the resources again below: Gemini (text...\n",
      "> Adding chunk: Their  multimodal demos  demonstrate joint imag...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: Tweet AI Chatbot Starter (from the DataStax tea...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: Tweet AI Chatbot Starter (from the DataStax tea...\n",
      "> Adding chunk: Query ‚ÄúHow does GPT4 do on the bar exam?‚Äù Respo...\n",
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n",
      "> Adding chunk: Query ‚ÄúHow does GPT4 do on the bar exam?‚Äù Respo...\n",
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: LlamaCloud - Built for Enterprise LLM App Build...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: LlamaCloud - Built for Enterprise LLM App Build...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: LlamaIndex 0.7.0: Better Enabling Bottoms-Up LL...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: LlamaIndex 0.7.0: Better Enabling Bottoms-Up LL...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: f_lang_match = Feedback(hugs.language_match).on...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: f_lang_match = Feedback(hugs.language_match).on...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: f_lang_match = Feedback(hugs.language_match).on...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: f_lang_match = Feedback(hugs.language_match).on...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Notebook . üó∫Ô∏è Guides: Guide  to Integrating Lla...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Notebook . üó∫Ô∏è Guides: Guide  to Integrating Lla...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Say that you want to build a chatbot Define the...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Say that you want to build a chatbot Define the...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: Let‚Äôs take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì10‚Äì31\n",
      "Greetings Llam...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì10‚Äì31\n",
      "Greetings Llam...\n",
      "> Adding chunk: Tweet . Fine-Tuning Guides: OpenAI Fine-Tuning:...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama...\n",
      "> Adding chunk: Tweet . Fine-Tuning Guides: OpenAI Fine-Tuning:...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama...\n",
      "> Adding chunk: Tonic Validate  tutorial on Implementing integr...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì30\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Tonic Validate  tutorial on Implementing integr...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024‚Äì01‚Äì30\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Tonic Validate  tutorial on Implementing integr...\n",
      "> Adding chunk: Tweet ‚úçÔ∏è Tutorials: Build a best-in-class RAG a...\n",
      "> Adding chunk: Tonic Validate  tutorial on Implementing integr...\n",
      "> Adding chunk: Tweet ‚úçÔ∏è Tutorials: Build a best-in-class RAG a...\n"
     ]
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    synthetic_response_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>3.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   3.960000\n",
       "mean_relevancy_score     0.900000\n",
       "mean_faithfulness_score  0.966667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the new feature released by LlamaInde...</td>\n",
       "      <td>The new feature, QueryFusionRetriever, allows ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex newsletter 2023‚Äì10‚Äì24\\nHello Llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discuss the advancements made by LlamaIndex in...</td>\n",
       "      <td>LlamaIndex has made significant advancements i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing LlamaCloud and LlamaParse\\nToday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain the three main sections of the OpenAI ...</td>\n",
       "      <td>The OpenAI Cookbook for evaluating RAG systems...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the OpenAI Cookbook suggest evaluatin...</td>\n",
       "      <td>The OpenAI Cookbook suggests evaluating the pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has LlamaIndex evolved over the past year ...</td>\n",
       "      <td>LlamaIndex has experienced significant growth ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[It‚Äôs what gets us up in the morning and keeps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can you explain the significance of the Retrie...</td>\n",
       "      <td>RAG technology plays a crucial role in LlamaIn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[MultiModal RAG for Advanced Video Processing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the partnership with Google Gemini be...</td>\n",
       "      <td>The partnership with Google Gemini benefits Ll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Linking the resources again below: Gemini (te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Describe the Multi-Doc SEC 10Q Dataset launche...</td>\n",
       "      <td>The Multi-Doc SEC 10Q Dataset, launched by Taq...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tweet . We introduced day-0 integrations with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>The MemoryCache project by Mozilla utilizes Pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Query ‚ÄúHow does GPT4 do on the bar exam?‚Äù Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Discuss the significance of integrating Na2SQL...</td>\n",
       "      <td>The integration of Na2SQL with Llama Index is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Transforming Natural Language to SQL and Insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does LlamaCloud help in reducing productio...</td>\n",
       "      <td>According to the latest updates, LlamaCloud he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaCloud - Built for Enterprise LLM App Bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>The new features introduced in LlamaCloud to e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This feature enables transparency, re-use, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does LlamaIndex help in building LLM apps ...</td>\n",
       "      <td>LlamaIndex is a popular open-source framework ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Evaluate LLM Apps with LlamaIndex a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Can you explain the process of wrapping a Llam...</td>\n",
       "      <td>To wrap a LlamaIndex app with TruLens, you can...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Evaluate LLM Apps with LlamaIndex a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the feedback function in TruLlama hel...</td>\n",
       "      <td>The feedback function in TruLlama helps improv...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Wrap A LlamaIndex App with TruLens With TruLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the significance of tracking a...</td>\n",
       "      <td>Tracking and evaluating app versions using Tru...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[min )\\n\\n\\nfeedbacks = [f_lang_match, f_qa_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explain the basic structure of LlamaIndex's Ag...</td>\n",
       "      <td>The basic structure of LlamaIndex's Agentic RA...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How does the meta-agent or top-level agent in ...</td>\n",
       "      <td>The meta-agent or top-level agent in the Agent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Explain the concept of Agentic RAG as describe...</td>\n",
       "      <td>Agentic RAG is a concept that integrates agent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does the implementation by LlamaIndex demo...</td>\n",
       "      <td>The implementation by LlamaIndex demonstrates ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the integration of NVIDIA NIM with Ll...</td>\n",
       "      <td>The integration of NVIDIA NIM with LlamaIndex ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What benefits do developers gain from using NV...</td>\n",
       "      <td>By integrating NVIDIA NIM with LlamaIndex, dev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How does Llama Packs aim to simplify the proce...</td>\n",
       "      <td>Llama Packs aim to simplify the process of bui...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we‚Äôre excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Can you explain the two ways in which Llama Pa...</td>\n",
       "      <td>Llama Packs can be described and utilized by u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we‚Äôre excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Explain the process of downloading and initial...</td>\n",
       "      <td>To download and initialize a Llama Pack, you c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let‚Äôs take a look at the downloaded pack in  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How can a user customize a Llama Pack, such as...</td>\n",
       "      <td>To customize a Llama Pack, such as swapping ou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let‚Äôs take a look at the downloaded pack in  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What are some of the key feature releases and ...</td>\n",
       "      <td>The LLaMA Index 0.9 release features streamlin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2023‚Äì11‚Äì21\\nHello Llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Describe the MechGPT project by Professor Mark...</td>\n",
       "      <td>The MechGPT project by Professor Markus J. Bue...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Tweet . Fine-Tuning Guides: OpenAI Fine-Tunin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What are some of the tutorials covered in the ...</td>\n",
       "      <td>According to the newsletter, the following tut...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate  tutorial on Implementing inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Who are the speakers featured in the webinars ...</td>\n",
       "      <td>According to the context, the speaker featured...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate  tutorial on Implementing inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the new feature released by LlamaInde...   \n",
       "1   Discuss the advancements made by LlamaIndex in...   \n",
       "2   Explain the three main sections of the OpenAI ...   \n",
       "3   How does the OpenAI Cookbook suggest evaluatin...   \n",
       "4   How has LlamaIndex evolved over the past year ...   \n",
       "5   Can you explain the significance of the Retrie...   \n",
       "6   How does the partnership with Google Gemini be...   \n",
       "7   Describe the Multi-Doc SEC 10Q Dataset launche...   \n",
       "8   How does the MemoryCache project by Mozilla ut...   \n",
       "9   Discuss the significance of integrating Na2SQL...   \n",
       "10  How does LlamaCloud help in reducing productio...   \n",
       "11  What new features have been introduced in Llam...   \n",
       "12  How does LlamaIndex help in building LLM apps ...   \n",
       "13  Can you explain the process of wrapping a Llam...   \n",
       "14  How does the feedback function in TruLlama hel...   \n",
       "15  Can you explain the significance of tracking a...   \n",
       "16  Explain the basic structure of LlamaIndex's Ag...   \n",
       "17  How does the meta-agent or top-level agent in ...   \n",
       "18  Explain the concept of Agentic RAG as describe...   \n",
       "19  How does the implementation by LlamaIndex demo...   \n",
       "20  How does the integration of NVIDIA NIM with Ll...   \n",
       "21  What benefits do developers gain from using NV...   \n",
       "22  How does Llama Packs aim to simplify the proce...   \n",
       "23  Can you explain the two ways in which Llama Pa...   \n",
       "24  Explain the process of downloading and initial...   \n",
       "25  How can a user customize a Llama Pack, such as...   \n",
       "26  What are some of the key feature releases and ...   \n",
       "27  Describe the MechGPT project by Professor Mark...   \n",
       "28  What are some of the tutorials covered in the ...   \n",
       "29  Who are the speakers featured in the webinars ...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   The new feature, QueryFusionRetriever, allows ...              1.0   \n",
       "1   LlamaIndex has made significant advancements i...              1.0   \n",
       "2   The OpenAI Cookbook for evaluating RAG systems...              1.0   \n",
       "3   The OpenAI Cookbook suggests evaluating the pe...              1.0   \n",
       "4   LlamaIndex has experienced significant growth ...              1.0   \n",
       "5   RAG technology plays a crucial role in LlamaIn...              1.0   \n",
       "6   The partnership with Google Gemini benefits Ll...              1.0   \n",
       "7   The Multi-Doc SEC 10Q Dataset, launched by Taq...              1.0   \n",
       "8   The MemoryCache project by Mozilla utilizes Pr...              0.0   \n",
       "9   The integration of Na2SQL with Llama Index is ...              1.0   \n",
       "10  According to the latest updates, LlamaCloud he...              1.0   \n",
       "11  The new features introduced in LlamaCloud to e...              1.0   \n",
       "12  LlamaIndex is a popular open-source framework ...              1.0   \n",
       "13  To wrap a LlamaIndex app with TruLens, you can...              1.0   \n",
       "14  The feedback function in TruLlama helps improv...              1.0   \n",
       "15  Tracking and evaluating app versions using Tru...              1.0   \n",
       "16  The basic structure of LlamaIndex's Agentic RA...              1.0   \n",
       "17  The meta-agent or top-level agent in the Agent...              1.0   \n",
       "18  Agentic RAG is a concept that integrates agent...              1.0   \n",
       "19  The implementation by LlamaIndex demonstrates ...              1.0   \n",
       "20  The integration of NVIDIA NIM with LlamaIndex ...              1.0   \n",
       "21  By integrating NVIDIA NIM with LlamaIndex, dev...              1.0   \n",
       "22  Llama Packs aim to simplify the process of bui...              1.0   \n",
       "23  Llama Packs can be described and utilized by u...              1.0   \n",
       "24  To download and initialize a Llama Pack, you c...              1.0   \n",
       "25  To customize a Llama Pack, such as swapping ou...              1.0   \n",
       "26  The LLaMA Index 0.9 release features streamlin...              1.0   \n",
       "27  The MechGPT project by Professor Markus J. Bue...              0.0   \n",
       "28  According to the newsletter, the following tut...              0.0   \n",
       "29  According to the context, the speaker featured...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 2.5                 1.0   \n",
       "1                 3.0                 1.0   \n",
       "2                 NaN                 1.0   \n",
       "3                 NaN                 1.0   \n",
       "4                 4.5                 1.0   \n",
       "5                 NaN                 1.0   \n",
       "6                 NaN                 1.0   \n",
       "7                 4.5                 1.0   \n",
       "8                 4.0                 1.0   \n",
       "9                 4.5                 1.0   \n",
       "10                2.5                 1.0   \n",
       "11                3.5                 1.0   \n",
       "12                4.5                 1.0   \n",
       "13                4.5                 1.0   \n",
       "14                4.0                 1.0   \n",
       "15                4.5                 1.0   \n",
       "16                4.5                 1.0   \n",
       "17                4.0                 1.0   \n",
       "18                4.5                 1.0   \n",
       "19                4.0                 1.0   \n",
       "20                4.5                 1.0   \n",
       "21                4.5                 1.0   \n",
       "22                4.5                 1.0   \n",
       "23                4.0                 1.0   \n",
       "24                4.5                 1.0   \n",
       "25                4.0                 1.0   \n",
       "26                NaN                 1.0   \n",
       "27                3.0                 0.0   \n",
       "28                4.0                 1.0   \n",
       "29                2.5                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [LlamaIndex newsletter 2023‚Äì10‚Äì24\\nHello Llama...  \n",
       "1   [Introducing LlamaCloud and LlamaParse\\nToday ...  \n",
       "2   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "3   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "4   [It‚Äôs what gets us up in the morning and keeps...  \n",
       "5   [MultiModal RAG for Advanced Video Processing ...  \n",
       "6   [Linking the resources again below: Gemini (te...  \n",
       "7   [Tweet . We introduced day-0 integrations with...  \n",
       "8   [Query ‚ÄúHow does GPT4 do on the bar exam?‚Äù Res...  \n",
       "9   [Transforming Natural Language to SQL and Insi...  \n",
       "10  [LlamaCloud - Built for Enterprise LLM App Bui...  \n",
       "11  [This feature enables transparency, re-use, an...  \n",
       "12  [Build and Evaluate LLM Apps with LlamaIndex a...  \n",
       "13  [Build and Evaluate LLM Apps with LlamaIndex a...  \n",
       "14  [Wrap A LlamaIndex App with TruLens With TruLe...  \n",
       "15  [min )\\n\\n\\nfeedbacks = [f_lang_match, f_qa_re...  \n",
       "16  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "17  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "18  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "19  [Introducing llama-agents: A Powerful Framewor...  \n",
       "20  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "21  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "22  [Introducing Llama Packs\\nToday we‚Äôre excited ...  \n",
       "23  [Introducing Llama Packs\\nToday we‚Äôre excited ...  \n",
       "24  [Let‚Äôs take a look at the downloaded pack in  ...  \n",
       "25  [Let‚Äôs take a look at the downloaded pack in  ...  \n",
       "26  [LlamaIndex Newsletter 2023‚Äì11‚Äì21\\nHello Llama...  \n",
       "27  [Tweet . Fine-Tuning Guides: OpenAI Fine-Tunin...  \n",
       "28  [Tonic Validate  tutorial on Implementing inte...  \n",
       "29  [Tonic Validate  tutorial on Implementing inte...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2876bc3c-251b-4686-9321-6a7a0c081019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node e009b8ae-616f-47c3-9cf9-20fa1c683ac3] [Similarity score:             0.855356] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 0020f723-6a45-4396-9543-f414375e8936] [Similarity score:             0.781137] LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Community ü¶ô, We are excited to collaborate with Deep...\n",
      "> Top 2 nodes:\n",
      "> [Node 685a355d-9332-41a9-96bd-a4903ec86777] [Similarity score:             0.824755] Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "         return  self._retrieve(query_bundle)\n",
      "\n",
      "     async ...\n",
      "> [Node d175dd37-b2cb-4772-9709-4cc90227bac0] [Similarity score:             0.81401] # Extract keys from queries and relevant_docs that need to be removed \n",
      "    queries_relevant_docs_...\n",
      "> Top 2 nodes:\n",
      "> [Node 78ead847-3d01-4db3-8483-59a88f95b1d7] [Similarity score:             0.797389] First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powere...\n",
      "> [Node 9929ccb4-3d3b-4e9d-80ac-dc86ff8df3c1] [Similarity score:             0.793224] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "curated_response_eval_prediction_dataset = await curated_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37a3e3347374c6f8e2ac558521ae9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: First we‚Äôll bring in our dependencies and set u...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: First we‚Äôll bring in our dependencies and set u...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "        ...\n",
      "> Adding chunk: # Extract keys from queries and relevant_docs t...\n",
      "> Adding chunk: Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "        ...\n",
      "> Adding chunk: # Extract keys from queries and relevant_docs t...\n"
     ]
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    curated_response_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.666667\n",
       "mean_relevancy_score     1.000000\n",
       "mean_faithfulness_score  1.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>Distributed Service-Oriented Architecture, Communication via standardized API interfaces, Pass messages between agents using a message queue, Define agentic and explicit orchestration flows, Ease of deployment, Scalability and resource management.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator import  dotenv\\ndotenv.load_dotenv()  # our .env file defines OPENAI_API_KEY \\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    AgentOrchestrator,\\n)\\n from  llama_index.core.agent  import  FunctionCallingAgentWorker\\n from  llama_index.core.tools  import  FunctionTool\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # turn on logging so we can see the system working \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Set up the message queue and control plane \\nmessage_queue = SimpleMessageQueue()\\ncontrol_plane = ControlPlaneServer(\\n    message_queue=message_queue,\\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\\n) Next we create our tools using LlamaIndex‚Äôs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice: # create a tool \\n def   get_the_secret_fact () -&gt;  str :\\n     \"\"\"Returns the secret fact.\"\"\" \\n     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \\n\\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # Define an agent \\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nagent = worker.as_agent()\\n\\n # Create an agent service \\nagent_service = AgentService(\\n    agent=agent,\\n    message_queue=message_queue,\\n    description= \"General purpose assistant\" ,\\n    service_name= \"assistant\" ,\\n) Finally we launch the service and the control plane. Note that here we‚Äôre using a helper function to run a single query through the system and then exit; next we‚Äôll show how to deploy this to production. # Set up the launcher for local testing \\n from  llama_agents  import  LocalLauncher\\n\\nlauncher = LocalLauncher(\\n    [agent_service],\\n    control_plane,\\n    message_queue,\\n)\\n\\n # Run a single query through the system \\nresult = launcher.launch_single( \"What's the secret fact?\", Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, LlamaIndex Newsletter 2023‚Äì12‚Äì05\\nHello Llama Community ü¶ô, We are excited to collaborate with DeepLearningAI and TruEraAI to launch an extensive course on advanced Retrieval-Augmented Generation (RAG) and its evaluations. The course includes Sentence Window Retrieval, Auto-merging Retrieval, and Evaluations with TruLensML, providing practical tools for enhanced learning and application. To make the most of this learning opportunity, we invite you to  take the course . We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. ü§©  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  Tweet . Core  guide   for Full-Stack LLM App Development : Simplifies complex app development with tools like ‚Äòcreate-llama‚Äô for full-stack apps, ‚ÄòSEC Insights‚Äô for multi-document processing, and ‚ÄòLlamaIndex Chat‚Äô for chatbot customization. ‚ú® Feature Releases and Enhancements: We‚Äôve launched seven advanced retrieval LlamaPacks, serving as templates to easily build advanced RAG systems. These packs simplify the process to almost a single line of code, moving away from the traditional notebook approach.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Implemented by the user.\\n\\n        \"\"\" \\n         return  self._retrieve(query_bundle)\\n\\n     async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&amp;gt;  List [NodeWithScore]:\\n         if   isinstance (str_or_query_bundle,  str ):\\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\\n         return   await  self._aretrieve(str_or_query_bundle)\\n\\ncustom_retriever = CustomRetriever(vector_retriever) Evaluation: To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics: retriever_evaluator = RetrieverEvaluator.from_metric_names(\\n    [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\\n)\\neval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset) Results: We put various embedding models and rerankers to the test. Here are the models we considered: Embedding Models : OpenAI Embedding Voyage Embedding CohereAI Embedding  (v2.0/ v3.0) Jina Embeddings  (small/ base) BAAI/bge-large-en Google PaLM Embedding Rerankers : CohereAI bge-reranker-base bge-reranker-large It‚Äôs worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools., # Extract keys from queries and relevant_docs that need to be removed \\n    queries_relevant_docs_keys_to_remove = {\\n        k  for  k, v  in  qa_dataset.queries.items()\\n         if   'Here are 2'   in  v  or   'Here are two'   in  v\\n    }\\n\\n     # Filter queries and relevant_docs using dictionary comprehensions \\n    filtered_queries = {\\n        k: v  for  k, v  in  qa_dataset.queries.items()\\n         if  k  not   in  queries_relevant_docs_keys_to_remove\\n    }\\n    filtered_relevant_docs = {\\n        k: v  for  k, v  in  qa_dataset.relevant_docs.items()\\n         if  k  not   in  queries_relevant_docs_keys_to_remove\\n    }\\n\\n     # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data \\n     return  EmbeddingQAFinetuneDataset(\\n        queries=filtered_queries,\\n        corpus=qa_dataset.corpus,\\n        relevant_docs=filtered_relevant_docs\\n    )\\n\\n # filter out pairs with phrases `Here are 2 questions based on provided context` \\nqa_dataset = filter_qa_dataset(qa_dataset) Custom Retriever: To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base  VectorIndexRetriever . Upon retrieving the nodes, we then introduce a reranker to further refine the results. It‚Äôs worth noting that for this particular experiment, we‚Äôve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with  OpenAIEmbedding , please refer to the  notebook  for code with other embeddings.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 query  \\\n",
       "0                                                                                                               What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                         What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "\n",
       "                                                                                                                                                                                                                                                    answer  \\\n",
       "0  Distributed Service-Oriented Architecture, Communication via standardized API interfaces, Pass messages between agents using a message queue, Define agentic and explicit orchestration flows, Ease of deployment, Scalability and resource management.   \n",
       "1                                                              The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.   \n",
       "2                                                                                                          The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \\\n",
       "0              1.0                4.5                 1.0   \n",
       "1              1.0                4.5                 1.0   \n",
       "2              1.0                5.0                 1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  contexts  \n",
       "0  [First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator import  dotenv\\ndotenv.load_dotenv()  # our .env file defines OPENAI_API_KEY \\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    AgentOrchestrator,\\n)\\n from  llama_index.core.agent  import  FunctionCallingAgentWorker\\n from  llama_index.core.tools  import  FunctionTool\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # turn on logging so we can see the system working \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Set up the message queue and control plane \\nmessage_queue = SimpleMessageQueue()\\ncontrol_plane = ControlPlaneServer(\\n    message_queue=message_queue,\\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\\n) Next we create our tools using LlamaIndex‚Äôs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice: # create a tool \\n def   get_the_secret_fact () ->  str :\\n     \"\"\"Returns the secret fact.\"\"\" \\n     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \\n\\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # Define an agent \\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nagent = worker.as_agent()\\n\\n # Create an agent service \\nagent_service = AgentService(\\n    agent=agent,\\n    message_queue=message_queue,\\n    description= \"General purpose assistant\" ,\\n    service_name= \"assistant\" ,\\n) Finally we launch the service and the control plane. Note that here we‚Äôre using a helper function to run a single query through the system and then exit; next we‚Äôll show how to deploy this to production. # Set up the launcher for local testing \\n from  llama_agents  import  LocalLauncher\\n\\nlauncher = LocalLauncher(\\n    [agent_service],\\n    control_plane,\\n    message_queue,\\n)\\n\\n # Run a single query through the system \\nresult = launcher.launch_single( \"What's the secret fact?\", Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents.]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô, LlamaIndex Newsletter 2023‚Äì12‚Äì05\\nHello Llama Community ü¶ô, We are excited to collaborate with DeepLearningAI and TruEraAI to launch an extensive course on advanced Retrieval-Augmented Generation (RAG) and its evaluations. The course includes Sentence Window Retrieval, Auto-merging Retrieval, and Evaluations with TruLensML, providing practical tools for enhanced learning and application. To make the most of this learning opportunity, we invite you to  take the course . We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. ü§©  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  Tweet . Core  guide   for Full-Stack LLM App Development : Simplifies complex app development with tools like ‚Äòcreate-llama‚Äô for full-stack apps, ‚ÄòSEC Insights‚Äô for multi-document processing, and ‚ÄòLlamaIndex Chat‚Äô for chatbot customization. ‚ú® Feature Releases and Enhancements: We‚Äôve launched seven advanced retrieval LlamaPacks, serving as templates to easily build advanced RAG systems. These packs simplify the process to almost a single line of code, moving away from the traditional notebook approach.]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Implemented by the user.\\n\\n        \"\"\" \\n         return  self._retrieve(query_bundle)\\n\\n     async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\\n         if   isinstance (str_or_query_bundle,  str ):\\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\\n         return   await  self._aretrieve(str_or_query_bundle)\\n\\ncustom_retriever = CustomRetriever(vector_retriever) Evaluation: To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics: retriever_evaluator = RetrieverEvaluator.from_metric_names(\\n    [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\\n)\\neval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset) Results: We put various embedding models and rerankers to the test. Here are the models we considered: Embedding Models : OpenAI Embedding Voyage Embedding CohereAI Embedding  (v2.0/ v3.0) Jina Embeddings  (small/ base) BAAI/bge-large-en Google PaLM Embedding Rerankers : CohereAI bge-reranker-base bge-reranker-large It‚Äôs worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools., # Extract keys from queries and relevant_docs that need to be removed \\n    queries_relevant_docs_keys_to_remove = {\\n        k  for  k, v  in  qa_dataset.queries.items()\\n         if   'Here are 2'   in  v  or   'Here are two'   in  v\\n    }\\n\\n     # Filter queries and relevant_docs using dictionary comprehensions \\n    filtered_queries = {\\n        k: v  for  k, v  in  qa_dataset.queries.items()\\n         if  k  not   in  queries_relevant_docs_keys_to_remove\\n    }\\n    filtered_relevant_docs = {\\n        k: v  for  k, v  in  qa_dataset.relevant_docs.items()\\n         if  k  not   in  queries_relevant_docs_keys_to_remove\\n    }\\n\\n     # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data \\n     return  EmbeddingQAFinetuneDataset(\\n        queries=filtered_queries,\\n        corpus=qa_dataset.corpus,\\n        relevant_docs=filtered_relevant_docs\\n    )\\n\\n # filter out pairs with phrases `Here are 2 questions based on provided context` \\nqa_dataset = filter_qa_dataset(qa_dataset) Custom Retriever: To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base  VectorIndexRetriever . Upon retrieving the nodes, we then introduce a reranker to further refine the results. It‚Äôs worth noting that for this particular experiment, we‚Äôve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with  OpenAIEmbedding , please refer to the  notebook  for code with other embeddings.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "113a264d-1cea-4e41-82ac-22135f12f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "         return  self._retrieve(query_bundle)\n",
      "\n",
      "     async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n",
      "         if   isinstance (str_or_query_bundle,  str ):\n",
      "            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
      "         return   await  self._aretrieve(str_or_query_bundle)\n",
      "\n",
      "custom_retriever = CustomRetriever(vector_retriever) Evaluation: To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics: retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
      "    [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\n",
      ")\n",
      "eval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset) Results: We put various embedding models and rerankers to the test. Here are the models we considered: Embedding Models : OpenAI Embedding Voyage Embedding CohereAI Embedding  (v2.0/ v3.0) Jina Embeddings  (small/ base) BAAI/bge-large-en Google PaLM Embedding Rerankers : CohereAI bge-reranker-base bge-reranker-large It‚Äôs worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools.\n",
      "----------\n",
      "# Extract keys from queries and relevant_docs that need to be removed \n",
      "    queries_relevant_docs_keys_to_remove = {\n",
      "        k  for  k, v  in  qa_dataset.queries.items()\n",
      "         if   'Here are 2'   in  v  or   'Here are two'   in  v\n",
      "    }\n",
      "\n",
      "     # Filter queries and relevant_docs using dictionary comprehensions \n",
      "    filtered_queries = {\n",
      "        k: v  for  k, v  in  qa_dataset.queries.items()\n",
      "         if  k  not   in  queries_relevant_docs_keys_to_remove\n",
      "    }\n",
      "    filtered_relevant_docs = {\n",
      "        k: v  for  k, v  in  qa_dataset.relevant_docs.items()\n",
      "         if  k  not   in  queries_relevant_docs_keys_to_remove\n",
      "    }\n",
      "\n",
      "     # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data \n",
      "     return  EmbeddingQAFinetuneDataset(\n",
      "        queries=filtered_queries,\n",
      "        corpus=qa_dataset.corpus,\n",
      "        relevant_docs=filtered_relevant_docs\n",
      "    )\n",
      "\n",
      " # filter out pairs with phrases `Here are 2 questions based on provided context` \n",
      "qa_dataset = filter_qa_dataset(qa_dataset) Custom Retriever: To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base  VectorIndexRetriever . Upon retrieving the nodes, we then introduce a reranker to further refine the results. It‚Äôs worth noting that for this particular experiment, we‚Äôve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with  OpenAIEmbedding , please refer to the  notebook  for code with other embeddings.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for context in curated_deep_eval_df.iloc[2]['contexts']:\n",
    "    print(context)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
