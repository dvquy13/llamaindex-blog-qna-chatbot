{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if TESTING:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/22 14:24:55 INFO mlflow.tracking.fluent: Experiment with name 'Chain Frost - LlamaIndex Blog QnA Chatbot' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "LOG_TO_MLFLOW = True\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run()\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Automate online tasks with MultiOn and LlamaIndex',\n",
       "  'content': 'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.',\n",
       "  'author': 'MultiOn',\n",
       "  'date': 'May 23, 2024',\n",
       "  'tags': ['automation', 'Agents']},\n",
       " {'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       "  'content': 'We‚Äôre happy to announce the recent integration of LlamaIndex with PostgresML ‚Äî a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows. The problem with typical RAG workflows Typical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users. Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges: Increased dev time to master new technologies Complicated maintenance and scalability issues due to multiple points of failure Costly vendors required for multiple services The diagram above illustrates the complexity, showing how each component interacts across different services ‚Äî exacerbating these problems. Solution The PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows. By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call ‚Äî resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow. About PostgresML PostgresML [ github  ||  website  ||  docs ] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides. Key highlights: Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning PostgresML has a range of capabilities. In the following sections, we‚Äôll guide you through just one use case ‚Äì RAG ‚Äì and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app. How it works in LlamaIndex Let‚Äôs look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham‚Äôs essays. Step 1: Get Your Database Connection String If you haven‚Äôt already,  create your PostgresML account . You‚Äôll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Alternatively, you can pass the pgml_database_url argument when creating the index. Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed Index component: pip install llama_index llama-index-indices-managed-postgresml Then load in the data: mkdir  data\\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt Finally create the PostgresML Managed Index: from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\\n\\n\\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\\nindex = PostgresMLIndex.from_documents(\\n    documents, collection_name= \"llama-index-example\" \\n) Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see:  https://postgresml.org/docs/api/client-sdk/pipelines  Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 . Step 3: Querying Now that we have created our index we can use it for retrieval and querying: retriever = index.as_retriever()\\ndocs = retriever.retrieve( \"Was the author puzzled by the IBM 1401?\" )\\n for  doc  in  docs:\\n     print (doc) PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up. Using the PostgresML Index as a query_engine is just as easy: response = index.as_query_engine().query( \"Was the author puzzled by the IBM 1401?\" )\\n print (response) Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming: query_engine = index.as_query_engine(streaming= True )\\nresults = query_engine.query( \"Was the author puzzled by the IBM 1401?\" )\\n for  text  in  results.response_gen:\\n     print (text, end= \"\" , flush= True ) Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable. Key takeaways The PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend. To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro  guide  to setup your account, and the examples above with your own data.',\n",
       "  'author': 'PostgresML',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': ['Managed Indexes']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-06-04',\n",
       "  'content': \"Hello, LlamaIndex Family! ü¶ô We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need. ü§©\\xa0 The highlights: Elevating Knowledge Graphs:  The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities.  Docs ,  Tweet . Spreadsheet Insights with LlamaParse:  LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling.  Notebook ,  Tweet . Code Generation with Codestral:  Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages.  Docs ,  Tweet . ‚ú® Feature Releases and Enhancements: We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support.  Docs ,  Tweet . We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance.  Notebook ,  Tweet . We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages.  Docs ,  Tweet . We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript.  Blogpost ,  Tweet . We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex.  Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results. Guide  to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results. Guide  to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights. üñ•Ô∏è\\xa0Demos: Omakase RAG Orchestrator , a project developed by  Amir Mehr , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel. gmail-extractor , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision. ‚úçÔ∏è Tutorials: Sherlock Xu‚Äôs  tutorial  from BentoML on Serving A LlamaIndex RAG App as REST APIs. üìë\\xa0Papers: FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text. üìπ\\xa0Webinar: Webinar  with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents üìÖ\\xa0 Events: Join  Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Jun 4, 2024',\n",
       "  'tags': []},\n",
       " {'title': 'Batch inference with MyMagic AI and LlamaIndex',\n",
       "  'content': 'This is a guest post from MyMagic AI. MyMagic AI  allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for  batch  inference (also known as  offline  or  delayed  inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it\\'s integrated directly into LlamaIndex! Part 1: batch inference How It Works: 1. Setup : Organize Your Data in an AWS S3 or GCS Bucket: Create a folder using your user ID assigned to you upon registration. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. Purpose of the \\'Session\\' Folder: This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. Granting Access to MyMagic AI: To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the  MyMagic AI documentation . 2. Install : Install both MyMagic AI‚Äôs API integration and LlamaIndex library: pip install llama-index\\npip install llama-index-llms-mymagic 3. API Request:  The llamaIndex library is a wrapper around MyMagic AI‚Äôs API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details. import  asyncio\\n from  llama_index.llms.mymagic  import  MyMagicAI\\n\\nllm = MyMagicAI(\\n    api_key= \"user_...\" ,  # provided by MyMagic AI upon sign-up \\n    storage_provider= \"s3\" ,\\n    bucket_name= \"batch-bucket\" ,  # you may name anything \\n    session= \"my-session\" ,\\n    role_arn= \"arn:aws:iam::<your account id>:role/mymagic-role\" ,\\n    system_prompt= \"You are an AI assistant that helps to summarize the documents without essential loss of information\" ,  # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create \\n    region= \"eu-west-2\" ,\\n) We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests. resp = llm.complete(\\n    question= \"Summarise this in one sentence.\" ,\\n    model= \"mixtral8x7\" , \\n    max_tokens= 20 ,   # default is 10 \\n)\\n print (resp)\\n async   def   main ():\\n    aresp =  await  llm.acomplete(\\n        question= \"Summarize this in one sentence.\" ,\\n        model= \"llama7b\" ,\\n        max_tokens= 20 ,\\n    )\\n     print (aresp)\\n\\nasyncio.run(main()) This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI‚Äôs backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data). Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a ‚Äúbatch way‚Äù. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once. Use Cases While there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing. 1. Extraction Imagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones. 2. Classification For businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning. 3. Embedding Embedding text files for further machine learning applications is another powerful use case of MyMagic AI\\'s API. You will be ready for your vector db in a matter of days not weeks. 4. Training (Fine-tuning) Data Generation Imagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI‚Äôs API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5. 5. Transcription MyMagic AI‚Äôs API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket. Part 2: Integration with LlamaIndex‚Äôs RAG Pipeline The output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex\\'s RAG pipeline for effective data storage and retrieval. This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled: pip install llama-index-embeddings-huggingface\\ncurl -fsSL https://ollama.com/install.sh | sh\\nollama pull llama3 For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json: {\\n  \"id_review1\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\\n  },\\n  \"id_review2\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen\\'s control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson\\'s performance. It concludes by calling the film a great comedy to watch with friends.\"\\n  },\\n  \"id_review3\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell\\'s flat. Overall, it is a terrific and well-written piece.\"\\n  },\\n  \"id_review4\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"Petter Mattei\\'s \\'Love in the Time of Money\\' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler\\'s play on the same theme. Mattei\\'s work is appreciated, and viewers look forward to his future projects.\"\\n  },\\n  \"id_review5\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes the TV show \\'Oz\\', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn\\'t dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\\n  },\\n  \"token_count\": 3391\\n}\\n Now let‚Äôs embed and store this document and ask questions using LlamaIndex‚Äôs query engine. Bring in our dependencies: import  os\\n\\n from  llama_index.embeddings.huggingface  import  HuggingFaceEmbedding\\n from  llama_index.core.indices.vector_store  import  VectorStoreIndex\\n from  llama_index.core.settings  import  Settings\\n from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.llms.ollama  import  Ollama Configure the embedding model and Llama3 model embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-base-en-v1.5\" )\\nllm = Ollama(model= \"llama3\" , request_timeout= 300.0 ) Update settings for the indexing pipeline: Settings.llm = llm\\nSettings.embed_model = embed_model\\nSettings.chunk_size =  512   # This parameter defines the size of text chunks for embedding \\n\\ndocuments = SimpleDirectoryReader( \"reviews_1_5.json\" ).load_data()  #Modify path for your case Now create our index, our query engine and run a query: index = VectorStoreIndex.from_documents(documents, show_progress= True )\\n\\nquery_engine = index.as_query_engine(similarity_top_k= 3 )\\n\\nresponse = query_engine.query( \"What is the least favourite movie?\" )\\n print (response) Output: Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10. Now we know that the review 1 is the least favorite movie among these reviews. Next Steps This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.  Get started with MyMagic AI‚Äôs API  today!',\n",
       "  'author': 'MyMagic AI',\n",
       "  'date': 'May 22, 2024',\n",
       "  'tags': ['MyMagic AI', 'Batch inference']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-05-28',\n",
       "  'content': \"Greetings, LlamaIndex Family! ü¶ô Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week. ü§©\\xa0 The highlights: Secure Code Execution with AzureCodeInterpreterTool:  Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution. Build Automated Email Agents:  Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails. LlamaFS for Organized Files:  Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API. RAGApp's No-Code Chatbots:  Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible. ‚ú® Feature Releases and Enhancements: We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you‚Äôre ready to go.  Blogpost ,  Tweet . We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance.  Docs ,  Tweet . We have integrated the Vespa vector store, supporting hybrid search with BM25.  Docs ,  Tweet . We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities.  Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities. Guide  to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates. üñ•Ô∏è\\xa0Demos: LlamaFS, a project developed by  Alex Reibman  and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency.  Code ,  Tweet . RAGApp, a project developed by  Marcus Schiesser , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API.  Code ,  Tweet . ‚úçÔ∏è Tutorials: Phil Chirchir‚Äôs   tutorial  on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse. Pavan Kumar‚Äôs   tutorial  on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine‚Äôs capabilities. Jayita Bhattacharyya‚Äôs  tutorial  on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit. üìπ\\xa0Webinar: Webinar  with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags'])\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='dea8ec25-11a4-457e-9d1b-50460cb002fb', embedding=None, metadata={'title': 'Automate online tasks with MultiOn and LlamaIndex', 'author': 'MultiOn', 'date': 'May 23, 2024', 'tags': 'automation, Agents'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Automate online tasks with MultiOn and LlamaIndex\\nIntroduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       " 'author': 'PostgresML',\n",
       " 'date': 'May 28, 2024',\n",
       " 'tags': 'Managed Indexes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OPTION = 'openai'\n",
    "# llm_option = 'ollama'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4ab185c-c7aa-4b81-9f5b-15dd4c48725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 14:24:58.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mUsing OpenAI LLM...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if LLM_OPTION == 'ollama':\n",
    "    logger.info(f\"Using local Ollama LLM...\")\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    OLLAMA_MODEL_NAME = 'llama3'\n",
    "    llm = Ollama(base_url=base_url, model=model_name, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "    Settings.llm = llm\n",
    "    embedding = OllamaEmbedding(\n",
    "        model_name=OLLAMA_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "    Settings.embed_model = embedding\n",
    "    if LOG_TO_MLFLOW:\n",
    "        mlflow.log_param(\"OLLAMA_MODEL_NAME\", OLLAMA_MODEL_NAME)\n",
    "elif LLM_OPTION == 'openai':\n",
    "    logger.info(f\"Using OpenAI LLM...\")\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embedding = OpenAIEmbedding()\n",
    "    OPENAI_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "    llm = OpenAI(model=OPENAI_MODEL_NAME)\n",
    "    if LOG_TO_MLFLOW:\n",
    "        mlflow.log_param(\"OPENAI_MODEL_NAME\", OPENAI_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f72b3d52-7be9-49e1-bf4e-4cd586635d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5a5fb8a-636a-463c-a938-1864cff81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "COLLECTION = 'mvp'\n",
    "NOTEBOOK_CACHE_DP = 'data/001'\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a1d0d1-142d-4e7a-b626-ef154ba08e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 14:24:59.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mDeleting existing ChromaDB collection...\u001b[0m\n",
      "\u001b[32m2024-07-22 14:24:59.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mDeleting persisted nodes object at data/001/nodes.pkl...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = chromadb.PersistentClient(path=f\"{NOTEBOOK_CACHE_DP}/chroma_db\")\n",
    "collection_exists = COLLECTION in [c.name for c in db.list_collections()]\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    logger.info(f\"Deleting existing ChromaDB collection...\")\n",
    "    db.delete_collection(COLLECTION)\n",
    "    logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "    os.remove(NODES_PERSIST_FP)\n",
    "else:\n",
    "    logger.info(f\"Use existing ChromaDB collection\")\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ecdec51-2720-4628-a43f-caaa98310630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0f36679-603e-40d6-862b-5d9313b3c1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 14:25:00.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCreating new ChromaDB index...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: All parameters are required\n",
      "        \n",
      "        If...\n",
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n",
      "> Adding chunk: The email was authenticated and passed SPF and ...\n",
      "> Adding chunk: As these technologies evolve, they will continu...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Step 2: Create the PostgresML Managed Index Fir...\n",
      "> Adding chunk: The PostgresML Managed Index is doing embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.38s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "if chroma_collection.count() > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing ChromaDB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new ChromaDB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(chunk_size=512, chunk_overlap=10),\n",
    "            TitleExtractor(),\n",
    "            embedding,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "    \n",
    "    # Need to use await and arun here to run the pipeline else error\n",
    "    # Ref: https://docs.llamaindex.ai/en/stable/examples/ingestion/async_ingestion_pipeline/\n",
    "    # Ref: https://github.com/run-llama/llama_index/issues/13904#issuecomment-2145561710\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a338d-469e-4461-9511-553dd6c1f766",
   "metadata": {},
   "source": [
    "#### Inspect nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f68dde-bd70-49b2-a62b-40a527fc44ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# embeddings are excluded by default for performance, if need then explicitly ask for it in `include`\n",
    "# chroma_collection.get(include=['embeddings'])\n",
    "chroma_collection.get()['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 2\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = 0.7\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.7029416244989046] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.6983769899484777] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.702942] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.698377] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 14:25:12.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mMultiOn is the author of the document discussing the automation of tasks with MultiOn and LlamaIndex.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"What is MultiOn?\"\n",
    "response = query_engine.query(question)\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RECREATE_RETRIEVAL_EVAL_DATASET = True\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 14:25:12.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:10<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        nodes, llm=llm, num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b77bd622-ef88-44d3-8e5a-80fd10b9d768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.7601724451671024] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.7320236136357314] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.760172] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.732024] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.7686286115565645] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.768469815735006] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.768629] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.76847] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node f382f8b8-74ca-47ff-a072-2f3ab09f4732] [Similarity score: 0.7154316094693713] All parameters are required\n",
      "        \n",
      "        If you need to reply to an email, ask this tool to b...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.7069626306756908] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node f382f8b8-74ca-47ff-a072-2f3ab09f4732] [Similarity score:             0.715432] All parameters are required\n",
      "        \n",
      "        If you need to reply to an email, ask this tool to b...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.706963] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.6686502446630052] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node f382f8b8-74ca-47ff-a072-2f3ab09f4732] [Similarity score: 0.6635667925733604] All parameters are required\n",
      "        \n",
      "        If you need to reply to an email, ask this tool to b...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.66865] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node f382f8b8-74ca-47ff-a072-2f3ab09f4732] [Similarity score:             0.663567] All parameters are required\n",
      "        \n",
      "        If you need to reply to an email, ask this tool to b...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.6317200778957383] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score: 0.6256278862468907] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.63172] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score:             0.625628] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score: 0.6626594624353536] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.6286528687967611] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score:             0.662659] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.628653] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score: 0.6111444140762247] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.6092021006292914] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score:             0.611144] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.609202] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score: 0.6683933253113493] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.6243015163377146] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score:             0.668393] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.624302] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.652868231343393] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.6122606917165837] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.652868] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.612261] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.6666573247410373] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.6058961251682105] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.666657] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.605896] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.8705428972103986] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.8488801036601985] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.870543] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.84888] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.7935824456514432] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.7716837223548082] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.793582] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.771684] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.8263571217029845] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.7931957706591864] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.826357] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.793196] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.7904622209035371] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.7841661127000106] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.790462] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.784166] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.7572006572810154] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node 3f0dcda1-cb3c-4366-83e4-10f01195ce06] [Similarity score: 0.7447021219861868] The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one netwo...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.757201] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node 3f0dcda1-cb3c-4366-83e4-10f01195ce06] [Similarity score:             0.744702] The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one netwo...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.7564952435459256] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.7555620319723293] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.756495] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.755562] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.8315178088116779] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.828417466282813] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.831518] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.828417] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.7906965170708331] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node 3f0dcda1-cb3c-4366-83e4-10f01195ce06] [Similarity score: 0.7863218958654469] The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one netwo...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.790697] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node 3f0dcda1-cb3c-4366-83e4-10f01195ce06] [Similarity score:             0.786322] The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one netwo...\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_2_retrieval_eval</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.435525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  top_2_retrieval_eval  0.833333  0.666667   0.416667  0.833333  0.666667   \n",
       "\n",
       "       ndcg  \n",
       "0  0.435525  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aevaluate_labelled_rag_dataset(response_eval_dataset, query_engine, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Make predictions with the dataset\n",
    "    response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "        predictor=query_engine, batch_size=batch_size, show_progress=True\n",
    "    )\n",
    "\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RECREATE_SYNTHETIC_EVAL_DATASET = True\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 14:26:00.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mCreating synthetic response eval dataset...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcfee5c3f454600930b1b6fcde80cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: ========================\n",
      " \n",
      "I have opened the la...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: documents = SimpleDirectoryReader( \"data\" ).loa...\n",
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: ========================\n",
      " \n",
      "I have opened the la...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: documents = SimpleDirectoryReader( \"data\" ).loa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.49it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # set context for llm provider\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        documents,\n",
    "        llm=llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.7596490551728567] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.7502007361666977] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> Top 2 nodes:\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.759649] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.750201] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.8252573722447051] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.8033591511992114] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.825257] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.803359] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.7331501664733657] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score: 0.7267057390136465] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> Top 2 nodes:\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.73315] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score:             0.726706] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score: 0.7379644874141085] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score: 0.720382380142986] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node fb3fd619-9777-4fa0-9903-4ef9e4d658bb] [Similarity score:             0.737964] The email was authenticated and passed SPF and DKIM checks.\n",
      "\n",
      "In response to the last email, I wou...\n",
      "> [Node ca7e2271-1946-4225-a666-869bba529af6] [Similarity score:             0.720382] print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user mess...\n",
      "> Top 2 nodes:\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.8684781680714966] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.8671857935565598] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.868478] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.867186] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                          | 5/8 [00:04<00:01,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.7821356009124653] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.7497006863665036] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.782136] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.749701] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node f8994407-be6c-4c46-bf83-e1c660951a6e] [Similarity score: 0.7996859571107091] Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed ...\n",
      "> [Node 3f0dcda1-cb3c-4366-83e4-10f01195ce06] [Similarity score: 0.7970893817535508] The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one netwo...\n",
      "> Top 2 nodes:\n",
      "> [Node f8994407-be6c-4c46-bf83-e1c660951a6e] [Similarity score:             0.799686] Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed ...\n",
      "> [Node 3f0dcda1-cb3c-4366-83e4-10f01195ce06] [Similarity score:             0.797089] The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one netwo...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.8713891783838247] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.8671497836549598] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.871389] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.86715] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcaca0be9574cf4826c3e8a2f5dd3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: As these technologies evolve, they will continu...\n",
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: As these technologies evolve, they will continu...\n",
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: As these technologies evolve, they will continu...\n",
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: As these technologies evolve, they will continu...\n",
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n",
      "> Adding chunk: The email was authenticated and passed SPF and ...\n",
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n",
      "> Adding chunk: The email was authenticated and passed SPF and ...\n",
      "> Adding chunk: The email was authenticated and passed SPF and ...\n",
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n",
      "> Adding chunk: The email was authenticated and passed SPF and ...\n",
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: Step 2: Create the PostgresML Managed Index Fir...\n",
      "> Adding chunk: The PostgresML Managed Index is doing embedding...\n",
      "> Adding chunk: Step 2: Create the PostgresML Managed Index Fir...\n",
      "> Adding chunk: The PostgresML Managed Index is doing embedding...\n"
     ]
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = await aevaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    query_engine,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.571429\n",
       "mean_relevancy_score     1.000000\n",
       "mean_faithfulness_score  1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does MultiOn empower developers to automat...</td>\n",
       "      <td>MultiOn empowers developers to automate online...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of LlamaIndex in complementin...</td>\n",
       "      <td>LlamaIndex complements MultiOn by providing an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the MultiOn agent summarize the email...</td>\n",
       "      <td>The MultiOn agent summarizes the email chain w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the MultiOn agent facilitate the send...</td>\n",
       "      <td>The MultiOn agent facilitates the sending of t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does the integration of LlamaIndex with Po...</td>\n",
       "      <td>The integration of LlamaIndex with PostgresML ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explain the challenges associated with typical...</td>\n",
       "      <td>The challenges associated with typical RAG wor...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the PostgresML Managed Index simplify...</td>\n",
       "      <td>The PostgresML Managed Index simplifies the RA...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Explain the process of querying using the Post...</td>\n",
       "      <td>The querying process using the PostgresML Inde...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  How does MultiOn empower developers to automat...   \n",
       "1  What is the role of LlamaIndex in complementin...   \n",
       "2  How does the MultiOn agent summarize the email...   \n",
       "3  How does the MultiOn agent facilitate the send...   \n",
       "4  How does the integration of LlamaIndex with Po...   \n",
       "5  Explain the challenges associated with typical...   \n",
       "6  How does the PostgresML Managed Index simplify...   \n",
       "7  Explain the process of querying using the Post...   \n",
       "\n",
       "                                              answer  relevancy_score  \\\n",
       "0  MultiOn empowers developers to automate online...              1.0   \n",
       "1  LlamaIndex complements MultiOn by providing an...              1.0   \n",
       "2  The MultiOn agent summarizes the email chain w...              1.0   \n",
       "3  The MultiOn agent facilitates the sending of t...              1.0   \n",
       "4  The integration of LlamaIndex with PostgresML ...              1.0   \n",
       "5  The challenges associated with typical RAG wor...              1.0   \n",
       "6  The PostgresML Managed Index simplifies the RA...              1.0   \n",
       "7  The querying process using the PostgresML Inde...              1.0   \n",
       "\n",
       "   correctness_score  faithfulness_score  \n",
       "0                5.0                 1.0  \n",
       "1                4.5                 1.0  \n",
       "2                4.5                 1.0  \n",
       "3                NaN                 1.0  \n",
       "4                4.5                 1.0  \n",
       "5                4.5                 1.0  \n",
       "6                4.5                 1.0  \n",
       "7                4.5                 1.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.7102608983008766] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.6991652827223744] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.710261] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.699165] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score: 0.6852199499977049] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score: 0.6657382438174269] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> Top 2 nodes:\n",
      "> [Node 9b9f091e-632a-463e-a178-3f56b407c0aa] [Similarity score:             0.68522] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> [Node 6c5ed3b0-e702-43e5-abbc-be393ae04e19] [Similarity score:             0.665738] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                                    | 1/3 [00:01<00:02,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score: 0.6402868722827324] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score: 0.6343866618327142] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n",
      "> Top 2 nodes:\n",
      "> [Node 6f824aaa-0440-48f2-a4e1-9cd25a5bf407] [Similarity score:             0.640287] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We‚Äôre happy to announce t...\n",
      "> [Node afc179af-5233-405c-b132-86094639742a] [Similarity score:             0.634387] On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0669e74029084440abeee19f884d962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n"
     ]
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = await aevaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    query_engine,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score        1.5\n",
       "mean_relevancy_score          0.0\n",
       "mean_faithfulness_score       0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>Empty Response</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system ...</td>\n",
       "      <td>The two critical areas of RAG system performan...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate...</td>\n",
       "      <td>Empty Response</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0             What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system ...   \n",
       "2  What are the two main metrics used to evaluate...   \n",
       "\n",
       "                                              answer  relevancy_score  \\\n",
       "0                                     Empty Response              0.0   \n",
       "1  The two critical areas of RAG system performan...              0.0   \n",
       "2                                     Empty Response              0.0   \n",
       "\n",
       "   correctness_score  faithfulness_score  \n",
       "0                1.0                 0.0  \n",
       "1                2.5                 0.0  \n",
       "2                1.0                 0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
