{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f554fb-79c8-4201-a0c9-e7464118c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c852ac68-9eb6-46f7-b33c-a1a12f8af228",
   "metadata": {},
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:55:46.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mCOLLECTION='huggingface__BAAI_bge_large_en_v1_5__exp_006_semantic_chunking'\u001b[0m\n",
      "\u001b[32m2024-07-25 12:55:46.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mNODES_PERSIST_FP='data/001/exp_006_semantic_chunking/nodes.pkl'\u001b[0m\n",
      "\u001b[32m2024-07-25 12:55:46.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mRETRIEVAL_EVAL_DATASET_FP='data/001/exp_006_semantic_chunking/llamaindex_blog_retrieval_eval_dataset.json'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "TESTING = False\n",
    "DEBUG = False\n",
    "OBSERVABILITY = True\n",
    "LOG_TO_MLFLOW = True\n",
    "\n",
    "# Run metadata\n",
    "RUN_NAME = \"exp_007_semantic_chunking_full_refresh\"\n",
    "RUN_DESCRIPTION = \"\"\"\n",
    "# Try Semantic Chunking\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_006\n",
    "- Previously do not recreate the response eval dataset. This experiment will recreate them since the nodes are generated differently by a new chunker.\n",
    "\"\"\"\n",
    "\n",
    "# Vector Store Index\n",
    "RECREATE_INDEX = False\n",
    "if not RECREATE_INDEX:\n",
    "    COLLECTION = \"huggingface__BAAI_bge_large_en_v1_5__exp_006_semantic_chunking\"\n",
    "    NODES_PERSIST_FP = 'data/001/exp_006_semantic_chunking/nodes.pkl'\n",
    "    logger.info(f\"{COLLECTION=}\")\n",
    "    logger.info(f\"{NODES_PERSIST_FP=}\")\n",
    "\n",
    "# Retrieval eval\n",
    "RECREATE_RETRIEVAL_EVAL_DATASET = False\n",
    "# Currently can not reuse retrieval_eval_dataset because the retrieval evaluation is based on ids\n",
    "if not RECREATE_RETRIEVAL_EVAL_DATASET:\n",
    "    RETRIEVAL_EVAL_DATASET_FP = f\"data/001/exp_006_semantic_chunking/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "    logger.info(f\"{RETRIEVAL_EVAL_DATASET_FP=}\")\n",
    "\n",
    "# Response eval\n",
    "RECREATE_SYNTHETIC_EVAL_DATASET = True\n",
    "if not RECREATE_SYNTHETIC_EVAL_DATASET:\n",
    "    RESPONSE_EVAL_DATASET_FP = f\"dataset at data/001/exp_007_semantic_chunking_full_refresh/llamaindex_blog_response_eval_dataset.json\"\n",
    "    logger.info(f\"{RESPONSE_EVAL_DATASET_FP=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ae48fa-a9cb-4f32-a91e-6a63c9b53506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721886852.718689 3152400 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "if OBSERVABILITY:\n",
    "    import phoenix as px\n",
    "    px.launch_app()\n",
    "    import llama_index.core\n",
    "    llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc26a2-7df5-4963-855d-92ec99c958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CACHE_DP = f'data/001/{RUN_NAME}'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"NOTEBOOK_CACHE_DP\", NOTEBOOK_CACHE_DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs-v2.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'content': \"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\",\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['AI', 'Evaluation', 'Rag'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       "  'content': \"Greetings, LlamaIndex community! ü¶ô Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more. ü§©\\xa0 The highlights: DeepLearningAI Course:  JavaScript RAG Web Apps with\\xa0LlamaIndex collaborative course with DeepLearningAI.  Course ,  Tweet . RAFTDatasetPack LlamaPack : Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents.  LlamaPack ,  Tweet . Memory Efficiency with Cohere Embeddings:  Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations.  Notebook ,  Tweet . Python Docs Makeover:  Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details.  API Ref ,  Tweet ,  Docs ‚ú® Feature Releases and Enhancements: We introduced RAFT - Retrieval Augmented Fine Tuning, a method from  Tianjun Zhang \\xa0and  Shishir Patil \\xa0to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation.  LlamaPack ,  Tweet . We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.  Course ,  Tweet . We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG.  Notebook ,  Tweet We launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation.  API Ref ,  Tweet ,  Docs üé•\\xa0Demos: RestAI , a project by  Pedro Dias  is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI. Ragdoll  and  Ragdoll Studio  by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI. üó∫Ô∏è Guides: Guide  to Designing RAG Systems by  Micha≈Ç Oleszak  for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation. ‚úçÔ∏è Tutorials: Sujit Patil   tutorial  on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment. Florian June's  tutorial  on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch. Laurie's  video tutorial  on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries. Akriti‚Äôs   tutorial  on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant. Marco Bertelli's  tutorial  on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience. Sudarshan Koirala‚Äôs   tutorial  on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking. üé•\\xa0 Webinars: Register for a webinar  with  Tianjun Zhang \\xa0and  Shishir Patil \\xa0on how to do retrieval-augmented fine-tuning (RAFT). Webinar  with  Daniel  on  CodeGPT  - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components. Vectara‚Äôs   Panel Discussion  on 'Why RAG will Never Die?‚Äô.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Apr 2, 2024',\n",
       "  'tags': ['LLM'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'},\n",
       " {'title': 'LlamaIndex Newsletter 2024-03-19',\n",
       "  'content': \"Greetings, LlamaIndex enthusiasts! ü¶ô Welcome to another exciting weekly update from the world of LlamaVerse! We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our  launch post  for all the details! ü§©\\xa0 The highlights: New observability with Instrumentation:  Enhanced developer workflow with a new Instrumentation module for improved observability.  Docs ,  Tweet . LlamaParse accepts natural language parsing instructions : Easily extract math snippets from PDFs into LaTeX with LlamaParse.  Blogpost ,  Tweet . Financial Data Parsing:  Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis.  Notebook ,  Tweet . ‚ú® Feature Releases and Enhancements: We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application.  Docs ,  Tweet . We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline.  Blogpost ,  Tweet . We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data.  Notebook ,  Tweet . We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code.  Blogpost ,  Docs ,  Tweet . Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts.  LlamaPack ,  Tweet üé•\\xa0Demos: Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency.  Blogpost ,  Code ,  Tweet . üó∫Ô∏è Guides: Guide  to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique. ‚úçÔ∏è Tutorials: Thomas Reid ‚Äôs  tutorial  on using LlamaParse can help properly extract text from a Tesla quarterly filings. Sudarshan Koirala   video tutorial  on RAG with LlamaParse, Qdrant, and Groq. Kyosuke Morita  tutorial  showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex. Cobus Greyling   tutorial  on Agentic RAG: Context-Augmented OpenAI Agents. Roey Ben Chaim ‚Äôs  tutorial  on PII Detector: hacking privacy in RAG. üé•\\xa0 Webinars: Webinar  with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT üìÖ\\xa0Events: We are hosting a RAG  meetup  in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': ['LlamaParse', 'AI', 'LLM', 'Newsletter'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-03-19'},\n",
       " {'title': 'One-click Open Source RAG Observability with Langfuse',\n",
       "  'content': 'This is a guest post from the team at Langfuse There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once. Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your  production  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point? Thus, we started working on  Langfuse.com  ( GitHub ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either  self-host  Langfuse or use the  cloud instance  maintained by us. We are thrilled to announce our new integration with LlamaIndex today. This feature was  highly requested  by our community and aligns with our project\\'s focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase! The challenge We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine. from  llama_index.core  import  SimpleDirectoryReader\\n from  llama_index.core  import  VectorStoreIndex\\n\\ndocuments = SimpleDirectoryReader( \"./data\" ).load_data()\\n\\nindex = VectorStoreIndex.from_documents(documents)\\n\\nchat_engine = index.as_chat_engine()\\n\\n print (chat_engine.chat( \"What problems can I solve with RAG?\" ))\\n print (chat_engine.chat( \"How do I optimize my RAG application?\" )) In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine. This is awesome to get started, but we quickly run into questions like: ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù ‚ÄúHow is chat memory managed?‚Äù ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù One-click OSS observability to the rescue We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager. Preparation Install the community package (pip install llama-index-callbacks-langfuse) Copy/paste the environment variables from the Langfuse project settings to your Python project: \\'LANGFUSE_SECRET_KEY\\', \\'LANGFUSE_PUBLIC_KEY\\' and \\'LANGFUSE_HOST\\' Now, you only need to set the global langfuse handler: from  llama_index.core  import  set_global_handler\\n\\nset_global_handler( \"langfuse\" ) And voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns. Group multiple chat threads into a session Working with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is  session tracking  for conversational applications to see the traces in context of a full message thread. To activate it, just add an id that identifies the session as a trace param before calling the chat_engine. from  llama_index.core  import  global_handler\\n\\nglobal_handler.set_trace_params(\\n  session_id= \"your-session-id\" \\n)\\n\\nchat_engine.chat( \"What did he do growing up?\" )\\nchat_engine.chat( \"What did he do at USC?\" )\\nchat_engine.chat( \"How old is he?\" ) Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing: Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces. Trace more complex applications and use other Langfuse features for prompt management and evaluation This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable. We have also built additional features to version control and collaborate on prompts (langfuse  prompt management ), track  experiments , and  evaluate  production traces. For RAG specifically, we collaborated with the RAGAS team and it‚Äôs easy to run their popular eval suite on traces captured with Langfuse (see  cookbook ). Get started The easiest way to get started is to follow the  cookbook  and check out the  docs . Feedback? Ping us We‚Äôd love to hear any feedback. Come join us on our  community discord  or add your thoughts to this  GitHub thread .',\n",
       "  'author': 'Langfuse',\n",
       "  'date': 'Mar 18, 2024',\n",
       "  'tags': ['LLM', 'Observability'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse'},\n",
       " {'title': 'Retrieving Privacy-Safe Documents Over A Network',\n",
       "  'content': 'In a  recent blog post , we introduced our  llama-index-networks  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores‚Äîthat one may not otherwise have access to‚Äîmeans more accurate responses to an even wider breadth of queries. A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we‚Äôll be relying on some recent developments in the area of Privacy-Enhancing Techniques. The story of Alex, Bob and Beth continues To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply. We showed then how such data a collaboration could be permitted through  llama-index-networks  by taking the following steps: Bob and Beth both build their respective QueryEngine‚Äôs (RAG in llama-index lingo) Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets. Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Sidebar: differential privacy primer In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual‚Äôs inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm. By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion). A light introduction to differential privacy. A light introduction to differential privacy. Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth‚Äôs data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled ‚ÄúPrivacy-preserving in-context learning with differentially private few-shot generation‚Äù by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network! There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration. Bob and Beth create privacy-safe synthetic copies of their private datasets Bob and Beth both build their respective QueryEngine‚Äôs over their synthetic datasets Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs Creating differentially private synthetic copies of a private dataset Fortunately, for step 0., we can make use of the  DiffPrivateSimpleDataset  pack. from  llama_index.core.llama_datasets.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n\\n # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,   # OpenAI only allows for top 5 next token \\n)                     # as opposed to entire vocabulary \\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nbeth_private_dataset: LabelledSimpleDataset = ...  # a dataset that contains \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # examples with two attributes \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t # `text` and `reference_label` \\n\\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,     # params for preparing required prompts \\n    simple_dataset=simple_dataset,   # to generate the synthetic examples  \\n)\\n\\nbeth_synthetic_dataset =  await  beth_synthetic_generator.arun(\\n\\t\\tsize= 3 ,   # number of synthetic observations to create \\n\\t\\tsigma= 0.5    # param that determines the level of privacy \\n) With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It‚Äôs worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.) Example: Symptom2Disease In this section of the blog post, we go over an actual example application of the privacy-safe networks over the  Symptom2Disease  dataset. This dataset consists of 1,200 examples each containing a ‚Äúsymptoms‚Äù description as well as the associated ‚Äúdisease‚Äù label ‚Äî the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network. Generate privacy-safe synthetic observations of Symptom2Disease We use the training subset and apply the  DiffPrivateSimpleDatasetPack  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a  LabelledSimpleDataset  object. import  pandas  as  pd\\n from  sklearn.model_selection  import  train_test_split\\n from  llama_index.core.llama_dataset.simple  import  (\\n    LabelledSimpleDataExample,\\n    LabelledSimpleDataset,\\n)\\n from  llama_index.core.llama_dataset.base  import  CreatedBy, CreatedByType\\n\\n # load the Symptom2Disease.csv file \\ndf = pd.read_csv( \"Symptom2Disease.csv\" )\\ntrain, test = train_test_split(df, test_size= 0.2 )\\n\\n # create a LabelledSimpleDataset (which is what the pack works with) \\nexamples = []\\n for  index, row  in  df.iterrows():\\n    example = LabelledSimpleDataExample(\\n        reference_label=row[ \"label\" ],\\n        text=row[ \"text\" ],\\n        text_by=CreatedBy( type =CreatedByType.HUMAN),\\n    )\\n    examples.append(example)\\n\\nsimple_dataset = LabelledSimpleDataset(examples=examples) Now we can use the llama-pack to create our synthetic observations. import  llama_index.core.instrumentation  as  instrument\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\\n from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\\n from  llama_index.llms.openai  import  OpenAI\\n import  tiktoken\\n from  .event_handler  import  DiffPrivacyEventHandler\\n import  asyncio\\n import  os\\n\\nNUM_SPLITS =  3 \\nT_MAX =  150 \\n\\nllm = OpenAI(\\n    model= \"gpt-3.5-turbo-instruct\" ,\\n    max_tokens= 1 ,\\n    logprobs= True ,\\n    top_logprobs= 5 ,\\n)\\ntokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\\n\\nprompt_bundle = PromptBundle(\\n    instruction=(\\n         \"You are a patient experiencing symptoms of a specific disease. \" \\n         \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\\\n\" \\n         \"Start your answer directly after \\'Symptoms: \\'. Begin your answer with [RESULT].\\\\n\" \\n    ),\\n    label_heading= \"Disease\" ,\\n    text_heading= \"Symptoms\" ,\\n)\\n\\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\\n    llm=llm,\\n    tokenizer=tokenizer,\\n    prompt_bundle=prompt_bundle,\\n    simple_dataset=simple_dataset,\\n)\\n\\nsynthetic_dataset =  await  dp_simple_dataset_pack.arun(\\n    sizes= 3 ,\\n    t_max=T_MAX,\\n    sigma= 1.5 ,\\n    num_splits=NUM_SPLITS,\\n    num_samples_per_split= 8 ,   # number of private observations to create a \\n)                              # synthetic obsevation \\nsynthetic_dataset.save_json( \"synthetic_dataset.json\" ) Create a network with two contributors Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well. Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we‚Äôre using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user). Step 1:  Contributor‚Äôs build their Retriever over their synthetic datasets. import  os\\n from  llama_index.core  import  VectorStoreIndex\\n from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\\n from  llama_index.core.schema  import  TextNode\\n\\n\\n # load the synthetic dataset \\nsynthetic_dataset = LabelledSimpleDataset.from_json(\\n     \"./data/contributor1_synthetic_dataset.json\" \\n)\\n\\n\\nnodes = [\\n    TextNode(text=el.text, metadata={ \"reference_label\" : el.reference_label})\\n     for  el  in  synthetic_dataset[:]\\n]\\n\\nindex = VectorStoreIndex(nodes=nodes)\\nsimilarity_top_k =  int (os.environ.get( \"SIMILARITY_TOP_K\" ))\\nretriever = index.as_retriever(similarity_top_k=similarity_top_k) Step 2:  Contributor‚Äôs expose their Retrievers behind a ContributorRetrieverService from  llama_index.networks.contributor.retriever.service  import  (\\n    ContributorRetrieverService,\\n    ContributorRetrieverServiceSettings,\\n)\\n\\nsettings = ContributorRetrieverServiceSettings()  # loads from .env file \\nservice = ContributorRetrieverService(config=settings, retriever=retriever)\\napp = service.app Step 3:  Define the NetworkRetriever that connects to the ContributorRetrieverServices from  llama_index.networks.network.retriever  import  NetworkRetriever\\n from  llama_index.networks.contributor.retriever  import  ContributorRetrieverClient\\n from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\\n\\n # ContributorRetrieverClient\\'s connect to the ContributorRetrieverService \\ncontributors = [\\n    ContributorRetrieverClient.from_config_file(\\n        env_file= f\"./client-env-files/.env.contributor_ {ix} .client\" \\n    )\\n     for  ix  in   range ( 1 ,  3 )\\n]\\nreranker = CohereRerank(top_n= 5 )\\nnetwork_retriever = NetworkRetriever(\\n    contributors=contributors, node_postprocessors=[reranker]\\n) With the  NetworkRetriever  established, we can retrieve synthetic observations from the two contributors data against a query. related_records = network_retriever.aretrieve( \"Vomitting and nausea\" )\\n print (related_records)  # contain symptoms/disease records that are similar to \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  # to the queried symptoms. Evaluating the  NetworkRetriever To evaluate the efficacy of the  NetworkRetriever  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. hit rate:  a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. mean reciprocal rank:  similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. In addition to evaluating the  NetworkRetriever  we consider the two baselines that represent Retrieving only over the individual Contributor‚Äôs synthetic datasets. Retriever evaluations, with sigma equal to 1.5. In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever‚Äôs in the test set. This shouldn‚Äôt be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor‚Äôs synthetic observations‚Äîthis is the point after all of a network! Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated). Finally, this level of privacy can be controlled via the noise parameter  sigma . In the example above we used a  sigma  of 1.5, which for this dataset amounts to an  epsilon  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are  generally considered to be quite private .) Below, we share the evaluations that result from using a  sigma  of 0.5, which amounts to an  epsilon  of 15.9‚Äîhigher values of  epsilon  or privacy-loss means less privacy. # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon \\nepsilon = dp_simple_dataset_pack.sigma_to_eps(\\n\\t\\tsigma= 0.5 ,\\n\\t\\tmechanism= \"gaussian\" ,\\n\\t\\tsize= 3 * 24 ,\\n\\t\\tmax_token_cnt= 150    # number of max tokens to generate per synthetic example \\n) Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5. So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening. # synthetic example epsilon = 1.3 \\n{\\n     \"reference_label\" :  \"Psoriasis\" ,\\n     \"text\" :  \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\" ,\\n     \"text_by\" : {\\n         \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n         \"type\" :  \"ai\" \\n    }\\n},\\n\\n # synthetic example epsilon = 15.9 \\n{\\n   \"reference_label\" :  \"Migraine\" ,\\n   \"text\" :  \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\" ,\\n   \"text_by\" : {\\n     \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\\n     \"type\" :  \"ai\" \\n  }\\n}, We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM. In summary We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. Learn more! To delve deeper into the materials of this blog post, we share a few links below: Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( link ) Demo notebooks for the  DiffPrivateSimpleDataset  ( link ) The source code for creating the synthetic Symptom2Disease observations using the  DiffPrivateSimpleDataset  ( link )',\n",
       "  'author': 'Andrei',\n",
       "  'date': 'Mar 20, 2024',\n",
       "  'tags': ['Privacy', 'llama-index-networks'],\n",
       "  'url': 'https://www.llamaindex.ai/blog/retrieving-privacy-safe-documents-over-a-network'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:13.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=160\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_input_data\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags']),\n",
    "        'url': record['url']\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='24a0c6e4-def2-4cd2-9600-ba71e14a9804', embedding=None, metadata={'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations\\nThis is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LlamaIndex Newsletter 2024-04-02',\n",
       " 'author': 'LlamaIndex',\n",
       " 'date': 'Apr 2, 2024',\n",
       " 'tags': 'LLM',\n",
       " 'url': 'https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-04-02'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ee5bd7-3f29-4b0e-b2e3-19d69ed7adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_documents\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_OPTION = 'openai'\n",
    "# LLM_OPTION = 'ollama'\n",
    "LLM_OPTION = 'togetherai'\n",
    "\n",
    "# LLM_MODEL_NAME = 'llama3'\n",
    "# LLM_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "LLM_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct-Lite'\n",
    "\n",
    "# EMBED_OPTION = 'openai'\n",
    "# EMBED_OPTION = 'togetherai'\n",
    "# EMBED_OPTION = 'ollama'\n",
    "EMBED_OPTION = 'huggingface'\n",
    "\n",
    "# EMBED_MODEL_NAME = 'llama3'\n",
    "# EMBED_MODEL_NAME = 'togethercomputer/m2-bert-80M-2k-retrieval'\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)\n",
    "    mlflow.log_param(\"LLM_MODEL_NAME\", LLM_MODEL_NAME)\n",
    "    mlflow.log_param(\"EMBED_OPTION\", EMBED_OPTION)\n",
    "    mlflow.log_param(\"EMBED_MODEL_NAME\", EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831156c6-08f7-4ed6-9063-6c7d7f00b312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:20.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mLLM:\n",
      "TogetherLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x76f2c01bd750>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x76f2cc385d00>, completion_to_prompt=<function default_completion_to_prompt at 0x76f2cc2200e0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Meta-Llama-3-8B-Instruct-Lite', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='3cf613093b6eb9b479c341126dc8d3761c67f9340d0a4a8e1fdc62ed41b58126', api_base='https://api.together.xyz/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=False, tokenizer=None)\u001b[0m\n",
      "\u001b[32m2024-07-25 12:54:20.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEmbed model:\n",
      "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x76f2a8929250>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# LLM options\n",
    "if LLM_OPTION == 'ollama':\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    llm = Ollama(base_url=base_url, model=LLM_MODEL_NAME, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "elif LLM_OPTION == 'openai':\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    llm = OpenAI(model=LLM_MODEL_NAME)\n",
    "elif LLM_OPTION == 'togetherai':\n",
    "    from llama_index.llms.together import TogetherLLM\n",
    "    llm = TogetherLLM(model=LLM_MODEL_NAME)\n",
    "\n",
    "# Embed options\n",
    "if EMBED_OPTION == 'huggingface':\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME\n",
    "    )\n",
    "elif EMBED_OPTION == 'openai':\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embed_model = OpenAIEmbedding()\n",
    "elif EMBED_OPTION == 'togetherai':\n",
    "    from llama_index.embeddings.together import TogetherEmbedding\n",
    "    embed_model = TogetherEmbedding(EMBED_MODEL_NAME)\n",
    "elif EMBED_OPTION == 'ollama':\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    embed_model = OllamaEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "\n",
    "logger.info(f\"LLM:\\n{repr(llm)}\")\n",
    "logger.info(f\"Embed model:\\n{repr(embed_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db5ed88-2448-4308-97e0-73bc9451c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:20.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1membed_model_dim=1024\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embed_model_dim = len(embed_model.get_text_embedding('sample text to find embedding dimensions'))\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "logger.info(f\"{embed_model_dim=}\")\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"embedding_model_dim\", embed_model_dim)\n",
    "    mlflow.log_param(\"LLM_MODEL\", repr(llm))\n",
    "    mlflow.log_param(\"EMBEDDING_MODEL\", repr(embed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10ea7756-4ee7-4731-a4bb-aec6b63d960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    # Create a translation table that maps each punctuation character to an underscore\n",
    "    translator = str.maketrans(string.punctuation, '_' * len(string.punctuation))\n",
    "    # Translate the text using the translation table\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f12cfb-6275-4edb-b2c9-2bbfdee9f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:20.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mCOLLECTION='huggingface__BAAI_bge_large_en_v1_5__exp_006_semantic_chunking'\u001b[0m\n",
      "\u001b[32m2024-07-25 12:54:20.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mNODES_PERSIST_FP='data/001/exp_006_semantic_chunking/nodes.pkl'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_INDEX:\n",
    "    collection_raw_name = f\"{EMBED_OPTION}__{EMBED_MODEL_NAME}__{RUN_NAME}\"\n",
    "    COLLECTION = substitute_punctuation(collection_raw_name)\n",
    "    NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "logger.info(f\"{COLLECTION=}\")\n",
    "logger.info(f\"{NODES_PERSIST_FP=}\")\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(f\"COLLECTION\", COLLECTION)\n",
    "    mlflow.log_param(f\"NODES_PERSIST_FP\", NODES_PERSIST_FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a05d2-4b9e-4295-9dc5-e1b3bdf22be0",
   "metadata": {},
   "source": [
    "## Qdrant as VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0be2dff6-23da-448b-866b-d791bb54812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec5d7b9-7ad3-427f-a423-0fee1f7f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:21.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUse existing Qdrant collection\u001b[0m\n",
      "WARNI [llama_index.vector_stores.qdrant.base] Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "collection_exists = qdrantdb.collection_exists(COLLECTION)\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing Qdrant collection...\")\n",
    "        qdrantdb.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "    logger.info(f\"Creating new Qdrant collection...\")\n",
    "    qdrantdb.create_collection(\n",
    "        COLLECTION,\n",
    "        vectors_config=VectorParams(size=embed_model_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Use existing Qdrant collection\")\n",
    "db_collection = qdrantdb.get_collection(COLLECTION)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=COLLECTION,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6e0c6f0-92a6-430b-a509-682e5884b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "\n",
    "# CHUNKER = SentenceSplitter\n",
    "# CHUNKER_CONFIG = {\n",
    "#     \"chunk_size\": 512,\n",
    "#     \"chunk_overlap\": 10\n",
    "# }\n",
    "# Observation: When using SemanticSplitterNodeParser it could take about 5 mins\n",
    "# eating GPU to do something (assuming prep work) and after that would run the ingestion pipeline \n",
    "CHUNKER = SemanticSplitterNodeParser\n",
    "CHUNKER_CONFIG = {\n",
    "    \"buffer_size\": 1,\n",
    "    \"breakpoint_percentile_threshold\": 95,\n",
    "    \"embed_model\": embed_model\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        logged_v = v\n",
    "        if not (isinstance(v, int) or isinstance(v, str)):\n",
    "            logged_v = repr(v)\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", logged_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e60fed4-7ea5-463c-bd19-d5b6da2cb04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:21.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading index from existing DB...\u001b[0m\n",
      "\u001b[32m2024-07-25 12:54:21.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mLoading cached `nodes` at data/001/exp_006_semantic_chunking/nodes.pkl...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        logger.info(f\"Loading cached `nodes` at {NODES_PERSIST_FP}...\")\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            CHUNKER(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = None\n",
    "    # Currently setting num_workers leads to error `AttributeError: 'HuggingFaceEmbedding' object has no attribute '_model'`\n",
    "    # num_workers = os.cpu_count() - 1\n",
    "    # logger.info(f\"Running Ingestion Pipeline with {num_workers=}...\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa39ef55-4f0f-46c0-8777-5b11955978cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:54:22.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexing 160 into VectorStoreIndex took 1s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_nodes\", len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40da3529-cf6a-41ce-853e-1526faf2e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.append_reference.custom_query_engine import ManualAppendReferenceQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 5\n",
    "RERANK_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "# APPEND_REF_MODE = 'response_synthesizer'\n",
    "APPEND_REF_MODE = 'query_engine'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    mlflow.log_param(\"RERANK_TOP_K\", RERANK_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "reranker = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-large\", top_n=RERANK_TOP_K)\n",
    "node_postprocessors.append(reranker)\n",
    "\n",
    "if APPEND_REF_MODE == 'response_synthesizer':\n",
    "    response_synthesizer = ManualAppendReferenceSynthesizer(verbose=0)\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "elif APPEND_REF_MODE == 'query_engine':\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = ManualAppendReferenceQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "else:\n",
    "    response_synthesizer = get_response_synthesizer()\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=node_postprocessors,\n",
    "    )\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"reranker\", repr(reranker))\n",
    "    mlflow.log_param(\"response_synthesizer\", repr(response_synthesizer))\n",
    "    mlflow.log_param(\"query_engine\", repr(query_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63bb5789-dde7-465c-86c2-94aadbf5b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To address points of failures in RAG pipeline, you can leverage the QueryPipeline's declarative orchestration abstraction. This allows you to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity, making it easier to identify and troubleshoot potential issues. By using the QueryPipeline, you can reduce boilerplate code and increase readability, which can help you pinpoint and resolve problems more efficiently. Additionally, the QueryPipeline's end-to-end observability feature enables you to get callback integration across the entire pipeline, even for arbitrarily nested DAGs, making it easier to debug and identify potential failures.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [Introducing Query Pipelines](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537)\n",
       "- [RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex](https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c1d767a4-bd20-4c9d-94e0-638fb3132515<br>**Similarity:** -4.749475955963135<br>**Text:** Check out our comprehensive  introduction guide , as well as our  docs page  for more details. Ex...<br>**Metadata:** {'title': 'Introducing Query Pipelines', 'author': 'Jerry Liu', 'date': 'Jan 8, 2024', 'tags': 'Llamaindex, Retrieval Augmented, LLM, AI', 'url': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537', 'document_title': 'Based on the analysis, I would suggest the following comprehensive title:\\n\\n\"Introducing Query Pipelines: A Declarative Orchestration Abstraction for Advanced RAG Pipelines - Query Pipeline and its Components: Running, Defining Custom Queries, and Supported Modules - Conclusion and Additional Resources for Query Pipelines\"\\n\\nThis title captures the main themes and entities found in the context, including the introduction of Query Pipelines, its components, and the conclusion and additional resources related to query pipelines. It provides a clear and concise summary of the content, making it easy to understand the topic and its scope.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 793c1e5e-6147-4514-9e43-8d35ab064df1<br>**Similarity:** -5.54605770111084<br>**Text:** RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...<br>**Metadata:** {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex', 'author': 'Harshad Suryawanshi', 'date': 'Feb 2, 2024', 'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation', 'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089', 'document_title': 'Based on the context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool Powered by LlamaIndex: A Collaborative Platform for Developers and AI Enthusiasts to Accelerate Idea-to-Implementation Pipelines\"\\n\\nThis title captures the main entities and themes present in the context, including RAGArch, LlamaIndex, no-code platform, collaboration, developers, AI enthusiasts, and idea-to-implementation pipelines. It provides a clear and concise summary of the platform\\'s purpose and functionality, making it an effective title for the document.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'c1d767a4-bd20-4c9d-94e0-638fb3132515': {'title': 'Introducing Query Pipelines',\n",
       "  'author': 'Jerry Liu',\n",
       "  'date': 'Jan 8, 2024',\n",
       "  'tags': 'Llamaindex, Retrieval Augmented, LLM, AI',\n",
       "  'url': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537',\n",
       "  'document_title': 'Based on the analysis, I would suggest the following comprehensive title:\\n\\n\"Introducing Query Pipelines: A Declarative Orchestration Abstraction for Advanced RAG Pipelines - Query Pipeline and its Components: Running, Defining Custom Queries, and Supported Modules - Conclusion and Additional Resources for Query Pipelines\"\\n\\nThis title captures the main themes and entities found in the context, including the introduction of Query Pipelines, its components, and the conclusion and additional resources related to query pipelines. It provides a clear and concise summary of the content, making it easy to understand the topic and its scope.'},\n",
       " '793c1e5e-6147-4514-9e43-8d35ab064df1': {'title': 'RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex',\n",
       "  'author': 'Harshad Suryawanshi',\n",
       "  'date': 'Feb 2, 2024',\n",
       "  'tags': 'Rag, No Code, Llamaindex, OpenAI, Code Generation',\n",
       "  'url': 'https://www.llamaindex.ai/blog/ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089',\n",
       "  'document_title': 'Based on the context, I would suggest the following comprehensive title:\\n\\n\"RAGArch: A No-Code RAG Pipeline Configuration and One-Click Code Generation Tool Powered by LlamaIndex: A Collaborative Platform for Developers and AI Enthusiasts to Accelerate Idea-to-Implementation Pipelines\"\\n\\nThis title captures the main entities and themes present in the context, including RAGArch, LlamaIndex, no-code platform, collaboration, developers, AI enthusiasts, and idea-to-implementation pipelines. It provides a clear and concise summary of the platform\\'s purpose and functionality, making it an effective title for the document.'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can we address points of failures in RAG pipeline?\"\n",
    "response = query_engine.query(question)\n",
    "display_response(response, show_source=True, show_metadata=True, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = min(len(nodes), RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "RETRIEVAL_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "# RETRIEVAL_EVAL_LLM_MODEL = 'gpt-4'\n",
    "RETRIEVAL_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:55:59.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mLoading retrieval_eval_nodes from data/001/exp_006_semantic_chunking/llamaindex_blog_retrieval_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes\n",
    "else:\n",
    "    logger.info(f\"Loading retrieval_eval_nodes from {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    with open(RETRIEVAL_EVAL_DATASET_FP, 'r') as f:\n",
    "        retrieval_eval_nodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:56:00.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading existing synthetic retrieval eval dataset at data/001/exp_006_semantic_chunking/llamaindex_blog_retrieval_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "QUESTION_GEN_QUERY = \"\"\"\n",
    "You are a Retriever Evaluator. Your task is to generate {num_questions_per_chunk} questions to assess the accuracy/relevancy of an information retrieval system.\n",
    "The information retrieval system would then be asked your generated question and assessed on how well it can look up and return the correct context.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- Restrict the generated questions to the context information provided.\n",
    "- Do not mention anything about the context in the generated questions.\n",
    "- The generated questions should be diverse in nature and in difficulty across the documents.\n",
    "- When being asked the generated question, a human with no prior knowledge can still answer perfectly given the input context.\n",
    "\"\"\"\n",
    "QA_GENERATE_PROMPT_TMPL = f\"\"\"\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{{context_str}}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "{QUESTION_GEN_QUERY}\n",
    "\"\"\"\n",
    "\n",
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    retrieval_eval_llm = OpenAI(model=RETRIEVAL_EVAL_LLM_MODEL, **RETRIEVAL_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes,\n",
    "        llm=retrieval_eval_llm,\n",
    "        num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK,\n",
    "        qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL\n",
    "    )\n",
    "    logger.info(f\"Persisting synthetic retrieval eval dataset to {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef47844c-5787-458e-9860-4c1f6a0f1935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'492819bb-c1ee-43da-a90c-0f89d14442e0': 'What new features have been introduced in LlamaCloud, and which platforms have been integrated with it?',\n",
       " 'd1542279-9c49-4576-83fb-65b9c706251f': 'How has Scaleport AI utilized LlamaCloud and LlamaIndex to enhance their development speed and sales across different industries?',\n",
       " 'a4f2465b-5c0a-4373-bc3a-736b61269f99': 'How can the alignment and safety of LLMs and LMMs be evaluated?',\n",
       " '652a5aeb-c95f-4199-a9bf-354c7b397753': 'What are some important dimensions to consider when evaluating LLMs and LMMs, aside from knowledge and reasoning capabilities?',\n",
       " '8d6701d8-2439-4d22-b319-ff9aaa145949': 'Who conducted a workshop at the LlamaIndex + Replit Pune Generative AI meetup?',\n",
       " '42a0603b-004d-40ae-ad08-8e4ac7ba3ad6': 'Which individuals were involved in the webinar on LLM Challenges in Production?',\n",
       " '543c9bd0-1725-48c0-9375-7ab9cc1c87f4': 'How does the MultiOn agent manage the action of sending an email through the web browser?',\n",
       " 'b7a74a1a-f305-4d83-a72a-5fb28e232e0a': 'Where can developers find information about the integration of MultiOn and LlamaIndex for automating online tasks?',\n",
       " 'f9d5c5e8-356d-4643-aefc-7d59c44d958c': 'What is the cost for evaluation of the Prometheus Model and GPT-4?',\n",
       " 'cc56a45f-68a4-453a-a2be-78158d2e191d': 'How does the Prometheus model differ from GPT-4 in terms of feedback accuracy and penalties for generated answers lacking certain facts?',\n",
       " '96749351-0a8e-492f-9c96-b3bc05945d8b': 'How does the system ensure that each user only receives answers from the documents they have indexed, and not from documents indexed by other users?',\n",
       " 'fbbda37a-58dd-4ca3-a585-a4ac0046a9cb': 'What role does metadata play in managing Multi-Tenancy in the RAG system, and how is it used during the query phase to filter and retrieve user-specific information?',\n",
       " 'd8be41d5-4ef8-44a2-a826-25a3d58a51c4': 'What is the significance of the alpaca as a personal mascot?',\n",
       " 'ac898cb6-d4ed-431d-a2db-3f2e3a7e114f': 'How are alpacas and llamas described in relation to each other?',\n",
       " '2a08e2a7-2863-4f46-9397-6e5adc9ca2d9': 'How does the RetrieverEvaluator module enhance retrieval evaluations and what specific functionalities does it offer?',\n",
       " 'cb8ed3ec-f713-4671-810b-8bf0a344bab0': 'Can you explain the purpose and capabilities of the SemanticSimilarityEvaluator module introduced for LLM/RAG outputs?',\n",
       " '7c4dc2b0-8704-47a9-aa50-eeecae8cfba5': 'How does the Launch of Seven Advanced Retrieval LlamaPacks simplify the building of advanced RAG systems?',\n",
       " '9c83b15a-d77e-4fff-9b5b-f83a5d43d4df': 'What enhancement was achieved in structured metadata extraction to boost RAG performance?',\n",
       " '06ef7c97-6d8d-4957-99cb-458211c4e365': \"What is the purpose of the reasoning loop in the agent's decision-making process?\",\n",
       " 'bf9177b9-5e00-4c3c-9224-f342497e5d38': 'How does the ReAct agent differ from the OpenAI Function agent in terms of functionality and usage?'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71cd5482-b261-42a0-b822-af1f1303bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    eval_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        eval_dict = {\n",
    "            \"query\": eval_result.query,\n",
    "            \"expected_ids\": eval_result.expected_ids,\n",
    "            \"retrieved_texts\": eval_result.retrieved_texts,\n",
    "            **metric_dict\n",
    "        }\n",
    "        eval_dicts.append(eval_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(eval_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df, full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_5_retrieval_eval</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>0.204842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate     mrr  precision  recall      ap      ndcg\n",
       "0  top_5_retrieval_eval       0.7  0.5725       0.14     0.7  0.5725  0.204842"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df, retrieval_eval_results_full_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc7a1e36-ff9e-4dac-a383-d77f55e77e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_ids</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>[503ebcc2-3c47-4215-a6c8-d5cd3a52ac00]</td>\n",
       "      <td>[Introducing LlamaCloud and LlamaParse\\nToday ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How has Scaleport AI utilized LlamaCloud and L...</td>\n",
       "      <td>[503ebcc2-3c47-4215-a6c8-d5cd3a52ac00]</td>\n",
       "      <td>[Case Study: How Scaleport.ai Accelerated Deve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.146068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can the alignment and safety of LLMs and L...</td>\n",
       "      <td>[e0877b54-7ce0-4778-bef7-2051f8ef9ebf]</td>\n",
       "      <td>[Though studies have shown that strong LLMs ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some important dimensions to consider...</td>\n",
       "      <td>[e0877b54-7ce0-4778-bef7-2051f8ef9ebf]</td>\n",
       "      <td>[Though studies have shown that strong LLMs ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who conducted a workshop at the LlamaIndex + R...</td>\n",
       "      <td>[7522899d-0ec2-40f0-b367-c8e3342fa283]</td>\n",
       "      <td>[Ravi Theja  conducted a  workshop  at LlamaIn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which individuals were involved in the webinar...</td>\n",
       "      <td>[7522899d-0ec2-40f0-b367-c8e3342fa283]</td>\n",
       "      <td>[Raymond \\n P.S. ‚Äî  This article is titled Par...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the MultiOn agent manage the action o...</td>\n",
       "      <td>[8aac6c35-0afc-41b8-9412-d7778d8b2cb9]</td>\n",
       "      <td>[3. Send Email through MultiOn : Finally, the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Where can developers find information about th...</td>\n",
       "      <td>[8aac6c35-0afc-41b8-9412-d7778d8b2cb9]</td>\n",
       "      <td>[Automate online tasks with MultiOn and LlamaI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.213986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the cost for evaluation of the Prometh...</td>\n",
       "      <td>[575a1ebd-5b35-4c14-a826-7ef4ec3b4654]</td>\n",
       "      <td>[This is in line with the information provided...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the Prometheus model differ from GPT-...</td>\n",
       "      <td>[575a1ebd-5b35-4c14-a826-7ef4ec3b4654]</td>\n",
       "      <td>[This is in line with the information provided...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does the system ensure that each user only...</td>\n",
       "      <td>[e3ad9dd8-2c6d-482e-9497-94252f75b327]</td>\n",
       "      <td>[Retrieving Privacy-Safe Documents Over A Netw...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What role does metadata play in managing Multi...</td>\n",
       "      <td>[e3ad9dd8-2c6d-482e-9497-94252f75b327]</td>\n",
       "      <td>[Building Multi-Tenancy RAG System with LlamaI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the significance of the alpaca as a pe...</td>\n",
       "      <td>[fb7263e0-633f-4891-8dc4-2dc9d363164e]</td>\n",
       "      <td>[Get started in  Python  or  JavaScript ! P.S....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How are alpacas and llamas described in relati...</td>\n",
       "      <td>[fb7263e0-633f-4891-8dc4-2dc9d363164e]</td>\n",
       "      <td>[Get started in  Python  or  JavaScript ! P.S....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the RetrieverEvaluator module enhance...</td>\n",
       "      <td>[b1788bca-a4eb-4657-8554-6631fb3fdc58]</td>\n",
       "      <td>[Keyword Queries GTR retriever recall rate 5. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the purpose and capabilities o...</td>\n",
       "      <td>[b1788bca-a4eb-4657-8554-6631fb3fdc58]</td>\n",
       "      <td>[Docs ,  Tweet . Blockchain:  LlamaIndex data ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does the Launch of Seven Advanced Retrieva...</td>\n",
       "      <td>[f3802677-a18d-4a3c-a066-ffa5b53296a3]</td>\n",
       "      <td>[Building Scalable RAG Applications with Llama...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What enhancement was achieved in structured me...</td>\n",
       "      <td>[f3802677-a18d-4a3c-a066-ffa5b53296a3]</td>\n",
       "      <td>[Fine-Tuning Embeddings for RAG with Synthetic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the purpose of the reasoning loop in t...</td>\n",
       "      <td>[946dc08f-868e-46b1-907f-dce09412d0f8]</td>\n",
       "      <td>[Asking the continuation agent to decide what ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.131205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does the ReAct agent differ from the OpenA...</td>\n",
       "      <td>[946dc08f-868e-46b1-907f-dce09412d0f8]</td>\n",
       "      <td>[The details behind our tool abstractions are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.213986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   What new features have been introduced in Llam...   \n",
       "1   How has Scaleport AI utilized LlamaCloud and L...   \n",
       "2   How can the alignment and safety of LLMs and L...   \n",
       "3   What are some important dimensions to consider...   \n",
       "4   Who conducted a workshop at the LlamaIndex + R...   \n",
       "5   Which individuals were involved in the webinar...   \n",
       "6   How does the MultiOn agent manage the action o...   \n",
       "7   Where can developers find information about th...   \n",
       "8   What is the cost for evaluation of the Prometh...   \n",
       "9   How does the Prometheus model differ from GPT-...   \n",
       "10  How does the system ensure that each user only...   \n",
       "11  What role does metadata play in managing Multi...   \n",
       "12  What is the significance of the alpaca as a pe...   \n",
       "13  How are alpacas and llamas described in relati...   \n",
       "14  How does the RetrieverEvaluator module enhance...   \n",
       "15  Can you explain the purpose and capabilities o...   \n",
       "16  How does the Launch of Seven Advanced Retrieva...   \n",
       "17  What enhancement was achieved in structured me...   \n",
       "18  What is the purpose of the reasoning loop in t...   \n",
       "19  How does the ReAct agent differ from the OpenA...   \n",
       "\n",
       "                              expected_ids  \\\n",
       "0   [503ebcc2-3c47-4215-a6c8-d5cd3a52ac00]   \n",
       "1   [503ebcc2-3c47-4215-a6c8-d5cd3a52ac00]   \n",
       "2   [e0877b54-7ce0-4778-bef7-2051f8ef9ebf]   \n",
       "3   [e0877b54-7ce0-4778-bef7-2051f8ef9ebf]   \n",
       "4   [7522899d-0ec2-40f0-b367-c8e3342fa283]   \n",
       "5   [7522899d-0ec2-40f0-b367-c8e3342fa283]   \n",
       "6   [8aac6c35-0afc-41b8-9412-d7778d8b2cb9]   \n",
       "7   [8aac6c35-0afc-41b8-9412-d7778d8b2cb9]   \n",
       "8   [575a1ebd-5b35-4c14-a826-7ef4ec3b4654]   \n",
       "9   [575a1ebd-5b35-4c14-a826-7ef4ec3b4654]   \n",
       "10  [e3ad9dd8-2c6d-482e-9497-94252f75b327]   \n",
       "11  [e3ad9dd8-2c6d-482e-9497-94252f75b327]   \n",
       "12  [fb7263e0-633f-4891-8dc4-2dc9d363164e]   \n",
       "13  [fb7263e0-633f-4891-8dc4-2dc9d363164e]   \n",
       "14  [b1788bca-a4eb-4657-8554-6631fb3fdc58]   \n",
       "15  [b1788bca-a4eb-4657-8554-6631fb3fdc58]   \n",
       "16  [f3802677-a18d-4a3c-a066-ffa5b53296a3]   \n",
       "17  [f3802677-a18d-4a3c-a066-ffa5b53296a3]   \n",
       "18  [946dc08f-868e-46b1-907f-dce09412d0f8]   \n",
       "19  [946dc08f-868e-46b1-907f-dce09412d0f8]   \n",
       "\n",
       "                                      retrieved_texts  hit_rate   mrr  \\\n",
       "0   [Introducing LlamaCloud and LlamaParse\\nToday ...       0.0  0.00   \n",
       "1   [Case Study: How Scaleport.ai Accelerated Deve...       1.0  0.25   \n",
       "2   [Though studies have shown that strong LLMs ca...       1.0  1.00   \n",
       "3   [Though studies have shown that strong LLMs ca...       1.0  1.00   \n",
       "4   [Ravi Theja  conducted a  workshop  at LlamaIn...       1.0  1.00   \n",
       "5   [Raymond \\n P.S. ‚Äî  This article is titled Par...       0.0  0.00   \n",
       "6   [3. Send Email through MultiOn : Finally, the ...       1.0  1.00   \n",
       "7   [Automate online tasks with MultiOn and LlamaI...       1.0  0.50   \n",
       "8   [This is in line with the information provided...       1.0  1.00   \n",
       "9   [This is in line with the information provided...       1.0  1.00   \n",
       "10  [Retrieving Privacy-Safe Documents Over A Netw...       0.0  0.00   \n",
       "11  [Building Multi-Tenancy RAG System with LlamaI...       1.0  1.00   \n",
       "12  [Get started in  Python  or  JavaScript ! P.S....       1.0  1.00   \n",
       "13  [Get started in  Python  or  JavaScript ! P.S....       1.0  1.00   \n",
       "14  [Keyword Queries GTR retriever recall rate 5. ...       0.0  0.00   \n",
       "15  [Docs ,  Tweet . Blockchain:  LlamaIndex data ...       1.0  1.00   \n",
       "16  [Building Scalable RAG Applications with Llama...       0.0  0.00   \n",
       "17  [Fine-Tuning Embeddings for RAG with Synthetic...       0.0  0.00   \n",
       "18  [Asking the continuation agent to decide what ...       1.0  0.20   \n",
       "19  [The details behind our tool abstractions are ...       1.0  0.50   \n",
       "\n",
       "    precision  recall    ap      ndcg  \n",
       "0         0.0     0.0  0.00  0.000000  \n",
       "1         0.2     1.0  0.25  0.146068  \n",
       "2         0.2     1.0  1.00  0.339160  \n",
       "3         0.2     1.0  1.00  0.339160  \n",
       "4         0.2     1.0  1.00  0.339160  \n",
       "5         0.0     0.0  0.00  0.000000  \n",
       "6         0.2     1.0  1.00  0.339160  \n",
       "7         0.2     1.0  0.50  0.213986  \n",
       "8         0.2     1.0  1.00  0.339160  \n",
       "9         0.2     1.0  1.00  0.339160  \n",
       "10        0.0     0.0  0.00  0.000000  \n",
       "11        0.2     1.0  1.00  0.339160  \n",
       "12        0.2     1.0  1.00  0.339160  \n",
       "13        0.2     1.0  1.00  0.339160  \n",
       "14        0.0     0.0  0.00  0.000000  \n",
       "15        0.2     1.0  1.00  0.339160  \n",
       "16        0.0     0.0  0.00  0.000000  \n",
       "17        0.0     0.0  0.00  0.000000  \n",
       "18        0.2     1.0  0.20  0.131205  \n",
       "19        0.2     1.0  0.50  0.213986  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_results_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)\n",
    "    retrieval_eval_results_full_df.to_html(f\"{NOTEBOOK_CACHE_DP}/retrieval_eval_results_full_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/retrieval_eval_results_full_df.html\", \"retrieval_eval_results_full_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cffd06-4258-4798-bd27-18636a3a9b29",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deadc982-b78c-4882-80dc-498358c14a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_ids</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>[503ebcc2-3c47-4215-a6c8-d5cd3a52ac00]</td>\n",
       "      <td>[Introducing LlamaCloud and LlamaParse\\nToday ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which individuals were involved in the webinar...</td>\n",
       "      <td>[7522899d-0ec2-40f0-b367-c8e3342fa283]</td>\n",
       "      <td>[Raymond \\n P.S. ‚Äî  This article is titled Par...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does the system ensure that each user only...</td>\n",
       "      <td>[e3ad9dd8-2c6d-482e-9497-94252f75b327]</td>\n",
       "      <td>[Retrieving Privacy-Safe Documents Over A Netw...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the RetrieverEvaluator module enhance...</td>\n",
       "      <td>[b1788bca-a4eb-4657-8554-6631fb3fdc58]</td>\n",
       "      <td>[Keyword Queries GTR retriever recall rate 5. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does the Launch of Seven Advanced Retrieva...</td>\n",
       "      <td>[f3802677-a18d-4a3c-a066-ffa5b53296a3]</td>\n",
       "      <td>[Building Scalable RAG Applications with Llama...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What enhancement was achieved in structured me...</td>\n",
       "      <td>[f3802677-a18d-4a3c-a066-ffa5b53296a3]</td>\n",
       "      <td>[Fine-Tuning Embeddings for RAG with Synthetic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   What new features have been introduced in Llam...   \n",
       "5   Which individuals were involved in the webinar...   \n",
       "10  How does the system ensure that each user only...   \n",
       "14  How does the RetrieverEvaluator module enhance...   \n",
       "16  How does the Launch of Seven Advanced Retrieva...   \n",
       "17  What enhancement was achieved in structured me...   \n",
       "\n",
       "                              expected_ids  \\\n",
       "0   [503ebcc2-3c47-4215-a6c8-d5cd3a52ac00]   \n",
       "5   [7522899d-0ec2-40f0-b367-c8e3342fa283]   \n",
       "10  [e3ad9dd8-2c6d-482e-9497-94252f75b327]   \n",
       "14  [b1788bca-a4eb-4657-8554-6631fb3fdc58]   \n",
       "16  [f3802677-a18d-4a3c-a066-ffa5b53296a3]   \n",
       "17  [f3802677-a18d-4a3c-a066-ffa5b53296a3]   \n",
       "\n",
       "                                      retrieved_texts  hit_rate  mrr  \\\n",
       "0   [Introducing LlamaCloud and LlamaParse\\nToday ...       0.0  0.0   \n",
       "5   [Raymond \\n P.S. ‚Äî  This article is titled Par...       0.0  0.0   \n",
       "10  [Retrieving Privacy-Safe Documents Over A Netw...       0.0  0.0   \n",
       "14  [Keyword Queries GTR retriever recall rate 5. ...       0.0  0.0   \n",
       "16  [Building Scalable RAG Applications with Llama...       0.0  0.0   \n",
       "17  [Fine-Tuning Embeddings for RAG with Synthetic...       0.0  0.0   \n",
       "\n",
       "    precision  recall   ap  ndcg  \n",
       "0         0.0     0.0  0.0   0.0  \n",
       "5         0.0     0.0  0.0   0.0  \n",
       "10        0.0     0.0  0.0   0.0  \n",
       "14        0.0     0.0  0.0   0.0  \n",
       "16        0.0     0.0  0.0   0.0  \n",
       "17        0.0     0.0  0.0   0.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_irrelevance_df = (\n",
    "    retrieval_eval_results_full_df\n",
    "    .loc[lambda df: df['hit_rate'].lt(1)]\n",
    "    .sort_values(['hit_rate', 'mrr', 'precision', 'recall', 'ap', 'ndcg'])\n",
    ")\n",
    "retrieval_eval_irrelevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "632a0bf9-0ce7-4272-bc7b-8c2d10623ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============Error #1=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What new features have been introduced in LlamaCloud, and which platforms have been integrated with it?\n",
      "\n",
      "Expected Contexts:\n",
      "LlamaIndex Newsletter 2024-07-23\n",
      "Hello, Llama Followers! ü¶ô Welcome to this week‚Äôs edition of the LlamaIndex newsletter! We‚Äôre thrilled to share some exciting updates about our products, including LlamaCloud, LlamaParse, and LlamaAgents. You‚Äôll also find success stories with LlamaCloud, extensive guides, in-depth tutorials, and information about upcoming hackathons. ü§©¬† The highlights: LlamaCloud Updates:  New features including LlamaCloud Chat, enhanced Teams collaboration, and expanded integrations with Notion, Slack, Jira, and SharePoint.  Blogpost ,  Tweet . Scaleport AI‚Äôs Accelerated Development with LlamaCloud:  Scaleport AI boosts development speed and sales with LlamaCloud and LlamaIndex, improving data handling and OCR accuracy across multiple industries.  Blogpost . Claude Sonnet-3.5 Integration with LlamaParse:  Integration of Claude Sonnet-3.5 with LlamaParse improves chart understanding and data extraction capabilities.  Notebook ,  Tweet . Multimodal RAG Cookbook:  A new guide for processing text, diagrams, charts, and tables in slide decks using LlamaParse, LlamaIndex, and GPT-4o.  Notebook ,  Tweet . Human in the Loop with LlamaAgents:  Implementation includes HumanService for math queries and agent handling for other inquiries, managed via Gradio app and RabbitMQ.  Code . ‚ú® Feature Releases and Enhancements: We have released new features on LlamaCloud like LlamaCloud Chat for instant conversational data access, enhanced Teams functionality for collaboration, and expanded data integration with connectors for Notion, Slack, Jira, and improved SharePoint support.  Blogpost ,  Tweet . We integrated Claude Sonnet-3.5 with LlamaParse to enhance document parsing capabilities, offering advanced chart understanding and structured data extraction with improved validation and scalability.  Notebook ,  Tweet . We have released a cookbook on Multimodal RAG for processing slide decks rich in text, diagrams, charts, and tables using LlamaParse, LlamaIndex, and GPT-4o, blending text and image data for comprehensive document analysis.  Notebook ,  Tweet . We have implemented Human in the Loop with LlamaAgents in our new example that integrates a HumanService object for handling math queries and an agent for other queries, all managed through a Gradio app and RabbitMQ messaging.  Code . \n",
      "\n",
      "Retrieved Contexts:\n",
      "Introducing LlamaCloud and LlamaParse\n",
      "Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring  production-grade   context-augmentation  to your LLM and RAG applications. Using LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components: LlamaParse:  Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You‚Äôll be able to answer complex questions that simply weren‚Äôt possible previously. Managed Ingestion and Retrieval API:  An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in  LlamaHub , including LlamaParse, and our data storage integrations. LlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use;  contact us  for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested,  get in touch . \n",
      "\n",
      "Let‚Äôs do a brief tour through the core components! Ingestion:  Declare a managed pipeline to process and transform/chunk/embed data backed by our 150+ data sources in LlamaHub and our 40+ storage integrations as destinations. Automatically handle syncing and load balancing. \n",
      "\n",
      "Define through the UI or our open-source library. Retrieval:  Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language. Playground:  Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop. LlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment. LlamaCloud Retrieval: Access advanced retrieval over your storage system via an API. We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you‚Äôre interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come  talk to us . Launch Partners and Collaborators We opened up access to LlamaParse at  our hackathon  we co-hosted with  Futureproof Labs  and  Datastax  at the beginning of February. We saw some incredible applications of LlamaParse in action,  including parsing building codes for Accessory Dwelling Unit (ADU) planning ,  parsing real-estate disclosures for home buying , and dozens more. Eric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into Mendable‚Äôs data stack: ‚ÄúWe integrated LlamaParse into our  open source data connector repo  which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.‚Äù We‚Äôre also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem, from storage to compute. DataStax Datastax has incorporated LlamaParse into their RAGStack to bring a privacy-preserving out-of-the-box RAG solution for enterprises: \"Last week one of our customers Imprompt has launched a pioneering 'Chat-to-Everything' platform leveraging RAGStack powered by LlamaIndex to enhance their enterprise offerings while prioritizing privacy.\" said Davor Bonaci, CTO and executive vice president, DataStax. \"We're thrilled to partner with LlamaIndex to bring a streamlined solution to market. By incorporating LlamaIndex into RAGStack, we are providing enterprise developers with a comprehensive Gen AI stack that simplifies the complexities of RAG implementation, while offering long-term support and compatibility assurance.‚Äù MongoDB ‚ÄúMongoDB‚Äôs partnership with LlamaIndex allows for the ingestion of data into the MongoDB Atlas Vector database, and the retrieval of the index from Atlas via LlamaParse and LlamaCloud, enabling the development of RAG systems and other AI applications,‚Äù said Greg Maxson, Global Lead, AI Ecosystems at MongoDB. ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline implementations, and more cost effectively develop large language model applications, ultimately accelerating generative AI app development and more quickly bringing apps to market.‚Äù Qdrant Andr√© Zayarni, CEO of Qdrant, says ‚ÄúThe Qdrant team is excited to partner with LlamaIndex to combine the power of optimal data preprocessing, vectorization, and ingestion with Qdrant for a powerful fullstack RAG solution.‚Äù NVIDIA We are also excited to collaborate with NVIDIA to integrate LlamaIndex with the  NVIDIA AI Enterprise  software platform for production AI: ‚ÄúLlamaCloud will help enterprises get generative AI applications from development into production with connectors that link proprietary data to the power of large language models,‚Äù said Justin Boitano, vice president, Enterprise and Edge Computing, NVIDIA. ‚ÄúPairing LlamaCloud with NVIDIA AI Enterprise can accelerate the end-to-end LLM pipeline ‚Äî including data processing, embedding creation, indexing, and model inference on accelerated computing across clouds, data centers and out to the edge.‚Äù FAQ Is this competitive with vector databases? No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are working hard to do the following: Integrate LlamaCloud with storage providers of existing design partners Make LlamaCloud available in a more ‚Äúself-serve‚Äù manner. Next Steps As mentioned in the above sections, LlamaParse is available for public preview starting today with a usage cap. LlamaCloud is in a private preview mode; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested come talk to us! LlamaParse:  Repo ,  Cookbook ,  Contact Us LlamaCloud:  Contact Us\n",
      "\n",
      "Want to discuss unlimited commercial use?  Contact us  and let's chat! Note: we support private deployments for a select number of enterprises Stay Updated : Follow us on  Twitter  and join our  Discord community  to stay in the loop. In the meantime, anyone can create an account at  https://cloud.llamaindex.ai/ . While you‚Äôre waiting for official LlamaCloud access, anyone can immediately start using our LlamaParse APIs. We‚Äôre shipping a  lot  of features in the next few weeks. We look forward to seeing the context-augmented LLM applications that you can build on top of LlamaCloud! üöÄü¶ô FAQ Have you got some examples of how to use LlamaCloud? We sure do! One of the strengths of LlamaCloud is how easily the endpoints integrate into your existing code. Our  llamacloud-demo repo  has lots of examples from  getting started  to  running evaluations . Is this competitive with vector databases? No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are integrating LlamaCloud with storage providers based on customer requests.\n",
      "\n",
      "The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need a production-quality data processing layer. LlamaCloud is that data processing and management layer for your AI knowledge assistants. Since  launching a LlamaCloud waitlist last week , we‚Äôve gotten hundreds of signups and published case studies showing how it cuts  production development hours by 50% . On top of that, our team has shipped a slew of new features at a breakneck pace in the past week. We‚Äôre excited to highlight these new features that collectively help you  set up a chat interface in minutes,   increase developer collaboration within your team, and access more data and metadata. Set up a Chat Interface in Minutes We are releasing  LlamaCloud Chat,  which gives you an easy-to-use chat interface over your data. This chat interface is a conversational RAG pipeline built over the advanced retrieval interface that a given pipeline provides, and has out-of-the-box support for streaming and citations - it‚Äôs powered by the same DNA as  create-llama , our fully open-source set up tool  for LLM applications. The LlamaCloud UI already lets you set up a data pipeline over any data in minutes, and now you get a full-blown ChatGPT over your data in minutes. Besides the chat UI, you also have additional flexibility: You can customize metadata filters in the retrieval parameters You can view retrieved nodes and their source files Besides chunk-level retrieval, you can now do  file-level retrieval  (more on this soon!) LlamaCloud is fundamentally a developer tool: with these updates, we enable developers to spend less time on data pipeline setup and iteration, and more time on writing the orchestration logic on top of this interface. Increased Developer Collaboration The team selection interface Organization settings We‚Äôve added  organizational features  into LlamaCloud, enabling any individual user to create an organization and add other users to the organization. Any user within an organization will have a view of all the organization‚Äôs projects and indexes within each project. This allows your team to have a single-source of truth for your data pipelines. In the past each developer would spend time re-indexing/experimenting with the same sources of data. This feature enables transparency, re-use, and generally more rapid development velocity. Improved Data and Metadata Access We‚Äôve made several updates here - we‚Äôve added more data connectors and added features to let you more easily access and customize metadata. We added a Notion, Slack, and Jira Connector Our Sharepoint connector now natively integrates with user IDs that you can filter for, enabling you to build LLM applications with access control. You can now attach metadata to any uploaded file as a CSV - do this through the UI or our API! Want to see what LlamaCloud can do for you? \n",
      "\n",
      "\n",
      "\n",
      "============Error #2=============\n",
      "\n",
      "\n",
      "Query:\n",
      "Which individuals were involved in the webinar on LLM Challenges in Production?\n",
      "\n",
      "Expected Contexts:\n",
      "Ravi Theja  conducted a  workshop  at LlamaIndex + Replit Pune Generative AI meetup. Jerry Liu   session  on Building a Lending Criteria Chatbot in Production with Stelios from MQube. Webinars : Webinar  on How to Win an LLM Hackathon by Alex Reibman, Rahul Parundekar, Caroline Frasca, and Yi Ding. Webinar  on LLM Challenges in Production with Mayo Oshin, AI Jason, and Dylan.\n",
      "\n",
      "Retrieved Contexts:\n",
      "Raymond \n",
      " P.S. ‚Äî  This article is titled Part 1 for a reason. What do you want to see in Part 2? Tell me below!\n",
      "\n",
      "Data Querying:  Data retrieval, response synthesis, multi-step interactions over your data. LlamaIndex allows you to seamlessly integrate individual or enterprise data, including files, workplace apps, and databases, with LLM applications. We also offer an extensive array of integrations with other storage providers and downstream applications. 100+ data loaders 13+ vector database providers Integrations with observability and experimentation frameworks (e.g. prompt tracking and system tracing) Integrations as a  ChatGPT Retrieval Plugin  or with  Poe The end result is that you can build a variety of amazing knowledge-intensive LLM applications. This ranges from a search engine over your data, to chatbot-style interfaces, to structured analytics helpers, to autonomous knowledge agents. What‚Äôs next? There are  so  many things that we want to do to more fully realize our vision of unlocking LLM capabilities on top of your data. We‚Äôll broadly break this down into two categories: 1) our continued commitment to the open-source developer community, and 2) solving the data problem at scale for enterprises. Build the best open source data framework and developer community At a high-level, we want to continue iterating on our core feature capabilities, improving reliability, and satisfy both the needs of beginner and advanced users. Handle complex queries:  We want to continue advancing the idea of ‚Äúquerying your data‚Äù, whether it‚Äôs through leveraging agent-style interactions for data retrieval and synthesis or program synthesis/DSL. Multi-modal data management:  The future of foundation models is multimodal, not just contained to LLMs. There are many types of semi-structured data (e.g. semi-structured data like JSONs, yaml files) as well as ‚Äúcomplex‚Äù unstructured data (audio, images, video) that we‚Äôd love to have native support for. Better evaluation of LLM data systems:  Properly evaluating LLM calls is already tricky (how do you best evaluate the quality of a generated output? Some  libraries  for handling this). This becomes even more tricky when you chain LLM calls within an overall data system. We want to invest efforts into this area to provide greater transparency to our users. Optimization of Latency/Cost:  Users are faced with a plethora of choices when it comes to building a data-driven LLM app: the choice of LLM model, embedding model, vector database, etc. They must choose in accordance to a variety of factors, from latency and cost to privacy. Ease of use for both beginner users and advanced users:  Our goal is to make the utilization of LLM capabilities accessible and user-friendly for individuals at all skill levels. We will develop clear tutorials, examples, and tools to simplify the learning curve and convey the value of all of our features. Solving the data problem at scale for Enterprises As we‚Äôre iterating on the open-source project, we also want to identify the surrounding pain points in being able to build and deploy data-powered LLM apps to production. Our solution to this will build upon the success of our open-source project and be a natural evolution to the enterprise setting. Production-ready data ingestion and management:  We want to handle data updates, data consistency, and scalability to larger volumes of data parsing. We also want to continue expanding on the right storage abstractions for multi-modal data. Scale to Large Data Volumes:  Enterprises will typically have orders of magnitude more data than an individual. We want to invest in hosted infrastructure/deployment solutions around our core package so that you don‚Äôt have to. Domain-specific LLM solutions:  We want to offer packaged solutions to enable users to easily build LLM apps in different domains, from healthcare to finance to legal. If you‚Äôre building LLM apps in the enterprise setting, we‚Äôd love to chat and learn more about pain points + desired features! \n",
      "\n",
      "Make it real. \n",
      " \n",
      "\n",
      "Docs ,  Tweet . üëÄ Community Demos : Automated LeetCode Crash Course:  The Project integrates advanced ML with traditional algorithms to streamline LeetCode study for technical interviews. It involves extracting and summarizing LeetCode problems using an LLM, organizing these summaries in a vector store, and employing scikit-learn for clustering.  Blog ,  Code . \n",
      "\n",
      "Check out our  form here . Join the Llama Gang! ü¶ô Join the Llama(Index) gang as we embark on this journey to solve problems at the intersection of LLMs and data. We are not just building tools for ML practitioners/researchers; the emerging LLM + data architecture stacks have implications for  all  of software development. As a result, we are operating at the intersection of incredibly fun and challenging problems from a variety of different fields: Foundation Model Development Information Retrieval + Recommendation Systems Data Systems MLOps DevOps Interested in checking out the project? Find our project on  Github  and check out our  Docs Check out our brand new landing page:  https://llamaindex.ai Join our  Discord  or Follow our  Twitter Also, we‚Äôre hiring! We‚Äôre looking for founding engineers ‚Äî experience in one or more of AI, data systems, and full-stack/front-end is nice to have but not a requirement. If you‚Äôre interested,  fill out our form here .\n",
      "\n",
      "\n",
      "\n",
      "============Error #3=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the system ensure that each user only receives answers from the documents they have indexed, and not from documents indexed by other users?\n",
      "\n",
      "Expected Contexts:\n",
      "Building Multi-Tenancy RAG System with LlamaIndex\n",
      "Introduction: The concept of Multi-Tenancy in RAG (Retriever-Augmented Generation) systems has become increasingly vital, especially when it comes to data security and privacy. Multi-Tenancy, in simple terms, refers to a system‚Äôs ability to serve multiple users (‚Äòtenants‚Äô) independently and securely. Consider this scenario: In a RAG system, there are two users, User-1 and User-2. Both have their own set of documents which they have indexed into the system. The critical aspect of Multi-Tenancy here is that when User-1 queries the system, they should only receive answers from the documents they have indexed, and not from the documents indexed by User-2, and vice versa. This separation is crucial for maintaining data confidentiality and security, as it prevents the accidental or unauthorized cross-referencing of private information between different users. In the context of Multi-Tenancy in RAG systems, this means designing a system that not only understands and retrieves information effectively but also strictly adheres to user-specific data boundaries. Each user‚Äôs interaction with the system is isolated, ensuring that the retriever component of the RAG pipeline accesses only the information relevant and permitted for that particular user. This approach is important in scenarios where sensitive or proprietary data is involved, as it safeguards against data leaks and privacy breaches. In this blog post, we will look into Building a Multi-Tenancy RAG System with LlamaIndex. Solving Multi-Tenancy Challenges The key to managing Multi-Tenancy lies within the metadata. When indexing documents, we incorporate user-specific information into the metadata before adding it to the index. This ensures that each document is uniquely tied to an individual user. During the query phase, the retriever uses this metadata to filter and only access documents associated with the querying user. Subsequently, it performs a semantic search to retrieve the most relevant information segments, or ‚Äòtop_k chunks‚Äô, for that user. By implementing this approach, we effectively prevent the unauthorized cross-referencing of private information between different users, upholding the integrity and confidentiality of each user‚Äôs data. Now that we‚Äôve discussed the concept, let‚Äôs dive into the construction of a Multi-Tenancy RAG system. For an in-depth step-by-step guide, feel free to follow along with the subsequent instructions in our  Google Colab Notebook . Download Data: We will use  An LLM Compiler for Parallel Function Calling  and  Dense X Retrieval: What Retrieval Granularity Should We Use?  \n",
      "\n",
      "Retrieved Contexts:\n",
      "Retrieving Privacy-Safe Documents Over A Network\n",
      "In a  recent blog post , we introduced our  llama-index-networks  library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores‚Äîthat one may not otherwise have access to‚Äîmeans more accurate responses to an even wider breadth of queries. A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we‚Äôll be relying on some recent developments in the area of Privacy-Enhancing Techniques. The story of Alex, Bob and Beth continues To illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply. We showed then how such data a collaboration could be permitted through  llama-index-networks  by taking the following steps: Bob and Beth both build their respective QueryEngine‚Äôs (RAG in llama-index lingo) Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs In part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets. \n",
      "\n",
      "Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy. Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Part 2: of Alex, Bob and Beth. \n",
      "\n",
      "Evaluating the  NetworkRetriever To evaluate the efficacy of the  NetworkRetriever  we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank. hit rate:  a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set. mean reciprocal rank:  similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set. In addition to evaluating the  NetworkRetriever  we consider the two baselines that represent Retrieving only over the individual Contributor‚Äôs synthetic datasets. Retriever evaluations, with sigma equal to 1.5. In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever‚Äôs in the test set. This shouldn‚Äôt be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor‚Äôs synthetic observations‚Äîthis is the point after all of a network! Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated). Finally, this level of privacy can be controlled via the noise parameter  sigma . In the example above we used a  sigma  of 1.5, which for this dataset amounts to an  epsilon  (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are  generally considered to be quite private .) Below, we share the evaluations that result from using a  sigma  of 0.5, which amounts to an  epsilon  of 15.9‚Äîhigher values of  epsilon  or privacy-loss means less privacy. # use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon \n",
      "epsilon = dp_simple_dataset_pack.sigma_to_eps(\n",
      "\t\tsigma= 0.5 ,\n",
      "\t\tmechanism= \"gaussian\" ,\n",
      "\t\tsize= 3 * 24 ,\n",
      "\t\tmax_token_cnt= 150    # number of max tokens to generate per synthetic example \n",
      ") Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5. So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening. # synthetic example epsilon = 1.3 \n",
      "{\n",
      "     \"reference_label\" :  \"Psoriasis\" ,\n",
      "     \"text\" :  \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\" ,\n",
      "     \"text_by\" : {\n",
      "         \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\n",
      "         \"type\" :  \"ai\" \n",
      "    }\n",
      "},\n",
      "\n",
      " # synthetic example epsilon = 15.9 \n",
      "{\n",
      "   \"reference_label\" :  \"Migraine\" ,\n",
      "   \"text\" :  \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\" ,\n",
      "   \"text_by\" : {\n",
      "     \"model_name\" :  \"gpt-3.5-turbo-instruct\" ,\n",
      "     \"type\" :  \"ai\" \n",
      "  }\n",
      "}, We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM. In summary We used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible. We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to. We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever. Learn more! To delve deeper into the materials of this blog post, we share a few links below: Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! ( link ) Demo notebooks for the  DiffPrivateSimpleDataset  ( link ) The source code for creating the synthetic Symptom2Disease observations using the  DiffPrivateSimpleDataset  ( link )\n",
      "\n",
      "This time Bob and Beth have sensitive data that they want to share, but can‚Äôt unless protective measures are applied before sharing across the network. Sidebar: differential privacy primer In short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual‚Äôs inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm. By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion). A light introduction to differential privacy. A light introduction to differential privacy. Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth‚Äôs data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled ‚ÄúPrivacy-preserving in-context learning with differentially private few-shot generation‚Äù by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network! There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration. Bob and Beth create privacy-safe synthetic copies of their private datasets Bob and Beth both build their respective QueryEngine‚Äôs over their synthetic datasets Bob and Beth both expose their QueryEngine behind a ContributorService Alex builds a NetworkQueryEngine that connects to Bob and Beth‚Äôs ContributorService‚Äôs Creating differentially private synthetic copies of a private dataset Fortunately, for step 0., we can make use of the  DiffPrivateSimpleDataset  pack. from  llama_index.core.llama_datasets.simple  import  LabelledSimpleDataset\n",
      " from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\n",
      " from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\n",
      " from  llama_index.llms.openai  import  OpenAI\n",
      " import  tiktoken\n",
      "\n",
      " # Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies \n",
      "\n",
      "llm = OpenAI(\n",
      "    model= \"gpt-3.5-turbo-instruct\" ,\n",
      "    max_tokens= 1 ,\n",
      "    logprobs= True ,\n",
      "    top_logprobs= 5 ,   # OpenAI only allows for top 5 next token \n",
      ")                     # as opposed to entire vocabulary \n",
      "tokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\n",
      "\n",
      "beth_private_dataset: LabelledSimpleDataset = ...  # a dataset that contains \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # examples with two attributes \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # `text` and `reference_label` \n",
      "\n",
      "beth_synthetic_generator = DiffPrivateSimpleDatasetPack(\n",
      "    llm=llm,\n",
      "    tokenizer=tokenizer,\n",
      "    prompt_bundle=prompt_bundle,     # params for preparing required prompts \n",
      "    simple_dataset=simple_dataset,   # to generate the synthetic examples  \n",
      ")\n",
      "\n",
      "beth_synthetic_dataset =  await  beth_synthetic_generator.arun(\n",
      "\t\tsize= 3 ,   # number of synthetic observations to create \n",
      "\t\tsigma= 0.5    # param that determines the level of privacy \n",
      ") With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It‚Äôs worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! \n",
      "\n",
      "(This is due to the post-processing property of differential privacy.) Example: Symptom2Disease In this section of the blog post, we go over an actual example application of the privacy-safe networks over the  Symptom2Disease  dataset. This dataset consists of 1,200 examples each containing a ‚Äúsymptoms‚Äù description as well as the associated ‚Äúdisease‚Äù label ‚Äî the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network. Generate privacy-safe synthetic observations of Symptom2Disease We use the training subset and apply the  DiffPrivateSimpleDatasetPack  on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a  LabelledSimpleDataset  object. import  pandas  as  pd\n",
      " from  sklearn.model_selection  import  train_test_split\n",
      " from  llama_index.core.llama_dataset.simple  import  (\n",
      "    LabelledSimpleDataExample,\n",
      "    LabelledSimpleDataset,\n",
      ")\n",
      " from  llama_index.core.llama_dataset.base  import  CreatedBy, CreatedByType\n",
      "\n",
      " # load the Symptom2Disease.csv file \n",
      "df = pd.read_csv( \"Symptom2Disease.csv\" )\n",
      "train, test = train_test_split(df, test_size= 0.2 )\n",
      "\n",
      " # create a LabelledSimpleDataset (which is what the pack works with) \n",
      "examples = []\n",
      " for  index, row  in  df.iterrows():\n",
      "    example = LabelledSimpleDataExample(\n",
      "        reference_label=row[ \"label\" ],\n",
      "        text=row[ \"text\" ],\n",
      "        text_by=CreatedBy( type =CreatedByType.HUMAN),\n",
      "    )\n",
      "    examples.append(example)\n",
      "\n",
      "simple_dataset = LabelledSimpleDataset(examples=examples) Now we can use the llama-pack to create our synthetic observations. import  llama_index.core.instrumentation  as  instrument\n",
      " from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\n",
      " from  llama_index.packs.diff_private_simple_dataset.base  import  PromptBundle\n",
      " from  llama_index.packs.diff_private_simple_dataset  import  DiffPrivateSimpleDatasetPack\n",
      " from  llama_index.llms.openai  import  OpenAI\n",
      " import  tiktoken\n",
      " from  .event_handler  import  DiffPrivacyEventHandler\n",
      " import  asyncio\n",
      " import  os\n",
      "\n",
      "NUM_SPLITS =  3 \n",
      "T_MAX =  150 \n",
      "\n",
      "llm = OpenAI(\n",
      "    model= \"gpt-3.5-turbo-instruct\" ,\n",
      "    max_tokens= 1 ,\n",
      "    logprobs= True ,\n",
      "    top_logprobs= 5 ,\n",
      ")\n",
      "tokenizer = tiktoken.encoding_for_model( \"gpt-3.5-turbo-instruct\" )\n",
      "\n",
      "prompt_bundle = PromptBundle(\n",
      "    instruction=(\n",
      "         \"You are a patient experiencing symptoms of a specific disease. \" \n",
      "         \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\n\" \n",
      "         \"Start your answer directly after 'Symptoms: '. Begin your answer with [RESULT].\\n\" \n",
      "    ),\n",
      "    label_heading= \"Disease\" ,\n",
      "    text_heading= \"Symptoms\" ,\n",
      ")\n",
      "\n",
      "dp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\n",
      "    llm=llm,\n",
      "    tokenizer=tokenizer,\n",
      "    prompt_bundle=prompt_bundle,\n",
      "    simple_dataset=simple_dataset,\n",
      ")\n",
      "\n",
      "synthetic_dataset =  await  dp_simple_dataset_pack.arun(\n",
      "    sizes= 3 ,\n",
      "    t_max=T_MAX,\n",
      "    sigma= 1.5 ,\n",
      "    num_splits=NUM_SPLITS,\n",
      "    num_samples_per_split= 8 ,   # number of private observations to create a \n",
      ")                              # synthetic obsevation \n",
      "synthetic_dataset.save_json( \"synthetic_dataset.json\" ) Create a network with two contributors Next, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well. Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we‚Äôre using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user). Step 1:  Contributor‚Äôs build their Retriever over their synthetic datasets. import  os\n",
      " from  llama_index.core  import  VectorStoreIndex\n",
      " from  llama_index.core.llama_dataset.simple  import  LabelledSimpleDataset\n",
      " from  llama_index.core.schema  import  TextNode\n",
      "\n",
      "\n",
      " # load the synthetic dataset \n",
      "synthetic_dataset = LabelledSimpleDataset.from_json(\n",
      "     \"./data/contributor1_synthetic_dataset.json\" \n",
      ")\n",
      "\n",
      "\n",
      "nodes = [\n",
      "    TextNode(text=el.text, metadata={ \"reference_label\" : el.reference_label})\n",
      "     for  el  in  synthetic_dataset[:]\n",
      "]\n",
      "\n",
      "index = VectorStoreIndex(nodes=nodes)\n",
      "similarity_top_k =  int (os.environ.get( \"SIMILARITY_TOP_K\" ))\n",
      "retriever = index.as_retriever(similarity_top_k=similarity_top_k) Step 2:  Contributor‚Äôs expose their Retrievers behind a ContributorRetrieverService from  llama_index.networks.contributor.retriever.service  import  (\n",
      "    ContributorRetrieverService,\n",
      "    ContributorRetrieverServiceSettings,\n",
      ")\n",
      "\n",
      "settings = ContributorRetrieverServiceSettings()  # loads from .env file \n",
      "service = ContributorRetrieverService(config=settings, retriever=retriever)\n",
      "app = service.app Step 3:  Define the NetworkRetriever that connects to the ContributorRetrieverServices from  llama_index.networks.network.retriever  import  NetworkRetriever\n",
      " from  llama_index.networks.contributor.retriever  import  ContributorRetrieverClient\n",
      " from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\n",
      "\n",
      " # ContributorRetrieverClient's connect to the ContributorRetrieverService \n",
      "contributors = [\n",
      "    ContributorRetrieverClient.from_config_file(\n",
      "        env_file= f\"./client-env-files/.env.contributor_ {ix} .client\" \n",
      "    )\n",
      "     for  ix  in   range ( 1 ,  3 )\n",
      "]\n",
      "reranker = CohereRerank(top_n= 5 )\n",
      "network_retriever = NetworkRetriever(\n",
      "    contributors=contributors, node_postprocessors=[reranker]\n",
      ") With the  NetworkRetriever  established, we can retrieve synthetic observations from the two contributors data against a query. related_records = network_retriever.aretrieve( \"Vomitting and nausea\" )\n",
      " print (related_records)  # contain symptoms/disease records that are similar to \n",
      "\t\t\t\t\t\t\t\t\t\t\t  # to the queried symptoms. \n",
      "\n",
      "\n",
      "\n",
      "============Error #4=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the RetrieverEvaluator module enhance retrieval evaluations and what specific functionalities does it offer?\n",
      "\n",
      "Expected Contexts:\n",
      "Docs ,  Tweet . Blockchain:  LlamaIndex data agents can be now used to analyze any blockchain subgraph using natural language queries.  Tweet . üîé  RAG Evaluation Enhancements: RetrieverEvaluator : We introduced ‚ÄúRetrieverEvaluator‚Äù for enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.  Docs ,  Tweet . SemanticSimilarityEvaluator : We introduced a new semantic similarity evaluator ‚Äî SemanticSimilarityEvaluator for LLM/RAG outputs, comparing embedding similarity between reference and generated answers.  Docs ,  Tweet . \n",
      "\n",
      "Retrieved Contexts:\n",
      "Keyword Queries GTR retriever recall rate 5. Queries With Misspellings What is the advntage of prposition retrieval over sentnce or passage retrieval? 6. Exact Sub-string Searches first kwords for the GTR retriever. Finer-grained Retrieval Evaluation Metrics: We will utilize Hit Rate and MRR metrics for retrieval evaluation. Let‚Äôs get into understanding these metrics. Hit Rate: Hit Rate measures the proportion of queries for which the correct chunk/ context appears within the top-k results chunks/ contexts. Put simply, it evaluates how frequently our system correctly identifies the chunk within its top-k chunks. Mean Reciprocal Rank (MRR): MRR assesses a system‚Äôs accuracy by taking into account the position of the highest-ranking relevant chunk/ context for each query. It calculates the average of the inverse of these positions across all queries. For instance, if the first relevant chunk/ context is at the top of the list, its reciprocal rank is 1. If it‚Äôs the second item, the reciprocal rank becomes 1/2, and this pattern continues accordingly. The remainder of this blog post is divided into two main sections: Implementing  Alpha  Tuning in Hybrid Search for Various Query Types. Analyzing the results of two different document datasets: Indexing a Single Document:  The  LLM Compiler Paper . Indexing Three Documents:  The  LLM Compiler ,  Llama Beyond English , and  Dense X Retrieval  Papers. You can also continue following along in the  Google Colab Notebook  from this point forward. Implementation We will adopt a systematic approach to implement the experimental workflow, which involves the following steps: Data Download. Data Loading. \n",
      "\n",
      "Cohere Reranker. Fine-tuned reranker (Custom reranker) without hard negatives. Fine-tuned reranker (Custom reranker) with hard negatives selected at random. Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity. Let‚Äôs define the rerankers. RERANKERS = {\n",
      "    \"WithoutReranker\": \"None\",\n",
      "    \"CohereRerank\": reranker_base,\n",
      "    \"CohereRerank_0\": reranker_model_0,\n",
      "    \"CohereRerank_5_random\": reranker_model_5_random,\n",
      "    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n",
      "} Create an Index and Retriever for evaluation purposes. # Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\n",
      "index_embed_model = CohereEmbedding(\n",
      "    cohere_api_key=cohere_api_key,\n",
      "    model_name=\"embed-english-v3.0\",\n",
      "    input_type=\"search_document\",\n",
      ")\n",
      "\n",
      "query_embed_model = CohereEmbedding(\n",
      "    cohere_api_key=cohere_api_key,\n",
      "    model_name=\"embed-english-v3.0\",\n",
      "    input_type=\"search_query\",\n",
      ")\n",
      "\n",
      "service_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\n",
      "service_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n",
      "\n",
      "vector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\n",
      "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query) Define a function to display the results def   display_results ( embedding_name, reranker_name, eval_results ):\n",
      "     \"\"\"Display results from evaluate.\"\"\" \n",
      "\n",
      "    metric_dicts = []\n",
      "     for  eval_result  in  eval_results:\n",
      "        metric_dict = eval_result.metric_vals_dict\n",
      "        metric_dicts.append(metric_dict)\n",
      "\n",
      "    full_df = pd.DataFrame(metric_dicts)\n",
      "\n",
      "    hit_rate = full_df[ \"hit_rate\" ].mean()\n",
      "    mrr = full_df[ \"mrr\" ].mean()\n",
      "\n",
      "    metric_df = pd.DataFrame(\n",
      "        { \"Embedding\" : [embedding_name],  \"Reranker\" : [reranker_name],  \"hit_rate\" : [hit_rate],  \"mrr\" : [mrr]}\n",
      "    )\n",
      "\n",
      "     return  metric_df Loop over different rerankers and evaluate retrieval performance using Custom Retriever. results_df = pd.DataFrame()\n",
      "\n",
      "embed_name =  'CohereEmbedding' \n",
      "\n",
      " # Loop over rerankers \n",
      " for  rerank_name, reranker  in  RERANKERS.items():\n",
      "\n",
      "     print ( f\"Running Evaluation for Reranker:  {rerank_name} \" )\n",
      "\n",
      "     # Define Retriever \n",
      "     class   CustomRetriever ( BaseRetriever ):\n",
      "         \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\" \n",
      "\n",
      "         def   __init__ ( \n",
      "            self,\n",
      "            vector_retriever: VectorIndexRetriever,\n",
      "         ) -&gt;  None :\n",
      "             \"\"\"Init params.\"\"\" \n",
      "\n",
      "            self._vector_retriever = vector_retriever\n",
      "\n",
      "         def   _retrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n",
      "             \"\"\"Retrieve nodes given query.\"\"\" \n",
      "\n",
      "            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
      "\n",
      "             if  reranker !=  'None' :\n",
      "                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
      "             else :\n",
      "                retrieved_nodes = retrieved_nodes[: 5 ]\n",
      "\n",
      "             return  retrieved_nodes\n",
      "\n",
      "         async   def   _aretrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n",
      "             \"\"\"Asynchronously retrieve nodes given query.\n",
      "            \"\"\" \n",
      "             return  self._retrieve(query_bundle)\n",
      "\n",
      "         async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n",
      "             if   isinstance (str_or_query_bundle,  str ):\n",
      "                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
      "             return   await  self._aretrieve(str_or_query_bundle)\n",
      "\n",
      "    custom_retriever = CustomRetriever(vector_retriever)\n",
      "\n",
      "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
      "        [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\n",
      "    )\n",
      "    eval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n",
      "\n",
      "    current_df = display_results(embed_name, rerank_name, eval_results)\n",
      "    results_df = pd.concat([results_df, current_df], ignore_index= True ) Results: From the above table (1- without reranker, 2 ‚Äî with base cohere reranker, 3‚Äì5: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It‚Äôs crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker. Summary: In this blog post, we‚Äôve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community‚Äôs use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.\n",
      "\n",
      "One can then apply a desired weighting scheme to establish a single aggregated retrieval score per metric. Hit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75 Table 3: Retrieval evaluation in multi-modal scenario. Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge) Multi-modal models (i.e., LMMs) like OpenAI‚Äôs GPT-4V or open-source alternatives like LLaVA are able to take in both input and image context to produce an answer the user query. As in text-only RAG, we are also concerned about the ‚Äúrelevancy‚Äù and ‚Äúfaithfulness‚Äù of these generated answers. But in order to be able to compute such metrics in the multi-modal case, we would need a judge model that is also able to take in the context images and text data. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in order to compute relevancy and faithfulness as well as other related metrics! Relevancy (multi-modal): considers  textual and visual context  and evaluates how much the generated response matches the query. Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved  textual and visual context . If you want to test these out, then you‚Äôre in luck as we‚Äôve recently released our beta Multi-Modal Evaluator abstractions! See the code snippet below for how one can use these abstractions to perform their respective evaluations on a generated response to a given query. from  llama_index.evaluation.multi_modal  import  (\n",
      "\tMultiModalRelevancyEvaluator,\n",
      "\tMultiModalFaithfulnessEvaluator\n",
      ")\n",
      " from  llama_index.multi_modal_llm  import  OpenAIMultiModal\n",
      "\n",
      "relevancy_judge = MultiModalRelevancyEvaluator(\n",
      "    multi_modal_llm=OpenAIMultiModal(\n",
      "        model= \"gpt-4-vision-preview\" ,\n",
      "        max_new_tokens= 300 ,\n",
      "    )\n",
      ")\n",
      "\n",
      "faithfulness_judge = MultiModalRelevancyEvaluator(\n",
      "    multi_modal_llm=OpenAIMultiModal(\n",
      "        model= \"gpt-4-vision-preview\" ,\n",
      "        max_new_tokens= 300 ,\n",
      "    )\n",
      ")\n",
      "\n",
      " # Generated response to a query and its retrieved context information \n",
      "query = ...\n",
      "response = ...\n",
      "contexts = ...   # retrieved text contexts \n",
      "image_paths = ...   # retrieved image contexts \n",
      "\n",
      " # Evaluations \n",
      "relevancy_eval = relevancy_judge.evaluate(\n",
      " query=query,\n",
      " response=response,\n",
      " contexts=contexts,\n",
      " image_paths=image_paths\n",
      ")\n",
      "\n",
      "faithfulness_eval = faithfulness_judge.evaluate(\n",
      " query=query,\n",
      " response=response,\n",
      " contexts=contexts,\n",
      " image_paths=image_paths\n",
      ") A Few Important Remarks First, it is worth mentioning that using LLMs or LMMs to judge generated responses has its drawbacks. These judges are generative models themselves and can suffer from hallucinations and other inconsistencies. \n",
      "\n",
      "Though studies have shown that strong LLMs can align to human judgments at a relatively high rate ( Zheng et al., 2023 ), using them in production systems should be handled with higher standards of care. At time of writing, there has no been study to show that strong LMMs can also align well to human judgements. Secondly, the evaluation of a generator touches mostly on the evaluation of its knowledge and reasoning capabilities. There are other important dimensions on which to evaluate LLMs and LMMs, including Alignment and Safety ‚Äî see  Evaluating LMMs: A Comprehensive Survey  for more information. Go forth and evaluate In this post, we covered how evaluation can be performed on multi-modal RAG systems. We believe that separating out the retrieval evaluations per modalities for increased visibility as well as the LMM-As-A-Judge represent a sensible extension of the evaluation framework for text-only RAG. We encourage you to check out our practical notebook guides as well as docs for more information on how you can not only build Multi-Modal RAGs but also adequately evaluate them! Notebook guide for evaluating Multi-Modal RAG systems with LlamaIndex Intro to Multi-Modal RAG Docs/guides on Multi-Modal Abstractions\n",
      "\n",
      "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex\n",
      "Introduction: Achieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is heavily dependent on robust retrieval performance. As we explored in our previous  blog post , rerankers have a significant impact on boosting retrieval performance. But what if we could take it a step further? What if our reranker was not just any reranker, but one tuned specifically to our domain or dataset? Could this specialization enhance the retrieval performance even more? To answer these questions, we turn to CohereAI‚Äôs beta release of fine-tuning reranker(Custom reranker) models. By integrating these with LlamaIndex, we now offer the ability to build your very own Cohere custom reranker using our streamlined process. In this blog post, we‚Äôll guide you through the steps to create a Cohere custom reranker with LlamaIndex and evaluate the retrieval performance. For a hands-on walkthrough, you can follow the tutorial on  Google Colab Notebook . Let‚Äôs start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex. N OTE: This is a guide for fine-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. \n",
      "\n",
      "\n",
      "\n",
      "============Error #5=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the Launch of Seven Advanced Retrieval LlamaPacks simplify the building of advanced RAG systems?\n",
      "\n",
      "Expected Contexts:\n",
      "We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. ü§©  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  \n",
      "\n",
      "Retrieved Contexts:\n",
      "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines\n",
      "Introduction We are seeing a huge wave of developers building Retrieval Augmented Generation (RAG) applications. The RAG tech stack generally contains a retrieval pipeline, LLM and prompt, among which LLM is accessible and developers are comfortable with prompt customization. However, developers new to search and index often need extensive help to build an effective  retrieval  pipeline. A production-ready retrieval pipeline typically consists of the following components: Document loader that parses and splits the long text Embedding model serving as core indexing component A vector database that stores the vector embeddings Advanced components to future optimize retrieval quality, such as re-ranker model to judge semantic similarity better It‚Äôs challenging to operate this complex tech stack. It involves managing software package dependencies, hosting services in Kubernetes clusters, and monitoring the performance of ML models. The high DevOps cost distracts developers from the most critical part of the user experience of RAG applications: prompt engineering, answer generation, and user interface. While experienced search infrastructure engineers may still manage a complicated tech stack for its flexibility, Zilliz believes that most RAG developers could benefit from a retrieval API service that is user-friendly and allows for lighter customization. Integrating  Zilliz Cloud Pipelines  and  LlamaIndex  brings a new approach to solving this problem. Zilliz Cloud Pipelines is a fully managed, scalable retrieval service. LlamaIndex is a flexible RAG framework that provides libraries and tools for organizing business logics such as retrieval and prompt engineering. The API service of Zilliz Cloud Pipelines is abstracted as a ManagedIndex in LlamaIndex. RAG developers using  ZillizCloudPipelineIndex  can easily scale the app from one user to millions of users without the hassle of setting up and maintaining the complex retrieval tech stack. It hides the technical complexity behind a few function calls, so that developers can focus on the core user experience of their RAG apps. In this blog, we show how to use  ZillizCloudPipelineIndex  to build a high quality RAG chatbot. The chatbot is scalable and supports multi-tenancy through metadata filtering. Set up Since Zilliz Cloud Pipelines is an API service, first you need to set up a  Zilliz Cloud  account and create a free serverless cluster. Now you can construct  ZillizCloudPipelineIndex  and get the handler to index docs and query later. from llama_index.indices import ZillizCloudPipelineIndex\n",
      "\n",
      "zcp_index = ZillizCloudPipelineIndex(\n",
      "    project_id=\" &lt; YOUR_ZILLIZ_PROJECT_ID &gt; \",\n",
      "    cluster_id=\" &lt; YOUR_ZILLIZ_CLUSTER_ID &gt; \",\n",
      "    token=\" &lt; YOUR_ZILLIZ_API_KEY &gt; \",\n",
      ")\n",
      "zcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"}) You can copy the Project ID, Cluster ID and API Key from your Zilliz account as follows: Ingest Documents Suppose your application has multiple users, and you would like to tag each user‚Äôs document to provide isolation. Your application logic can be implemented as follows. For simplicity, here we demo ingesting public documents. Currently, Zilliz Cloud Pipelines  supports  documents stored and managed in AWS S3 and Google Cloud Storage. Local document upload will also be supported soon. \n",
      "\n",
      "(We‚Äôve also launched a  new version of our website  ü¶ô!) RAG is Only as Good as your Data A core promise of LLMs is the ability to automate knowledge search, synthesis, extraction, and planning over any source of unstructured data. Over the past year a new data stack has emerged to power these context-augmented LLM applications, popularly referred to as Retrieval-Augmented Generation (RAG). This stack includes loading data, processing it, embedding it, and loading into a vector database. This enables downstream orchestration of retrieval and prompting to provide context within an LLM app. This stack is different from any ETL stack before it, because unlike traditional software, every decision in the data stack directly  affects the accuracy  of the full LLM-powered system. Every decision like chunk size and embedding model affects LLM outputs, and since LLMs are black boxes, you can‚Äôt unit test your way to correct behavior. We‚Äôve spent the past year hard at work at the forefront of providing tooling and educating users on how to build high-performing, advanced RAG for various use cases. We crossed the 2M monthly download mark, and are used by large enterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and Biases, DataStax, and many more. But while getting started with our famous 5-line starter example is easy, building production-grade RAG remains a complex and subtle problem. In our hundreds of user conversations, we learned the biggest pain points: Results aren‚Äôt accurate enough:  The application was not able to produce satisfactory results for a long-tail of input tasks/queries. The number of parameters to tune is overwhelming:  It‚Äôs not clear which parameters across the data parsing, ingestion, retrieval. PDFs are specifically a problem:  I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it? Data syncing is a challenge:  Production data often updates regularly, and continuously syncing new data brings a new set of challenges. These are the problems we set out to solve with LlamaCloud. Data Pipelines to Bring you to Production We built LlamaCloud and LlamaParse as the data pipelines to get your RAG application to production more quickly. LlamaParse LlamaParse is a state-of-the-art parser designed to specifically unlock RAG over complex PDFs with embedded tables and charts. This simply wasn‚Äôt possible before with other approaches, and we‚Äôre incredibly excited about this technology. LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document. For the past few months we‚Äôve been obsessed with this problem. \n",
      "\n",
      "# user1 ingests a document, it is technical documentation  for  v2 .3  version. \n",
      "zcp_index.insert_doc_url(\n",
      "    url= \"https://publicdataset.zillizcloud.com/milvus_doc.md\" ,\n",
      "    metadata={ \"user_id\" :  \"user1\" ,  \"version\" :  \"2.3\" },\n",
      ")\n",
      "# user2 ingests a document, it is technical documentation  for  v2 .2  version. \n",
      "zcp_index.insert_doc_url(\n",
      "    url= \"https://publicdataset.zillizcloud.com/milvus_doc_22.md\" ,\n",
      "    metadata={ \"user_id\" :  \"user2\" ,  \"version\" :  \"2.2\" },\n",
      ") Query To conduct semantic search with  ZillizCloudPipelineIndex , you can use it  as_query_engine()  by specifying a few parameters: search_top_k: How many text nodes/chunks to retrieve. Optional, defaults to DEFAULT_SIMILARITY_TOP_K (2). \n",
      "\n",
      "Define through the UI or our open-source library. Retrieval:  Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language. Playground:  Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop. LlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment. LlamaCloud Retrieval: Access advanced retrieval over your storage system via an API. We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you‚Äôre interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come  talk to us . Launch Partners and Collaborators We opened up access to LlamaParse at  our hackathon  we co-hosted with  Futureproof Labs  and  Datastax  at the beginning of February. We saw some incredible applications of LlamaParse in action,  including parsing building codes for Accessory Dwelling Unit (ADU) planning ,  parsing real-estate disclosures for home buying , and dozens more. Eric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into Mendable‚Äôs data stack: ‚ÄúWe integrated LlamaParse into our  open source data connector repo  which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.‚Äù We‚Äôre also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem, from storage to compute. DataStax Datastax has incorporated LlamaParse into their RAGStack to bring a privacy-preserving out-of-the-box RAG solution for enterprises: \"Last week one of our customers Imprompt has launched a pioneering 'Chat-to-Everything' platform leveraging RAGStack powered by LlamaIndex to enhance their enterprise offerings while prioritizing privacy.\" said Davor Bonaci, CTO and executive vice president, DataStax. \"We're thrilled to partner with LlamaIndex to bring a streamlined solution to market. By incorporating LlamaIndex into RAGStack, we are providing enterprise developers with a comprehensive Gen AI stack that simplifies the complexities of RAG implementation, while offering long-term support and compatibility assurance.‚Äù MongoDB ‚ÄúMongoDB‚Äôs partnership with LlamaIndex allows for the ingestion of data into the MongoDB Atlas Vector database, and the retrieval of the index from Atlas via LlamaParse and LlamaCloud, enabling the development of RAG systems and other AI applications,‚Äù said Greg Maxson, Global Lead, AI Ecosystems at MongoDB. ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline implementations, and more cost effectively develop large language model applications, ultimately accelerating generative AI app development and more quickly bringing apps to market.‚Äù Qdrant Andr√© Zayarni, CEO of Qdrant, says ‚ÄúThe Qdrant team is excited to partner with LlamaIndex to combine the power of optimal data preprocessing, vectorization, and ingestion with Qdrant for a powerful fullstack RAG solution.‚Äù NVIDIA We are also excited to collaborate with NVIDIA to integrate LlamaIndex with the  NVIDIA AI Enterprise  software platform for production AI: ‚ÄúLlamaCloud will help enterprises get generative AI applications from development into production with connectors that link proprietary data to the power of large language models,‚Äù said Justin Boitano, vice president, Enterprise and Edge Computing, NVIDIA. ‚ÄúPairing LlamaCloud with NVIDIA AI Enterprise can accelerate the end-to-end LLM pipeline ‚Äî including data processing, embedding creation, indexing, and model inference on accelerated computing across clouds, data centers and out to the edge.‚Äù FAQ Is this competitive with vector databases? No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are working hard to do the following: Integrate LlamaCloud with storage providers of existing design partners Make LlamaCloud available in a more ‚Äúself-serve‚Äù manner. Next Steps As mentioned in the above sections, LlamaParse is available for public preview starting today with a usage cap. LlamaCloud is in a private preview mode; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested come talk to us! LlamaParse:  Repo ,  Cookbook ,  Contact Us LlamaCloud:  Contact Us\n",
      "\n",
      "Shipping your Retrieval-Augmented Generation app to production with create-llama\n",
      "It‚Äôs a llama on a ship, geddit? Last week  we released create-llama , a command-line tool to generate a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). The response was fantastic, so we‚Äôll be following up with more templates and more features. \n",
      "\n",
      "\n",
      "\n",
      "============Error #6=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What enhancement was achieved in structured metadata extraction to boost RAG performance?\n",
      "\n",
      "Expected Contexts:\n",
      "We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. ü§©  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  \n",
      "\n",
      "Retrieved Contexts:\n",
      "Fine-Tuning Embeddings for RAG with Synthetic Data\n",
      "UPDATE 9/10/2023:  We‚Äôve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our  embedding fine-tuning guides  in the core documentation. We‚Äôve created a  comprehensive, end-to-end guide  showing you how to fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!). The result is a  5‚Äì10% performance increase in retrieval evaluation metrics  ‚Äî our finetuned  bge  model almost reaches  text-embedding-ada-002  levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole. This tutorial is helpful to  anyone  building RAG systems: If you‚Äôre new to finetuning, no problem! We have  step by step notebooks  walking through the key steps. Simply substitute the file links for your own data, and just run every cell. Finetuning embedding models is lightweight and doesn‚Äôt require a GPU. These notebooks were tested on an M2 Macbook Pro. Resources Repo:  https://github.com/run-llama/finetune-embedding Notebooks:  Dataset Generation ,  Finetuning ,  Evaluation Background/Context The Current RAG Stack RAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not present in its training corpus. It pairs a  retrieval model  over a knowledge bank with the LLM through its input prompt space. RAG stacks typically look like the following: Indexing : Prepare a corpus of unstructured text, parse/chunk it. Then  embed  each chunk and put in a vector database. Query-time:  Retrieve  context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space. (Of course RAG can be much more advanced than this, and LlamaIndex provides tools for both  simple and advanced RAG ) Unfortunately RAG is easy to prototype by cobbling together the different components, but hard to productionize. The simple stack has many failure modes and oftentimes the issue lies with bad retrieval ‚Äî if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad. How Can We Make Retrieval Better? \n",
      "\n",
      "More concretely, we first process the given documents into a corpus of text chunks. We do this with the  SimpleNodeParser  module in LlamaIndex: parser =  SimpleNodeParser ()\n",
      "nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
      "corpus = {\n",
      "  node. node_id:  node.get_content(metadata_mode= MetadataMode . NONE ) \n",
      "   for  node  in  nodes\n",
      "} Then for each text chunk, we use LLM to generate a few hypothetical questions that can be answered with information form that text chunk. The example prompt is shown below as well. prompt_template = prompt_template  or   \"\" \"\\\n",
      "  Context information is below.\n",
      "  \n",
      "  ---------------------\n",
      "  {context_str}\n",
      "  ---------------------\n",
      "  \n",
      "  Given the context information and not prior knowledge.\n",
      "  generate only questions based on the below query.\n",
      "  \n",
      "  You are a Teacher/ Professor. Your task is to setup \\\n",
      "  {num_questions_per_chunk} questions for an upcoming \\\n",
      "  quiz/examination. The questions should be diverse in nature \\\n",
      "  across the document. Restrict the questions to the \\\n",
      "  context information provided.\" \n",
      "   \"\" \"\n",
      "\n",
      "# for a given node, extract questions (do this over all nodes in outer loop)\n",
      "query = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n",
      "response = llm.complete(query)\n",
      "\n",
      "result = str(response).strip().split(\" \\n \")\n",
      "questions = [\n",
      "    re.sub(r\" ^\\d+[\\).\\s] \", \" \", question).strip() for question in result\n",
      "]\n",
      "questions = [question for question in questions if len(question) &gt; 0]\n",
      " Finally, we collect all pairs of questions and text chunks as the dataset. Example query, chunk, and mapping is shown below. \n",
      "\n",
      "\n",
      "It‚Äôs a  hit  if the results contain relevant_doc. InformationRetrievalEvaluator  from sentence_transformers. This provides a comprehensive suite of metrics such as cosine similarity accuracy, precision, recall at different top-k values. Results In terms of hit-rate metric, the base model gets 78% hit-rate on the validation dataset, and the fine-tuned model gets 84%.  text-embedding-ada-002  gets 87%, which means that our fine-tuned model is only 3% off! Hit-rate for `text-embedding-ada-002`, base model, finetuned model The InformationRetrievalEvaluator shows a similar improvement across an entire suite of metrics. The fine-tuned model increases evaluation metrics by 5‚Äì10% compared to the base-model. Evaluation suite from `InformationRetrievalEvaluator` Conclusion We successfully finetuned an embedding model over unlabeled, unstructured data to give better retrieval performance for downstream RAG systems. We show a 5‚Äì10% improvement across all metrics! Resources (copied from intro) Repo:  https://github.com/run-llama/finetune-embedding Notebooks:  Dataset Generation ,  Finetuning ,  Evaluation\n",
      "\n",
      "We can try more sophisticated retrieval algorithms (e.g. hybrid search, reranking). An  insight  from our recent  production RAG webinar , however, is that the embeddings themselves may not live in an optimal latent space for your data. Embeddings generated by pre-trained models may be close/far from each other based on the pre-training objective, but may not completely align with your own retrieval objective. For instance, if you‚Äôre building search over ML ArXiv papers, you may want the embeddings to align semantically with specific ML concepts (e.g. ‚ÄúLLMs‚Äù, ‚ÄúNLP‚Äù) and not filler words ‚ÄúThis paper is‚Ä¶‚Äù). Finetuning is a way to solve that. The concept of finetuning has become increasingly popular in the LLM space, with  technological advancements  as well as  easy-to-use services . In this tutorial, we focus on  finetuning the embedding model.  We show how finetuning the embedding model can lead to better retrieval performance. Challenges/Considerations When you finetune embeddings, you need training examples. In the case of embeddings, this typically means that you have both ‚Äúpositive‚Äù and ‚Äúnegative‚Äù examples ‚Äî pairs of texts that should be close to each other and far from each other. An issue is that we don‚Äôt have these positive or negative examples apriori. Given a dataset of unstructured text, is it possible to  automatically  generate these example pairs? With LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions from unstructured text chunks. These (question, chunk) pairs are then used as positive examples as training signals for the model (negative examples are randomly sampled across other chunks). The next section shows a full walkthrough across all of our modules. Walkthrough At a high-level, we do the following: Generating synthetic dataset for training and evaluation ( Notebook ) Finetuning an opensource embedding model ( Notebook ) Evaluating the embedding model ( Notebook ) Generating synthetic dataset for training and evaluation The key idea here is that we can leverage an LLM to generate hypothetical questions that are best answered by a given piece of context. This allows us to generate synthetic positive pairs of (query, relevant documents) in a scalable way without requiring human labellers. \n",
      "\n",
      "Build ParamTuner object \n",
      " ### 3. Execute hyperparameter tuning with ParamTuner.tune() \n",
      "\n",
      " # 1. Define objective function \n",
      " def   objective_function ( params_dict ):\n",
      "    chunk_size = params_dict[ \"chunk_size\" ]\n",
      "    docs = params_dict[ \"docs\" ]\n",
      "    top_k = params_dict[ \"top_k\" ]\n",
      "    eval_qs = params_dict[ \"eval_qs\" ]\n",
      "    ref_response_strs = params_dict[ \"ref_response_strs\" ]\n",
      "\n",
      "     # build RAG pipeline \n",
      "    index = _build_index(chunk_size, docs)   # helper function not shown here \n",
      "    query_engine = index.as_query_engine(similarity_top_k=top_k)\n",
      "  \n",
      "     # perform inference with RAG pipeline on a provided questions `eval_qs` \n",
      "    pred_response_objs = get_responses(\n",
      "        eval_qs, query_engine, show_progress= True \n",
      "    )\n",
      "\n",
      "     # perform evaluations of predictions by comparing them to reference \n",
      "     # responses `ref_response_strs` \n",
      "    evaluator = SemanticSimilarityEvaluator(...)\n",
      "    eval_batch_runner = BatchEvalRunner(\n",
      "        { \"semantic_similarity\" : evaluator}, workers= 2 , show_progress= True \n",
      "    )\n",
      "    eval_results = eval_batch_runner.evaluate_responses(\n",
      "        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n",
      "    )\n",
      "\n",
      "     # get semantic similarity metric \n",
      "    mean_score = np.array(\n",
      "        [r.score  for  r  in  eval_results[ \"semantic_similarity\" ]]\n",
      "    ).mean()\n",
      "\n",
      "     return  RunResult(score=mean_score, params=params_dict)\n",
      "\n",
      " # 2. Build ParamTuner object \n",
      "param_dict = { \"chunk_size\" : [ 256 ,  512 ,  1024 ]}  # params/values to search over \n",
      "fixed_param_dict = {  # fixed hyperparams \n",
      "   \"top_k\" :  2 ,\n",
      "     \"docs\" : docs,\n",
      "     \"eval_qs\" : eval_qs[: 10 ],\n",
      "     \"ref_response_strs\" : ref_response_strs[: 10 ],\n",
      "}\n",
      "param_tuner = ParamTuner(\n",
      "    param_fn=objective_function,\n",
      "    param_dict=param_dict,\n",
      "    fixed_param_dict=fixed_param_dict,\n",
      "    show_progress= True ,\n",
      ")\n",
      "\n",
      " # 3. Execute hyperparameter search \n",
      "results = param_tuner.tune()\n",
      "best_result = results.best_run_result\n",
      "best_chunk_size = results.best_run_result.params[ \"chunk_size\" ] 2. Structured External Knowledge:  In complex scenarios, it may be necessary to build your external knowledge with a bit more structure than the basic vector index so as to permit recursive retrievals or routed retrieval when dealing with sensibly separated external knowledge sources. LlamaIndex Recursive Retrieval Recipe  ( notebook guide ) : from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n",
      " from  llama_index.node_parser  import  SentenceSplitter\n",
      " from  llama_index.schema  import  IndexNode\n",
      "\n",
      " ### Recipe \n",
      " ### Build a recursive retriever that retrieves using small chunks \n",
      " ### but passes associated larger chunks to the generation stage \n",
      "\n",
      " # load data \n",
      "documents = SimpleDirectoryReader(\n",
      "  input_file= \"some_data_path/llama2.pdf\" \n",
      ").load_data()\n",
      "\n",
      " # build parent chunks via NodeParser \n",
      "node_parser = SentenceSplitter(chunk_size= 1024 )\n",
      "base_nodes = node_parser.get_nodes_from_documents(documents)\n",
      "\n",
      " # define smaller child chunks \n",
      "sub_chunk_sizes = [ 256 ,  512 ]\n",
      "sub_node_parsers = [\n",
      "    SentenceSplitter(chunk_size=c, chunk_overlap= 20 )  for  c  in  sub_chunk_sizes\n",
      "]\n",
      "all_nodes = []\n",
      " for  base_node  in  base_nodes:\n",
      "     for  n  in  sub_node_parsers:\n",
      "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
      "        sub_inodes = [\n",
      "            IndexNode.from_text_node(sn, base_node.node_id)  for  sn  in  sub_nodes\n",
      "        ]\n",
      "        all_nodes.extend(sub_inodes)\n",
      "     # also add original node to node \n",
      "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
      "    all_nodes.append(original_node)\n",
      "\n",
      " # define a VectorStoreIndex with all of the nodes \n",
      "vector_index_chunk = VectorStoreIndex(\n",
      "    all_nodes, service_context=service_context\n",
      ")\n",
      "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k= 2 )\n",
      "\n",
      " # build RecursiveRetriever \n",
      "all_nodes_dict = {n.node_id: n  for  n  in  all_nodes}\n",
      "retriever_chunk = RecursiveRetriever(\n",
      "     \"vector\" ,\n",
      "    retriever_dict={ \"vector\" : vector_retriever_chunk},\n",
      "    node_dict=all_nodes_dict,\n",
      "    verbose= True ,\n",
      ")\n",
      "\n",
      " # build RetrieverQueryEngine using recursive_retriever \n",
      "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
      "    retriever_chunk, service_context=service_context\n",
      ")\n",
      "\n",
      " # perform inference with advanced RAG (i.e. query engine) \n",
      "response = query_engine_chunk.query(\n",
      "     \"Can you tell me about the key concepts for safety finetuning\" \n",
      ") Other useful links We have several of guides demonstrating the application of other advanced techniques to help ensure accurate retrieval in complex cases. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in retrieval_eval_irrelevance_df.reset_index(drop=True).iterrows():\n",
    "    print(f\"\\n\\n============Error #{i+1}=============\\n\\n\")\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    expected_contexts = [json.loads(record.payload['_node_content'])['text'] for record in qdrantdb.retrieve(COLLECTION, ids=row.expected_ids)]\n",
    "    expected_contexts = '\\n\\n'.join(expected_contexts)\n",
    "    print(f\"Expected Contexts:\\n{expected_contexts}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.retrieved_texts)\n",
    "    print(f\"Retrieved Contexts:\\n{contexts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    "),\n",
    "# Below question is hard because LLM needs to follow the URL in the blog to get the information to answer\n",
    "(\"How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.\",\n",
    "\"\"\"\n",
    "The MemoryCache project by Mozilla aims to transform local desktop environments into on-device AI agents, utilizing PrivateGPT_AI and LlamaIndex to enhance personal knowledge management. It saves browser history and other local files to the user‚Äôs machine, allowing a local AI model to ingest and augment responses. This approach maintains privacy by avoiding cloud-based processing, focusing instead on generating insights from personal data. The project emphasizes creating a personalized AI experience that mirrors the original vision of personal computers as companions for thought.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_rag_dataset(response_eval_dataset, response_eval_prediction_dataset, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        # \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions),\n",
    "        total=len(response_eval_dataset.examples)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "        evals[\"contexts\"].append(prediction.contexts)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "        \"contexts\": evals['contexts'],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "        pd.Series(evals['contexts'], name='contexts')\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "# RESPONSE_EVAL_LLM_MODEL = 'gpt-4'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 1\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = min(len(documents), RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:56:19.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-25 12:56:58.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mCreating synthetic response eval dataset...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390d325e924948209e1295acb8e68df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.63s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.05s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.43s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.97s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.04s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.84s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.63s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.53s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.13it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.50s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.74s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.00s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.27it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.19s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.64s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.63s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.71it/s]\n",
      "\u001b[32m2024-07-25 12:57:47.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mPersisting synthetic response eval dataset at data/001/exp_007_semantic_chunking_full_refresh/llamaindex_blog_response_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=response_eval_llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        question_gen_query=QUESTION_GEN_QUERY,  # Reuse the same format from the above Retrieval Question Gen Query\n",
    "        show_progress=True,\n",
    "        workers=(os.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    logger.info(f\"Persisting synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "940feda2-1aad-427c-895f-2d4cdf43918f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:12<00:00,  1.26it/s]\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:10<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_prediction_dataset = await synthetic_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68f9f241bf84423b4ca740cca0525cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    synthetic_response_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>3.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   3.793103\n",
       "mean_relevancy_score     0.827586\n",
       "mean_faithfulness_score  0.793103"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the property graph index different from...</td>\n",
       "      <td>\\nThe property graph index is different from t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[In the previous integration, the graph was re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of using the SchemaLLMPath...</td>\n",
       "      <td>\\nThe purpose of using the SchemaLLMPathExtrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, let‚Äôs examine our retriever options. At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you accelerate the process of extracti...</td>\n",
       "      <td>\\nYou can accelerate the process of extracting...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Note the increased cost of $0.60 USD per page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you customize the property graph index...</td>\n",
       "      <td>\\nYou can customize the property graph index i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[O‚ÄôBrien expressed confidence that the dispute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What methods are used in the custom retriever ...</td>\n",
       "      <td>\\nThe custom retriever uses the `entity_extrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, let‚Äôs examine our retriever options. At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How many sections are included in the OpenAI C...</td>\n",
       "      <td>\\nThe OpenAI Cookbook for evaluating RAG syste...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does KOSMOS-2 contribute to the multi-moda...</td>\n",
       "      <td>\\nKOSMOS-2 contributes to the multi-modal prot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Building My Own ChatGPT Vision with PaLM, KOS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the application handle user interface...</td>\n",
       "      <td>\\nThe application does not explicitly discuss ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It's remarkable how seamless the integration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the application handle user interacti...</td>\n",
       "      <td>\\nThe application ensures an engaging and cont...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Your responses always descriptive. \" \\n      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How has the field of AI and large language mod...</td>\n",
       "      <td>\\nThe field of AI and large language models ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Notebook ,  Tweet . ‚úçÔ∏è Tutorials: Bhavesh Bha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of using Medium articles f...</td>\n",
       "      <td>\\nThe purpose of using Medium articles from 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How many vector stores are instantiated for te...</td>\n",
       "      <td>\\nTwo vector stores are instantiated for text ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We could use a vision model to filter out irr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does the integration between Create-llama ...</td>\n",
       "      <td>\\nThe integration between Create-llama and Lla...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Blogpost ,  Tweet . create-llama  Integrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of using LLMs for retrieva...</td>\n",
       "      <td>\\nThe purpose of using LLMs for retrieval and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the two ways of feeding text into the...</td>\n",
       "      <td>\\nDoc: 9, Relevance: 7\\nDoc: 3, Relevance: 4\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[A question is also provided.\\n  Respond with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the benefit of using LLMs for rerankin...</td>\n",
       "      <td>\\nLLMs can return more relevant documents than...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the length of the 2021 Lyft SEC 10-K d...</td>\n",
       "      <td>\\nThe 2021 Lyft SEC 10-K document is not expli...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It goes into details about the US-China trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of Tonic Validate in relat...</td>\n",
       "      <td>\\nTonic Validate is a RAG benchmarking and eva...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate x LlamaIndex: Implementing int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What steps are involved in setting up Tonic Va...</td>\n",
       "      <td>\\nTo set up Tonic Validate, you will need to f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I highly recommend utilizing the UI to make v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the purpose of setting up Tonic Validate?</td>\n",
       "      <td>\\nThe purpose of setting up Tonic Validate is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I highly recommend utilizing the UI to make v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the Tonic ValidateEvaluator score Lla...</td>\n",
       "      <td>\\nThe Tonic ValidateEvaluator scores LlamaInde...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[This creative approach to funding allowed the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the recommended approach for adding QA...</td>\n",
       "      <td>\\nThe recommended approach for adding QA testi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[As is common in modern software development p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How is LlamaIndex helping enterprises deploy g...</td>\n",
       "      <td>\\nBy integrating with NVIDIA NIM inference mic...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of the Llama Datasets coll...</td>\n",
       "      <td>\\nThe purpose of the Llama Datasets collection...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Two new llama-datasets and a Gemini vs. GPT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What new offering integrates local LLMs and em...</td>\n",
       "      <td>\\nLlamaCloud.\\n\\n\\nSources:\\n- [Introducing Ll...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[(We‚Äôve also launched a  new version of our we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Zephyr 7b LLM address the obstacles f...</td>\n",
       "      <td>\\nZephyr 7b LLM addresses the obstacles faced ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Becoming Proficient in Document Extraction\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the integration of Zephyr and LlamaIn...</td>\n",
       "      <td>\\nThe integration of Zephyr and LlamaIndex enh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does a tree index organize data?</td>\n",
       "      <td>\\nA tree index organizes data in a hierarchica...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[tree index is a data structure that organizes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the author's name of the document on b...</td>\n",
       "      <td>\\nThe author's name of the document on becomin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[If you find my content valuable, don‚Äôt hesita...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How is the property graph index different from...   \n",
       "1   What is the purpose of using the SchemaLLMPath...   \n",
       "2   How can you accelerate the process of extracti...   \n",
       "3   How can you customize the property graph index...   \n",
       "4   What methods are used in the custom retriever ...   \n",
       "5   How many sections are included in the OpenAI C...   \n",
       "6   How does KOSMOS-2 contribute to the multi-moda...   \n",
       "7   How does the application handle user interface...   \n",
       "8   How does the application handle user interacti...   \n",
       "9   How has the field of AI and large language mod...   \n",
       "10  What is the purpose of using Medium articles f...   \n",
       "11  How many vector stores are instantiated for te...   \n",
       "12  How does the integration between Create-llama ...   \n",
       "13  What is the purpose of using LLMs for retrieva...   \n",
       "14  What are the two ways of feeding text into the...   \n",
       "15  What is the benefit of using LLMs for rerankin...   \n",
       "16  What is the length of the 2021 Lyft SEC 10-K d...   \n",
       "17  What is the purpose of Tonic Validate in relat...   \n",
       "18  What steps are involved in setting up Tonic Va...   \n",
       "19  What is the purpose of setting up Tonic Validate?   \n",
       "20  How does the Tonic ValidateEvaluator score Lla...   \n",
       "21  What is the recommended approach for adding QA...   \n",
       "22  How is LlamaIndex helping enterprises deploy g...   \n",
       "23  What is the purpose of the Llama Datasets coll...   \n",
       "24  What new offering integrates local LLMs and em...   \n",
       "25  How does Zephyr 7b LLM address the obstacles f...   \n",
       "26  How does the integration of Zephyr and LlamaIn...   \n",
       "27               How does a tree index organize data?   \n",
       "28  What is the author's name of the document on b...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   \\nThe property graph index is different from t...              1.0   \n",
       "1   \\nThe purpose of using the SchemaLLMPathExtrac...              1.0   \n",
       "2   \\nYou can accelerate the process of extracting...              1.0   \n",
       "3   \\nYou can customize the property graph index i...              1.0   \n",
       "4   \\nThe custom retriever uses the `entity_extrac...              1.0   \n",
       "5   \\nThe OpenAI Cookbook for evaluating RAG syste...              1.0   \n",
       "6   \\nKOSMOS-2 contributes to the multi-modal prot...              1.0   \n",
       "7   \\nThe application does not explicitly discuss ...              0.0   \n",
       "8   \\nThe application ensures an engaging and cont...              1.0   \n",
       "9   \\nThe field of AI and large language models ha...              1.0   \n",
       "10  \\nThe purpose of using Medium articles from 20...              1.0   \n",
       "11  \\nTwo vector stores are instantiated for text ...              1.0   \n",
       "12  \\nThe integration between Create-llama and Lla...              0.0   \n",
       "13  \\nThe purpose of using LLMs for retrieval and ...              1.0   \n",
       "14  \\nDoc: 9, Relevance: 7\\nDoc: 3, Relevance: 4\\n...              1.0   \n",
       "15  \\nLLMs can return more relevant documents than...              1.0   \n",
       "16  \\nThe 2021 Lyft SEC 10-K document is not expli...              0.0   \n",
       "17  \\nTonic Validate is a RAG benchmarking and eva...              1.0   \n",
       "18  \\nTo set up Tonic Validate, you will need to f...              1.0   \n",
       "19  \\nThe purpose of setting up Tonic Validate is ...              1.0   \n",
       "20  \\nThe Tonic ValidateEvaluator scores LlamaInde...              1.0   \n",
       "21  \\nThe recommended approach for adding QA testi...              1.0   \n",
       "22  \\nBy integrating with NVIDIA NIM inference mic...              1.0   \n",
       "23  \\nThe purpose of the Llama Datasets collection...              1.0   \n",
       "24  \\nLlamaCloud.\\n\\n\\nSources:\\n- [Introducing Ll...              0.0   \n",
       "25  \\nZephyr 7b LLM addresses the obstacles faced ...              1.0   \n",
       "26  \\nThe integration of Zephyr and LlamaIndex enh...              1.0   \n",
       "27  \\nA tree index organizes data in a hierarchica...              1.0   \n",
       "28  \\nThe author's name of the document on becomin...              0.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 4.5                 1.0   \n",
       "1                 4.0                 1.0   \n",
       "2                 2.0                 1.0   \n",
       "3                 4.0                 1.0   \n",
       "4                 3.0                 1.0   \n",
       "5                 4.5                 1.0   \n",
       "6                 4.5                 1.0   \n",
       "7                 2.0                 0.0   \n",
       "8                 4.5                 1.0   \n",
       "9                 4.0                 1.0   \n",
       "10                3.5                 1.0   \n",
       "11                5.0                 1.0   \n",
       "12                3.0                 1.0   \n",
       "13                4.5                 1.0   \n",
       "14                3.0                 0.0   \n",
       "15                3.5                 1.0   \n",
       "16                1.0                 0.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.0                 1.0   \n",
       "19                3.0                 1.0   \n",
       "20                4.0                 0.0   \n",
       "21                4.5                 1.0   \n",
       "22                4.0                 1.0   \n",
       "23                4.0                 1.0   \n",
       "24                3.0                 0.0   \n",
       "25                4.5                 1.0   \n",
       "26                4.5                 1.0   \n",
       "27                4.5                 1.0   \n",
       "28                5.0                 0.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [In the previous integration, the graph was re...  \n",
       "1   [Now, let‚Äôs examine our retriever options. At ...  \n",
       "2   [Note the increased cost of $0.60 USD per page...  \n",
       "3   [O‚ÄôBrien expressed confidence that the dispute...  \n",
       "4   [Now, let‚Äôs examine our retriever options. At ...  \n",
       "5   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "6   [Building My Own ChatGPT Vision with PaLM, KOS...  \n",
       "7   [It's remarkable how seamless the integration ...  \n",
       "8   [Your responses always descriptive. \" \\n      ...  \n",
       "9   [Notebook ,  Tweet . ‚úçÔ∏è Tutorials: Bhavesh Bha...  \n",
       "10  [LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...  \n",
       "11  [We could use a vision model to filter out irr...  \n",
       "12  [Blogpost ,  Tweet . create-llama  Integrated ...  \n",
       "13  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "14  [A question is also provided.\\n  Respond with ...  \n",
       "15  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "16  [It goes into details about the US-China trade...  \n",
       "17  [Tonic Validate x LlamaIndex: Implementing int...  \n",
       "18  [I highly recommend utilizing the UI to make v...  \n",
       "19  [I highly recommend utilizing the UI to make v...  \n",
       "20  [This creative approach to funding allowed the...  \n",
       "21  [As is common in modern software development p...  \n",
       "22  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "23  [Two new llama-datasets and a Gemini vs. GPT s...  \n",
       "24  [(We‚Äôve also launched a  new version of our we...  \n",
       "25  [Becoming Proficient in Document Extraction\\nI...  \n",
       "26  [Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...  \n",
       "27  [tree index is a data structure that organizes...  \n",
       "28  [If you find my content valuable, don‚Äôt hesita...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14e8cb-d4fc-4e60-9c11-4af8fe304bb0",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52959440-a2c1-4ae9-9203-6f103789a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_response_eval_dataset_dict = dict()\n",
    "for example in synthetic_response_eval_dataset.examples:\n",
    "    synthetic_response_eval_dataset_dict[example.query] = {\n",
    "        \"reference_answer\": example.reference_answer,\n",
    "        \"reference_contexts\": example.reference_contexts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a36d56f-6210-42b1-a878-606b4f0e4c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the length of the 2021 Lyft SEC 10-K d...</td>\n",
       "      <td>\\nThe 2021 Lyft SEC 10-K document is not expli...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It goes into details about the US-China trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the application handle user interface...</td>\n",
       "      <td>\\nThe application does not explicitly discuss ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It's remarkable how seamless the integration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What new offering integrates local LLMs and em...</td>\n",
       "      <td>\\nLlamaCloud.\\n\\n\\nSources:\\n- [Introducing Ll...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[(We‚Äôve also launched a  new version of our we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does the integration between Create-llama ...</td>\n",
       "      <td>\\nThe integration between Create-llama and Lla...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Blogpost ,  Tweet . create-llama  Integrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the author's name of the document on b...</td>\n",
       "      <td>\\nThe author's name of the document on becomin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[If you find my content valuable, don‚Äôt hesita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you accelerate the process of extracti...</td>\n",
       "      <td>\\nYou can accelerate the process of extracting...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Note the increased cost of $0.60 USD per page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the two ways of feeding text into the...</td>\n",
       "      <td>\\nDoc: 9, Relevance: 7\\nDoc: 3, Relevance: 4\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[A question is also provided.\\n  Respond with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What methods are used in the custom retriever ...</td>\n",
       "      <td>\\nThe custom retriever uses the `entity_extrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, let‚Äôs examine our retriever options. At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the purpose of setting up Tonic Validate?</td>\n",
       "      <td>\\nThe purpose of setting up Tonic Validate is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I highly recommend utilizing the UI to make v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of using Medium articles f...</td>\n",
       "      <td>\\nThe purpose of using Medium articles from 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the benefit of using LLMs for rerankin...</td>\n",
       "      <td>\\nLLMs can return more relevant documents than...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the Tonic ValidateEvaluator score Lla...</td>\n",
       "      <td>\\nThe Tonic ValidateEvaluator scores LlamaInde...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[This creative approach to funding allowed the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of using the SchemaLLMPath...</td>\n",
       "      <td>\\nThe purpose of using the SchemaLLMPathExtrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, let‚Äôs examine our retriever options. At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you customize the property graph index...</td>\n",
       "      <td>\\nYou can customize the property graph index i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[O‚ÄôBrien expressed confidence that the dispute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How has the field of AI and large language mod...</td>\n",
       "      <td>\\nThe field of AI and large language models ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Notebook ,  Tweet . ‚úçÔ∏è Tutorials: Bhavesh Bha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What steps are involved in setting up Tonic Va...</td>\n",
       "      <td>\\nTo set up Tonic Validate, you will need to f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I highly recommend utilizing the UI to make v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How is LlamaIndex helping enterprises deploy g...</td>\n",
       "      <td>\\nBy integrating with NVIDIA NIM inference mic...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of the Llama Datasets coll...</td>\n",
       "      <td>\\nThe purpose of the Llama Datasets collection...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Two new llama-datasets and a Gemini vs. GPT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the property graph index different from...</td>\n",
       "      <td>\\nThe property graph index is different from t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[In the previous integration, the graph was re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How many sections are included in the OpenAI C...</td>\n",
       "      <td>\\nThe OpenAI Cookbook for evaluating RAG syste...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does KOSMOS-2 contribute to the multi-moda...</td>\n",
       "      <td>\\nKOSMOS-2 contributes to the multi-modal prot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Building My Own ChatGPT Vision with PaLM, KOS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the application handle user interacti...</td>\n",
       "      <td>\\nThe application ensures an engaging and cont...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Your responses always descriptive. \" \\n      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of using LLMs for retrieva...</td>\n",
       "      <td>\\nThe purpose of using LLMs for retrieval and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of Tonic Validate in relat...</td>\n",
       "      <td>\\nTonic Validate is a RAG benchmarking and eva...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate x LlamaIndex: Implementing int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the recommended approach for adding QA...</td>\n",
       "      <td>\\nThe recommended approach for adding QA testi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[As is common in modern software development p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Zephyr 7b LLM address the obstacles f...</td>\n",
       "      <td>\\nZephyr 7b LLM addresses the obstacles faced ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Becoming Proficient in Document Extraction\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the integration of Zephyr and LlamaIn...</td>\n",
       "      <td>\\nThe integration of Zephyr and LlamaIndex enh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does a tree index organize data?</td>\n",
       "      <td>\\nA tree index organizes data in a hierarchica...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[tree index is a data structure that organizes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How many vector stores are instantiated for te...</td>\n",
       "      <td>\\nTwo vector stores are instantiated for text ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We could use a vision model to filter out irr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "16  What is the length of the 2021 Lyft SEC 10-K d...   \n",
       "7   How does the application handle user interface...   \n",
       "24  What new offering integrates local LLMs and em...   \n",
       "12  How does the integration between Create-llama ...   \n",
       "28  What is the author's name of the document on b...   \n",
       "2   How can you accelerate the process of extracti...   \n",
       "14  What are the two ways of feeding text into the...   \n",
       "4   What methods are used in the custom retriever ...   \n",
       "19  What is the purpose of setting up Tonic Validate?   \n",
       "10  What is the purpose of using Medium articles f...   \n",
       "15  What is the benefit of using LLMs for rerankin...   \n",
       "20  How does the Tonic ValidateEvaluator score Lla...   \n",
       "1   What is the purpose of using the SchemaLLMPath...   \n",
       "3   How can you customize the property graph index...   \n",
       "9   How has the field of AI and large language mod...   \n",
       "18  What steps are involved in setting up Tonic Va...   \n",
       "22  How is LlamaIndex helping enterprises deploy g...   \n",
       "23  What is the purpose of the Llama Datasets coll...   \n",
       "0   How is the property graph index different from...   \n",
       "5   How many sections are included in the OpenAI C...   \n",
       "6   How does KOSMOS-2 contribute to the multi-moda...   \n",
       "8   How does the application handle user interacti...   \n",
       "13  What is the purpose of using LLMs for retrieva...   \n",
       "17  What is the purpose of Tonic Validate in relat...   \n",
       "21  What is the recommended approach for adding QA...   \n",
       "25  How does Zephyr 7b LLM address the obstacles f...   \n",
       "26  How does the integration of Zephyr and LlamaIn...   \n",
       "27               How does a tree index organize data?   \n",
       "11  How many vector stores are instantiated for te...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "16  \\nThe 2021 Lyft SEC 10-K document is not expli...              0.0   \n",
       "7   \\nThe application does not explicitly discuss ...              0.0   \n",
       "24  \\nLlamaCloud.\\n\\n\\nSources:\\n- [Introducing Ll...              0.0   \n",
       "12  \\nThe integration between Create-llama and Lla...              0.0   \n",
       "28  \\nThe author's name of the document on becomin...              0.0   \n",
       "2   \\nYou can accelerate the process of extracting...              1.0   \n",
       "14  \\nDoc: 9, Relevance: 7\\nDoc: 3, Relevance: 4\\n...              1.0   \n",
       "4   \\nThe custom retriever uses the `entity_extrac...              1.0   \n",
       "19  \\nThe purpose of setting up Tonic Validate is ...              1.0   \n",
       "10  \\nThe purpose of using Medium articles from 20...              1.0   \n",
       "15  \\nLLMs can return more relevant documents than...              1.0   \n",
       "20  \\nThe Tonic ValidateEvaluator scores LlamaInde...              1.0   \n",
       "1   \\nThe purpose of using the SchemaLLMPathExtrac...              1.0   \n",
       "3   \\nYou can customize the property graph index i...              1.0   \n",
       "9   \\nThe field of AI and large language models ha...              1.0   \n",
       "18  \\nTo set up Tonic Validate, you will need to f...              1.0   \n",
       "22  \\nBy integrating with NVIDIA NIM inference mic...              1.0   \n",
       "23  \\nThe purpose of the Llama Datasets collection...              1.0   \n",
       "0   \\nThe property graph index is different from t...              1.0   \n",
       "5   \\nThe OpenAI Cookbook for evaluating RAG syste...              1.0   \n",
       "6   \\nKOSMOS-2 contributes to the multi-modal prot...              1.0   \n",
       "8   \\nThe application ensures an engaging and cont...              1.0   \n",
       "13  \\nThe purpose of using LLMs for retrieval and ...              1.0   \n",
       "17  \\nTonic Validate is a RAG benchmarking and eva...              1.0   \n",
       "21  \\nThe recommended approach for adding QA testi...              1.0   \n",
       "25  \\nZephyr 7b LLM addresses the obstacles faced ...              1.0   \n",
       "26  \\nThe integration of Zephyr and LlamaIndex enh...              1.0   \n",
       "27  \\nA tree index organizes data in a hierarchica...              1.0   \n",
       "11  \\nTwo vector stores are instantiated for text ...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "16                1.0                 0.0   \n",
       "7                 2.0                 0.0   \n",
       "24                3.0                 0.0   \n",
       "12                3.0                 1.0   \n",
       "28                5.0                 0.0   \n",
       "2                 2.0                 1.0   \n",
       "14                3.0                 0.0   \n",
       "4                 3.0                 1.0   \n",
       "19                3.0                 1.0   \n",
       "10                3.5                 1.0   \n",
       "15                3.5                 1.0   \n",
       "20                4.0                 0.0   \n",
       "1                 4.0                 1.0   \n",
       "3                 4.0                 1.0   \n",
       "9                 4.0                 1.0   \n",
       "18                4.0                 1.0   \n",
       "22                4.0                 1.0   \n",
       "23                4.0                 1.0   \n",
       "0                 4.5                 1.0   \n",
       "5                 4.5                 1.0   \n",
       "6                 4.5                 1.0   \n",
       "8                 4.5                 1.0   \n",
       "13                4.5                 1.0   \n",
       "17                4.5                 1.0   \n",
       "21                4.5                 1.0   \n",
       "25                4.5                 1.0   \n",
       "26                4.5                 1.0   \n",
       "27                4.5                 1.0   \n",
       "11                5.0                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "16  [It goes into details about the US-China trade...  \n",
       "7   [It's remarkable how seamless the integration ...  \n",
       "24  [(We‚Äôve also launched a  new version of our we...  \n",
       "12  [Blogpost ,  Tweet . create-llama  Integrated ...  \n",
       "28  [If you find my content valuable, don‚Äôt hesita...  \n",
       "2   [Note the increased cost of $0.60 USD per page...  \n",
       "14  [A question is also provided.\\n  Respond with ...  \n",
       "4   [Now, let‚Äôs examine our retriever options. At ...  \n",
       "19  [I highly recommend utilizing the UI to make v...  \n",
       "10  [LlamaIndex Newsletter 2023‚Äì12‚Äì12\\nHowdy, Llam...  \n",
       "15  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "20  [This creative approach to funding allowed the...  \n",
       "1   [Now, let‚Äôs examine our retriever options. At ...  \n",
       "3   [O‚ÄôBrien expressed confidence that the dispute...  \n",
       "9   [Notebook ,  Tweet . ‚úçÔ∏è Tutorials: Bhavesh Bha...  \n",
       "18  [I highly recommend utilizing the UI to make v...  \n",
       "22  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "23  [Two new llama-datasets and a Gemini vs. GPT s...  \n",
       "0   [In the previous integration, the graph was re...  \n",
       "5   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "6   [Building My Own ChatGPT Vision with PaLM, KOS...  \n",
       "8   [Your responses always descriptive. \" \\n      ...  \n",
       "13  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "17  [Tonic Validate x LlamaIndex: Implementing int...  \n",
       "21  [As is common in modern software development p...  \n",
       "25  [Becoming Proficient in Document Extraction\\nI...  \n",
       "26  [Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...  \n",
       "27  [tree index is a data structure that organizes...  \n",
       "11  [We could use a vision model to filter out irr...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df.sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e56956b8-7c12-4548-b913-ae3bc0f77fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============Error #1===============\n",
      "\n",
      "\n",
      "Query:\n",
      "What is the length of the 2021 Lyft SEC 10-K document?\n",
      "\n",
      "Context:\n",
      "It goes into details about the US-China trade war, slowing economic growth, Brexit, and more (keep in mind 2019 is pre-COVID). Token Usage and Latency The document contains around ~170K tokens. For some reason, this number is not reflected on the Anthropic usage logs (the ‚ÄúPrompt Tokens‚Äù section seems capped at 10240). But the Prompt Length (in characters) is logged, as well as the model latency. Given the pricing, ~170K tokens would be equivalent to $1.5‚Äì2 USD. A query through one Uber SEC-10K takes around  150   seconds , including all LLM calls. This is actually a bit faster than repeated calls to ChatGPT/davinci. Each ChatGPT/davinci call (with the 4K token window maximized), empirically can take 6‚Äì10 seconds to complete ‚Üí  125‚Äì250 seconds ( or more). Analyzing Multiple Documents A popular example in our  previous blog post  was showcasing that you could compare/contrast different documents with LlamaIndex graph structures. We test whether we can do that here as well, by feeding in multiple SEC reports into Claude-v1 100k. Caveat:  Considering that one UBER SEC-10K filing doesn‚Äôt even fit in the context window, we‚Äôll of course also need to implement response synthesis strategies in order to handle ingesting multiple 10K filings. We build a list index over all 4 10K filings: 2019, 2020, 2021, and 2022. list_index = GPTListIndex.from_documents(all_docs, service_context=service_context)\n",
      " print ( len (list_index.index_struct.nodes)) We then ask our question using our Tree Summarize response mode. query = \"How are the risk factors changing across years? Compare/contrast the risk factors across the SEC filings.\"\n",
      "query_engine = list_index.as_query_engine(response_mode=\"tree_summarize\")\n",
      "response = query_engine.query(query) The full answer is given below: The risk factors disclosed in Uber ' s SEC filings have evolved over time based on Uber ' s business  and  industry changes. Some of the key differences in risk factors across the filings are:\n",
      "\n",
      " 2017   10 -K:\n",
      "- Focused heavily on risks related to negative publicity, competition, dependence on independent contractors,  and  regulatory challenges as Uber was still facing backlash from various PR crises  and  regulatory pushback. \n",
      "- Also highlighted risks from intellectual property litigation given various IP disputes at the time.\n",
      "\n",
      " 2018   10 -K:\n",
      "- Added more risks related to autonomous vehicles as Uber ramped up its self-driving car efforts. Specifically called out risks from accidents, technical challenges,  and  competition in the AV space.\n",
      "\n",
      "\n",
      "This means no embeddings, and no fancy retrieval mechanisms. Ideally, we can directly insert an entire 10-k filing (or even all four 10-k filings) into the prompt. However, we found that a single UBER 10-k filing actually consists of ~ 160k tokens, which is greater than the 100k context window.  This means that we still have to chunk up each filing! We end up using our  list index data structure  ‚Äî we split each text up into massive ~100k token chunks, and use our  response synthesis strategies  to synthesize an answer across multiple chunks. We run some queries over each filing as well as over multiple filings, similar to our original blog post. \n",
      "\n",
      "Answer:\n",
      "\n",
      "The 2021 Lyft SEC 10-K document is not explicitly mentioned in the provided context. However, the context does mention the 2019, 2020, 2021, and 2022 Uber SEC-10K filings.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Testing Anthropic Claude‚Äôs 100k-token window on SEC 10-K Filings](https://www.llamaindex.ai/blog/testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "The 2021 Lyft SEC 10-K document is 238 pages long.\n",
      "\n",
      "Expected Contexts:\n",
      "['The top retrieved contexts are shown in the images below. We see that in embedding-based retrieval, the top two texts contain semantics of the car crash but give no details as to who was actually responsible. Only the third text contains the proper answer. Retrieved context using top-k embedding lookup (baseline) In contrast, the two-stage approach returns just one relevant context, and it contains the correct answer. Retrieved context using two-stage pipeline (embedding lookup then rerank) 2021 Lyft SEC 10-K We want to ask some questions over the 2021 Lyft SEC 10-K, specifically about the COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a ctrl-f for ‚ÄúCOVID-19‚Äù returns 127 matches. We use a similar setup as the Gatsby example above. The main differences are that we set the chunk size to 128 instead of 512, we set k=5 for the embedding retrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage approach. We then ask the following questions and analyze the results. Question: ‚ÄúWhat initiatives are the company focusing on independently of COVID-19?‚Äù Results for the baseline are shown in the image above. We see that results corresponding to indices 0, 1, 3, 4, are about measures directly in response to Covid-19, even though the question was specifically about company initiatives that were independent of the COVID-19 pandemic. Retrieved context using top-k embedding lookup (baseline) We get more relevant results in approach 2, by widening the top-k to 40 and then using an LLM to filter for the top-5 contexts. The independent company initiatives include ‚Äúexpansion of Light Vehicles‚Äù (1), ‚Äúincremental investments in brand/marketing‚Äù (2), international expansion (3), and accounting for misc. risks such as natural disasters and operational risks in terms of financial performance (4). Retrieved context using two-stage pipeline (embedding lookup then rerank) Conclusion That‚Äôs it for now! We‚Äôve added some initial functionality to help support LLM-augmented retrieval pipelines, but of course there‚Äôs a ton of future steps that we couldn‚Äôt quite get to. Some questions we‚Äôd love to explore: How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.) What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance. Exploring different prompts and text summarization methods to help determine document relevance Exploring if there‚Äôs a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?) Resources You can play around with the notebooks yourself! Great Gatsby Notebook 2021 Lyft 10-K Notebook']\n",
      "\n",
      "\n",
      "\n",
      "==============Error #2===============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the application handle user interface design?\n",
      "\n",
      "Context:\n",
      "It's remarkable how seamless the integration feels, and it's obvious that Apple has invested a lot in refining the experience.\"},\n",
      "\n",
      "    # SamsungTV Reviews\n",
      "    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"Samsung's display technology has always been at the forefront, but with this TV, they've outdone themselves. Every visual is crisp, the colors are vibrant, and the depth of the blacks is simply mesmerizing. The smart features only add to the luxurious viewing experience.\"},\n",
      "    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"This isn't just a TV; it's a centerpiece for the living room. The ultra-slim bezels and the sleek design make it a visual treat even when it's turned off. And when it's on, the 4K resolution delivers a cinematic experience right at home.\"},\n",
      "    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"The sound quality, often an oversight in many TVs, matches the visual prowess. It creates an enveloping atmosphere that's hard to get without an external sound system. Combined with its user-friendly interface, it's the TV I've always dreamt of.\"},\n",
      "\n",
      "    # Ergonomic Chair Reviews\n",
      "    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"Shifting to this ergonomic chair was a decision I wish I'd made earlier. Not only does it look sophisticated in its design, but the level of comfort is unparalleled. Long hours at the desk now feel less daunting, and my back is definitely grateful.\"},\n",
      "    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"The meticulous craftsmanship of this chair is evident. Every component, from the armrests to the wheels, feels premium. The adjustability features mean I can tailor it to my needs, ensuring optimal posture and comfort throughout the day.\"},\n",
      "    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"I was initially drawn to its aesthetic appeal, but the functional benefits have been profound. The breathable material ensures no discomfort even after prolonged use, and the robust build gives me confidence that it's a chair built to last.\"},\n",
      "] Setting up an In-Memory Database To process our data, we‚Äôre using an in-memory SQLite database. SQLAlchemy provides an efficient way to model, create, and interact with this database. Here‚Äôs how our  product_reviews  table structure looks: id  (Integer, Primary Key) category  (String) product_name  (String) review  (String, Not Null) Once we‚Äôve defined our table structure, we populate it with our sample dataset. engine = create_engine( \"sqlite:///:memory:\" )\n",
      "metadata_obj = MetaData()\n",
      "\n",
      " # create product reviews SQL table \n",
      "table_name =  \"product_reviews\" \n",
      "city_stats_table = Table(\n",
      "    table_name,\n",
      "    metadata_obj,\n",
      "    Column( \"id\" , Integer(), primary_key= True ),\n",
      "    Column( \"category\" , String( 16 ), primary_key= True ),\n",
      "    Column( \"product_name\" , Integer),\n",
      "    Column( \"review\" , String( 16 ), nullable= False )\n",
      ")\n",
      "metadata_obj.create_all(engine)\n",
      "\n",
      "sql_database = SQLDatabase(engine, include_tables=[ \"product_reviews\" ])\n",
      "\n",
      " for  row  in  rows:\n",
      "    stmt = insert(city_stats_table).values(**row)\n",
      "     with  engine.connect()  as  connection:\n",
      "        cursor = connection.execute(stmt)\n",
      "        connection.commit() Analysing Product Reviews ‚Äî Text2SQL + RAG Deriving insights from data often requires intricate questioning. SQL + RAG in LlamaIndex simplifies this by breaking it into a three-step process: Decomposition of the Question: Primary Query Formation: Frame the main question in natural language to extract preliminary data from the SQL table. Secondary Query Formation: Construct an auxiliary question to refine or interpret the results of the primary query. 2. Data Retrieval : Run the primary query using the Text2SQL LlamaIndex module to obtain the initial set of results. 3. Final Answer Generation:  Use List Index to further refine the results based on the secondary question, leading to the conclusive answer. Let‚Äôs start doing it step by step. \n",
      "\n",
      "Let‚Äôs delve into these experiments. You can also follow along with this blog post in our  Google Colab Notebook . N OTE: This blog post aims to inform the community about GPT-4V‚Äôs performance, though the results might not be universally applicable. We strongly advise conducting tests with similar questions on your own dataset before drawing conclusions. Experiment: What we‚Äôll do is take an image of each of a bar chart, scatter plot, and table and analyze them by asking three types of questions. General Question: Simply ask, ‚ÄúAnalyse the image.‚Äù Specific Question: Specific Question the performance of a certain category by providing more details. Chain of Thought Prompting: Use a step-by-step reasoning method to walk through the analysis. These guidelines aim to test how different questioning techniques might improve the precision of the information we gather from the images. Data for experiments: To test out we selected the following images from  Llama2  and  MistralAI  papers. Violation percentage of safety with different LLMs across categories. (Llama2 paper) Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper) Performances of different LLMs across various NLP tasks. (Llama2 paper) Let‚Äôs go through each of these data points one by one. Image 1 ‚Äî Violation percentage of safety with different LLMs across categories. General Question: Question:  Analyse the Image. Answer: The image you‚Äôve provided is a bar chart displaying data grouped into three categories of online violations: hateful and harmful, illicit and criminal activity, and unqualified advice. Each of these categories has a number in parentheses, indicating the sample size of reported incidents for each type of violation (395, 728, and 311, respectively). The x-axis of the chart specifies various social media platforms or services, such as ‚ÄúVideo sharing‚Äù, ‚ÄúSocial networking‚Äù, ‚ÄúGaming‚Äù, ‚ÄúDating‚Äù, ‚ÄúForums & boards‚Äù, ‚ÄúCommercial Websites‚Äù, ‚ÄúMedia sharing‚Äù, ‚ÄúP2P/File sharing‚Äù, ‚ÄúWiki‚Äù, and ‚ÄúOther‚Äù. It appears to measure how often these categories of violations occur on each type of platform. The y-axis measures the percentage of violations reported, ranging from 0% to 60%. Each platform/service has three bars corresponding to the three violation categories, showing their respective percentages. The bars also have error bars, which typically represent the variability of the data, such as standard deviation, standard error, or confidence interval, indicating the precision of the estimates. The chart has a legend indicating the color corresponding to each of the three categories of violations. This visual representation helps to compare the prevalence of different types of violations across the different types of online services. Without specific numbers, it‚Äôs not possible to provide exact percentages, but we can observe trends, such as: ‚Äî ‚ÄúForums & boards‚Äù and ‚ÄúSocial networking‚Äù platforms have notably higher percentages across all three types of violations compared to other platforms. ‚Äî ‚ÄúCommercial Websites‚Äù seem to have lower percentages of reported hateful and harmful activities and unqualified advice but higher percentages of illicit and criminal activities when compared to other platforms like ‚ÄúWiki‚Äù or ‚ÄúP2P/File sharing‚Äù. ‚Äî ‚ÄúGaming‚Äù appears to have a moderate percentage of hateful and harmful violations, lower levels of illicit and criminal activity, and relatively higher levels of unqualified advice. Overall, this chart is a visual tool that likely aims to inform about the prevalence of certain types of online violations across different digital platforms. Observation: As you can see though the categories hateful and harmful, illicit and criminal activity, and unqualified advice but it hallucinated with x-axis values with ‚Äî ‚ÄúVideo sharing‚Äù, ‚ÄúSocial networking‚Äù, ‚ÄúGaming‚Äù, ‚ÄúDating‚Äù, ‚ÄúForums & boards‚Äù, ‚ÄúCommercial Websites‚Äù, ‚ÄúMedia sharing‚Äù, ‚ÄúP2P/File sharing‚Äù, ‚ÄúWiki‚Äù, and ‚ÄúOther‚Äù. Specific Questions: Question:  Which model among llama2 and vicuna models does better in terms of violation percentages in Hateful and harmful category. Answer: From the provided bar chart, we can analyze the violation percentage for both the Llama2 and Vicuna models in the ‚ÄúHateful and harmful‚Äù category. To determine which model does better (i.e., has a lower violation percentage), you would look at the height of the bars corresponding to each model within that category. In the ‚ÄúHateful and harmful‚Äù category (which is the first group of bars on the left), you can compare the blue bar (representing Llama2) to the light blue bar (representing Vicuna). The model with the shorter bar in this category will have a lower violation percentage, and hence, better performance with respect to minimizing hateful and harmful content. Please note that I cannot physically see the chart, so I‚Äôm unable to specify which model has the lower violation percentage. If you provide the actual percentages or describe the relative bar lengths for Llama2 and Vicuna in the ‚ÄúHateful and harmful‚Äù category, I could then tell you which model performs better in that respect. Observation: It failed to accurately capture the information, mistakenly identifying the light blue bar as representing Vicuna when, in fact, it is the light blue bar that represents Llama2. Let us give more information on the question and see if it can improve the answer. Question:  In the image provided to you depicts about the violation rate performance of various AI models across Hateful and harmful, Illicit and criminal activity, Unqualified advice categories. Hateful and harmful category is in first column. Bars with light blue are with Llama2 model and dark blue are with Vicuna models. With this information, Can you compare about Llama2 and Vicuna models in Hateful and harmful category. Answer: In the image you provided, which shows a bar graph for the violation rate performance of various AI models in the Hateful and harmful category, we can analyze the light blue bars that represent the Llama2 model and the dark blue bars that represent the Vicuna model. Based on the visual information given, I can compare the two models within this category by observing the heights of the light blue bars (Llama2) versus the heights of the dark blue bars (Vicuna) for each subsection within the category. A lower bar indicates a lower violation rate, which could be interpreted as better performance in minimizing violations for the given criteria. Since I can‚Äôt give you the exact numbers or percentages, I would describe their relative performances. It appears that for some subsections, the Llama2 bars are shorter than the Vicuna bars, suggesting that the Llama2 model could have a lower violation rate in those areas. Conversely, in other subsections, the Vicuna bars might be shorter than the Llama2 bars, indicating a lower violation rate for the Vicuna model in those areas. The exact subsections where one model outperforms the other would depend on their relative bar heights, which should be compared individually for the given information. Observation: It did answer the question correctly. \n",
      "\n",
      "Answer:\n",
      "\n",
      "The application does not explicitly discuss the user interface design. The provided context is focused on the integration of natural language processing with SQL query execution for text analysis and summarization, as well as experiments with GPT-4V and its capabilities in interpreting visual data. There is no mention of user interface design in the given context.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews](https://www.llamaindex.ai/blog/llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b)\n",
      "- [GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.](https://www.llamaindex.ai/blog/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "The application handles user interface design by crafting a sidebar that displays information about the creator of the app and provides access to other projects for credibility and engagement. Additionally, the application ensures an engaging and controlled user experience by limiting the number of messages to 20 per session in the demo version. If this limit is reached, it gracefully notifies the user and disables further input to manage resources effectively. When the message count is within the limit, the application provides a clear chat option and handles the image upload process, processing the image to get a caption, setting up the chat engine, and updating the user interface to reflect the successful upload.\n",
      "\n",
      "Expected Contexts:\n",
      "['You can upload an image and start chatting with the LLM about the image\" ) 2. User Interface Next, we craft the sidebar and the main area, ensuring that the user knows who created the app and has access to other projects, enhancing credibility and engagement. # Sidebar\\nst.sidebar.markdown(\\'## Created By\\')\\nst.sidebar.markdown(\"[Harshad Suryawanshi](https://www.linkedin.com/in/harshadsuryawanshi/)\")\\nst.sidebar.markdown(\\'## Other Projects\\')\\n# ...sidebar content continues 3. Image Upload and Processing Upon uploading an image, the app not only displays it but also invokes the  get_image_caption  function to generate a relevant caption. This function, decorated with  @st.cache for caching, uses the KOSMOS-2 model through Replicate to provide a brief description of the uploaded image. The description is then used as the basis for the initial conversation with the user. @st.cache \\n def   get_image_caption ( image_data ):\\n    input_data = {\\n         \"image\" : image_data,\\n         \"description_type\" :  \"Brief\" \\n    }\\n    output = replicate.run(\\n         \"lucataco/kosmos-2:3e7b211c29c092f4bcc8853922cc986baa52efe255876b80cac2c2fbb4aff805\" ,\\n         input =input_data\\n    )\\n     # Split the output string on the newline character and take the first item \\n    text_description = output.split( \\'\\\\n\\\\n\\' )[ 0 ]\\n     return  text_description 4. Conversational Flow with PaLM and Llamaindex With the image caption in hand, the  create_chat_engine  function is called to set up the chat engine. This function is crucial as it establishes the context for the conversation and initializes the PaLM API for interaction. @st.cache_resource \\n def   create_chat_engine ( img_desc, api_key ):\\n    llm = PaLM(api_key=api_key)\\n    service_context = ServiceContext.from_defaults(llm=llm)\\n    doc = Document(text=img_desc)\\n    index = VectorStoreIndex.from_documents([doc], service_context=service_context)\\n    chatmemory = ChatMemoryBuffer.from_defaults(token_limit= 1500 )\\n    \\n    chat_engine = index.as_chat_engine(\\n        chat_mode= \"context\" ,\\n        system_prompt=(\\n             f\"You are a chatbot, able to have normal interactions, as well as talk. \" \\n             \"You always answer in great detail and are polite. Your responses always descriptive. \" \\n             \"Your job is to talk about an image the user has uploaded. Image description: {img_desc}.\" \\n        ),\\n        verbose= True ,\\n        memory=chatmemory\\n    )\\n     return  chat_engine The  create_chat_engine  function builds the infrastructure for our app\\'s conversation capabilities. It starts by instantiating a PaLM object with the provided API key, setting up the service context, and creating a document with the image description. This document is then indexed to prepare it for Llamaindex‚Äôs context chat engine. Finally, the chat engine is configured with a prompt that instructs the AI on how to engage in the conversation, referencing the image description and defining the chatbot\\'s behavior. 5. User Interaction and Message Handling The application ensures an engaging and controlled user experience by limiting the number of messages to 20 per session in the demo version. If this limit is reached, it gracefully notifies the user and disables further input to manage resources effectively. if  message_count &gt;=  20 :\\n    st.error( \"Notice: The maximum message limit for this demo version has been reached.\" )\\n     # Disabling the uploader and input by not displaying them \\n    image_uploader_placeholder = st.empty()   # Placeholder for the uploader \\n    chat_input_placeholder = st.empty()       # Placeholder for the chat input However, when the message count is within the limit, the application provides a clear chat option and handles the image upload process. Upon uploading, it immediately processes the image to get a caption, sets up the chat engine, and updates the user interface to reflect the successful upload.']\n",
      "\n",
      "\n",
      "\n",
      "==============Error #3===============\n",
      "\n",
      "\n",
      "Query:\n",
      "What new offering integrates local LLMs and embeddings into a fully local RAG pipeline?\n",
      "\n",
      "Context:\n",
      "(We‚Äôve also launched a  new version of our website  ü¶ô!) RAG is Only as Good as your Data A core promise of LLMs is the ability to automate knowledge search, synthesis, extraction, and planning over any source of unstructured data. Over the past year a new data stack has emerged to power these context-augmented LLM applications, popularly referred to as Retrieval-Augmented Generation (RAG). This stack includes loading data, processing it, embedding it, and loading into a vector database. This enables downstream orchestration of retrieval and prompting to provide context within an LLM app. This stack is different from any ETL stack before it, because unlike traditional software, every decision in the data stack directly  affects the accuracy  of the full LLM-powered system. Every decision like chunk size and embedding model affects LLM outputs, and since LLMs are black boxes, you can‚Äôt unit test your way to correct behavior. We‚Äôve spent the past year hard at work at the forefront of providing tooling and educating users on how to build high-performing, advanced RAG for various use cases. We crossed the 2M monthly download mark, and are used by large enterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and Biases, DataStax, and many more. But while getting started with our famous 5-line starter example is easy, building production-grade RAG remains a complex and subtle problem. In our hundreds of user conversations, we learned the biggest pain points: Results aren‚Äôt accurate enough:  The application was not able to produce satisfactory results for a long-tail of input tasks/queries. The number of parameters to tune is overwhelming:  It‚Äôs not clear which parameters across the data parsing, ingestion, retrieval. PDFs are specifically a problem:  I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it? Data syncing is a challenge:  Production data often updates regularly, and continuously syncing new data brings a new set of challenges. These are the problems we set out to solve with LlamaCloud. Data Pipelines to Bring you to Production We built LlamaCloud and LlamaParse as the data pipelines to get your RAG application to production more quickly. LlamaParse LlamaParse is a state-of-the-art parser designed to specifically unlock RAG over complex PDFs with embedded tables and charts. This simply wasn‚Äôt possible before with other approaches, and we‚Äôre incredibly excited about this technology. LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document. For the past few months we‚Äôve been obsessed with this problem. \n",
      "\n",
      "Image input to LLM. The LLM got two identical images as input, which just shows that I reuse some of my diagrams. However, I am pleasantly surprised by CLIP embeddings as they were able to retrieve he most relevant image out of the collection. In a more production setting, you might want to clean and deduplicate images, but that is beyond the scope of this article. Conclusion LLMs are evolving faster than what we are historically used to and are spanning across multiple modalities. I firmly believe that by the end of the next year, LLMs will be soon able to comprehend videos, and be therefore able to pick up non-verbal cues while talking to you. On the other hand, we can use images as input to RAG pipeline and enhance the variety of information passed to an LLM, making responses better and more accurate. The multimodal RAG pipelines implementation with LlamaIndex and Neo4j is as easy as it gets. The code is available on  GitHub .\n",
      "\n",
      "Answer:\n",
      "\n",
      "LlamaCloud.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Introducing LlamaCloud and LlamaParse](https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b)\n",
      "- [Multimodal RAG pipeline with LlamaIndex and Neo4j](https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "The new offering that integrates local LLMs and embeddings into a fully local RAG pipeline is the Ollama LlamaPack.\n",
      "\n",
      "Expected Contexts:\n",
      "['Current examples include  embedded-tables  for analyzing complex tables in large PDFs, and  multi-document-agent  for comparing multiple documents.  Tweet . We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds.  Tweet . We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities.  Docs ,  Tweet . We launched the revamped  LlamaHub , a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets.  Tweet . We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects.  Blog ,  Repo ,  Tweet We released support for exact match and range queries in 4 vector databases including Weaviate, Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying.  Tweet . üó∫Ô∏è Guides: Guide  on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages. Guide  on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It‚Äôs part of our Snowflake BUILD talk and PyData Global talk. ‚úçÔ∏è Tutorials: Blog  by  Waii.ai  on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends. Wenqi Glantz‚Äôs  tutorial  on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine. An hour comprehensive workshop  tutorial  by  AIMakerspace  on RAG strategies over complex documents through recursive retrieval. Laurie‚Äôs   video  on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses. Ravi Theja‚Äôs   video  on Understanding LlamaIndex 0.9v abstractions and features. ü§ù Integrations: We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions.  Blog . We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code.  Docs ,  Tweet . We integrated FlagEmbeddingReranker to further boost your RAG pipeline.  Notebook ,  Tweet . üé•  Webinars: Webinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open-source multi-modal models of LLaVa, which are competitive with GPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by Haotian Zhang from the LlamaIndex team. üè¢ Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.']\n",
      "\n",
      "\n",
      "\n",
      "==============Error #4===============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the integration between Create-llama and E2B enhance agent capabilities?\n",
      "\n",
      "Context:\n",
      "Blogpost ,  Tweet . create-llama  Integrated with LlamaCloud:  Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance.  Tweet . ‚ú® Feature Releases and Enhancements: We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability.  \n",
      "\n",
      "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents. First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator import  dotenv\n",
      "dotenv.load_dotenv()  # our .env file defines OPENAI_API_KEY \n",
      " from  llama_agents  import  (\n",
      "    AgentService,\n",
      "    ControlPlaneServer,\n",
      "    SimpleMessageQueue,\n",
      "    AgentOrchestrator,\n",
      ")\n",
      " from  llama_index.core.agent  import  FunctionCallingAgentWorker\n",
      " from  llama_index.core.tools  import  FunctionTool\n",
      " from  llama_index.llms.openai  import  OpenAI\n",
      " import  logging\n",
      "\n",
      " # turn on logging so we can see the system working \n",
      "logging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\n",
      "\n",
      " # Set up the message queue and control plane \n",
      "message_queue = SimpleMessageQueue()\n",
      "control_plane = ControlPlaneServer(\n",
      "    message_queue=message_queue,\n",
      "    orchestrator=AgentOrchestrator(llm=OpenAI()),\n",
      ") Next we create our tools using LlamaIndex‚Äôs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice: # create a tool \n",
      " def   get_the_secret_fact () ->  str :\n",
      "     \"\"\"Returns the secret fact.\"\"\" \n",
      "     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \n",
      "\n",
      "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
      "\n",
      " # Define an agent \n",
      "worker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\n",
      "agent = worker.as_agent()\n",
      "\n",
      " # Create an agent service \n",
      "agent_service = AgentService(\n",
      "    agent=agent,\n",
      "    message_queue=message_queue,\n",
      "    description= \"General purpose assistant\" ,\n",
      "    service_name= \"assistant\" ,\n",
      ") Finally we launch the service and the control plane. Note that here we‚Äôre using a helper function to run a single query through the system and then exit; next we‚Äôll show how to deploy this to production. # Set up the launcher for local testing \n",
      " from  llama_agents  import  LocalLauncher\n",
      "\n",
      "launcher = LocalLauncher(\n",
      "    [agent_service],\n",
      "    control_plane,\n",
      "    message_queue,\n",
      ")\n",
      "\n",
      " # Run a single query through the system \n",
      "result = launcher.launch_single( \"What's the secret fact?\" )\n",
      " print (result) Deploying Your Multi-Agent System Once you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. \n",
      "\n",
      "Answer:\n",
      "\n",
      "The integration between Create-llama and LlamaCloud streamlines the setup and maintenance of data pipelines, allowing for faster and more efficient system management.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [LlamaIndex Newsletter 2024-07-02](https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02)\n",
      "- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "The integration between Create-llama and E2B enhances agent capabilities by turning agents into advanced data analysts. This integration enables agents to write Python code for data analysis and generate detailed files like graph images. This allows agents to expand their scope of capabilities and accomplish more complex tasks related to data analysis.\n",
      "\n",
      "Expected Contexts:\n",
      "[\"LlamaIndex Newsletter 2024-06-11\\nHello Llama Fansü¶ô Step into this week's edition of the LlamaIndex newsletter, where we bring you a slew of exciting updates, in-depth guides, demos, enriching educational tutorials, and webinars designed to enhance your experience and understanding of our platforms and tools. ü§©\\xa0 The highlights: Enhanced Memory Modules:  New memory modules in LlamaIndex boost agentic RAG capabilities with Vector Memory for message storage and retrieval, and Simple Composable Memory for integrating multiple memory sources.  Notebook1 ,  Notebook2 ,  Tweet . Create-llama and E2B Integration:  Launched integration turns agents into advanced data analysts, enabling Python coding for data analysis and generating detailed files like graph images.  Tweet . LlamaParse and Knowledge Graphs:   Guide  on integrating LlamaParse with Knowledge Graphs to develop RAG pipelines and agents for complex query handling. Prometheus-2 RAG Evaluation:   Guide  on using Prometheus-2, an affordable, transparent LLM based on Mistral models for effective RAG application evaluation with customized criteria. Agentic RAG :   Video tutorial  on Agentic RAG covering memory, planning, and reasoning, enhancing knowledge retrieval and agent capabilities. ‚ú® Feature Releases and Enhancements: We have introduced new memory modules in LlamaIndex for enhancing agentic RAG capabilities. The Vector Memory module enables storage and retrieval of user messages using vector search, while the Simple Composable Memory module allows for integration of multiple memory sources.  Notebook1 ,  Notebook2 ,  Tweet . We have launched an integration between Create-llama and E2B‚Äôs sandbox, transforming agents into powerful data analysts. This new feature allows agents to write Python code for data analysis and return comprehensive files, like graph images, enhancing the scope of what agents can accomplish.  Tweet . We have launched an integration with Nomic-Embed-Vision that transforms Nomic-Embed-Text into a multimodal embedding that excels in handling image, text, and combined tasks, outperforming OpenAI CLIP with open access for all.  Notebook . üó∫Ô∏è Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pipeline for sophisticated query retrieval, and create an agent capable of answering complex queries effectively. Guide  to Using Prometheus-2 for RAG Evaluation for assessing RAG applications, built on Mistral base models, it offers an affordable and transparent solution for evaluation, capable of direct assessments, pairwise rankings, and tailored criteria, ensuring alignment with human judgments. Guide  to Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively. üñ•Ô∏è\\xa0Demos: Laurie Voss ‚Äôs  LLM-powered file organizer project  that categorizes files into folders based on LLM-decided categories without renaming them, ensuring important filenames remain intact. It organizes your files in multiple passes to balance folder sizes, resulting in descriptive yet practical folder names to help you find files easily. ‚úçÔ∏è Tutorials: Laurie Voss ‚Äôs  video tutorial  on transitioning from basic RAG to fully agentic knowledge retrieval, featuring real-world code examples that cover routing, memory, planning, tool use, and advanced agentic reasoning methods like Chain of Thought and Tree of Thought, along with insights into observability, controllability, and customizability. Prince krampah 's  tutorials  on Agentic RAG Systems, offering comprehensive insights into advanced system building with detailed explanations on router query engines, function calling, and multi-step reasoning across complex documents. kingzzm‚Äôs   tutorial  on Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively. Rajdeep Borgohain 's  tutorial  to build a customer-support voicebot with advanced features like speech-to-text and text-to-speech, integrated into a RAG pipeline for efficient handling of customer support exchanges using Inferless, LlamaIndex, faster-whisper, Piper, and Pinecone. Pavan Mantha 's  tutorial  on securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant. üìπ\\xa0Webinar: Join us  for our webinar with  Tomaz Bratanic  from Neo4j on LlamaIndex property graph for insights into high-level and low-level graph construction, retrieval, and knowledge graph agents.\"]\n",
      "\n",
      "\n",
      "\n",
      "==============Error #5===============\n",
      "\n",
      "\n",
      "Query:\n",
      "What is the author's name of the document on becoming proficient in document extraction?\n",
      "\n",
      "Context:\n",
      "If you find my content valuable, don‚Äôt hesitate to leave a star.\n",
      "   \n",
      "   \n",
      "    Patreon: If you‚Äôd like to provide additional support, you can consider\n",
      "    becoming a patron on my Patreon page at\n",
      "     https://www.patreon.com/AndyShanu .\n",
      "   \n",
      "   \n",
      "     Medium : You\n",
      "    can read my latest articles and insights on Medium at\n",
      "     https://medium.com/@andysingal .\n",
      "   \n",
      "   \n",
      "     The Kaggle :\n",
      "    Check out my Kaggle profile for data science and machine learning projects\n",
      "    at\n",
      "     https://www.kaggle.com/alphasingal .\n",
      "   \n",
      "   \n",
      "     Hugging Face :\n",
      "    For natural language processing and AI-related projects, you can explore my\n",
      "    Huggingface profile at\n",
      "     https://huggingface.co/Andyrasika .\n",
      "   \n",
      "   \n",
      "    YouTube: To watch my video content, visit my YouTube channel at\n",
      "     https://www.youtube.com/@andy111007 .\n",
      "   \n",
      "   \n",
      "    LinkedIn: To stay updated on my latest projects and posts, you can follow me\n",
      "    on LinkedIn. Here is the link to my profile:\n",
      "     https://www.linkedin.com/in/ankushsingal/.\" \n",
      "   \n",
      " \n",
      " \n",
      "  Requests and questions: If you have a project in mind that you‚Äôd like me to\n",
      "  work on or if you have any questions about the concepts I‚Äôve explained, don‚Äôt\n",
      "  hesitate to let me know. I‚Äôm always looking for new ideas for future Notebooks\n",
      "  and I love helping to resolve any doubts you might have.\n",
      " \n",
      " \n",
      "  Remember, each ‚ÄúLike‚Äù, ‚ÄúShare‚Äù, and ‚ÄúStar‚Äù greatly contributes to my work and\n",
      "  motivates me to continue producing more quality content. Thank you for your\n",
      "  support!\n",
      " \n",
      " \n",
      "  If you enjoyed this story, feel free\n",
      "   to subscribe \n",
      "  to Medium, and you will get notifications when my new articles will be\n",
      "  published, as well as full access to thousands of stories from other authors.\n",
      " \n",
      " Resource: \n",
      " \n",
      "   \n",
      "     Data used for above code \n",
      "   \n",
      "   \n",
      "     llama-index\n",
      "\n",
      "Multilingual Proficiency: \n",
      " \n",
      " \n",
      "  Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency\n",
      "  facilitates seamless content extraction from documents in various languages,\n",
      "  extending global accessibility for businesses dealing with multilingual\n",
      "  documentation.\n",
      " \n",
      " \n",
      "   \n",
      "   Source: Created by Author using MidJourney \n",
      " \n",
      " \n",
      "   Implementation of Code \n",
      " \n",
      " \n",
      "  The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\n",
      "  transformation in document extraction. By merging Zephyr‚Äôs advanced OCR\n",
      "  capabilities with LlamaIndex‚Äôs image enhancement and data organization\n",
      "  features, this integration presents a comprehensive solution:\n",
      " \n",
      " \n",
      "   \n",
      "     Augmented Precision : The fusion of Zephyr‚Äôs machine\n",
      "    learning expertise and LlamaIndex‚Äôs image enhancement markedly heightens the\n",
      "    accuracy of extracted data, diminishing errors and enhancing overall\n",
      "    efficiency.\n",
      "   \n",
      "   \n",
      "     Efficient Workflow : Users experience an optimized workflow,\n",
      "    enabling swift extraction and conversion of image-based documents into\n",
      "    structured, actionable data, facilitating expedited decision-making\n",
      "    processes.\n",
      "   \n",
      "   \n",
      "     Adaptability Across Document Varieties : This integration\n",
      "    empowers users to handle diverse document formats and languages\n",
      "    effortlessly, granting access to previously challenging document types for\n",
      "    extraction and analysis.\n",
      "   \n",
      " \n",
      " \n",
      "   \n",
      "   Source: Image created by Author using MidJourney \n",
      " \n",
      " \n",
      "   Step 1: Install and Import Libraries \n",
      " \n",
      " !pip install llama-index transformers accelerate sentencepiece bitsandbytes -q \n",
      " \n",
      "   Step 2: Load the Model \n",
      " \n",
      " import torch from transformers import BitsAndBytesConfig from llama_index.prompts import PromptTemplate from llama_index.llms import HuggingFaceLLM quantization_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_compute_dtype=torch.float16,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_use_double_quant=True, ) def messages_to_prompt(messages):   prompt = \"\"   for message in messages:     if message.role == 'system':       prompt += f\"<|system|>\\n{message.content}</s>\\n\"     elif message.role == 'user':       prompt += f\"<|user|>\\n{message.content}</s>\\n\"     elif message.role == 'assistant':       prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"   # ensure we start with a system prompt, insert blank if needed   if not prompt.startswith(\"<|system|>\\n\"):     prompt = \"<|system|>\\n</s>\\n\" + prompt   # add final assistant prompt   prompt = prompt + \"<|assistant|>\\n\"   return prompt llm = HuggingFaceLLM(     model_name=\"HuggingFaceH4/zephyr-7b-alpha\",     tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",     query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),     context_window=3900,     max_new_tokens=2000,     model_kwargs={\"quantization_config\": quantization_config},     # tokenizer_kwargs={},     generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},     messages_to_prompt=messages_to_prompt,     device_map=\"auto\", ) \n",
      " from llama_index import ServiceContext, set_global_service_context service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\") \n",
      " set_global_service_context(service_context) \n",
      " \n",
      "   Step 3: Storing your index \n",
      " \n",
      " from llama_index import SimpleDirectoryReader, VectorStoreIndex from llama_index.readers.file.base import (     DEFAULT_FILE_READER_CLS,     ImageReader, ) from llama_index.response.notebook_utils import (     display_response,     display_image, ) from llama_index.indices.query.query_transform.base import (     ImageOutputQueryTransform, ) filename_fn = lambda filename: {\"file_name\": filename} llama_reader = SimpleDirectoryReader(     input_dir=\"/content/llama\",     file_metadata=filename_fn, ) llama_documents = llama_reader.load_data() llama_index = VectorStoreIndex.from_documents(llama_documents) \n",
      " \n",
      "   Step 4: Query  Transformations \n",
      " \n",
      " from llama_index.query_engine import TransformQueryEngine query_engine = llama_index.as_query_engine(similarity_top_k=2) query_engine = TransformQueryEngine(     query_engine, query_transform=ImageOutputQueryTransform(width=400) ) llama_response = query_engine.query(     \"Show an image to illustrate how tree index works and explain briefly\", ) display_response(llama_response) #Output Final Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works. \n",
      "\n",
      "Answer:\n",
      "\n",
      "The author's name of the document on becoming proficient in document extraction is Ankush k Singal.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Becoming Proficient in Document Extraction](https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "Ankush k Singal\n",
      "\n",
      "Expected Contexts:\n",
      "['Hugging Face :\\n    For natural language processing and AI-related projects, you can explore my\\n    Huggingface profile at\\n     https://huggingface.co/Andyrasika .\\n   \\n   \\n    YouTube: To watch my video content, visit my YouTube channel at\\n     https://www.youtube.com/@andy111007 .\\n   \\n   \\n    LinkedIn: To stay updated on my latest projects and posts, you can follow me\\n    on LinkedIn. Here is the link to my profile:\\n     https://www.linkedin.com/in/ankushsingal/.\" \\n   \\n \\n \\n  Requests and questions: If you have a project in mind that you‚Äôd like me to\\n  work on or if you have any questions about the concepts I‚Äôve explained, don‚Äôt\\n  hesitate to let me know. I‚Äôm always looking for new ideas for future Notebooks\\n  and I love helping to resolve any doubts you might have.\\n \\n \\n  Remember, each ‚ÄúLike‚Äù, ‚ÄúShare‚Äù, and ‚ÄúStar‚Äù greatly contributes to my work and\\n  motivates me to continue producing more quality content. Thank you for your\\n  support!\\n \\n \\n  If you enjoyed this story, feel free\\n   to subscribe \\n  to Medium, and you will get notifications when my new articles will be\\n  published, as well as full access to thousands of stories from other authors.\\n \\n Resource: \\n \\n   \\n     Data used for above code \\n   \\n   \\n     llama-index']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_irrelevance_df = (\n",
    "    synthetic_deep_eval_df\n",
    "    .loc[lambda df: df['relevancy_score'].lt(1)]\n",
    "    .sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])\n",
    ")\n",
    "\n",
    "for i, row in synthetic_response_eval_irrelevance_df.reset_index(drop=True).iterrows():\n",
    "    print(f\"\\n\\n==============Error #{i+1}===============\\n\\n\")\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.contexts)\n",
    "    print(f\"Context:\\n{contexts}\\n\")\n",
    "    print(f\"Answer:\\n{row.answer}\\n----\\n\")\n",
    "    expected = synthetic_response_eval_dataset_dict.get(row.query)\n",
    "    if not expected:\n",
    "        logger.error(f\"Could not find query {row.query} in synthetic_response_eval_dataset_dict!\")\n",
    "        continue\n",
    "    expected_answer = expected['reference_answer']\n",
    "    print(f\"Expected Answer:\\n{expected_answer}\\n\")\n",
    "    expected_contexts = expected['reference_contexts']\n",
    "    print(f\"Expected Contexts:\\n{expected_contexts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2876bc3c-251b-4686-9321-6a7a0c081019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "curated_response_eval_prediction_dataset = await curated_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0945ae48584d52a4bef94afd9b2212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    curated_response_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score        4.5\n",
       "mean_relevancy_score          1.0\n",
       "mean_faithfulness_score       1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>\\nDistributed Service-Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\\n\\nDefine agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of deployment: launch, scale, and monitor each agent and your control plane independently.\\n\\nScalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.\\n\\n\\nSources:\\n- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\\n\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents. First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator import  dotenv\\ndotenv.load_dotenv()  # our .env file defines OPENAI_API_KEY \\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    AgentOrchestrator,\\n)\\n from  llama_index.core.agent  import  FunctionCallingAgentWorker\\n from  llama_index.core.tools  import  FunctionTool\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # turn on logging so we can see the system working \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Set up the message queue and control plane \\nmessage_queue = SimpleMessageQueue()\\ncontrol_plane = ControlPlaneServer(\\n    message_queue=message_queue,\\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\\n) Next we create our tools using LlamaIndex‚Äôs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice: # create a tool \\n def   get_the_secret_fact () -&gt;  str :\\n     \"\"\"Returns the secret fact.\"\"\" \\n     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \\n\\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # Define an agent \\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nagent = worker.as_agent()\\n\\n # Create an agent service \\nagent_service = AgentService(\\n    agent=agent,\\n    message_queue=message_queue,\\n    description= \"General purpose assistant\" ,\\n    service_name= \"assistant\" ,\\n) Finally we launch the service and the control plane. Note that here we‚Äôre using a helper function to run a single query through the system and then exit; next we‚Äôll show how to deploy this to production. # Set up the launcher for local testing \\n from  llama_agents  import  LocalLauncher\\n\\nlauncher = LocalLauncher(\\n    [agent_service],\\n    control_plane,\\n    message_queue,\\n)\\n\\n # Run a single query through the system \\nresult = launcher.launch_single( \"What's the secret fact?\" )\\n print (result) Deploying Your Multi-Agent System Once you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. , We‚Äôre actively seeking public feedback on what works for you and what doesn‚Äôt. Dive in! llama-agents  provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production,  llama-agents  offers the tools you need to bring your AI vision to life. Check out  the repo  to learn more, especially our library of  examples . We're excited to see what the community builds with  llama-agents . Happy coding!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>\\nThe two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\\n\\n\\nSources:\\n- [OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)\\n\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. , Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>\\nThe two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\\n\\n\\nSources:\\n- [Boosting RAG: Picking the Best Embedding &amp; Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\\n\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools. bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better. Foundation is Key : Choosing the right embedding for the initial search is essential; even the best reranker can‚Äôt help much if the basic search results aren‚Äôt good. Working Together:  To get the best out of retrievers, it‚Äôs important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing., Boosting RAG: Picking the Best Embedding &amp; Reranker models\\nUPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we‚Äôll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let‚Äôs first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let‚Äôs delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on. Now that we‚Äôve established the scope and familiarized ourselves with the metrics, it‚Äôs time to dive into the experiment. For a hands-on experience, you can also follow along using our  Google Colab Notebook Setting Up the Environment !pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf Setting Up the Keys openai_api_key =  'YOUR OPENAI API KEY' \\ncohere_api_key =  'YOUR COHEREAI API KEY' \\nanthropic_api_key =  'YOUR ANTHROPIC API KEY' \\nopenai.api_key = openai_api_key Download the Data We will use Llama2 paper for this experiment. Let‚Äôs download the paper. ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.</td>\n",
       "      <td>\\nThe MemoryCache project by Mozilla utilizes PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy by integrating private data and knowledge sources. This project allows users to store and query their private data, enabling them to process the data once and then query it for various downstream applications. The project's key features include the ability to connect private knowledge sources using LlamaIndex connectors, load in documents, and utilize LLMs to construct final answers.\\n\\n\\nSources:\\n- [Build a ChatGPT with your Private Data using LlamaIndex and MongoDB](https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c)\\n\\n\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The requirement to continuously maintain updates from the underlying data sources. Being able to persist this data enables processing the data once and then being able to query it for various downstream applications. MongoDB Atlas MongoDB offers a free forever Atlas cluster in the public cloud service of your choice. This can be accomplished very quickly by following this  tutorial . , Or you can get started directly  here . Use of LLMs LlamaIndex uses LangChain‚Äôs (another popular framework for building Generative AI applications) LLM modules and allows for customizing the underlying LLM to be used (default being OpenAI‚Äôs text-davinci-003 model). The chosen LLM is always used by LlamaIndex to construct the final answer and is sometimes used during index creation as well. The workflow Connect private knowledge sources using LlamaIndex connectors (offered through  LlamaHub ). Load in the Documents. ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                query  \\\n",
       "0                                                                                                                                                                              What are key features of llama-agents?   \n",
       "1                                                                 What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                                                                                        What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "3  How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   answer  \\\n",
       "0  \\nDistributed Service-Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\\n\\nDefine agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\\n\\nEase of deployment: launch, scale, and monitor each agent and your control plane independently.\\n\\nScalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.\\n\\n\\nSources:\\n- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\\n\\n\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\nThe two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\\n\\n\\nSources:\\n- [OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)\\n\\n\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\nThe two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\\n\\n\\nSources:\\n- [Boosting RAG: Picking the Best Embedding & Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\\n\\n\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                          \\nThe MemoryCache project by Mozilla utilizes PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy by integrating private data and knowledge sources. This project allows users to store and query their private data, enabling them to process the data once and then query it for various downstream applications. The project's key features include the ability to connect private knowledge sources using LlamaIndex connectors, load in documents, and utilize LLMs to construct final answers.\\n\\n\\nSources:\\n- [Build a ChatGPT with your Private Data using LlamaIndex and MongoDB](https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c)\\n\\n\\n   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \\\n",
       "0              1.0                NaN                 1.0   \n",
       "1              1.0                5.0                 1.0   \n",
       "2              1.0                NaN                 1.0   \n",
       "3              1.0                4.0                 1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    contexts  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents. First we‚Äôll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestrator import  dotenv\\ndotenv.load_dotenv()  # our .env file defines OPENAI_API_KEY \\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    AgentOrchestrator,\\n)\\n from  llama_index.core.agent  import  FunctionCallingAgentWorker\\n from  llama_index.core.tools  import  FunctionTool\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # turn on logging so we can see the system working \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Set up the message queue and control plane \\nmessage_queue = SimpleMessageQueue()\\ncontrol_plane = ControlPlaneServer(\\n    message_queue=message_queue,\\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\\n) Next we create our tools using LlamaIndex‚Äôs existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice: # create a tool \\n def   get_the_secret_fact () ->  str :\\n     \"\"\"Returns the secret fact.\"\"\" \\n     return   \"The secret fact is: A baby llama is called a 'Cria'.\" \\n\\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\\n\\n # Define an agent \\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\\nagent = worker.as_agent()\\n\\n # Create an agent service \\nagent_service = AgentService(\\n    agent=agent,\\n    message_queue=message_queue,\\n    description= \"General purpose assistant\" ,\\n    service_name= \"assistant\" ,\\n) Finally we launch the service and the control plane. Note that here we‚Äôre using a helper function to run a single query through the system and then exit; next we‚Äôll show how to deploy this to production. # Set up the launcher for local testing \\n from  llama_agents  import  LocalLauncher\\n\\nlauncher = LocalLauncher(\\n    [agent_service],\\n    control_plane,\\n    message_queue,\\n)\\n\\n # Run a single query through the system \\nresult = launcher.launch_single( \"What's the secret fact?\" )\\n print (result) Deploying Your Multi-Agent System Once you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. , We‚Äôre actively seeking public feedback on what works for you and what doesn‚Äôt. Dive in! llama-agents  provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production,  llama-agents  offers the tools you need to bring your AI vision to life. Check out  the repo  to learn more, especially our library of  examples . We're excited to see what the community builds with  llama-agents . Happy coding!]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. , Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô]  \n",
       "2  [However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools. bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better. Foundation is Key : Choosing the right embedding for the initial search is essential; even the best reranker can‚Äôt help much if the basic search results aren‚Äôt good. Working Together:  To get the best out of retrievers, it‚Äôs important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing., Boosting RAG: Picking the Best Embedding & Reranker models\\nUPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we‚Äôll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let‚Äôs first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let‚Äôs delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on. Now that we‚Äôve established the scope and familiarized ourselves with the metrics, it‚Äôs time to dive into the experiment. For a hands-on experience, you can also follow along using our  Google Colab Notebook Setting Up the Environment !pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf Setting Up the Keys openai_api_key =  'YOUR OPENAI API KEY' \\ncohere_api_key =  'YOUR COHEREAI API KEY' \\nanthropic_api_key =  'YOUR ANTHROPIC API KEY' \\nopenai.api_key = openai_api_key Download the Data We will use Llama2 paper for this experiment. Let‚Äôs download the paper. ]  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [The requirement to continuously maintain updates from the underlying data sources. Being able to persist this data enables processing the data once and then being able to query it for various downstream applications. MongoDB Atlas MongoDB offers a free forever Atlas cluster in the public cloud service of your choice. This can be accomplished very quickly by following this  tutorial . , Or you can get started directly  here . Use of LLMs LlamaIndex uses LangChain‚Äôs (another popular framework for building Generative AI applications) LLM modules and allows for customizing the underlying LLM to be used (default being OpenAI‚Äôs text-davinci-003 model). The chosen LLM is always used by LlamaIndex to construct the final answer and is sometimes used during index creation as well. The workflow Connect private knowledge sources using LlamaIndex connectors (offered through  LlamaHub ). Load in the Documents. ]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "113a264d-1cea-4e41-82ac-22135f12f729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on. The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR): Analysis: Performance by Embedding: OpenAI : Showcases top-tier performance, especially with the  CohereRerank  (0.926966 hit rate, 0.86573 MRR) and  bge-reranker-large  (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools. bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere‚Äôs latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs. Rerankers, especially  CohereRerank , have demonstrated their capability to transform any embedding into a competitive one. Overall Superiority : When considering both hit rate and MRR, the combinations of  OpenAI + CohereRerank  and  JinaAI-Base + bge-reranker-large/ CohereRerank  emerge as top contenders. However, the consistent improvement brought by the  CohereRerank/ bge-reranker-large  rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use. In summary, to achieve the peak performance in both hit rate and MRR, the combination of  OpenAI  or  JinaAI-Base  embeddings with the  CohereRerank/bge-reranker-large  reranker stands out. Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them. Conclusions: In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions. Embeddings : The  OpenAI  and  JinaAI-Base  embeddings, especially when paired with the  CohereRerank/bge-reranker-large  reranker, set the gold standard for both hit rate and MRR. Rerankers : The influence of rerankers, particularly  CohereRerank/bge-reranker-large , cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better. Foundation is Key : Choosing the right embedding for the initial search is essential; even the best reranker can‚Äôt help much if the basic search results aren‚Äôt good. Working Together:  To get the best out of retrievers, it‚Äôs important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing.\n",
      "----------\n",
      "Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we‚Äôll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let‚Äôs first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let‚Äôs delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on. Now that we‚Äôve established the scope and familiarized ourselves with the metrics, it‚Äôs time to dive into the experiment. For a hands-on experience, you can also follow along using our  Google Colab Notebook Setting Up the Environment !pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf Setting Up the Keys openai_api_key =  'YOUR OPENAI API KEY' \n",
      "cohere_api_key =  'YOUR COHEREAI API KEY' \n",
      "anthropic_api_key =  'YOUR ANTHROPIC API KEY' \n",
      "openai.api_key = openai_api_key Download the Data We will use Llama2 paper for this experiment. Let‚Äôs download the paper. \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for context in curated_deep_eval_df.iloc[2]['contexts']:\n",
    "    print(context)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
