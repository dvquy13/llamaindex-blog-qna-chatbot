{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_TO_MLFLOW = True\n",
    "if LOG_TO_MLFLOW:\n",
    "    RUN_NAME = \"mvp_002\"\n",
    "    RUN_DESCRIPTION = \"\"\"\n",
    "Add debug=True and reusing persisted database\n",
    "\"\"\"\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Automate online tasks with MultiOn and LlamaIndex',\n",
       "  'content': 'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.',\n",
       "  'author': 'MultiOn',\n",
       "  'date': 'May 23, 2024',\n",
       "  'tags': ['automation', 'Agents']},\n",
       " {'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       "  'content': 'We‚Äôre happy to announce the recent integration of LlamaIndex with PostgresML ‚Äî a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows. The problem with typical RAG workflows Typical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users. Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges: Increased dev time to master new technologies Complicated maintenance and scalability issues due to multiple points of failure Costly vendors required for multiple services The diagram above illustrates the complexity, showing how each component interacts across different services ‚Äî exacerbating these problems. Solution The PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows. By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call ‚Äî resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow. About PostgresML PostgresML [ github  ||  website  ||  docs ] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides. Key highlights: Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning PostgresML has a range of capabilities. In the following sections, we‚Äôll guide you through just one use case ‚Äì RAG ‚Äì and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app. How it works in LlamaIndex Let‚Äôs look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham‚Äôs essays. Step 1: Get Your Database Connection String If you haven‚Äôt already,  create your PostgresML account . You‚Äôll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Alternatively, you can pass the pgml_database_url argument when creating the index. Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed Index component: pip install llama_index llama-index-indices-managed-postgresml Then load in the data: mkdir  data\\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt Finally create the PostgresML Managed Index: from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\\n\\n\\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\\nindex = PostgresMLIndex.from_documents(\\n    documents, collection_name= \"llama-index-example\" \\n) Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see:  https://postgresml.org/docs/api/client-sdk/pipelines  Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 . Step 3: Querying Now that we have created our index we can use it for retrieval and querying: retriever = index.as_retriever()\\ndocs = retriever.retrieve( \"Was the author puzzled by the IBM 1401?\" )\\n for  doc  in  docs:\\n     print (doc) PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up. Using the PostgresML Index as a query_engine is just as easy: response = index.as_query_engine().query( \"Was the author puzzled by the IBM 1401?\" )\\n print (response) Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming: query_engine = index.as_query_engine(streaming= True )\\nresults = query_engine.query( \"Was the author puzzled by the IBM 1401?\" )\\n for  text  in  results.response_gen:\\n     print (text, end= \"\" , flush= True ) Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable. Key takeaways The PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend. To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro  guide  to setup your account, and the examples above with your own data.',\n",
       "  'author': 'PostgresML',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': ['Managed Indexes']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-06-04',\n",
       "  'content': \"Hello, LlamaIndex Family! ü¶ô We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need. ü§©\\xa0 The highlights: Elevating Knowledge Graphs:  The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities.  Docs ,  Tweet . Spreadsheet Insights with LlamaParse:  LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling.  Notebook ,  Tweet . Code Generation with Codestral:  Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages.  Docs ,  Tweet . ‚ú® Feature Releases and Enhancements: We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support.  Docs ,  Tweet . We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance.  Notebook ,  Tweet . We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages.  Docs ,  Tweet . We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript.  Blogpost ,  Tweet . We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex.  Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results. Guide  to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results. Guide  to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights. üñ•Ô∏è\\xa0Demos: Omakase RAG Orchestrator , a project developed by  Amir Mehr , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel. gmail-extractor , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision. ‚úçÔ∏è Tutorials: Sherlock Xu‚Äôs  tutorial  from BentoML on Serving A LlamaIndex RAG App as REST APIs. üìë\\xa0Papers: FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text. üìπ\\xa0Webinar: Webinar  with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents üìÖ\\xa0 Events: Join  Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Jun 4, 2024',\n",
       "  'tags': []},\n",
       " {'title': 'Batch inference with MyMagic AI and LlamaIndex',\n",
       "  'content': 'This is a guest post from MyMagic AI. MyMagic AI  allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for  batch  inference (also known as  offline  or  delayed  inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it\\'s integrated directly into LlamaIndex! Part 1: batch inference How It Works: 1. Setup : Organize Your Data in an AWS S3 or GCS Bucket: Create a folder using your user ID assigned to you upon registration. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. Purpose of the \\'Session\\' Folder: This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. Granting Access to MyMagic AI: To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the  MyMagic AI documentation . 2. Install : Install both MyMagic AI‚Äôs API integration and LlamaIndex library: pip install llama-index\\npip install llama-index-llms-mymagic 3. API Request:  The llamaIndex library is a wrapper around MyMagic AI‚Äôs API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details. import  asyncio\\n from  llama_index.llms.mymagic  import  MyMagicAI\\n\\nllm = MyMagicAI(\\n    api_key= \"user_...\" ,  # provided by MyMagic AI upon sign-up \\n    storage_provider= \"s3\" ,\\n    bucket_name= \"batch-bucket\" ,  # you may name anything \\n    session= \"my-session\" ,\\n    role_arn= \"arn:aws:iam::<your account id>:role/mymagic-role\" ,\\n    system_prompt= \"You are an AI assistant that helps to summarize the documents without essential loss of information\" ,  # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create \\n    region= \"eu-west-2\" ,\\n) We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests. resp = llm.complete(\\n    question= \"Summarise this in one sentence.\" ,\\n    model= \"mixtral8x7\" , \\n    max_tokens= 20 ,   # default is 10 \\n)\\n print (resp)\\n async   def   main ():\\n    aresp =  await  llm.acomplete(\\n        question= \"Summarize this in one sentence.\" ,\\n        model= \"llama7b\" ,\\n        max_tokens= 20 ,\\n    )\\n     print (aresp)\\n\\nasyncio.run(main()) This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI‚Äôs backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data). Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a ‚Äúbatch way‚Äù. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once. Use Cases While there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing. 1. Extraction Imagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones. 2. Classification For businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning. 3. Embedding Embedding text files for further machine learning applications is another powerful use case of MyMagic AI\\'s API. You will be ready for your vector db in a matter of days not weeks. 4. Training (Fine-tuning) Data Generation Imagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI‚Äôs API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5. 5. Transcription MyMagic AI‚Äôs API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket. Part 2: Integration with LlamaIndex‚Äôs RAG Pipeline The output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex\\'s RAG pipeline for effective data storage and retrieval. This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled: pip install llama-index-embeddings-huggingface\\ncurl -fsSL https://ollama.com/install.sh | sh\\nollama pull llama3 For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json: {\\n  \"id_review1\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\\n  },\\n  \"id_review2\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen\\'s control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson\\'s performance. It concludes by calling the film a great comedy to watch with friends.\"\\n  },\\n  \"id_review3\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell\\'s flat. Overall, it is a terrific and well-written piece.\"\\n  },\\n  \"id_review4\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"Petter Mattei\\'s \\'Love in the Time of Money\\' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler\\'s play on the same theme. Mattei\\'s work is appreciated, and viewers look forward to his future projects.\"\\n  },\\n  \"id_review5\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes the TV show \\'Oz\\', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn\\'t dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\\n  },\\n  \"token_count\": 3391\\n}\\n Now let‚Äôs embed and store this document and ask questions using LlamaIndex‚Äôs query engine. Bring in our dependencies: import  os\\n\\n from  llama_index.embeddings.huggingface  import  HuggingFaceEmbedding\\n from  llama_index.core.indices.vector_store  import  VectorStoreIndex\\n from  llama_index.core.settings  import  Settings\\n from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.llms.ollama  import  Ollama Configure the embedding model and Llama3 model embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-base-en-v1.5\" )\\nllm = Ollama(model= \"llama3\" , request_timeout= 300.0 ) Update settings for the indexing pipeline: Settings.llm = llm\\nSettings.embed_model = embed_model\\nSettings.chunk_size =  512   # This parameter defines the size of text chunks for embedding \\n\\ndocuments = SimpleDirectoryReader( \"reviews_1_5.json\" ).load_data()  #Modify path for your case Now create our index, our query engine and run a query: index = VectorStoreIndex.from_documents(documents, show_progress= True )\\n\\nquery_engine = index.as_query_engine(similarity_top_k= 3 )\\n\\nresponse = query_engine.query( \"What is the least favourite movie?\" )\\n print (response) Output: Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10. Now we know that the review 1 is the least favorite movie among these reviews. Next Steps This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.  Get started with MyMagic AI‚Äôs API  today!',\n",
       "  'author': 'MyMagic AI',\n",
       "  'date': 'May 22, 2024',\n",
       "  'tags': ['MyMagic AI', 'Batch inference']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-05-28',\n",
       "  'content': \"Greetings, LlamaIndex Family! ü¶ô Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week. ü§©\\xa0 The highlights: Secure Code Execution with AzureCodeInterpreterTool:  Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution. Build Automated Email Agents:  Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails. LlamaFS for Organized Files:  Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API. RAGApp's No-Code Chatbots:  Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible. ‚ú® Feature Releases and Enhancements: We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you‚Äôre ready to go.  Blogpost ,  Tweet . We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance.  Docs ,  Tweet . We have integrated the Vespa vector store, supporting hybrid search with BM25.  Docs ,  Tweet . We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities.  Docs ,  Tweet . üó∫Ô∏è Guides: Guide  to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities. Guide  to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates. üñ•Ô∏è\\xa0Demos: LlamaFS, a project developed by  Alex Reibman  and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency.  Code ,  Tweet . RAGApp, a project developed by  Marcus Schiesser , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API.  Code ,  Tweet . ‚úçÔ∏è Tutorials: Phil Chirchir‚Äôs   tutorial  on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse. Pavan Kumar‚Äôs   tutorial  on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine‚Äôs capabilities. Jayita Bhattacharyya‚Äôs  tutorial  on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit. üìπ\\xa0Webinar: Webinar  with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': []}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:02:05.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=159\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags'])\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='e235237a-5a9e-4c90-8f91-4630573fbffe', embedding=None, metadata={'title': 'Automate online tasks with MultiOn and LlamaIndex', 'author': 'MultiOn', 'date': 'May 23, 2024', 'tags': 'automation, Agents'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Automate online tasks with MultiOn and LlamaIndex\\nIntroduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let‚Äôs explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you‚Äôre interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       " 'author': 'PostgresML',\n",
       " 'date': 'May 28, 2024',\n",
       " 'tags': 'Managed Indexes'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OPTION = 'openai'\n",
    "# llm_option = 'ollama'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ab185c-c7aa-4b81-9f5b-15dd4c48725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:02:16.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mUsing OpenAI LLM...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if LLM_OPTION == 'ollama':\n",
    "    logger.info(f\"Using local Ollama LLM...\")\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    OLLAMA_MODEL_NAME = 'llama3'\n",
    "    llm = Ollama(base_url=base_url, model=model_name, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "    Settings.llm = llm\n",
    "    embedding = OllamaEmbedding(\n",
    "        model_name=OLLAMA_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "    Settings.embed_model = embedding\n",
    "    if LOG_TO_MLFLOW:\n",
    "        mlflow.log_param(\"OLLAMA_MODEL_NAME\", OLLAMA_MODEL_NAME)\n",
    "elif LLM_OPTION == 'openai':\n",
    "    logger.info(f\"Using OpenAI LLM...\")\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embedding = OpenAIEmbedding()\n",
    "    OPENAI_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "    llm = OpenAI(model=OPENAI_MODEL_NAME)\n",
    "    if LOG_TO_MLFLOW:\n",
    "        mlflow.log_param(\"OPENAI_MODEL_NAME\", OPENAI_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f72b3d52-7be9-49e1-bf4e-4cd586635d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5a5fb8a-636a-463c-a938-1864cff81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "COLLECTION = 'mvp'\n",
    "NOTEBOOK_CACHE_DP = 'data/001'\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a1d0d1-142d-4e7a-b626-ef154ba08e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:02:31.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mUse existing ChromaDB collection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = chromadb.PersistentClient(path=f\"{NOTEBOOK_CACHE_DP}/chroma_db\")\n",
    "collection_exists = COLLECTION in [c.name for c in db.list_collections()]\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    logger.info(f\"Deleting existing ChromaDB collection...\")\n",
    "    db.delete_collection(COLLECTION)\n",
    "    logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "    os.remove(NODES_PERSIST_FP)\n",
    "else:\n",
    "    logger.info(f\"Use existing ChromaDB collection\")\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ecdec51-2720-4628-a43f-caaa98310630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa0a955-fbdf-4029-8e41-2efdea6ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0f36679-603e-40d6-862b-5d9313b3c1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:02:48.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mLoading index from existing ChromaDB...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if chroma_collection.count() > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing ChromaDB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new ChromaDB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embedding,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "    \n",
    "    # Need to use await and arun here to run the pipeline else error\n",
    "    # Ref: https://docs.llamaindex.ai/en/stable/examples/ingestion/async_ingestion_pipeline/\n",
    "    # Ref: https://github.com/run-llama/llama_index/issues/13904#issuecomment-2145561710\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a338d-469e-4461-9511-553dd6c1f766",
   "metadata": {},
   "source": [
    "#### Inspect nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f68dde-bd70-49b2-a62b-40a527fc44ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# embeddings are excluded by default for performance, if need then explicitly ask for it in `include`\n",
    "# chroma_collection.get(include=['embeddings'])\n",
    "chroma_collection.get()['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node eb1f08fa-46d9-47c1-a10a-3f65efd514bf] [Similarity score: 0.6978917999454536] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> [Node b6b6b342-99e9-4459-82d6-941a640ff819] [Similarity score: 0.6923464230331815] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n",
      "> Top 2 nodes:\n",
      "> [Node eb1f08fa-46d9-47c1-a10a-3f65efd514bf] [Similarity score:             0.697892] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> [Node b6b6b342-99e9-4459-82d6-941a640ff819] [Similarity score:             0.692346] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:04:24.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mMultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"What is MultiOn?\"\n",
    "response = query_engine.query(question)\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_RETRIEVAL_EVAL_DATASET = False\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "if RECREATE_RETRIEVAL_EVAL_DATASET:\n",
    "    RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "    RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "    \n",
    "    if LOG_TO_MLFLOW:\n",
    "        mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "        mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET:\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:05:51.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mLoading existing synthetic retrieval eval dataset at data/001/llamaindex_blog_retrieval_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes, llm=llm, num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b77bd622-ef88-44d3-8e5a-80fd10b9d768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node fdc7d228-4d26-40b7-9e89-a644305265e4] [Similarity score: 0.8067454274188276] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> [Node 1f4bd31e-e73c-4264-b765-d7d76f2359b9] [Similarity score: 0.788734662447601] Data Agents\n",
      "Today we‚Äôre incredibly excited to announce the launch of a big new capability within ...\n",
      "> Top 2 nodes:\n",
      "> [Node fdc7d228-4d26-40b7-9e89-a644305265e4] [Similarity score:             0.806745] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> [Node 1f4bd31e-e73c-4264-b765-d7d76f2359b9] [Similarity score:             0.788735] Data Agents\n",
      "Today we‚Äôre incredibly excited to announce the launch of a big new capability within ...\n",
      "> Top 2 nodes:\n",
      "> [Node 8b1f680b-98a8-4088-b680-0ec4dd4f627f] [Similarity score: 0.8044526440159468] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> [Node fdc7d228-4d26-40b7-9e89-a644305265e4] [Similarity score: 0.7584638049746459] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> Top 2 nodes:\n",
      "> [Node 8b1f680b-98a8-4088-b680-0ec4dd4f627f] [Similarity score:             0.804453] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> [Node fdc7d228-4d26-40b7-9e89-a644305265e4] [Similarity score:             0.758464] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> Top 2 nodes:\n",
      "> [Node 73f557d3-4dd7-42d1-ae74-b4e75a0ae3f2] [Similarity score: 0.7545659431014736] Guide . ‚ú® Feature Releases and Enhancements: We have introduced RankGPT in our advanced module th...\n",
      "> [Node 741b7c75-2b65-4438-b702-ac5cce00dc1d] [Similarity score: 0.7535100474899667] Notebook . üó∫Ô∏è Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pip...\n",
      "> Top 2 nodes:\n",
      "> [Node 73f557d3-4dd7-42d1-ae74-b4e75a0ae3f2] [Similarity score:             0.754566] Guide . ‚ú® Feature Releases and Enhancements: We have introduced RankGPT in our advanced module th...\n",
      "> [Node 741b7c75-2b65-4438-b702-ac5cce00dc1d] [Similarity score:             0.75351] Notebook . üó∫Ô∏è Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pip...\n",
      "> Top 2 nodes:\n",
      "> [Node 37d0cb58-215b-4aa1-9a76-c3bc0a2bdfa0] [Similarity score: 0.7905262624537555] In just six months, the project garnered an impressive following, with  16K Github Stars ,  20K T...\n",
      "> [Node 1ddc94f0-6657-4d95-a213-20c591651790] [Similarity score: 0.7838377230755988] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> Top 2 nodes:\n",
      "> [Node 37d0cb58-215b-4aa1-9a76-c3bc0a2bdfa0] [Similarity score:             0.790526] In just six months, the project garnered an impressive following, with  16K Github Stars ,  20K T...\n",
      "> [Node 1ddc94f0-6657-4d95-a213-20c591651790] [Similarity score:             0.783838] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> Top 2 nodes:\n",
      "> [Node a14a84bc-4d2e-4f74-b000-825c843dcf5c] [Similarity score: 0.6824134547421892] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node 8acd918e-4c5c-4a8f-a26d-63f399ed91b8] [Similarity score: 0.6655009737164711] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> Top 2 nodes:\n",
      "> [Node a14a84bc-4d2e-4f74-b000-825c843dcf5c] [Similarity score:             0.682413] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node 8acd918e-4c5c-4a8f-a26d-63f399ed91b8] [Similarity score:             0.665501] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> Top 2 nodes:\n",
      "> [Node a14a84bc-4d2e-4f74-b000-825c843dcf5c] [Similarity score: 0.8622628686205462] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node 8acd918e-4c5c-4a8f-a26d-63f399ed91b8] [Similarity score: 0.8562408710123093] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> Top 2 nodes:\n",
      "> [Node a14a84bc-4d2e-4f74-b000-825c843dcf5c] [Similarity score:             0.862263] To see this in action, let‚Äôs take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node 8acd918e-4c5c-4a8f-a26d-63f399ed91b8] [Similarity score:             0.856241] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> Top 2 nodes:\n",
      "> [Node 6c3c1bc2-f2d8-40a7-9c7a-d2dbb6c80662] [Similarity score: 0.7005425530151507] Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex\n",
      "In this technical walk...\n",
      "> [Node 8a543b31-11f6-4f33-b85f-99f5ef537ff0] [Similarity score: 0.6897434358508218] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node 6c3c1bc2-f2d8-40a7-9c7a-d2dbb6c80662] [Similarity score:             0.700543] Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex\n",
      "In this technical walk...\n",
      "> [Node 8a543b31-11f6-4f33-b85f-99f5ef537ff0] [Similarity score:             0.689743] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node d18acc51-4c4b-4242-b283-69ac77c1fed7] [Similarity score: 0.6173530594455052] However, as we progressed and aimed for higher benchmarks, we recognized the need to migrate to a...\n",
      "> [Node f12f1c47-20fa-4390-9398-e364392ab62c] [Similarity score: 0.616331406054132] If you would like additional reasoning or explanation, use a more prescriptive approach: Add deta...\n",
      "> Top 2 nodes:\n",
      "> [Node d18acc51-4c4b-4242-b283-69ac77c1fed7] [Similarity score:             0.617353] However, as we progressed and aimed for higher benchmarks, we recognized the need to migrate to a...\n",
      "> [Node f12f1c47-20fa-4390-9398-e364392ab62c] [Similarity score:             0.616331] If you would like additional reasoning or explanation, use a more prescriptive approach: Add deta...\n",
      "> Top 2 nodes:\n",
      "> [Node e2724d54-1f64-47d9-bafa-65db3572bc7d] [Similarity score: 0.6785849405482006] float16,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_use_double_quant=True, ) def messages_to_pro...\n",
      "> [Node f12f1c47-20fa-4390-9398-e364392ab62c] [Similarity score: 0.6456473141940214] If you would like additional reasoning or explanation, use a more prescriptive approach: Add deta...\n",
      "> Top 2 nodes:\n",
      "> [Node e2724d54-1f64-47d9-bafa-65db3572bc7d] [Similarity score:             0.678585] float16,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_use_double_quant=True, ) def messages_to_pro...\n",
      "> [Node f12f1c47-20fa-4390-9398-e364392ab62c] [Similarity score:             0.645647] If you would like additional reasoning or explanation, use a more prescriptive approach: Add deta...\n",
      "> Top 2 nodes:\n",
      "> [Node af1178e0-e024-4c3d-9291-25f7787f0675] [Similarity score: 0.6671297846421331] The independent company initiatives include ‚Äúexpansion of Light Vehicles‚Äù (1), ‚Äúincremental inves...\n",
      "> [Node 0ed0b7a3-4b62-41fa-b518-382b9009f3b2] [Similarity score: 0.6660838768282832] We also showcase some results of pure LLM-based retrieval (though we don‚Äôt showcase as many resul...\n",
      "> Top 2 nodes:\n",
      "> [Node af1178e0-e024-4c3d-9291-25f7787f0675] [Similarity score:             0.66713] The independent company initiatives include ‚Äúexpansion of Light Vehicles‚Äù (1), ‚Äúincremental inves...\n",
      "> [Node 0ed0b7a3-4b62-41fa-b518-382b9009f3b2] [Similarity score:             0.666084] We also showcase some results of pure LLM-based retrieval (though we don‚Äôt showcase as many resul...\n",
      "> Top 2 nodes:\n",
      "> [Node 9c2ca83a-eab0-491d-8014-9980eb746d0e] [Similarity score: 0.7046266256726763] We did not test out other agent interaction patterns besides ReAct: ‚Äúplan and solve‚Äù agents (thou...\n",
      "> [Node 8b1f680b-98a8-4088-b680-0ec4dd4f627f] [Similarity score: 0.7021697344434169] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> Top 2 nodes:\n",
      "> [Node 9c2ca83a-eab0-491d-8014-9980eb746d0e] [Similarity score:             0.704627] We did not test out other agent interaction patterns besides ReAct: ‚Äúplan and solve‚Äù agents (thou...\n",
      "> [Node 8b1f680b-98a8-4088-b680-0ec4dd4f627f] [Similarity score:             0.70217] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> Top 2 nodes:\n",
      "> [Node b72412f6-c2fc-438c-989f-6736acf0efcc] [Similarity score: 0.8079133618556177] Context-1:  Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron‚àóLouis Martin‚Ä†Kevin ...\n",
      "> [Node e5eca8e9-1297-4e77-b950-7a260fd5af2c] [Similarity score: 0.8061820709513321] ‚àóEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com ‚Ä†Second author Contribu...\n",
      "> Top 2 nodes:\n",
      "> [Node b72412f6-c2fc-438c-989f-6736acf0efcc] [Similarity score:             0.807913] Context-1:  Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron‚àóLouis Martin‚Ä†Kevin ...\n",
      "> [Node e5eca8e9-1297-4e77-b950-7a260fd5af2c] [Similarity score:             0.806182] ‚àóEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com ‚Ä†Second author Contribu...\n",
      "> Top 2 nodes:\n",
      "> [Node 0d0a13ff-5e56-4d3d-841e-b9e755a82760] [Similarity score: 0.7384351451280461] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node ecc83656-c9e3-43a6-99a5-295caf6a6df7] [Similarity score: 0.7343019637627693] Say that you want to build a chatbot Define the dataset (here it‚Äôs a web page, can also be a loca...\n",
      "> Top 2 nodes:\n",
      "> [Node 0d0a13ff-5e56-4d3d-841e-b9e755a82760] [Similarity score:             0.738435] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node ecc83656-c9e3-43a6-99a5-295caf6a6df7] [Similarity score:             0.734302] Say that you want to build a chatbot Define the dataset (here it‚Äôs a web page, can also be a loca...\n",
      "> Top 2 nodes:\n",
      "> [Node eeb40b92-118e-4dc7-97de-fb08e36ae744] [Similarity score: 0.7319546482385184] kingzzm‚Äôs   tutorial  on enhancing RAG Performance to overcome the issue of 'broken' context in R...\n",
      "> [Node bdf5fca6-8e6b-4de8-85be-b8c1bb3fcb21] [Similarity score: 0.7314758360506539] Evaluating Multi-Modal Retrieval-Augmented Generation\n",
      "A few days ago, we published a blog on  Mul...\n",
      "> Top 2 nodes:\n",
      "> [Node eeb40b92-118e-4dc7-97de-fb08e36ae744] [Similarity score:             0.731955] kingzzm‚Äôs   tutorial  on enhancing RAG Performance to overcome the issue of 'broken' context in R...\n",
      "> [Node bdf5fca6-8e6b-4de8-85be-b8c1bb3fcb21] [Similarity score:             0.731476] Evaluating Multi-Modal Retrieval-Augmented Generation\n",
      "A few days ago, we published a blog on  Mul...\n",
      "> Top 2 nodes:\n",
      "> [Node 488d28f6-c55a-4d20-9210-d4e2bc171110] [Similarity score: 0.7550027154155525] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> [Node 127ba93d-6270-4a3b-be98-6bdccb24e18d] [Similarity score: 0.7493181916589714] With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, a...\n",
      "> Top 2 nodes:\n",
      "> [Node 488d28f6-c55a-4d20-9210-d4e2bc171110] [Similarity score:             0.755003] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> [Node 127ba93d-6270-4a3b-be98-6bdccb24e18d] [Similarity score:             0.749318] With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, a...\n",
      "> Top 2 nodes:\n",
      "> [Node 7a510cdf-46aa-4cf7-b997-93ecaec997e2] [Similarity score: 0.821871192237042] These (question, chunk) pairs are then used as positive examples as training signals for the mode...\n",
      "> [Node 67ecd010-251f-4d8d-bbd0-c2d7a61e8039] [Similarity score: 0.804658587843203] Building and Evaluating a QA System with LlamaIndex\n",
      "Introduction LlamaIndex (GPT Index)  offers a...\n",
      "> Top 2 nodes:\n",
      "> [Node 7a510cdf-46aa-4cf7-b997-93ecaec997e2] [Similarity score:             0.821871] These (question, chunk) pairs are then used as positive examples as training signals for the mode...\n",
      "> [Node 67ecd010-251f-4d8d-bbd0-c2d7a61e8039] [Similarity score:             0.804659] Building and Evaluating a QA System with LlamaIndex\n",
      "Introduction LlamaIndex (GPT Index)  offers a...\n",
      "> Top 2 nodes:\n",
      "> [Node 0d0a13ff-5e56-4d3d-841e-b9e755a82760] [Similarity score: 0.7104108229869489] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node 488d28f6-c55a-4d20-9210-d4e2bc171110] [Similarity score: 0.7072820066698097] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node 0d0a13ff-5e56-4d3d-841e-b9e755a82760] [Similarity score:             0.710411] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node 488d28f6-c55a-4d20-9210-d4e2bc171110] [Similarity score:             0.707282] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node cff80aa5-f82a-4766-8c59-1c1e3791f665] [Similarity score: 0.8244552868994978] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> [Node 687d110b-6a80-4c41-8132-c1cf1b479d21] [Similarity score: 0.8204567501059012] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n",
      "> Top 2 nodes:\n",
      "> [Node cff80aa5-f82a-4766-8c59-1c1e3791f665] [Similarity score:             0.824455] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> [Node 687d110b-6a80-4c41-8132-c1cf1b479d21] [Similarity score:             0.820457] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n",
      "> Top 2 nodes:\n",
      "> [Node 687d110b-6a80-4c41-8132-c1cf1b479d21] [Similarity score: 0.7972364375050134] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n",
      "> [Node cff80aa5-f82a-4766-8c59-1c1e3791f665] [Similarity score: 0.7895098873125608] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> Top 2 nodes:\n",
      "> [Node 687d110b-6a80-4c41-8132-c1cf1b479d21] [Similarity score:             0.797236] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n",
      "> [Node cff80aa5-f82a-4766-8c59-1c1e3791f665] [Similarity score:             0.78951] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_2_retrieval_eval</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.266618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  top_2_retrieval_eval  0.473684  0.421053   0.236842  0.473684  0.421053   \n",
       "\n",
       "       ndcg  \n",
       "0  0.266618  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it‚Äôs about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system‚Äôs accuracy by looking at the rank of the highest-placed relevant document. Specifically, it‚Äôs the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it‚Äôs second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aevaluate_labelled_rag_dataset(response_eval_dataset, query_engine, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Make predictions with the dataset\n",
    "    response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "        predictor=query_engine, batch_size=batch_size, show_progress=True\n",
    "    )\n",
    "\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_SYNTHETIC_EVAL_DATASET = False\n",
    "RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:07:12.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-22 15:07:19.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading existing synthetic response eval dataset at data/001/llamaindex_blog_response_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # set context for llm provider\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 294ecbb3-ea9a-4cc9-889f-fad2fb89a032] [Similarity score: 0.7902765003425318] This stack is different from any ETL stack before it, because unlike traditional software, every ...\n",
      "> [Node 90bee6f8-8ac2-4da6-8127-f13e4488155d] [Similarity score: 0.7768721898180933] This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXi...\n",
      "> Top 2 nodes:\n",
      "> [Node 294ecbb3-ea9a-4cc9-889f-fad2fb89a032] [Similarity score:             0.790277] This stack is different from any ETL stack before it, because unlike traditional software, every ...\n",
      "> [Node 90bee6f8-8ac2-4da6-8127-f13e4488155d] [Similarity score:             0.776872] This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXi...\n",
      "> Top 2 nodes:\n",
      "> [Node b09a7306-7211-49b1-8153-df9fdd50c2f0] [Similarity score: 0.7442927068795645] It‚Äôs what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what...\n",
      "> [Node 72301e09-d5ca-480d-ac8c-8e5fc20d3f3e] [Similarity score: 0.740255919439983] A fantastic fusion of tech and medicine! SEC Insights AI  does SEC document analysis using LlamaI...\n",
      "> Top 2 nodes:\n",
      "> [Node b09a7306-7211-49b1-8153-df9fdd50c2f0] [Similarity score:             0.744293] It‚Äôs what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what...\n",
      "> [Node 72301e09-d5ca-480d-ac8c-8e5fc20d3f3e] [Similarity score:             0.740256] A fantastic fusion of tech and medicine! SEC Insights AI  does SEC document analysis using LlamaI...\n",
      "> Top 2 nodes:\n",
      "> [Node c64cb1f0-5316-46ba-a6d9-9534ce72ab4d] [Similarity score: 0.8306884676010349] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 741b7c75-2b65-4438-b702-ac5cce00dc1d] [Similarity score: 0.7597227376775914] Notebook . üó∫Ô∏è Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pip...\n",
      "> Top 2 nodes:\n",
      "> [Node c64cb1f0-5316-46ba-a6d9-9534ce72ab4d] [Similarity score:             0.830688] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 741b7c75-2b65-4438-b702-ac5cce00dc1d] [Similarity score:             0.759723] Notebook . üó∫Ô∏è Guides: Guide  to Integrating LlamaParse with Knowledge Graphs to develop a RAG pip...\n",
      "> Top 2 nodes:\n",
      "> [Node 07a02299-ab45-4803-bd48-5fde1a68c7d4] [Similarity score: 0.817949869375321] LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Community ü¶ô, We are excited to collaborate with Deep...\n",
      "> [Node 640701d0-cb00-4e7d-9eaf-9e2865330c6e] [Similarity score: 0.8105186789648405] The techniques include Hybrid Fusion, Query Rewriting + Fusion, Retrieval with Embedded Tables, A...\n",
      "> Top 2 nodes:\n",
      "> [Node 07a02299-ab45-4803-bd48-5fde1a68c7d4] [Similarity score:             0.81795] LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Community ü¶ô, We are excited to collaborate with Deep...\n",
      "> [Node 640701d0-cb00-4e7d-9eaf-9e2865330c6e] [Similarity score:             0.810519] The techniques include Hybrid Fusion, Query Rewriting + Fusion, Retrieval with Embedded Tables, A...\n",
      "> Top 2 nodes:\n",
      "> [Node 44e38a88-6828-4bab-8cfe-5beaa6b41719] [Similarity score: 0.7926220396949905] Querying a network of knowledge with llama-index-networks\n",
      "The main premise behind RAG is the inje...\n",
      "> [Node 0fdae8e2-3272-4565-babc-f5dad3939941] [Similarity score: 0.7857441614093106] A place where data suppliers package their data in the form of RAGs to data consumers look to exp...\n",
      "> Top 2 nodes:\n",
      "> [Node 44e38a88-6828-4bab-8cfe-5beaa6b41719] [Similarity score:             0.792622] Querying a network of knowledge with llama-index-networks\n",
      "The main premise behind RAG is the inje...\n",
      "> [Node 0fdae8e2-3272-4565-babc-f5dad3939941] [Similarity score:             0.785744] A place where data suppliers package their data in the form of RAGs to data consumers look to exp...\n",
      "> Top 2 nodes:\n",
      "> [Node 277220f5-b15c-456b-a351-258be045e859] [Similarity score: 0.7636120769954494] Tweet . We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mi...\n",
      "> [Node 4903b02d-4a6a-41ce-aaab-9cab7794cb8e] [Similarity score: 0.751940727582034] ‚úçÔ∏è Tutorials: Laurie‚Äôs   Advanced Querying & Retrieval Techniques comprehensive code-level tutori...\n",
      "> Top 2 nodes:\n",
      "> [Node 277220f5-b15c-456b-a351-258be045e859] [Similarity score:             0.763612] Tweet . We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mi...\n",
      "> [Node 4903b02d-4a6a-41ce-aaab-9cab7794cb8e] [Similarity score:             0.751941] ‚úçÔ∏è Tutorials: Laurie‚Äôs   Advanced Querying & Retrieval Techniques comprehensive code-level tutori...\n",
      "> Top 2 nodes:\n",
      "> [Node c64cb1f0-5316-46ba-a6d9-9534ce72ab4d] [Similarity score: 0.7845975657679731] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node e0da6e85-6a08-4b1f-a978-75f3f83a640a] [Similarity score: 0.7753241079438594] Below, we list a select few of the evaluation notebook guides. Answer Relevancy and Context Relev...\n",
      "> Top 2 nodes:\n",
      "> [Node c64cb1f0-5316-46ba-a6d9-9534ce72ab4d] [Similarity score:             0.784598] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node e0da6e85-6a08-4b1f-a978-75f3f83a640a] [Similarity score:             0.775324] Below, we list a select few of the evaluation notebook guides. Answer Relevancy and Context Relev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 7/8 [00:04<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node a869feb3-4f92-406f-bc83-4336f5fbecf2] [Similarity score: 0.7958722664575402] LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Llama Followers ü¶ô, We‚Äôre excited to bring you another...\n",
      "> [Node ca46dbfc-c51d-4174-8e17-bb516ba98ae3] [Similarity score: 0.7687064012879486] Linking the resources again below: Gemini (text-only) Guide Gemini (multi-modal) Guide Semantic R...\n",
      "> Top 2 nodes:\n",
      "> [Node a869feb3-4f92-406f-bc83-4336f5fbecf2] [Similarity score:             0.795872] LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Llama Followers ü¶ô, We‚Äôre excited to bring you another...\n",
      "> [Node ca46dbfc-c51d-4174-8e17-bb516ba98ae3] [Similarity score:             0.768706] Linking the resources again below: Gemini (text-only) Guide Gemini (multi-modal) Guide Semantic R...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.06it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score: 0.7829275434542138] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score: 0.7765200492167021] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score:             0.782928] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score:             0.77652] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node d3b4fbb6-70ff-4970-89a1-caf6b2d634a0] [Similarity score: 0.8352047269882725] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node f6bc96f3-6d28-4a39-8e43-55c7abe53b3f] [Similarity score: 0.8034565669392124] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> Top 2 nodes:\n",
      "> [Node d3b4fbb6-70ff-4970-89a1-caf6b2d634a0] [Similarity score:             0.835205] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node f6bc96f3-6d28-4a39-8e43-55c7abe53b3f] [Similarity score:             0.803457] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> Top 2 nodes:\n",
      "> [Node 268f8370-3989-4d4d-ab91-8ad34b7f9bf7] [Similarity score: 0.848671714931103] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node 504495df-6823-47fd-9c81-b8a7ced10178] [Similarity score: 0.7666758245337346] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 268f8370-3989-4d4d-ab91-8ad34b7f9bf7] [Similarity score:             0.848672] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node 504495df-6823-47fd-9c81-b8a7ced10178] [Similarity score:             0.766676] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 53b25f37-f5c2-4c89-9052-8c63b2201e17] [Similarity score: 0.7276074586972303] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> [Node d6f028a8-96e2-44a4-b841-b0c540145d42] [Similarity score: 0.7147993883079442] Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex\n",
      "Introduction Retrieval-augmente...\n",
      "> Top 2 nodes:\n",
      "> [Node 53b25f37-f5c2-4c89-9052-8c63b2201e17] [Similarity score:             0.727607] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> [Node d6f028a8-96e2-44a4-b841-b0c540145d42] [Similarity score:             0.714799] Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex\n",
      "Introduction Retrieval-augmente...\n",
      "> Top 2 nodes:\n",
      "> [Node 268f8370-3989-4d4d-ab91-8ad34b7f9bf7] [Similarity score: 0.7650013366175522] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node 504495df-6823-47fd-9c81-b8a7ced10178] [Similarity score: 0.7467527379852195] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 268f8370-3989-4d4d-ab91-8ad34b7f9bf7] [Similarity score:             0.765001] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node 504495df-6823-47fd-9c81-b8a7ced10178] [Similarity score:             0.746753] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 37d0cb58-215b-4aa1-9a76-c3bc0a2bdfa0] [Similarity score: 0.7745836455705898] In just six months, the project garnered an impressive following, with  16K Github Stars ,  20K T...\n",
      "> [Node d3b4fbb6-70ff-4970-89a1-caf6b2d634a0] [Similarity score: 0.7684403336594633] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> Top 2 nodes:\n",
      "> [Node 37d0cb58-215b-4aa1-9a76-c3bc0a2bdfa0] [Similarity score:             0.774584] In just six months, the project garnered an impressive following, with  16K Github Stars ,  20K T...\n",
      "> [Node d3b4fbb6-70ff-4970-89a1-caf6b2d634a0] [Similarity score:             0.76844] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> Top 2 nodes:\n",
      "> [Node e4c412e7-0c71-4e41-a04e-49a5b3470fe6] [Similarity score: 0.7491060955011721] Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\n",
      "Co-authors: Prakul Agarwal ‚Äî ...\n",
      "> [Node f143db7c-0750-47a2-9253-3d7067ebdfb4] [Similarity score: 0.7406759983617656] Or you can get started directly  here . Use of LLMs LlamaIndex uses LangChain‚Äôs (another popular ...\n",
      "> Top 2 nodes:\n",
      "> [Node e4c412e7-0c71-4e41-a04e-49a5b3470fe6] [Similarity score:             0.749106] Build a ChatGPT with your Private Data using LlamaIndex and MongoDB\n",
      "Co-authors: Prakul Agarwal ‚Äî ...\n",
      "> [Node f143db7c-0750-47a2-9253-3d7067ebdfb4] [Similarity score:             0.740676] Or you can get started directly  here . Use of LLMs LlamaIndex uses LangChain‚Äôs (another popular ...\n",
      "> Top 2 nodes:\n",
      "> [Node 60c24335-99d8-484a-9b8d-7574b25798a3] [Similarity score: 0.7093723392499391] Answer: To answer which model has higher performance in SAT-en in the 7B series models, I will fo...\n",
      "> [Node 67c882e4-a421-4081-b470-5d016487aa68] [Similarity score: 0.6941406629679887] Answer: Examine the Image: The image is a bar chart depicting violation percentages in three cate...\n",
      "> Top 2 nodes:\n",
      "> [Node 60c24335-99d8-484a-9b8d-7574b25798a3] [Similarity score:             0.709372] Answer: To answer which model has higher performance in SAT-en in the 7B series models, I will fo...\n",
      "> [Node 67c882e4-a421-4081-b470-5d016487aa68] [Similarity score:             0.694141] Answer: Examine the Image: The image is a bar chart depicting violation percentages in three cate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.20it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 3078847e-199e-4273-a522-2e3e2f5e648a] [Similarity score: 0.8642420660772184] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node dea8c47b-84b7-4768-b03d-568182ada8eb] [Similarity score: 0.756826920715372] ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline ...\n",
      "> Top 2 nodes:\n",
      "> [Node 3078847e-199e-4273-a522-2e3e2f5e648a] [Similarity score:             0.864242] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node dea8c47b-84b7-4768-b03d-568182ada8eb] [Similarity score:             0.756827] ‚ÄúNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline ...\n",
      "> Top 2 nodes:\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score: 0.740371794695132] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score: 0.7351298386190317] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score:             0.740372] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score:             0.73513] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score: 0.8317222958872582] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score: 0.8303631449791112] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> Top 2 nodes:\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score:             0.831722] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score:             0.830363] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> Top 2 nodes:\n",
      "> [Node e9fe28b6-1a04-4c12-a7e5-b58ef0423796] [Similarity score: 0.7337114016175477] Building the data framework for LLMs\n",
      "Today is an exciting day for LlamaIndex, and a big milestone...\n",
      "> [Node 7d40cab4-0892-4f1f-9fa1-78b3dabbaa88] [Similarity score: 0.7301314224809352] Building An Intelligent Query-Response System with LlamaIndex and OpenLLM\n",
      "Over the past year, Lar...\n",
      "> Top 2 nodes:\n",
      "> [Node e9fe28b6-1a04-4c12-a7e5-b58ef0423796] [Similarity score:             0.733711] Building the data framework for LLMs\n",
      "Today is an exciting day for LlamaIndex, and a big milestone...\n",
      "> [Node 7d40cab4-0892-4f1f-9fa1-78b3dabbaa88] [Similarity score:             0.730131] Building An Intelligent Query-Response System with LlamaIndex and OpenLLM\n",
      "Over the past year, Lar...\n",
      "> Top 2 nodes:\n",
      "> [Node 6e1236b4-b650-4e40-896f-8b7c9e024089] [Similarity score: 0.7421315867607672] Voyage AI Pack. Every Pack has a detailed README on how to use / modules. First, we download and ...\n",
      "> [Node 33ded8d6-c928-4c46-8078-078e931b5236] [Similarity score: 0.7376114176680696] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 6e1236b4-b650-4e40-896f-8b7c9e024089] [Similarity score:             0.742132] Voyage AI Pack. Every Pack has a detailed README on how to use / modules. First, we download and ...\n",
      "> [Node 33ded8d6-c928-4c46-8078-078e931b5236] [Similarity score:             0.737611] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 08a0b1fc-d3e7-4152-aec0-622e31cd1059] [Similarity score: 0.780590755201142] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 605984f0-4094-4928-801b-4bbc4d967ef7] [Similarity score: 0.7648415336073079] Codebase . üó∫Ô∏è Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node 08a0b1fc-d3e7-4152-aec0-622e31cd1059] [Similarity score:             0.780591] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 605984f0-4094-4928-801b-4bbc4d967ef7] [Similarity score:             0.764842] Codebase . üó∫Ô∏è Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node db5c3752-a874-4ad9-b9ae-f4524a702841] [Similarity score: 0.7819128401716526] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 9c496a2d-7cae-42ee-8ec2-639e80930872] [Similarity score: 0.75723600211852] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n",
      "> Top 2 nodes:\n",
      "> [Node db5c3752-a874-4ad9-b9ae-f4524a702841] [Similarity score:             0.781913] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> [Node 9c496a2d-7cae-42ee-8ec2-639e80930872] [Similarity score:             0.757236] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n",
      "> Top 2 nodes:\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score: 0.7903298947003353] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score: 0.7830979749985305] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> Top 2 nodes:\n",
      "> [Node c1f13df6-488f-425c-aa9c-5b1eb3e0af36] [Similarity score:             0.79033] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score:             0.783098] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.83it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node c32329ef-6d60-4636-a599-9ec12928ac95] [Similarity score: 0.68926271740739] from  transformers  import  pipeline\n",
      " from  transformers  import  AutoTokenizer, AutoModelForToke...\n",
      "> [Node 4a4de14c-9803-4597-ad0c-47a616719af3] [Similarity score: 0.6851382079260567] NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app\n",
      "We‚Äôre excited t...\n",
      "> Top 2 nodes:\n",
      "> [Node c32329ef-6d60-4636-a599-9ec12928ac95] [Similarity score:             0.689263] from  transformers  import  pipeline\n",
      " from  transformers  import  AutoTokenizer, AutoModelForToke...\n",
      "> [Node 4a4de14c-9803-4597-ad0c-47a616719af3] [Similarity score:             0.685138] NewsGPT(Neotice): Summarize news articles with LlamaIndex ‚Äî Hackathon winning app\n",
      "We‚Äôre excited t...\n",
      "> Top 2 nodes:\n",
      "> [Node cf13437a-eb99-40cc-bbab-804d77e1de98] [Similarity score: 0.79416673127969] LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Friends ü¶ô LlamaIndex is 1 year old this week! üéâ To c...\n",
      "> [Node 2d8ceeda-0623-498b-9090-53b2006f5cd2] [Similarity score: 0.7695210039407967] üé• Events: Ravi Theja gave talk on Building Multi-Tenancy RAG System with LlamaIndex and Qdrant at...\n",
      "> Top 2 nodes:\n",
      "> [Node cf13437a-eb99-40cc-bbab-804d77e1de98] [Similarity score:             0.794167] LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Friends ü¶ô LlamaIndex is 1 year old this week! üéâ To c...\n",
      "> [Node 2d8ceeda-0623-498b-9090-53b2006f5cd2] [Similarity score:             0.769521] üé• Events: Ravi Theja gave talk on Building Multi-Tenancy RAG System with LlamaIndex and Qdrant at...\n",
      "> Top 2 nodes:\n",
      "> [Node ce8edf9c-0f23-427f-aa5c-db1c832ad2fa] [Similarity score: 0.7153454140661204] Harshad Suryawanshi ‚Äôs  tutorial  covers Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and L...\n",
      "> [Node 0ced6110-5475-4efb-9e05-a661df84dd08] [Similarity score: 0.7112262390814935] LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama Fans! ü¶ô We hope you enjoyed our  OpenAI Dev Day s...\n",
      "> Top 2 nodes:\n",
      "> [Node ce8edf9c-0f23-427f-aa5c-db1c832ad2fa] [Similarity score:             0.715345] Harshad Suryawanshi ‚Äôs  tutorial  covers Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and L...\n",
      "> [Node 0ced6110-5475-4efb-9e05-a661df84dd08] [Similarity score:             0.711226] LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama Fans! ü¶ô We hope you enjoyed our  OpenAI Dev Day s...\n",
      "> Top 2 nodes:\n",
      "> [Node 9c496a2d-7cae-42ee-8ec2-639e80930872] [Similarity score: 0.7309023806099979] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n",
      "> [Node db5c3752-a874-4ad9-b9ae-f4524a702841] [Similarity score: 0.7217996123683568] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> Top 2 nodes:\n",
      "> [Node 9c496a2d-7cae-42ee-8ec2-639e80930872] [Similarity score:             0.730902] Take a look at this folder for a full set of examples:  https://github.com/run-llama/llama-hub/tr...\n",
      "> [Node db5c3752-a874-4ad9-b9ae-f4524a702841] [Similarity score:             0.7218] Introducing Llama Packs\n",
      "Today we‚Äôre excited to introduce  Llama Packs ü¶ôüì¶‚Äî  a community-driven hub...\n",
      "> Top 2 nodes:\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score: 0.801823118139039] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> [Node 4cebdb4b-59ab-4150-9f49-0815d04afc50] [Similarity score: 0.7978794980363344] Guide  on building a full-stack financial analysis bot using  create-llama  and Llama Index's RAG...\n",
      "> Top 2 nodes:\n",
      "> [Node e0f90e61-0084-497c-91aa-067748038341] [Similarity score:             0.801823] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> [Node 4cebdb4b-59ab-4150-9f49-0815d04afc50] [Similarity score:             0.797879] Guide  on building a full-stack financial analysis bot using  create-llama  and Llama Index's RAG...\n",
      "> Top 2 nodes:\n",
      "> [Node 53f21aea-0624-4a0b-8489-ce4f46746490] [Similarity score: 0.6681199672053417] Next Steps Our early users have already given us important feedback on what they‚Äôd like to see ne...\n",
      "> [Node 15e6e114-2c39-4eb1-a91e-4269caa7a06a] [Similarity score: 0.6668409614750742] Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n",
      "\n",
      "Response: \n",
      "The Ra...\n",
      "> Top 2 nodes:\n",
      "> [Node 53f21aea-0624-4a0b-8489-ce4f46746490] [Similarity score:             0.66812] Next Steps Our early users have already given us important feedback on what they‚Äôd like to see ne...\n",
      "> [Node 15e6e114-2c39-4eb1-a91e-4269caa7a06a] [Similarity score:             0.666841] Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n",
      "\n",
      "Response: \n",
      "The Ra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:04<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7652c3ab54804d78a05e2fa3b2e99da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: A place where data suppliers package their data...\n",
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: A place where data suppliers package their data...\n",
      "> Adding chunk: This stack is different from any ETL stack befo...\n",
      "> Adding chunk: This is a surprisingly prevalent use case acros...\n",
      "> Adding chunk: This stack is different from any ETL stack befo...\n",
      "> Adding chunk: This is a surprisingly prevalent use case acros...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Notebook . üó∫Ô∏è Guides: Guide  to Integrating Lla...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Notebook . üó∫Ô∏è Guides: Guide  to Integrating Lla...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: It‚Äôs what gets us up in the morning and keeps u...\n",
      "> Adding chunk: A fantastic fusion of tech and medicine! SEC In...\n",
      "> Adding chunk: It‚Äôs what gets us up in the morning and keeps u...\n",
      "> Adding chunk: A fantastic fusion of tech and medicine! SEC In...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: The techniques include Hybrid Fusion, Query Rew...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: The techniques include Hybrid Fusion, Query Rew...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n",
      "> Adding chunk: Linking the resources again below: Gemini (text...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì12‚Äì19\n",
      "What‚Äôs up, Lla...\n",
      "> Adding chunk: Linking the resources again below: Gemini (text...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: ‚úçÔ∏è Tutorials: Laurie‚Äôs   Advanced Querying & Re...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: ‚úçÔ∏è Tutorials: Laurie‚Äôs   Advanced Querying & Re...\n",
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n",
      "> Adding chunk: Or you can get started directly  here . Use of ...\n",
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n",
      "> Adding chunk: Or you can get started directly  here . Use of ...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: In just six months, the project garnered an imp...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: In just six months, the project garnered an imp...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Evaluating the Ideal Chunk Size for a RAG Syste...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Evaluating the Ideal Chunk Size for a RAG Syste...\n",
      "> Adding chunk: Answer: To answer which model has higher perfor...\n",
      "> Adding chunk: Answer: Examine the Image: The image is a bar c...\n",
      "> Adding chunk: Answer: To answer which model has higher perfor...\n",
      "> Adding chunk: Answer: Examine the Image: The image is a bar c...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . üó∫Ô∏è Guides: Guide  to Building an Age...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: ‚ÄúNow, developers can abstract complexities asso...\n",
      "> Adding chunk: Building the data framework for LLMs\n",
      "Today is a...\n",
      "> Adding chunk: Building An Intelligent Query-Response System w...\n",
      "> Adding chunk: Building the data framework for LLMs\n",
      "Today is a...\n",
      "> Adding chunk: Building An Intelligent Query-Response System w...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Take a look at this folder for a full set of ex...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Take a look at this folder for a full set of ex...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Next Steps Our early users have already given u...\n",
      "> Adding chunk: Q: \"Compare and contrast how the Ray docs and t...\n",
      "> Adding chunk: Next Steps Our early users have already given u...\n",
      "> Adding chunk: Q: \"Compare and contrast how the Ray docs and t...\n",
      "> Adding chunk: Take a look at this folder for a full set of ex...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: Take a look at this folder for a full set of ex...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we‚Äôre excited to ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Fr...\n",
      "> Adding chunk: üé• Events: Ravi Theja gave talk on Building Mult...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023‚Äì11‚Äì14\n",
      "Hello Llama Fr...\n",
      "> Adding chunk: üé• Events: Ravi Theja gave talk on Building Mult...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Guide  on building a full-stack financial analy...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Guide  on building a full-stack financial analy...\n",
      "> Adding chunk: from  transformers  import  pipeline\n",
      " from  tra...\n",
      "> Adding chunk: NewsGPT(Neotice): Summarize news articles with ...\n",
      "> Adding chunk: from  transformers  import  pipeline\n",
      " from  tra...\n",
      "> Adding chunk: NewsGPT(Neotice): Summarize news articles with ...\n",
      "> Adding chunk: Harshad Suryawanshi ‚Äôs  tutorial  covers Buildi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama...\n",
      "> Adding chunk: Harshad Suryawanshi ‚Äôs  tutorial  covers Buildi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023-11‚Äì07\n",
      "Hi again Llama...\n"
     ]
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = await aevaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    query_engine,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.134615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.134615\n",
       "mean_relevancy_score     0.933333\n",
       "mean_faithfulness_score  1.000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the new feature released by LlamaInde...</td>\n",
       "      <td>The new feature released by LlamaIndex, llama-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discuss the advancements made in LlamaIndex's ...</td>\n",
       "      <td>The advancements made in LlamaIndex's PDF pars...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain the three main sections of the OpenAI ...</td>\n",
       "      <td>The three main sections of the OpenAI Cookbook...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the OpenAI Cookbook suggest evaluatin...</td>\n",
       "      <td>The OpenAI Cookbook suggests evaluating RAG sy...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has LlamaIndex evolved over the past year ...</td>\n",
       "      <td>LlamaIndex has evolved over the past year by e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can you explain the significance of Retrieval-...</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) plays a s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the partnership with Google Gemini be...</td>\n",
       "      <td>The partnership with Google Gemini benefits Ll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Describe the new Multi-Doc SEC 10Q Dataset lau...</td>\n",
       "      <td>The new Multi-Doc SEC 10Q Dataset launched by ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>The MemoryCache project by Mozilla utilizes Pr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the key features of the LionAGI agent...</td>\n",
       "      <td>The LionAGI agent framework by Ocean Li incorp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does LlamaCloud help in increasing develop...</td>\n",
       "      <td>LlamaCloud helps in increasing developer colla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>New features introduced in LlamaCloud to impro...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does LlamaIndex help in building LLM apps ...</td>\n",
       "      <td>LlamaIndex helps in building LLM apps by provi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Can you explain the process of wrapping a Llam...</td>\n",
       "      <td>The process of wrapping a LlamaIndex app with ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the feedback function in TruLlama eva...</td>\n",
       "      <td>The feedback function in TruLlama evaluates th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the significance of identifyin...</td>\n",
       "      <td>Identifying instances where the model hallucin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does Agentic RAG incorporate agents into e...</td>\n",
       "      <td>Agentic RAG incorporates agents into existing ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Explain the process by which LlamaIndex's Agen...</td>\n",
       "      <td>The Agentic RAG approach by LlamaIndex breaks ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Explain the concept of Agentic RAG as describe...</td>\n",
       "      <td>Agentic RAG is a framework that incorporates a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does the implementation by LlamaIndex show...</td>\n",
       "      <td>The implementation by LlamaIndex showcases the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the integration of NVIDIA NIM with Ll...</td>\n",
       "      <td>The integration of NVIDIA NIM with LlamaIndex ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What benefits do developers gain from using NV...</td>\n",
       "      <td>Developers gain benefits from using NVIDIA NIM...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How does Llama Packs aim to simplify the proce...</td>\n",
       "      <td>Llama Packs aim to simplify the process of bui...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Can you explain the process of downloading and...</td>\n",
       "      <td>To download and initialize a Llama Pack using ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Explain the process of downloading and initial...</td>\n",
       "      <td>To download and initialize a Llama Pack like t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How can a user customize a Llama Pack, such as...</td>\n",
       "      <td>To customize a Llama Pack by swapping out the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the LlamaIndex newsletter celebrate i...</td>\n",
       "      <td>The LlamaIndex newsletter celebrates its one-y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Discuss the significance of the Multi-Modal RA...</td>\n",
       "      <td>The Multi-Modal RAG Stack and the OpenAIAssist...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How does the GPT Builder mentioned in the news...</td>\n",
       "      <td>The GPT Builder mentioned in the newsletter st...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Discuss the topics covered in the various tuto...</td>\n",
       "      <td>The tutorials mentioned in the newsletter cove...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the new feature released by LlamaInde...   \n",
       "1   Discuss the advancements made in LlamaIndex's ...   \n",
       "2   Explain the three main sections of the OpenAI ...   \n",
       "3   How does the OpenAI Cookbook suggest evaluatin...   \n",
       "4   How has LlamaIndex evolved over the past year ...   \n",
       "5   Can you explain the significance of Retrieval-...   \n",
       "6   How does the partnership with Google Gemini be...   \n",
       "7   Describe the new Multi-Doc SEC 10Q Dataset lau...   \n",
       "8   How does the MemoryCache project by Mozilla ut...   \n",
       "9   What are the key features of the LionAGI agent...   \n",
       "10  How does LlamaCloud help in increasing develop...   \n",
       "11  What new features have been introduced in Llam...   \n",
       "12  How does LlamaIndex help in building LLM apps ...   \n",
       "13  Can you explain the process of wrapping a Llam...   \n",
       "14  How does the feedback function in TruLlama eva...   \n",
       "15  Can you explain the significance of identifyin...   \n",
       "16  How does Agentic RAG incorporate agents into e...   \n",
       "17  Explain the process by which LlamaIndex's Agen...   \n",
       "18  Explain the concept of Agentic RAG as describe...   \n",
       "19  How does the implementation by LlamaIndex show...   \n",
       "20  How does the integration of NVIDIA NIM with Ll...   \n",
       "21  What benefits do developers gain from using NV...   \n",
       "22  How does Llama Packs aim to simplify the proce...   \n",
       "23  Can you explain the process of downloading and...   \n",
       "24  Explain the process of downloading and initial...   \n",
       "25  How can a user customize a Llama Pack, such as...   \n",
       "26  How does the LlamaIndex newsletter celebrate i...   \n",
       "27  Discuss the significance of the Multi-Modal RA...   \n",
       "28  How does the GPT Builder mentioned in the news...   \n",
       "29  Discuss the topics covered in the various tuto...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   The new feature released by LlamaIndex, llama-...              1.0   \n",
       "1   The advancements made in LlamaIndex's PDF pars...              1.0   \n",
       "2   The three main sections of the OpenAI Cookbook...              1.0   \n",
       "3   The OpenAI Cookbook suggests evaluating RAG sy...              1.0   \n",
       "4   LlamaIndex has evolved over the past year by e...              1.0   \n",
       "5   Retrieval-Augmented Generation (RAG) plays a s...              1.0   \n",
       "6   The partnership with Google Gemini benefits Ll...              1.0   \n",
       "7   The new Multi-Doc SEC 10Q Dataset launched by ...              1.0   \n",
       "8   The MemoryCache project by Mozilla utilizes Pr...              1.0   \n",
       "9   The LionAGI agent framework by Ocean Li incorp...              1.0   \n",
       "10  LlamaCloud helps in increasing developer colla...              1.0   \n",
       "11  New features introduced in LlamaCloud to impro...              1.0   \n",
       "12  LlamaIndex helps in building LLM apps by provi...              1.0   \n",
       "13  The process of wrapping a LlamaIndex app with ...              1.0   \n",
       "14  The feedback function in TruLlama evaluates th...              1.0   \n",
       "15  Identifying instances where the model hallucin...              0.0   \n",
       "16  Agentic RAG incorporates agents into existing ...              1.0   \n",
       "17  The Agentic RAG approach by LlamaIndex breaks ...              1.0   \n",
       "18  Agentic RAG is a framework that incorporates a...              1.0   \n",
       "19  The implementation by LlamaIndex showcases the...              1.0   \n",
       "20  The integration of NVIDIA NIM with LlamaIndex ...              1.0   \n",
       "21  Developers gain benefits from using NVIDIA NIM...              1.0   \n",
       "22  Llama Packs aim to simplify the process of bui...              1.0   \n",
       "23  To download and initialize a Llama Pack using ...              1.0   \n",
       "24  To download and initialize a Llama Pack like t...              0.0   \n",
       "25  To customize a Llama Pack by swapping out the ...              1.0   \n",
       "26  The LlamaIndex newsletter celebrates its one-y...              1.0   \n",
       "27  The Multi-Modal RAG Stack and the OpenAIAssist...              1.0   \n",
       "28  The GPT Builder mentioned in the newsletter st...              1.0   \n",
       "29  The tutorials mentioned in the newsletter cove...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \n",
       "0                 4.0                 1.0  \n",
       "1                 4.0                 1.0  \n",
       "2                 4.5                 1.0  \n",
       "3                 4.5                 1.0  \n",
       "4                 4.5                 1.0  \n",
       "5                 4.0                 1.0  \n",
       "6                 4.5                 1.0  \n",
       "7                 4.5                 1.0  \n",
       "8                 4.0                 1.0  \n",
       "9                 4.0                 1.0  \n",
       "10                4.5                 1.0  \n",
       "11                NaN                 1.0  \n",
       "12                4.0                 1.0  \n",
       "13                NaN                 1.0  \n",
       "14                2.0                 1.0  \n",
       "15                4.0                 1.0  \n",
       "16                4.5                 1.0  \n",
       "17                4.5                 1.0  \n",
       "18                NaN                 1.0  \n",
       "19                4.0                 1.0  \n",
       "20                4.5                 1.0  \n",
       "21                5.0                 1.0  \n",
       "22                4.5                 1.0  \n",
       "23                4.5                 1.0  \n",
       "24                2.0                 1.0  \n",
       "25                4.0                 1.0  \n",
       "26                4.0                 1.0  \n",
       "27                4.0                 1.0  \n",
       "28                5.0                 1.0  \n",
       "29                NaN                 1.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 08a0b1fc-d3e7-4152-aec0-622e31cd1059] [Similarity score: 0.7783543956834692] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node aa548ec3-ce01-4681-a11d-4438af0078c6] [Similarity score: 0.7576050237219937] ,\n",
      "    service_name= \"dumb_fact_agent\" ,\n",
      "    host= \"localhost\" ,\n",
      "    port= 8004 \n",
      ") And finally we ...\n",
      "> Top 2 nodes:\n",
      "> [Node 08a0b1fc-d3e7-4152-aec0-622e31cd1059] [Similarity score:             0.778354] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node aa548ec3-ce01-4681-a11d-4438af0078c6] [Similarity score:             0.757605] ,\n",
      "    service_name= \"dumb_fact_agent\" ,\n",
      "    host= \"localhost\" ,\n",
      "    port= 8004 \n",
      ") And finally we ...\n",
      "> Top 2 nodes:\n",
      "> [Node c64cb1f0-5316-46ba-a6d9-9534ce72ab4d] [Similarity score: 0.8118085001908792] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node c33db99d-e2f0-423c-8c29-60ea5d4e54bc] [Similarity score: 0.7640593000480705] Advanced RAG With the success requirements defined, we can then say that building advanced RAG is...\n",
      "> Top 2 nodes:\n",
      "> [Node c64cb1f0-5316-46ba-a6d9-9534ce72ab4d] [Similarity score:             0.811809] OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node c33db99d-e2f0-423c-8c29-60ea5d4e54bc] [Similarity score:             0.764059] Advanced RAG With the success requirements defined, we can then say that building advanced RAG is...\n",
      "> Top 2 nodes:\n",
      "> [Node 81fa9322-b712-426f-9da5-02dfa300d285] [Similarity score: 0.7540106983969525] Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Ji...\n",
      "> [Node 8a3b96fe-3816-45a1-9e73-760f742cb8a3] [Similarity score: 0.7365943502055999] Reranking involves using a semantic search model (specially tuned for the reranking task) that br...\n",
      "> Top 2 nodes:\n",
      "> [Node 81fa9322-b712-426f-9da5-02dfa300d285] [Similarity score:             0.754011] Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Ji...\n",
      "> [Node 8a3b96fe-3816-45a1-9e73-760f742cb8a3] [Similarity score:             0.736594] Reranking involves using a semantic search model (specially tuned for the reranking task) that br...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.00s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9944d3e7f9046bcaadd33019a73b363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: ,\n",
      "    service_name= \"dumb_fact_agent\" ,\n",
      "    hos...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: ,\n",
      "    service_name= \"dumb_fact_agent\" ,\n",
      "    hos...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Advanced RAG With the success requirements defi...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre e...\n",
      "> Adding chunk: Advanced RAG With the success requirements defi...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n"
     ]
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = await aevaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    query_engine,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score       4.75\n",
       "mean_relevancy_score         1.00\n",
       "mean_faithfulness_score      1.00"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>The key features of llama-agents include a Distributed Service Oriented Architecture where each agent can function as an independently running microservice, communication between agents via standardized API interfaces using a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment allowing for independent launching, scaling, and monitoring of each agent and the control plane, as well as built-in observability tools for scalability and resource management.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>Hit Rate and Mean Reciprocal Rank</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 query  \\\n",
       "0                                                                                                               What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                         What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       answer  \\\n",
       "0  The key features of llama-agents include a Distributed Service Oriented Architecture where each agent can function as an independently running microservice, communication between agents via standardized API interfaces using a central control plane orchestrator, the ability to define agentic and explicit orchestration flows, ease of deployment allowing for independent launching, scaling, and monitoring of each agent and the control plane, as well as built-in observability tools for scalability and resource management.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                 The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Hit Rate and Mean Reciprocal Rank   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \n",
       "0              1.0                4.5                 1.0  \n",
       "1              1.0                NaN                 1.0  \n",
       "2              1.0                5.0                 1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
