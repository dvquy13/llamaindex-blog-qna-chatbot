{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a424d031-221e-443a-9705-86592aedea8f",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c852ac68-9eb6-46f7-b33c-a1a12f8af228",
   "metadata": {},
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f23c48-7807-452e-a9fa-0a53167cdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbba9d1-4103-4efd-b0d6-25ba32c74f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0438668-7d1d-46cd-968b-3a3774e109b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb3d4f-6de7-462c-bc3b-b536b0a1580e",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860f66ff-51a8-42ac-ac6a-5a0261a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "DEBUG = True\n",
    "LOG_TO_MLFLOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3aeff7f-d3a6-4d94-9ce5-50853ca91714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "if DEBUG:\n",
    "    logging.getLogger('llama_index').addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    logging.getLogger('llama_index').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984eedcf-127a-4eda-8df4-163c94bfc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = \"exp_002_bge_large\"\n",
    "if LOG_TO_MLFLOW:\n",
    "    RUN_DESCRIPTION = \"\"\"\n",
    "# Qdrant with TogetherAI Llama3 model\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_001\n",
    "- Try larger embedding models to see if retrieval improves\n",
    "\"\"\"\n",
    "    mlflow.set_experiment(\"Chain Frost - LlamaIndex Blog QnA Chatbot\")\n",
    "    mlflow.start_run(run_name=RUN_NAME, description=RUN_DESCRIPTION)\n",
    "    mlflow.log_param(\"TESTING\", TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafc26a2-7df5-4963-855d-92ec99c958d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_CACHE_DP = f'data/001/{RUN_NAME}'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"NOTEBOOK_CACHE_DP\", NOTEBOOK_CACHE_DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c298c-20ea-45df-9b05-9d98d9c29a48",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4b4a7e-90d7-4f3c-87a3-0daf60c30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FP = '../crawl_llamaindex_blog/data/blogs.json'\n",
    "with open(DATA_FP, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a373ee9-9979-423f-9b08-0084fc496855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d84dba2-443a-4ff0-8611-7703b8a6829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Automate online tasks with MultiOn and LlamaIndex',\n",
       "  'content': 'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let’s explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you’re interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.',\n",
       "  'author': 'MultiOn',\n",
       "  'date': 'May 23, 2024',\n",
       "  'tags': ['automation', 'Agents']},\n",
       " {'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       "  'content': 'We’re happy to announce the recent integration of LlamaIndex with PostgresML — a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows. The problem with typical RAG workflows Typical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users. Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges: Increased dev time to master new technologies Complicated maintenance and scalability issues due to multiple points of failure Costly vendors required for multiple services The diagram above illustrates the complexity, showing how each component interacts across different services — exacerbating these problems. Solution The PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows. By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call — resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow. About PostgresML PostgresML [ github  ||  website  ||  docs ] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides. Key highlights: Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costs Model Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versions Model Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performance Feature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency system Python and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your application Serverless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioning PostgresML has a range of capabilities. In the following sections, we’ll guide you through just one use case – RAG – and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app. How it works in LlamaIndex Let’s look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham’s essays. Step 1: Get Your Database Connection String If you haven’t already,  create your PostgresML account . You’ll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Alternatively, you can pass the pgml_database_url argument when creating the index. Step 2: Create the PostgresML Managed Index First install Llama_index and the PostgresML Managed Index component: pip install llama_index llama-index-indices-managed-postgresml Then load in the data: mkdir  data\\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt Finally create the PostgresML Managed Index: from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\\n\\n\\ndocuments = SimpleDirectoryReader( \"data\" ).load_data()\\nindex = PostgresMLIndex.from_documents(\\n    documents, collection_name= \"llama-index-example\" \\n) Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see:  https://postgresml.org/docs/api/client-sdk/pipelines  Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 . Step 3: Querying Now that we have created our index we can use it for retrieval and querying: retriever = index.as_retriever()\\ndocs = retriever.retrieve( \"Was the author puzzled by the IBM 1401?\" )\\n for  doc  in  docs:\\n     print (doc) PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up. Using the PostgresML Index as a query_engine is just as easy: response = index.as_query_engine().query( \"Was the author puzzled by the IBM 1401?\" )\\n print (response) Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming: query_engine = index.as_query_engine(streaming= True )\\nresults = query_engine.query( \"Was the author puzzled by the IBM 1401?\" )\\n for  text  in  results.response_gen:\\n     print (text, end= \"\" , flush= True ) Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable. Key takeaways The PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend. To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro  guide  to setup your account, and the examples above with your own data.',\n",
       "  'author': 'PostgresML',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': ['Managed Indexes']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-06-04',\n",
       "  'content': \"Hello, LlamaIndex Family! 🦙 We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need. 🤩\\xa0 The highlights: Elevating Knowledge Graphs:  The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities.  Docs ,  Tweet . Spreadsheet Insights with LlamaParse:  LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling.  Notebook ,  Tweet . Code Generation with Codestral:  Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages.  Docs ,  Tweet . ✨ Feature Releases and Enhancements: We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support.  Docs ,  Tweet . We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance.  Notebook ,  Tweet . We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages.  Docs ,  Tweet . We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript.  Blogpost ,  Tweet . We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex.  Docs ,  Tweet . 🗺️ Guides: Guide  to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results. Guide  to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results. Guide  to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights. 🖥️\\xa0Demos: Omakase RAG Orchestrator , a project developed by  Amir Mehr , is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel. gmail-extractor , a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision. ✍️ Tutorials: Sherlock Xu’s  tutorial  from BentoML on Serving A LlamaIndex RAG App as REST APIs. 📑\\xa0Papers: FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text. 📹\\xa0Webinar: Webinar  with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents 📅\\xa0 Events: Join  Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'Jun 4, 2024',\n",
       "  'tags': []},\n",
       " {'title': 'Batch inference with MyMagic AI and LlamaIndex',\n",
       "  'content': 'This is a guest post from MyMagic AI. MyMagic AI  allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for  batch  inference (also known as  offline  or  delayed  inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it\\'s integrated directly into LlamaIndex! Part 1: batch inference How It Works: 1. Setup : Organize Your Data in an AWS S3 or GCS Bucket: Create a folder using your user ID assigned to you upon registration. Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks. Purpose of the \\'Session\\' Folder: This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like. Granting Access to MyMagic AI: To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the  MyMagic AI documentation . 2. Install : Install both MyMagic AI’s API integration and LlamaIndex library: pip install llama-index\\npip install llama-index-llms-mymagic 3. API Request:  The llamaIndex library is a wrapper around MyMagic AI’s API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details. import  asyncio\\n from  llama_index.llms.mymagic  import  MyMagicAI\\n\\nllm = MyMagicAI(\\n    api_key= \"user_...\" ,  # provided by MyMagic AI upon sign-up \\n    storage_provider= \"s3\" ,\\n    bucket_name= \"batch-bucket\" ,  # you may name anything \\n    session= \"my-session\" ,\\n    role_arn= \"arn:aws:iam::<your account id>:role/mymagic-role\" ,\\n    system_prompt= \"You are an AI assistant that helps to summarize the documents without essential loss of information\" ,  # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create \\n    region= \"eu-west-2\" ,\\n) We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests. resp = llm.complete(\\n    question= \"Summarise this in one sentence.\" ,\\n    model= \"mixtral8x7\" , \\n    max_tokens= 20 ,   # default is 10 \\n)\\n print (resp)\\n async   def   main ():\\n    aresp =  await  llm.acomplete(\\n        question= \"Summarize this in one sentence.\" ,\\n        model= \"llama7b\" ,\\n        max_tokens= 20 ,\\n    )\\n     print (aresp)\\n\\nasyncio.run(main()) This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI’s backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data). Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a “batch way”. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once. Use Cases While there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing. 1. Extraction Imagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones. 2. Classification For businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning. 3. Embedding Embedding text files for further machine learning applications is another powerful use case of MyMagic AI\\'s API. You will be ready for your vector db in a matter of days not weeks. 4. Training (Fine-tuning) Data Generation Imagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI’s API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5. 5. Transcription MyMagic AI’s API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket. Part 2: Integration with LlamaIndex’s RAG Pipeline The output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex\\'s RAG pipeline for effective data storage and retrieval. This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled: pip install llama-index-embeddings-huggingface\\ncurl -fsSL https://ollama.com/install.sh | sh\\nollama pull llama3 For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json: {\\n  \"id_review1\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\\n  },\\n  \"id_review2\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen\\'s control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson\\'s performance. It concludes by calling the film a great comedy to watch with friends.\"\\n  },\\n  \"id_review3\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell\\'s flat. Overall, it is a terrific and well-written piece.\"\\n  },\\n  \"id_review4\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"Petter Mattei\\'s \\'Love in the Time of Money\\' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler\\'s play on the same theme. Mattei\\'s work is appreciated, and viewers look forward to his future projects.\"\\n  },\\n  \"id_review5\": {\\n    \"query\": \"Summarize the document!\",\\n    \"output\": \"The document describes the TV show \\'Oz\\', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn\\'t dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\\n  },\\n  \"token_count\": 3391\\n}\\n Now let’s embed and store this document and ask questions using LlamaIndex’s query engine. Bring in our dependencies: import  os\\n\\n from  llama_index.embeddings.huggingface  import  HuggingFaceEmbedding\\n from  llama_index.core.indices.vector_store  import  VectorStoreIndex\\n from  llama_index.core.settings  import  Settings\\n from  llama_index.core.readers  import  SimpleDirectoryReader\\n from  llama_index.llms.ollama  import  Ollama Configure the embedding model and Llama3 model embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-base-en-v1.5\" )\\nllm = Ollama(model= \"llama3\" , request_timeout= 300.0 ) Update settings for the indexing pipeline: Settings.llm = llm\\nSettings.embed_model = embed_model\\nSettings.chunk_size =  512   # This parameter defines the size of text chunks for embedding \\n\\ndocuments = SimpleDirectoryReader( \"reviews_1_5.json\" ).load_data()  #Modify path for your case Now create our index, our query engine and run a query: index = VectorStoreIndex.from_documents(documents, show_progress= True )\\n\\nquery_engine = index.as_query_engine(similarity_top_k= 3 )\\n\\nresponse = query_engine.query( \"What is the least favourite movie?\" )\\n print (response) Output: Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10. Now we know that the review 1 is the least favorite movie among these reviews. Next Steps This shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data.  Get started with MyMagic AI’s API  today!',\n",
       "  'author': 'MyMagic AI',\n",
       "  'date': 'May 22, 2024',\n",
       "  'tags': ['MyMagic AI', 'Batch inference']},\n",
       " {'title': 'LlamaIndex Newsletter 2024-05-28',\n",
       "  'content': \"Greetings, LlamaIndex Family! 🦙 Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week. 🤩\\xa0 The highlights: Secure Code Execution with AzureCodeInterpreterTool:  Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution. Build Automated Email Agents:  Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails. LlamaFS for Organized Files:  Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API. RAGApp's No-Code Chatbots:  Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible. ✨ Feature Releases and Enhancements: We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you’re ready to go.  Blogpost ,  Tweet . We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance.  Docs ,  Tweet . We have integrated the Vespa vector store, supporting hybrid search with BM25.  Docs ,  Tweet . We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities.  Docs ,  Tweet . 🗺️ Guides: Guide  to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities. Guide  to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates. 🖥️\\xa0Demos: LlamaFS, a project developed by  Alex Reibman  and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency.  Code ,  Tweet . RAGApp, a project developed by  Marcus Schiesser , offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API.  Code ,  Tweet . ✍️ Tutorials: Phil Chirchir’s   tutorial  on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse. Pavan Kumar’s   tutorial  on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine’s capabilities. Jayita Bhattacharyya’s  tutorial  on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit. 📹\\xa0Webinar: Webinar  with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.\",\n",
       "  'author': 'LlamaIndex',\n",
       "  'date': 'May 28, 2024',\n",
       "  'tags': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb844-bd47-4762-8dbc-4b55488e8d10",
   "metadata": {},
   "source": [
    "# Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f5cbfd-43d1-4eda-9c76-05d219e94729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let’s explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you’re interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e65a39-1308-4459-97fb-d88fe3d74c81",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc404403-96d2-4ad2-9b71-aa4370c953f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 17:50:55.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mlen(input_data)=159\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"{len(input_data)=}\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_input_data\", len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61465bc-bc54-4845-89c5-bfe28b93ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags'])\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984356cd-5af4-4b94-8417-1d8743c830d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='1e37239b-4ae4-45f2-b788-9057ef1c3462', embedding=None, metadata={'title': 'Automate online tasks with MultiOn and LlamaIndex', 'author': 'MultiOn', 'date': 'May 23, 2024', 'tags': 'automation, Agents'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Automate online tasks with MultiOn and LlamaIndex\\nIntroduction MultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions. LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI. In this article, we\\'ll demonstrate how MultiOn\\'s capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions. Technical walkthrough: Integrating MultiOn with LlamaIndex Let’s explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing. Step 1: Setting Up the Environment  We begin by setting up our AI agent with the necessary configurations and API keys: import  openai\\n from  llama_index.agent.openai  import  OpenAIAgent\\nopenai.api_key =  \"sk-your-key\" \\n\\n from  llama_index.tools.multion  import  MultionToolSpec\\nmultion_tool = MultionToolSpec(api_key= \"your-multion-key\" ) Step 2: Integrating Gmail Search Tool  Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions: from  llama_index.tools.google  import  GmailToolSpec\\n from  llama_index.core.tools.ondemand_loader_tool  import  OnDemandLoaderTool\\n\\ngmail_tool = GmailToolSpec()\\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\\n    gmail_tool.to_tool_list()[ 1 ],\\n    name= \"gmail_search\" ,\\n    description= \"\"\"\\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\\n\\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\\n        All parameters are required\\n        \\n        If you need to reply to an email, ask this tool to build the reply directly\\n        Examples:\\n            query=\\'from:adam subject:dinner\\', max_results=5, query_str=\\'Where are adams favourite places to eat\\'\\n            query=\\'dentist appointment\\', max_results=1, query_str=\\'When is the next dentist appointment\\'\\n            query=\\'to:jerry\\', max_results=1, query_str=\\'summarize and then create a response email to jerrys latest email\\'\\n            query=\\'is:inbox\\', max_results=5, query_str=\\'Summarize these emails\\'\\n    \"\"\" \\n) Step 3: Initialize agent Initialise the agent with tools and a system prompt agent = OpenAIAgent.from_tools(\\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\\n    system_prompt= \"\"\"\\n\\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\\n\\t    \\n\\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\\n\\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\\n\\t    \\n\\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\\n    \"\"\" \\n) Step 4: Agent Execution Flow  With our tools integrated, the agent is now equipped to perform a series of tasks: 1. Search and Summarize Emails : The agent uses LlamaIndex\\'s Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response. print (agent.chat( \"browse to the latest email from Julian and open the email\" )) Added user message to memory: browse to the latest email from Julian and open the email\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\\nGot output: Open the email from Julian to view the latest communication.\\n========================\\n \\nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know. 2. Generate Response : Based on the summarized information, the agent crafts an appropriate response to the email chain. print (agent.chat(\\n\\t \"Summarize the email chain with julian and create a response to the last email that confirms all the details\" \\n)) Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\\n=== Calling Function ===\\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\\n\\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\\n========================\\n\\nBased on the email chain with Julian, here is a summary:\\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\\n- Instructions for joining the meeting were provided in the email.\\n- Attendees included Julian and Nassar, with Julian as the organizer.\\n- The email passed SPF and DKIM checks.\\n\\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you. 3. Send Email through MultiOn : Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser. print (agent.chat(\\n\\t \"pass the entire generated email to the browser and have it send the email as a reply to the chain\" \\n)) Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\\n=== Calling Function ===\\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\\nGot output: Email response sent to Julian\\n======================== Next Steps MultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the  LlamaHub page  here. If you’re interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our  notebook . The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbc54b70-1676-496d-b429-6bfaace26683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Simplify your RAG application architecture with LlamaIndex + PostgresML',\n",
       " 'author': 'PostgresML',\n",
       " 'date': 'May 28, 2024',\n",
       " 'tags': 'Managed Indexes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ee5bd7-3f29-4b0e-b2e3-19d69ed7adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_documents\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cfe0f-e8cf-44da-9bd6-b602bf98f1ec",
   "metadata": {},
   "source": [
    "## Setting LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce58b389-ddb6-4c14-816f-29376e0b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47872e1-ac65-476d-993f-d39e530ad32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_OPTION = 'openai'\n",
    "# LLM_OPTION = 'ollama'\n",
    "LLM_OPTION = 'togetherai'\n",
    "\n",
    "# LLM_MODEL_NAME = 'llama3'\n",
    "# LLM_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "LLM_MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct-Lite'\n",
    "\n",
    "# EMBED_OPTION = 'openai'\n",
    "# EMBED_OPTION = 'togetherai'\n",
    "# EMBED_OPTION = 'ollama'\n",
    "EMBED_OPTION = 'huggingface'\n",
    "\n",
    "# EMBED_MODEL_NAME = 'llama3'\n",
    "# EMBED_MODEL_NAME = 'togethercomputer/m2-bert-80M-2k-retrieval'\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"LLM_OPTION\", LLM_OPTION)\n",
    "    mlflow.log_param(\"LLM_MODEL_NAME\", LLM_MODEL_NAME)\n",
    "    mlflow.log_param(\"EMBED_OPTION\", EMBED_OPTION)\n",
    "    mlflow.log_param(\"EMBED_MODEL_NAME\", EMBED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831156c6-08f7-4ed6-9063-6c7d7f00b312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 17:51:07.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mLLM:\n",
      "TogetherLLM(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7402fd428850>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7402fdf19120>, completion_to_prompt=<function default_completion_to_prompt at 0x7402fdf834c0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='meta-llama/Meta-Llama-3-8B-Instruct-Lite', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='3cf613093b6eb9b479c341126dc8d3761c67f9340d0a4a8e1fdc62ed41b58126', api_base='https://api.together.xyz/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=False, tokenizer=None)\u001b[0m\n",
      "\u001b[32m2024-07-23 17:51:07.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEmbed model:\n",
      "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x74028dfb07d0>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# LLM options\n",
    "if LLM_OPTION == 'ollama':\n",
    "    LLM_SERVER_HOST = '192.168.100.14'\n",
    "    LLM_SERVER_PORT = 11434\n",
    "    base_url = f'http://{LLM_SERVER_HOST}:{LLM_SERVER_PORT}'\n",
    "    llm = Ollama(base_url=base_url, model=LLM_MODEL_NAME, request_timeout=60.0)\n",
    "    !ping -c 1 $LLM_SERVER_HOST\n",
    "elif LLM_OPTION == 'openai':\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    llm = OpenAI(model=LLM_MODEL_NAME)\n",
    "elif LLM_OPTION == 'togetherai':\n",
    "    from llama_index.llms.together import TogetherLLM\n",
    "    llm = TogetherLLM(model=LLM_MODEL_NAME)\n",
    "\n",
    "# Embed options\n",
    "if EMBED_OPTION == 'huggingface':\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME\n",
    "    )\n",
    "elif EMBED_OPTION == 'openai':\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    embed_model = OpenAIEmbedding()\n",
    "elif EMBED_OPTION == 'togetherai':\n",
    "    from llama_index.embeddings.together import TogetherEmbedding\n",
    "    embed_model = TogetherEmbedding(EMBED_MODEL_NAME)\n",
    "elif EMBED_OPTION == 'ollama':\n",
    "    from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "    embed_model = OllamaEmbedding(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        base_url=base_url,\n",
    "        ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    )\n",
    "\n",
    "logger.info(f\"LLM:\\n{repr(llm)}\")\n",
    "logger.info(f\"Embed model:\\n{repr(embed_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0db5ed88-2448-4308-97e0-73bc9451c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_model_dim = len(embed_model.get_text_embedding('sample text to find embedding dimensions'))\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"embedding_model_dim\", embed_model_dim)\n",
    "    mlflow.log_param(\"LLM_MODEL\", repr(llm))\n",
    "    mlflow.log_param(\"EMBEDDING_MODEL\", repr(embed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc548f-6c02-4755-b851-eec692090115",
   "metadata": {},
   "source": [
    "# Index embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a05d2-4b9e-4295-9dc5-e1b3bdf22be0",
   "metadata": {},
   "source": [
    "## Qdrant as VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0be2dff6-23da-448b-866b-d791bb54812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08b83e18-6391-408c-93d9-dcf225813302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 17:51:20.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1msubstitute_punctuation(collection_raw_name)='huggingface__BAAI_bge_large_en_v1_5'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def substitute_punctuation(text):\n",
    "    # Create a translation table that maps each punctuation character to an underscore\n",
    "    translator = str.maketrans(string.punctuation, '_' * len(string.punctuation))\n",
    "    # Translate the text using the translation table\n",
    "    return text.translate(translator)\n",
    "\n",
    "collection_raw_name = f\"{EMBED_OPTION}__{EMBED_MODEL_NAME}\"\n",
    "logger.info(f\"{substitute_punctuation(collection_raw_name)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da75b19f-ef70-4ca7-9564-0bf663466dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "COLLECTION = substitute_punctuation(collection_raw_name)\n",
    "\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "# NODES_PERSIST_FP = 'data/001/exp_001_qdrant_togetherai_llama3/nodes.pkl'\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(f\"COLLECTION\", COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec5d7b9-7ad3-427f-a423-0fee1f7f49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 17:51:21.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mDeleting existing Qdrant collection...\u001b[0m\n",
      "\u001b[32m2024-07-23 17:51:21.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mDeleting persisted nodes object at data/001/exp_002_bge_large/nodes.pkl...\u001b[0m\n",
      "\u001b[32m2024-07-23 17:51:21.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mCreating new Qdrant collection...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    # location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # url=\"http://<host>:<port>\"\n",
    "    # otherwise set Qdrant instance with host and port:\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")\n",
    "collection_exists = qdrantdb.collection_exists(COLLECTION)\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing Qdrant collection...\")\n",
    "        qdrantdb.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "    logger.info(f\"Creating new Qdrant collection...\")\n",
    "    qdrantdb.create_collection(\n",
    "        COLLECTION,\n",
    "        vectors_config=VectorParams(size=embed_model_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Use existing Qdrant collection\")\n",
    "db_collection = qdrantdb.get_collection(COLLECTION)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=COLLECTION,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e0c6f0-92a6-430b-a509-682e5884b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e60fed4-7ea5-463c-bd19-d5b6da2cb04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 17:51:36.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mCreating new DB index...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Automate online tasks with MultiOn and LlamaInd...\n",
      "> Adding chunk: All parameters are required\n",
      "        \n",
      "        If...\n",
      "> Adding chunk: print (agent.chat( \"browse to the latest email ...\n",
      "> Adding chunk: The email was authenticated and passed SPF and ...\n",
      "> Adding chunk: As these technologies evolve, they will continu...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: On the PostgresML cloud, you can perform vector...\n",
      "> Adding chunk: Step 2: Create the PostgresML Managed Index Fir...\n",
      "> Adding chunk: The PostgresML Managed Index is doing embedding...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-06-04\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: Blogpost ,  Tweet . We have integrated with Mil...\n",
      "> Adding chunk: Memary is a fully open-source reference impleme...\n",
      "> Adding chunk: Batch inference with MyMagic AI and LlamaIndex\n",
      "...\n",
      "> Adding chunk: import  asyncio\n",
      " from  llama_index.llms.mymagic...\n",
      "> Adding chunk: The user queries will be aggregated and appende...\n",
      "> Adding chunk: \",\n",
      "    \"output\": \"The document describes a fami...\n",
      "> Adding chunk: },\n",
      "  \"id_review5\": {\n",
      "    \"query\": \"Summarize th...\n",
      "> Adding chunk: Next Steps This shows how batch inference combi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-28\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Guide  to building Full-Stack Job Search Assist...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-06-18\n",
      "Hey Llama Foll...\n",
      "> Adding chunk: LlamaPack ,  Tweet . PingCap  has integrated th...\n",
      "> Adding chunk: Kingzzm ’s  tutorial  on Advanced RAG Patterns ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-06-11\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Notebook . 🗺️ Guides: Guide  to Integrating Lla...\n",
      "> Adding chunk: Pavan Mantha 's  tutorial  on securing RAG apps...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: First we’ll bring in our dependencies and set u...\n",
      "> Adding chunk: )\n",
      " print (result) Deploying Your Multi-Agent Sy...\n",
      "> Adding chunk: ,\n",
      "    service_name= \"dumb_fact_agent\" ,\n",
      "    hos...\n",
      "> Adding chunk: import  dotenv\n",
      "dotenv.load_dotenv()  # our .env...\n",
      "> Adding chunk: input ,\n",
      "    )\n",
      "    query_engine = RetrieverQuery...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-07-02\n",
      "Hello, Llama e...\n",
      "> Adding chunk: Codebase . 🗺️ Guides: Guide  to Building an Age...\n",
      "> Adding chunk: LlamaCloud - Built for Enterprise LLM App Build...\n",
      "> Adding chunk: The Rise of Centralized Knowledge Management We...\n",
      "> Adding chunk: Want to discuss unlimited commercial use?  Cont...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-07-09\n",
      "Hello, Llama L...\n",
      "> Adding chunk: Notebook ,  Tweet . [ Yi-01.AI ]( http://Yi-01....\n",
      "> Adding chunk: Ross A.’s  tutorial  on retrieval evaluations f...\n",
      "> Adding chunk: Building and Evaluating a QA System with LlamaI...\n",
      "> Adding chunk: Generate Answers/Source Nodes (Context) Using L...\n",
      "> Adding chunk: NO — Response and Source Nodes (Context) are no...\n",
      "> Adding chunk: This mode of evaluation will look at each sourc...\n",
      "> Adding chunk: A New Document Summary Index for LLM-powered QA...\n",
      "> Adding chunk: It also allows for a more flexible form of retr...\n",
      "> Adding chunk: The rest of this guide showcases the relevant c...\n",
      "> Adding chunk: Note that the LLM returns relevance scores in a...\n",
      "> Adding chunk: Testing Anthropic Claude’s 100k-token window on...\n",
      "> Adding chunk: Where Anthropic’s 100k model doesn’t do well: C...\n",
      "> Adding chunk: from  llama_index  import  download_loader\n",
      " fro...\n",
      "> Adding chunk: from  llama_index  import  PromptHelper, LLMPre...\n",
      "> Adding chunk: # NOTE: the default create/refine approach does...\n",
      "> Adding chunk: • Reliance on Drivers and Restaurants. Uber' s ...\n",
      "> Adding chunk: • Macroeconomic conditions. Ube r's business wa...\n",
      "> Adding chunk: Increased competition could make it difficult  ...\n",
      "> Adding chunk: Analyzing Multiple Documents A popular example ...\n",
      "> Adding chunk: 2019   10 -K: \n",
      "- Further expanded AV risks to i...\n",
      "> Adding chunk: In summary, Uber ' s risk factors changed over ...\n",
      "> Adding chunk: Using LLM’s for Retrieval and Reranking\n",
      "Summary...\n",
      "> Adding chunk: The first stage uses embedding-based retrieval ...\n",
      "> Adding chunk: There are two ways of feeding in the text to th...\n",
      "> Adding chunk: You use the LLM instead of embedding-based look...\n",
      "> Adding chunk: We also showcase some results of pure LLM-based...\n",
      "> Adding chunk: With embedding-based retrieval we set k=3. With...\n",
      "> Adding chunk: The independent company initiatives include “ex...\n",
      "> Adding chunk: Secure code execution in LlamaIndex with Azure ...\n",
      "> Adding chunk: Doing this will give you a pool management endp...\n",
      "> Adding chunk: PDT is UTC-7, and PST is UTC-8. I can use the c...\n",
      "> Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n",
      "> Adding chunk: Action: code_interpreter\n",
      "Action Input: {'python...\n",
      "> Adding chunk: Modifying files would not be useful if you coul...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Now, you can easily process complex documents l...\n",
      "> Adding chunk: Introducing the Property Graph Index: A Powerfu...\n",
      "> Adding chunk: from  llama_index.indices.property_graph  impor...\n",
      "> Adding chunk: 1. Keyword/Synonym-Based Retrieval : Expand you...\n",
      "> Adding chunk: from  llama_index.indices.property_graph  impor...\n",
      "> Adding chunk: Learn More Property Graph Index Overview Basic ...\n",
      "> Adding chunk: Dumber LLM Agents Need More Constraints and Bet...\n",
      "> Adding chunk: It repeats these steps in an iterative loop unt...\n",
      "> Adding chunk: These Tools can vary in complexity. For instanc...\n",
      "> Adding chunk: Smarter agents require fewer constraints.  We d...\n",
      "> Adding chunk: graph = ComposableGraph.from_indices(\n",
      "    GPTLi...\n",
      "> Adding chunk: GPT-3/GPT-4 ReAct Agent Setup # initializing ze...\n",
      "> Adding chunk: \",\n",
      ")\n",
      "\n",
      "# our \"router\" query engine is effectivel...\n",
      "> Adding chunk: Query 2 agent_chain.run(input=\"Analyze changes ...\n",
      "> Adding chunk: Query 1: agent_chain_gpt4.run(input=\"Analyze Ub...\n",
      "> Adding chunk: To provide a more comprehensive analysis, addit...\n",
      "> Adding chunk: COVID -19  pandemic: The ongoing pandemic remai...\n",
      "> Adding chunk: 7.  Integration  and  performance of acquired b...\n",
      "> Adding chunk: > Current query: Analyze Uber revenue growth an...\n",
      "> Adding chunk: We did not test out other agent interaction pat...\n",
      "> Adding chunk: Vellum <> LlamaIndex Integration\n",
      "Co-Authors: Ak...\n",
      "> Adding chunk: While doing that, however, it’s best practice t...\n",
      "> Adding chunk: If you would like additional reasoning or expla...\n",
      "> Adding chunk: For example, if you generate a first draft of e...\n",
      "> Adding chunk: Customizing property graph index in LlamaIndex\n",
      "...\n",
      "> Adding chunk: Environment setup In this blog post, we will us...\n",
      "> Adding chunk: entities =  Literal [ \"PERSON\" ,  \"LOCATION\" , ...\n",
      "> Adding chunk: from  llama_index.core  import  PropertyGraphIn...\n",
      "> Adding chunk: graph_store.structured_query( \"\"\"\n",
      "CREATE VECTOR...\n",
      "> Adding chunk: similarity_threshold =  0.9 \n",
      "word_edit_distance...\n",
      "> Adding chunk: You can tune  similarity_threshold  and  word_d...\n",
      "> Adding chunk: class   Entities ( BaseModel ):\n",
      "     \"\"\"List of...\n",
      "> Adding chunk: entities = self.entity_extraction(text=query_st...\n",
      "> Adding chunk: )\n",
      " print ( str (response))\n",
      " # Detected entities...\n",
      "> Adding chunk: LlamaIndex and Weaviate\n",
      "Co-authors: Jerry Liu (...\n",
      "> Adding chunk: Data Indexing Once the data is loaded, LlamaInd...\n",
      "> Adding chunk: from  llama_index.node_parser  import  SimpleNo...\n",
      "> Adding chunk: LLMs can be used to prompt the language model t...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-06-25\n",
      "Hello to All L...\n",
      "> Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n",
      "> Adding chunk: Llama Index & Prem AI Join Forces\n",
      "Co-authors:  ...\n",
      "> Adding chunk: Start Building Your App In this quick tutorial ...\n",
      "> Adding chunk: Prem's got you covered.\n",
      "\n",
      "Rapid Iterations, Inst...\n",
      "> Adding chunk: )\n",
      " print (response) The benefits of using Prem ...\n",
      "> Adding chunk: LlamaIndex Update — 06/26/2023\n",
      "Greetings, Llama...\n",
      "> Adding chunk: Docs ,  Tweet We now incorporate the FLARE tech...\n",
      "> Adding chunk: The router can pick up to a maximum number of c...\n",
      "> Adding chunk: Notebook ,  Blogpost Prem App has successfully ...\n",
      "> Adding chunk: We hope you found this information useful and a...\n",
      "> Adding chunk: Build and Scale a Powerful Query Engine with Ll...\n",
      "> Adding chunk: More specifically, we showcase a very relevant ...\n",
      "> Adding chunk: if  file.suffix.lower() ==  \".html\" :\n",
      "        l...\n",
      "> Adding chunk: This will split the documents into chunks. \n",
      " fr...\n",
      "> Adding chunk: encode_kwargs={ \"device\" :  \"cuda\" ,  \"batch_si...\n",
      "> Adding chunk: In this example, we use a simple in-memory vect...\n",
      "> Adding chunk: Given an existing question, it can decide to br...\n",
      "> Adding chunk: To do this, you just need to do the following s...\n",
      "> Adding chunk: Q: \"Compare and contrast how the Ray docs and t...\n",
      "> Adding chunk: This allows you to effortlessly ask questions a...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: f_lang_match = Feedback(hugs.language_match).on...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Enriching LlamaIndex Models with GraphQL and Gr...\n",
      "> Adding chunk: !pip install llama-index==0.6.19 python-dotenv ...\n",
      "> Adding chunk: Initializing CSV Loader and GPTVectorStoreIndex...\n",
      "> Adding chunk: As you can see in the response below it doesn’t...\n",
      "> Adding chunk: 7877589022795918)], extra_info={'3decbee1-98cc-...\n",
      "> Adding chunk: Then I added the code for  __init__  to take in...\n",
      "> Adding chunk: if  parameters  is   None :\n",
      "            paramet...\n",
      "> Adding chunk: GraphDBCypherReader = download_loader('GraphDBC...\n",
      "> Adding chunk: extra_info=None), Document(text='names:\\n- Yimo...\n",
      "> Adding chunk: embedding=None, doc_hash='4d493a9f33eb7a1c07175...\n",
      "> Adding chunk: GraphQL  is not a database query language, but ...\n",
      "> Adding chunk: phone :  String !\n",
      "     phones : [ String !]!\n",
      "  ...\n",
      "> Adding chunk: Answer:\n",
      "\"\"\")\n",
      "\n",
      "response.response I was surprised...\n",
      "> Adding chunk: LlamaIndex on TWIML AI: A Distilled Summary (us...\n",
      "> Adding chunk: It also provides an outer abstraction layer tha...\n",
      "> Adding chunk: What is the origin story of LlamaIndex? The ori...\n",
      "> Adding chunk: Jerry’s response is that there are probably a f...\n",
      "> Adding chunk: Additionally, we can use LlamaIndex to structur...\n",
      "> Adding chunk: Special Feature: Berkeley Hackathon Projects (L...\n",
      "> Adding chunk: Key Features and Technology Stack Context-Aware...\n",
      "> Adding chunk: With a solid foundation in place, Helmet AI hop...\n",
      "> Adding chunk: However, with a lot of dedication and troublesh...\n",
      "> Adding chunk: The Prosperity Engine: How Prosper AI Works Pro...\n",
      "> Adding chunk: However, as we progressed and aimed for higher ...\n",
      "> Adding chunk: The Road Ahead for Prosper AI As we set our sig...\n",
      "> Adding chunk: LlamaIndex Update — 07/11/2023\n",
      "Greetings once a...\n",
      "> Adding chunk: Primarily effective for simple charts, such as ...\n",
      "> Adding chunk: It includes native LLM abstractions for platfor...\n",
      "> Adding chunk: Michael Hunger tutorial  on   Load in data from...\n",
      "> Adding chunk: Build a ChatGPT with your Private Data using Ll...\n",
      "> Adding chunk: There are 2 main paradigms currently for extend...\n",
      "> Adding chunk: Or you can get started directly  here . Use of ...\n",
      "> Adding chunk: Query “How does GPT4 do on the bar exam?” Respo...\n",
      "> Adding chunk: Building Better Tools for LLM Agents\n",
      "Over the p...\n",
      "> Adding chunk: Example inputs:\n",
      "              \"(7 * 12 ^ 10) / ...\n",
      "> Adding chunk: In fact, the above tool definition is only 9 li...\n",
      "> Adding chunk: Techniques for building better tools Below are ...\n",
      "> Adding chunk: Print the returned draft's message and id.\n",
      "    ...\n",
      "> Adding chunk: We could simply pass along the null value and r...\n",
      "> Adding chunk: All of the above actions cause friction and fru...\n",
      "> Adding chunk: Currently, LLMs tend to have context windows fr...\n",
      "> Adding chunk: #    2.   `create_event` :  This  tool allows m...\n",
      "> Adding chunk: Data Agents\n",
      "Today we’re incredibly excited to a...\n",
      "> Adding chunk: As a result some of our existing query capabili...\n",
      "> Adding chunk: You can use them as the following: from  llama_...\n",
      "> Adding chunk: The ReAct agent uses an input prompt inspired b...\n",
      "> Adding chunk: The  __call__  function can take in any series ...\n",
      "> Adding chunk: )\n",
      "    ),\n",
      " ...\n",
      "] Tool Specs A  tool spec  is a P...\n",
      "> Adding chunk: The primary reason is to preserve the generalit...\n",
      "> Adding chunk: Finally, let’s send the email! This is a good s...\n",
      "> Adding chunk: As a tool spec, it implements  to_tool_list  , ...\n",
      "> Adding chunk: The agent then reasons that it needs to call th...\n",
      "> Adding chunk: We provide some additional tool abstractions to...\n",
      "> Adding chunk: Building the data framework for LLMs\n",
      "Today is a...\n",
      "> Adding chunk: In just six months, the project garnered an imp...\n",
      "> Adding chunk: We also offer an extensive array of integration...\n",
      "> Adding chunk: They must choose in accordance to a variety of ...\n",
      "> Adding chunk: Find our project on  Github  and check out our ...\n",
      "> Adding chunk: Combining Text-to-SQL with Semantic Search for ...\n",
      "> Adding chunk: Each of these stacks solves particular use case...\n",
      "> Adding chunk: Example queries suited for Retrieval Augmented ...\n",
      "> Adding chunk: A Query Engine to Combine Structured Analytics ...\n",
      "> Adding chunk: For instance if the original question is “Tell ...\n",
      "> Adding chunk: Setup Our experiment setup is very simple. We h...\n",
      "> Adding chunk: Query 1 query_engine.query(\n",
      "  'Tell me about th...\n",
      "> Adding chunk: The early 20th century saw Berlin as a hub for ...\n",
      "> Adding chunk: Introducing LlamaIndex.TS\n",
      "We are beyond excited...\n",
      "> Adding chunk: https://github.com/run-llama/ts-playground Main...\n",
      "> Adding chunk: LlamaIndex and Transformers Agents\n",
      "Summary Agen...\n",
      "> Adding chunk: from  datasets  import  load_dataset\n",
      " from  lla...\n",
      "> Adding chunk: query ( \"Draw me a picture of a happy dog\" ) Sn...\n",
      "> Adding chunk: )\n",
      "        service_context = ServiceContext.from...\n",
      "> Adding chunk: When asked to draw a picture of a mountain, thi...\n",
      "> Adding chunk: LlamaIndex + Metaphor: Towards Automating Knowl...\n",
      "> Adding chunk: Given a query, the agent will execute its reaso...\n",
      "> Adding chunk: Search:  The entrypoint to Metaphor — allows an...\n",
      "> Adding chunk: Example input: metaphor_tool.search('machine le...\n",
      "> Adding chunk: ===  Calling   Function  ===\n",
      " Calling   functio...\n",
      "> Adding chunk: lephenixto.com/', 'id': 'spCTcFr0GHlFUTzyngfRVw...\n",
      "> Adding chunk: Note that we can ask a followup question as wel...\n",
      "> Adding chunk: Our  LoadAndSearchToolSpec  wraps any given too...\n",
      "> Adding chunk: ===  Calling   Function  ===\n",
      " Calling   functio...\n",
      "> Adding chunk: Conclusion As shown above, the integration betw...\n",
      "> Adding chunk: Easily Finetune Llama 2 for Your Text-to-SQL Ap...\n",
      "> Adding chunk: This is exactly where fine-tuning comes in — gi...\n",
      "> Adding chunk: Make sure to check it out! As mentioned above, ...\n",
      "> Adding chunk: modal run src.load_data_sql --data-dir \"data_sq...\n",
      "> Adding chunk: The input prompt is then tokenized, and the lab...\n",
      "> Adding chunk: ,  'context' :  'CREATE TABLE table_name_12 (re...\n",
      "> Adding chunk: We create a toy  city_stats  table that contain...\n",
      "> Adding chunk: Jupyter notebook guide . Stack: [b-mc2/sql-crea...\n",
      "> Adding chunk: LlamaIndex: Harnessing the Power of Text2SQL an...\n",
      "> Adding chunk: \"},\n",
      "\n",
      "    # SamsungTV Reviews\n",
      "    {\"category\": \"...\n",
      "> Adding chunk: The breathable material ensures no discomfort e...\n",
      "> Adding chunk: Let’s start doing it step by step. Decomposing ...\n",
      "> Adding chunk: Provide the culture of countries\n",
      "  '''\n",
      "\n",
      "  messa...\n",
      "> Adding chunk: sql_response_list = ast.literal_eval(sql_respon...\n",
      "> Adding chunk: The ambient mode, gaming mode, and HDR content ...\n",
      "> Adding chunk: Fine-Tuning Embeddings for RAG with Synthetic D...\n",
      "> Adding chunk: (Of course RAG can be much more advanced than t...\n",
      "> Adding chunk: These (question, chunk) pairs are then used as ...\n",
      "> Adding chunk: \"\" \"\n",
      "\n",
      "# for a given node, extract questions (do...\n",
      "> Adding chunk: # define model \n",
      "model_id =  \"BAAI/bge-small-en\"...\n",
      "> Adding chunk: Resources (copied from intro) Repo:  https://gi...\n",
      "> Adding chunk: LlamaIndex: Automatic Knowledge Transfer (KT) G...\n",
      "> Adding chunk: However, using entire code bases for\n",
      "  explanat...\n",
      "> Adding chunk: However, a challenge arises. While\n",
      "   accumulat...\n",
      "> Adding chunk: To see this in action, let’s take a look at a s...\n",
      "> Adding chunk: Introducing Airbyte sources within LlamaIndex\n",
      "A...\n",
      "> Adding chunk: With this release, it’s easier than ever to run...\n",
      "> Adding chunk: This allows you to load only documents that cha...\n",
      "> Adding chunk: source  import  MyCustomSource  # plug in your ...\n",
      "> Adding chunk: LlamaIndex 0.7.0: Better Enabling Bottoms-Up LL...\n",
      "> Adding chunk: Slightly cleaner dev UX. Before, if you wanted ...\n",
      "> Adding chunk: Here’s on how you can use the LLM abstractions ...\n",
      "> Adding chunk: Here’s some resources to show both the LLM abst...\n",
      "> Adding chunk: Accumulate  - Query an LLM with the same prompt...\n",
      "> Adding chunk: Defining Metadata Fields document = Document(\n",
      " ...\n",
      "> Adding chunk: In addition to this, node post processors are n...\n",
      "> Adding chunk: Old from  llama_index  import  (\n",
      "    VectorStor...\n",
      "> Adding chunk: Now, the LLM Predictor class is mostly a lightw...\n",
      "> Adding chunk: LlamaIndex Update — 08/01/2023\n",
      "Greetings once a...\n",
      "> Adding chunk: This feature, compatible with all ReAct and Ope...\n",
      "> Adding chunk: Users can define custom retrievers within Llama...\n",
      "> Adding chunk: Anil Chandra Naidu ’s tutorial on  Retrievers  ...\n",
      "> Adding chunk: Data Agents + Zapier NLA\n",
      "Joint blog by LlamaInd...\n",
      "> Adding chunk: Zep and LlamaIndex: A Vector Store Walkthrough\n",
      "...\n",
      "> Adding chunk: from  llama_index.vector_stores  import  ZepVec...\n",
      "> Adding chunk: print ( str (response)) But one of the most sig...\n",
      "> Adding chunk: retriever = index.as_retriever(filters=filters)...\n",
      "> Adding chunk: Timescale Vector x LlamaIndex: Making PostgreSQ...\n",
      "> Adding chunk: Efficient similarity search with time-based fil...\n",
      "> Adding chunk: Try Timescale Vector for free today . Faster Ve...\n",
      "> Adding chunk: We can also specify the exact parameters for in...\n",
      "> Adding chunk: We can take advantage of this time metadata in ...\n",
      "> Adding chunk: from  llama_index.schema  import  TextNode, Nod...\n",
      "> Adding chunk: from  timescale_vector  import  client\n",
      " # Funct...\n",
      "> Adding chunk: # Create embeddings for nodes \n",
      " from  llama_ind...\n",
      "> Adding chunk: similarity_top_k= 5 )\n",
      "\n",
      "# Time filter variables ...\n",
      "> Adding chunk: -----------------------------------------------...\n",
      "> Adding chunk: 11.1 release… Success! Notice how only vectors ...\n",
      "> Adding chunk: When creating the query engine, we use Timescal...\n",
      "> Adding chunk: Take the next step in your learning journey by ...\n",
      "> Adding chunk: ChatGPT’s Knowledge is Two Years Old: What to d...\n",
      "> Adding chunk: We have integrated with over 20 open source vec...\n",
      "> Adding chunk: LlamaIndex update 2023–10–10\n",
      "Here’s our weekly ...\n",
      "> Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n",
      "> Adding chunk: This method fosters enhanced utilization of con...\n",
      "> Adding chunk: Docs ,  Tweet . TimescaleDB : We integrated wit...\n",
      "> Adding chunk: Evaluating the Ideal Chunk Size for a RAG Syste...\n",
      "> Adding chunk: For a practical evaluation in choosing the righ...\n",
      "> Adding chunk: # We will use GPT-4 for evaluating the response...\n",
      "> Adding chunk: total_response_time =  0 \n",
      "    total_faithfulnes...\n",
      "> Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n",
      "> Adding chunk: from_defaults(llm=gpt4)\n",
      "\n",
      " # Define Faithfulness...\n",
      "> Adding chunk: for  chunk_size  in  [ 128 ,  256 ,  512 ,  102...\n",
      "> Adding chunk: Fine-Tuning a Linear Adapter for Any Embedding ...\n",
      "> Adding chunk: The linear adapter can be used on top of any ex...\n",
      "> Adding chunk: The full guide is here:  https://gpt-index.read...\n",
      "> Adding chunk: from  llama_index.finetuning  import  Embedding...\n",
      "> Adding chunk: The reciprocal rank is defined as 1/rank. Of co...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–10–17\n",
      "Hello Llama En...\n",
      "> Adding chunk: Slides . Simon  conducted a workshop on Buildin...\n",
      "> Adding chunk: ✨ Feature Releases and Enhancements: Text to pg...\n",
      "> Adding chunk: Webinar  with Omar Khattab and Thomas Joshi on ...\n",
      "> Adding chunk: LlamaIndex + Vectara\n",
      "(co-authored by Ofer Mende...\n",
      "> Adding chunk: We never train on your data, so you can be sure...\n",
      "> Adding chunk: Then setup your Vectara customer_id, corpus_id ...\n",
      "> Adding chunk: The school district’s 1401 happened to be in th...\n",
      "> Adding chunk: \").response “Yes, learning Lisp was helpful for...\n",
      "> Adding chunk: Improving RAG effectiveness with Retrieval-Augm...\n",
      "> Adding chunk: Language Model Fine-tuning With our fine-tuning...\n",
      "> Adding chunk: Results In a comparative analysis of model perf...\n",
      "> Adding chunk: Through the dual fine-tuning of both the model ...\n",
      "> Adding chunk: Mastering PDFs: Extracting Sections, Headings, ...\n",
      "> Adding chunk: Do we need Retrieval-Augmented Generation (RAG)...\n",
      "> Adding chunk: “Content-aware” chunking . Set of methods for t...\n",
      "> Adding chunk: As a quick example, the following code snippet ...\n",
      "> Adding chunk: LlamaIndex newsletter 2023–10–24\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Tweet ,  Docs . 🗺️ Guides: Tutorial  guide on  ...\n",
      "> Adding chunk: We now accommodate custom models that align wit...\n",
      "> Adding chunk: LlamaIndex Update — 09/03/2023\n",
      "Hello LlamaIndex...\n",
      "> Adding chunk: Experience top-tier RAG by privately hosting op...\n",
      "> Adding chunk: BlogPost ,  Tweet . LlamaIndex has revamped its...\n",
      "> Adding chunk: Docs ,  Tweet . LlamaIndex introduces the  Open...\n",
      "> Adding chunk: BlogPost ,  Tweet . LlamaIndex integrates with ...\n",
      "> Adding chunk: Tweet . LITS now has additional session options...\n",
      "> Adding chunk: KDNuggests  blog post on  Build Your Own Pandas...\n",
      "> Adding chunk: A fantastic fusion of tech and medicine! SEC In...\n",
      "> Adding chunk: NewsGPT(Neotice): Summarize news articles with ...\n",
      "> Adding chunk: This data is systematically stored in the  Qdra...\n",
      "> Adding chunk: from  transformers  import  pipeline\n",
      " from  tra...\n",
      "> Adding chunk: Streaming Output with LlamaIndex and Streamlit ...\n",
      "> Adding chunk: Instead of using  st.write()  from the regular ...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n",
      "> Adding chunk: So, if the first relevant document is the top r...\n",
      "> Adding chunk: generate only questions based on the below quer...\n",
      "> Adding chunk: # Extract keys from queries and relevant_docs t...\n",
      "> Adding chunk: embed_model = OpenAIEmbedding()\n",
      "service_context...\n",
      "> Adding chunk: Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "        ...\n",
      "> Adding chunk: bge-large : Experiences significant improvement...\n",
      "> Adding chunk: Nearly all embeddings benefit from reranking, s...\n",
      "> Adding chunk: LlamaIndex Update — 20/09/2023\n",
      "Hello LlamaIndex...\n",
      "> Adding chunk: Tweet . Fine-Tuning Guides: OpenAI Fine-Tuning:...\n",
      "> Adding chunk: Integrations with External Platforms Integratio...\n",
      "> Adding chunk: LlamaIndex news special edition: OpenAI develop...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023-11–07\n",
      "Hi again Llama...\n",
      "> Adding chunk: Tweet . We introduced  ParamTuner , a hyperpara...\n",
      "> Adding chunk: Ravi Theja’s   tutorial  on the Router Query En...\n",
      "> Adding chunk: LlamaIndex + Laurie Voss: an alpaca joins the l...\n",
      "> Adding chunk: If you’re new to LlamaIndex, it’s a Python and ...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It’s our birthday! One year...\n",
      "> Adding chunk: It’s what gets us up in the morning and keeps u...\n",
      "> Adding chunk: Big plans With all that growth and all those fe...\n",
      "> Adding chunk: Building My Own ChatGPT Vision with PaLM, KOSMO...\n",
      "> Adding chunk: Unveiling  app.py : The Core of the Application...\n",
      "> Adding chunk: @st.cache \n",
      " def   get_image_caption ( image_dat...\n",
      "> Adding chunk: This document is then indexed to prepare it for...\n",
      "> Adding chunk: # ...code for handling user input and displayin...\n",
      "> Adding chunk: How I built the Streamlit LLM Hackathon winning...\n",
      "> Adding chunk: Setup \n",
      " \n",
      "  In case you want to refer the code t...\n",
      "> Adding chunk: FAISS is also very convenient to use. Hence, we...\n",
      "> Adding chunk: However, the response wouldn't\n",
      "  be comprehensi...\n",
      "> Adding chunk: Using this module is essential\n",
      "  because genera...\n",
      "> Adding chunk: class InnovationRnD(BaseModel):     r_and_d_act...\n",
      "> Adding chunk: session_state.index.as_query_engine(similarity_...\n",
      "> Adding chunk: write(\"## Risk Management\")             st.writ...\n",
      "> Adding chunk: You connect with me on\n",
      "   LinkedIn \n",
      "  and\n",
      "   Tw...\n",
      "> Adding chunk: Improving Retrieval Performance by Fine-tuning ...\n",
      "> Adding chunk: !mkdir -p 'data/10k/'\n",
      "!wget 'https://raw.github...\n",
      "> Adding chunk: \\\n",
      "Restrict the questions to the context informa...\n",
      "> Adding chunk: It should be noted that Hard negatives are opti...\n",
      "> Adding chunk: generate_cohere_reranker_finetuning_dataset(\n",
      "  ...\n",
      "> Adding chunk: finetune_model_no_hard_negatives = CohereRerank...\n",
      "> Adding chunk: reranker_base = CohereRerank(top_n= 5 )\n",
      "reranke...\n",
      "> Adding chunk: index_embed_model = CohereEmbedding(\n",
      "    cohere...\n",
      "> Adding chunk: results_df = pd.DataFrame()\n",
      "\n",
      "embed_name =  'Coh...\n",
      "> Adding chunk: return  self._retrieve(query_bundle)\n",
      "\n",
      "         ...\n",
      "> Adding chunk: NVIDIA Research: RAG with Long Context LLMs\n",
      "Int...\n",
      "> Adding chunk: NVIDIA’s work distinguishes itself by tapping i...\n",
      "> Adding chunk: The retrieval approach entailed segmenting each...\n",
      "> Adding chunk: Models with longer contexts (16K, 32K) outperfo...\n",
      "> Adding chunk: Best Model Performance : After enhancing with b...\n",
      "> Adding chunk: GPT4-V Experiments with General, Specific quest...\n",
      "> Adding chunk: Answer: The image you’ve provided is a bar char...\n",
      "> Adding chunk: Observation: As you can see though the categori...\n",
      "> Adding chunk: With this information, Can you compare about Ll...\n",
      "> Adding chunk: Answer: Examine the Image: The image is a bar c...\n",
      "> Adding chunk: Natural Language Understanding (Top left): All ...\n",
      "> Adding chunk: **MMLU (Top Left Graph)**: LLaMA2 shows a steep...\n",
      "> Adding chunk: The graphs are labeled with task-specific perfo...\n",
      "> Adding chunk: Identify Relevant Data: We need to focus on the...\n",
      "> Adding chunk: Image 3 — Performances of different LLMs across...\n",
      "> Adding chunk: In the SAT-en column (second from the right), t...\n",
      "> Adding chunk: Answer: To answer which model has higher perfor...\n",
      "> Adding chunk: Evaluating Multi-Modal Retrieval-Augmented Gene...\n",
      "> Adding chunk: The first two of these metrics recall and hit r...\n",
      "> Adding chunk: Hit Rate Mean Reciprocal Rank Text 0.95 0.88 Im...\n",
      "> Adding chunk: from  llama_index.evaluation.multi_modal  impor...\n",
      "> Adding chunk: We believe that separating out the retrieval ev...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–10–31\n",
      "Greetings Llam...\n",
      "> Adding chunk: Bharat Ramanathan  built  Wandbot , a live RAG ...\n",
      "> Adding chunk: Wenqi Glantz  also made a second  blog post  on...\n",
      "> Adding chunk: LongLLMLingua: Bye-bye to Middle Loss and Save ...\n",
      "> Adding chunk: Re-ranking is an intuitive concept. One intuiti...\n",
      "> Adding chunk: Compress unrelated and unimportant information ...\n",
      "> Adding chunk: The document with the ground truth is located o...\n",
      "> Adding chunk: As can be seen, compared to Retrieval-based met...\n",
      "> Adding chunk: Huiqiang Jiang, Qianhui Wu etc.\n",
      "[4] RECOMP: Imp...\n",
      "> Adding chunk: Becoming Proficient in Document Extraction\n",
      "Intr...\n",
      "> Adding chunk: This contextual understanding significantly red...\n",
      "> Adding chunk: float16,     bnb_4bit_quant_type=\"nf4\",     bnb...\n",
      "> Adding chunk: ) from llama_index.indices.query.query_transfor...\n",
      "> Adding chunk: Step 5: Lets read the  receipts \n",
      " \n",
      " from llama_...\n",
      "> Adding chunk: Medium : You\n",
      "    can read my latest articles an...\n",
      "> Adding chunk: Multi-Modal RAG\n",
      "(co-authored by Haotian Zhang, ...\n",
      "> Adding chunk: You can have chained/sequential calls that inte...\n",
      "> Adding chunk: It contains all the methods as our existing emb...\n",
      "> Adding chunk: We load the documents as a mix of text docs and...\n",
      "> Adding chunk: create-llama, a command line tool to generate L...\n",
      "> Adding chunk: There are a couple of other questions you’ll be...\n",
      "> Adding chunk: Multimodal RAG: Building ‘AInimal Go!’, a Pokém...\n",
      "> Adding chunk: Streamlit for UI Gif showing the demo in action...\n",
      "> Adding chunk: st.session_state[ 'assistant_avatar' ] = image_...\n",
      "> Adding chunk: 3. Animal Detection with ResNet18 After initial...\n",
      "> Adding chunk: Further in the script, this function is utilize...\n",
      "> Adding chunk: Remember to make {img_desc} sounds while talkin...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–11–28\n",
      "Hello to Our L...\n",
      "> Adding chunk: This pack includes Zephyr-7b as the LLM and bge...\n",
      "> Adding chunk: Tonic AI   analysis  on OpenAI Assistant API vs...\n",
      "> Adding chunk: Announcing LlamaIndex 0.9\n",
      "Our hard-working team...\n",
      "> Adding chunk: What is a  Transformation  though? It could be ...\n",
      "> Adding chunk: Here’s an example with a saving and loading a l...\n",
      "> Adding chunk: from  llama_index  import  Document\n",
      " from  llam...\n",
      "> Adding chunk: import  re\n",
      " from  llama_index  import  Document...\n",
      "> Adding chunk: We’ve done our best to minimize the impacts on ...\n",
      "> Adding chunk: There are also other installation options depen...\n",
      "> Adding chunk: See you on the Discord!\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: Shipping your Retrieval-Augmented Generation ap...\n",
      "> Adding chunk: Your deployed app should look like this: Congra...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–11–21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Register for free! ✨ Feature Releases and Enhan...\n",
      "> Adding chunk: Guide  on building a full-stack financial analy...\n",
      "> Adding chunk: Introducing RAGs: Your Personalized ChatGPT Exp...\n",
      "> Adding chunk: This part of the app provides an intuitive UI w...\n",
      "> Adding chunk: Say that you want to build a chatbot Define the...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we’re excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: Let’s take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: Take a look at this folder for a full set of ex...\n",
      "> Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n",
      "> Adding chunk: Their  multimodal demos  demonstrate joint imag...\n",
      "> Adding chunk: It contains the following features: supports bo...\n",
      "> Adding chunk: Please see our extensive notebook guides for mo...\n",
      "> Adding chunk: Simply define the index, insert nodes, and then...\n",
      "> Adding chunk: Linking the resources again below: Gemini (text...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–11–14\n",
      "Hello Llama Fr...\n",
      "> Adding chunk: Tweet . We integrated OpenAI’s parallel functio...\n",
      "> Adding chunk: Harshad Suryawanshi ’s  tutorial  covers Buildi...\n",
      "> Adding chunk: LlamaIndex: RAG Evaluation Showdown with GPT-4 ...\n",
      "> Adding chunk: You’ll find comprehensive details on this in th...\n",
      "> Adding chunk: from  llama_index.llms  import  HuggingFaceInfe...\n",
      "> Adding chunk: ###The instruction to evaluate: Your task is to...\n",
      "> Adding chunk: Score   NO :  If  the given piece  of  informat...\n",
      "> Adding chunk: 3. After writing a feedback, write a score that...\n",
      "> Adding chunk: ### The  instruction to  evaluate :  Your  task...\n",
      "> Adding chunk: token_counter = TokenCountingHandler(\n",
      "    token...\n",
      "> Adding chunk: async   def   batch_eval_runner ( \n",
      "    evaluato...\n",
      "> Adding chunk: Query:  Based on the abstract of “Llama 2: Open...\n",
      "> Adding chunk: However, it misses the detail about Llama 2-Cha...\n",
      "> Adding chunk: Context-1:  Llama 2 : Open Foundation and Fine-...\n",
      "> Adding chunk: ∗Equal contribution, corresponding authors: {ts...\n",
      "> Adding chunk: Wemeticulouslyelaboratedonthe methodsandtechniq...\n",
      "> Adding chunk: The context clearly states that Llama 2, a coll...\n",
      "> Adding chunk: The endpoint on HF is served on AWS Nvidia A100...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: class   StreamlitChatPack ( BaseLlamaPack ):\n",
      "\n",
      " ...\n",
      "> Adding chunk: I’ve used GPT3.5 here, but you can easily swap ...\n",
      "> Adding chunk: The app concludes by displaying the SQL queries...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–12–19\n",
      "What’s up, Lla...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: Tweet AI Chatbot Starter (from the DataStax tea...\n",
      "> Adding chunk: ✍️ Tutorials: Laurie’s   Advanced Querying & Re...\n",
      "> Adding chunk: Introducing Llama Datasets 🦙📝\n",
      "(Authors: Andrei ...\n",
      "> Adding chunk: Llama Datasets on LlamaHub Overview Today’s lau...\n",
      "> Adding chunk: You can easily plug in any query engine into  a...\n",
      "> Adding chunk: reference_answer_by = CreatedBy(type=CreatedByT...\n",
      "> Adding chunk: If you want to auto-generate this given some in...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–12–05\n",
      "Hello Llama Co...\n",
      "> Adding chunk: The techniques include Hybrid Fusion, Query Rew...\n",
      "> Adding chunk: This interactive application, developed by  Har...\n",
      "> Adding chunk: Wenqi Glantz  made a  tutorial  on Llama Packs:...\n",
      "> Adding chunk: Two new llama-datasets and a Gemini vs. GPT sho...\n",
      "> Adding chunk: The way to do that with our llama-dataset abstr...\n",
      "> Adding chunk: Benchmarking flow with LabelledPairwiseEvaluato...\n",
      "> Adding chunk: The below snippet of code is how you can replic...\n",
      "> Adding chunk: As for GPT-4 versus the reference GPT-4, this i...\n",
      "> Adding chunk: Note again that these are conditional on the pr...\n",
      "> Adding chunk: Running Mixtral 8x7 locally with LlamaIndex and...\n",
      "> Adding chunk: That’s where LlamaIndex comes in. The next few ...\n",
      "> Adding chunk: This will give you a pile of documents ready to...\n",
      "> Adding chunk: Start a new python file and load in dependencie...\n",
      "> Adding chunk: We’ll need two new dependencies: pip install fl...\n",
      "> Adding chunk: And add a route that accepts a query (as form d...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–12–12\n",
      "Howdy, Llama E...\n",
      "> Adding chunk: Blog ,  Tweet . We launched  RAGs v5 , enabling...\n",
      "> Adding chunk: Tweet . We introduced AutoTranslateDoc, an open...\n",
      "> Adding chunk: This provides you with a robust chat interface ...\n",
      "> Adding chunk: Bridging the Language Gap in Programming: Intro...\n",
      "> Adding chunk: Improving Accuracy and Consistency Our commitme...\n",
      "> Adding chunk: Future Enhancements: We are actively working on...\n",
      "> Adding chunk: Scaling LlamaIndex with AWS and Hugging Face\n",
      "Ov...\n",
      "> Adding chunk: My AWS console favourites Note on how deploymen...\n",
      "> Adding chunk: Once you have a quota for GPU instances (like G...\n",
      "> Adding chunk: First, you need to create your cluster. For wha...\n",
      "> Adding chunk: You’ll want the external IP for the load balanc...\n",
      "> Adding chunk: Create a python file called  lambda_function.py...\n",
      "> Adding chunk: Select  Create function Give it a name, select ...\n",
      "> Adding chunk: To ingest data, you can run: import  requests\n",
      " ...\n",
      "> Adding chunk: Conclusion Using this setup, I was able to redu...\n",
      "> Adding chunk: Building An Intelligent Query-Response System w...\n",
      "> Adding chunk: python -m venv llamaindex-openllm\n",
      "source llamai...\n",
      "> Adding chunk: Here’s how you can use it: from  llama_index.ll...\n",
      "> Adding chunk: OpenLLM is written in Python and is available u...\n",
      "> Adding chunk: Create a folder and let’s import the GitHub REA...\n",
      "> Adding chunk: The output should be consistent with the conten...\n",
      "> Adding chunk: LlamaIndex + Waii: Combining Structured Data fr...\n",
      "> Adding chunk: Limitations arise due to the restricted context...\n",
      "> Adding chunk: The Waii Service can be deployed as a hosted Sa...\n",
      "> Adding chunk: Let’s start with creating an agent which includ...\n",
      "> Adding chunk: Understand your dataset \n",
      " \n",
      "  The first step in ...\n",
      "> Adding chunk: Specific questions that can be addressed using ...\n",
      "> Adding chunk: Books: 1,472,911 7. Home: 1,471,348 8. Jewelry:...\n",
      "> Adding chunk: - \"Gift Cards & Other\" and \"Food & Beverage\" fr...\n",
      "> Adding chunk: Introducing Query Pipelines\n",
      "Today we introduce ...\n",
      "> Adding chunk: Source: “Advanced RAG Techniques: an Illustrate...\n",
      "> Adding chunk: [In the future] Caching:  This interface also a...\n",
      "> Adding chunk: from  llama_index.postprocessor  import  Cohere...\n",
      "> Adding chunk: output_dict = p. run_multi ({ \"llm\" : { \"topic\"...\n",
      "> Adding chunk: Conclusion + Resources That’s it! As mentioned ...\n",
      "> Adding chunk: How to train a custom GPT on your data with Emb...\n",
      "> Adding chunk: The steps to do this are: Extract all the URLs ...\n",
      "> Adding chunk: Case 4: Custom ChatGPT for Notion In many moder...\n",
      "> Adding chunk: Custom trained chatbots can help your business ...\n",
      "> Adding chunk: Free Advanced RAG Certification course with Act...\n",
      "> Adding chunk: Legal:  Patent Generation and Search Engine. Ga...\n",
      "> Adding chunk: Complimentary Free Trial of Deep Lake As a part...\n",
      "> Adding chunk: AI Voice Assistant: Enhancing Accessibility in ...\n",
      "> Adding chunk: The  App.js  script incorporates features like ...\n",
      "> Adding chunk: toLowerCase ();\n",
      "         if  (transcript. inclu...\n",
      "> Adding chunk: current . stop ();\n",
      "      }\n",
      "    }  catch  (error...\n",
      "> Adding chunk: log ( \"TTS starts speaking\" );\n",
      "         setShow...\n",
      "> Adding chunk: This change ensures that when the frontend make...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–01–16\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: This method combines textual and symbolic reaso...\n",
      "> Adding chunk: 🎥 Events: Ravi Theja gave talk on Building Mult...\n",
      "> Adding chunk: Multimodal RAG pipeline with LlamaIndex and Neo...\n",
      "> Adding chunk: The results are fed into a multimodal LLM, whic...\n",
      "> Adding chunk: sections = []\n",
      "    current_section = { \"header\" ...\n",
      "> Adding chunk: all_documents = []\n",
      "all_images = []\n",
      "\n",
      " # Director...\n",
      "> Adding chunk: # Takes 10 min without GPU / 1 min with GPU on ...\n",
      "> Adding chunk: Conclusion LLMs are evolving faster than what w...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–01–23\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Guide . ✨ Feature Releases and Enhancements: We...\n",
      "> Adding chunk: Guide  to Long-Context Embedding Models: The mo...\n",
      "> Adding chunk: Building a Slack bot that learns with LlamaInde...\n",
      "> Adding chunk: Otherwise we'll do nothing. @flask_app.route( \"...\n",
      "> Adding chunk: Click the “Permissions” link in the bottom righ...\n",
      "> Adding chunk: We’re going to use Slack’s handy Python SDK to ...\n",
      "> Adding chunk: Here's me trying it out: Success! We have a ver...\n",
      "> Adding chunk: )\n",
      "                                     return \n",
      "...\n",
      "> Adding chunk: Step 5: use LlamaIndex to store facts and answe...\n",
      "> Adding chunk: pip install qdrant-client  and bring in some ne...\n",
      "> Adding chunk: To do that we are going to stop inserting  Docu...\n",
      "> Adding chunk: A prompt template will automatically get the  c...\n",
      "> Adding chunk: We have to deploy this thing! Step 9: deploy to...\n",
      "> Adding chunk: Add a way to tell the bot to forget things (del...\n",
      "> Adding chunk: Building Scalable RAG Applications with LlamaIn...\n",
      "> Adding chunk: The chatbot is scalable and supports multi-tena...\n",
      "> Adding chunk: zcp_index.insert_doc_url(\n",
      "    url= \"https://pub...\n",
      "> Adding chunk: Zilliz Cloud Pipelines will soon support local ...\n",
      "> Adding chunk: Introducing the LlamaIndex retrieval-augmented ...\n",
      "> Adding chunk: LlamaIndex is designed  for  both beginner and ...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Do not rely on prior knowledge \n",
      " Working Exampl...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Building a Fully Open Source Retriever with Nom...\n",
      "> Adding chunk: For this example, we are going to use an essay ...\n",
      "> Adding chunk: ) returns a document that describes Paul’s firs...\n",
      "> Adding chunk: So I 'm not surprised I can' t remember  any  p...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–01–30\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Docs ,  Tweet . We have introduced JSONalyze, a...\n",
      "> Adding chunk: Tonic Validate  tutorial on Implementing integr...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–01–02\n",
      "Hello, Llama L...\n",
      "> Adding chunk: This surpasses traditional sequential methods i...\n",
      "> Adding chunk: It integrates various tools: LlamaIndex for ind...\n",
      "> Adding chunk: Wenqi Glantz   tutorial  on 10+ Ways to Run Ope...\n",
      "> Adding chunk: Tonic Validate x LlamaIndex: Implementing integ...\n",
      "> Adding chunk: To install it, we need to make sure we have Nod...\n",
      "> Adding chunk: After this, you can run the chatbot with: pytho...\n",
      "> Adding chunk: Download the  qa_pairs.json  file from the link...\n",
      "> Adding chunk: Next, we can add the code that queries LlamaInd...\n",
      "> Adding chunk: def   test_llama_index ():\n",
      "    questions, refer...\n",
      "> Adding chunk: As is common in modern software development pra...\n",
      "> Adding chunk: In this file, we will include the following cod...\n",
      "> Adding chunk: In GitHub, go to  Settings > Secrets and variab...\n",
      "> Adding chunk: LlamaIndex: Enhancing Retrieval Performance wit...\n",
      "> Adding chunk: Queries With Misspellings:  Queries containing ...\n",
      "> Adding chunk: You can also continue following along in the  G...\n",
      "> Adding chunk: def   get_weaviate_client ( api_key, url ):\n",
      "  a...\n",
      "> Adding chunk: Setup Weaviate Client url = 'cluster URL'\n",
      "api_k...\n",
      "> Adding chunk: retrieved_nodes = self._vector_retriever.retrie...\n",
      "> Adding chunk: # Alpha values and datasets to test \n",
      "alpha_valu...\n",
      "> Adding chunk: DataFrame({ 'Dataset' : [dataset],  'Alpha' : [...\n",
      "> Adding chunk: These experiments varied in alpha values, types...\n",
      "> Adding chunk: Keyword Queries — MRR and Hit Rate are higher w...\n",
      "> Adding chunk: RAGArch: Building a No-Code RAG Pipeline Config...\n",
      "> Adding chunk: Tools and Technologies The creation of RAGArch ...\n",
      "> Adding chunk: Total documents loaded:  { len (loaded_file)} \"...\n",
      "> Adding chunk: You can choose from Google’s Gemini Pro, Cohere...\n",
      "> Adding chunk: markdown ( \"\" \"\n",
      "                    [Embedding ...\n",
      "> Adding chunk: col2 = st.columns([ 4 , 1 ])\n",
      "     with  col2:\n",
      " ...\n",
      "> Adding chunk: 'max_chars' : max_chars}\n",
      "        \n",
      "     elif  pa...\n",
      "> Adding chunk: \" )\n",
      "             return   None ,  None \n",
      "       ...\n",
      "> Adding chunk: def   select_response_synthesis_method ():\n",
      "    ...\n",
      "> Adding chunk: It initializes the pipeline with the chosen LLM...\n",
      "> Adding chunk: node_parser import SentenceSplitter, CodeSplitt...\n",
      "> Adding chunk: get( 'language' ,  'python' )} , chunk_lines= {...\n",
      "> Adding chunk: Index('test')\\n\" \n",
      "        code_snippet +=  \"vec...\n",
      "> Adding chunk: With RAGArch, both seasoned developers and AI e...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–02–06\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: 🎥 Demo: LlamaBot:  Rohan  developed an open-sou...\n",
      "> Adding chunk: How to build LLM Agents in TypeScript with Llam...\n",
      "> Adding chunk: // Sum properties to give to the LLM \n",
      " const  s...\n",
      "> Adding chunk: Then you should see an output as: ===  Calling ...\n",
      "> Adding chunk: asQueryEngine ();\n",
      "\n",
      " // Create a QueryEngineTool...\n",
      "> Adding chunk: There is still a long way to go before they are...\n",
      "> Adding chunk: Pioneering the Future of Housing: Introducing G...\n",
      "> Adding chunk: Watch Our Demo \n",
      "\n",
      " \n",
      "\n",
      " Technologies At the heart ...\n",
      "> Adding chunk: Once you’ve setup your LlamaIndex and OpenAI AP...\n",
      "> Adding chunk: import  os\n",
      " from  pathlib  import  Path\n",
      " from  ...\n",
      "> Adding chunk: Please analyze the image, describle the layout,...\n",
      "> Adding chunk: Conclusions Our application markedly simplifies...\n",
      "> Adding chunk: LlamaIndex v0.10\n",
      "Today we’re excited to launch ...\n",
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n",
      "> Adding chunk: See below for more details. llama-index-packs  ...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: ) Dealing with Breaking Changes This update com...\n",
      "> Adding chunk: `download` syntax A popular UX for fetching int...\n",
      "> Adding chunk: This is especially useful for callbacks. All re...\n",
      "> Adding chunk: Migration to v0.10 If you want to use LlamaInde...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–02–13\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Docs . ✨ Feature Releases and Enhancements: We ...\n",
      "> Adding chunk: HelixML  tutorial  to Knowledge Memorization by...\n",
      "> Adding chunk: A Cheat Sheet and Some Recipes For Building Adv...\n",
      "> Adding chunk: Advanced RAG With the success requirements defi...\n",
      "> Adding chunk: Define objective function \n",
      " def   objective_fun...\n",
      "> Adding chunk: Execute hyperparameter search \n",
      "results = param_...\n",
      "> Adding chunk: node_id: n  for  n  in  all_nodes}\n",
      "retriever_ch...\n",
      "> Adding chunk: irrelevant information). LlamaIndex Information...\n",
      "> Adding chunk: LlamaIndex Re-Ranking For Better Generation Rec...\n",
      "> Adding chunk: # Build FLAREInstructQueryEngine \n",
      "documents = S...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–01–09\n",
      "Hola, LlamaInd...\n",
      "> Adding chunk: Notebook ,  Tweet . RAGatouille LlamaPack : Int...\n",
      "> Adding chunk: Docs ,  Tweet . Ian McCrystal  has added the St...\n",
      "> Adding chunk: ✍️ Tutorials: BentoML  tutorial  on Building An...\n",
      "> Adding chunk: Introducing LlamaCloud and LlamaParse\n",
      "Today is ...\n",
      "> Adding chunk: This stack is different from any ETL stack befo...\n",
      "> Adding chunk: This is a surprisingly prevalent use case acros...\n",
      "> Adding chunk: Next Steps Our early users have already given u...\n",
      "> Adding chunk: Launch Partners and Collaborators We opened up ...\n",
      "> Adding chunk: “Now, developers can abstract complexities asso...\n",
      "> Adding chunk: Building Multi-Tenancy RAG System with LlamaInd...\n",
      "> Adding chunk: Now that we’ve discussed the concept, let’s div...\n",
      "> Adding chunk: # For user Jerry \n",
      " for  document  in  documents...\n",
      "> Adding chunk: The Task Fetching Unit dispatches the function ...\n",
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: Alex has heard about these insightful documents...\n",
      "> Adding chunk: from  llama_index.networks.contributor  import ...\n",
      "> Adding chunk: A place where data suppliers package their data...\n",
      "> Adding chunk: Check out the demo to learn more! To see an act...\n",
      "> Adding chunk: Unlocking the 3rd Dimension for Generative AI (...\n",
      "> Adding chunk: This approach offers a more direct and potentia...\n",
      "> Adding chunk: (For me, I usually overestimate what I can achi...\n",
      "> Adding chunk: Our key objectives are to make neThing.xyz fast...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-12\n",
      "Salutations, L...\n",
      "> Adding chunk: Tweet . 🎥 Demos: Build an AI Browser Copilot : ...\n",
      "> Adding chunk: 📅 Events: We are hosting a RAG  meetup  in Pari...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: Do I have the wrong map? Example 3: mathematica...\n",
      "> Adding chunk: Check out this  demo notebook  where we demonst...\n",
      "> Adding chunk: PII Detector: hacking privacy in RAG\n",
      "A couple o...\n",
      "> Adding chunk: When the models generate text, there is a risk ...\n",
      "> Adding chunk: These were implemented as post processors that ...\n",
      "> Adding chunk: [ORG_521].dk/ As can be seen in this example, w...\n",
      "> Adding chunk: I could not have that. So, I added a counter an...\n",
      "> Adding chunk: My IBAN is GB90YNTU67299444055881. \n",
      "What's your...\n",
      "> Adding chunk: How It Ended Anyway, this project won the 3rd p...\n",
      "> Adding chunk: One-click Open Source RAG Observability with La...\n",
      "> Adding chunk: ))\n",
      " print (chat_engine.chat( \"How do I optimize...\n",
      "> Adding chunk: Trace more complex applications and use other L...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Blogpost ,  Docs ,  Tweet . Search-in-the-Chain...\n",
      "> Adding chunk: Supercharge your LlamaIndex RAG Pipeline with U...\n",
      "> Adding chunk: The evaluations demonstrated here will help you...\n",
      "> Adding chunk: Factual Accuracy : Now that we have checked if ...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n",
      "> Adding chunk: Much of the success in the field of Artificial ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–02–20: introducing L...\n",
      "> Adding chunk: Tweet ,  LlamaPack . RAG Production  Guide :  A...\n",
      "> Adding chunk: Ravi Theja   tutorial  video on Building Multi-...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024–02–27\n",
      "Yo, LlamaIndex...\n",
      "> Adding chunk: Notebook ,  Tweet . ColBERT Integration:  Docum...\n",
      "> Adding chunk: BlogPost ,  Tweet . 🗺️ Guides: Guide  to simpli...\n",
      "> Adding chunk: Secure RAG with LlamaIndex and LLM Guard by Pro...\n",
      "> Adding chunk: With this example, we show how you can use LLM ...\n",
      "> Adding chunk: Try out LLM Guard by going to our  library  or ...\n",
      "> Adding chunk: Bridging the Gap in Crisis Counseling: Introduc...\n",
      "> Adding chunk: Counselor copilot takes into account contact co...\n",
      "> Adding chunk: Lastly, we used the conversation content to fil...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-26\n",
      "Hi there, Llam...\n",
      "> Adding chunk: Blogpost ,  Tweet . We launched LlamaParse inte...\n",
      "> Adding chunk: 📅 Events: Join us  for a Panel discussion on 'W...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: The solution is divided into the following sect...\n",
      "> Adding chunk: clip = VideoFileClip(video_path)\n",
      "    clip.write...\n",
      "> Adding chunk: Building the Multi-Modal Index and Vector Store...\n",
      "> Adding chunk: from  llama_index  import  (\n",
      "    SimpleDirector...\n",
      "> Adding chunk: def   retrieve ( retriever_engine, query_str ):...\n",
      "> Adding chunk: Below is the prompt template : qa_tmpl_str = (\n",
      "...\n",
      "> Adding chunk: Convolution of Two Gaussians:  Discusses adding...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-05\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Tweet ,  notebook ,  package ,  blog post Mixed...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-02\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Notebook ,  Tweet We launched revamped Python d...\n",
      "> Adding chunk: Vectara’s   Panel Discussion  on 'Why RAG will ...\n",
      "> Adding chunk: Towards Long Context RAG \n",
      "Google recently relea...\n",
      "> Adding chunk: But we’re also excited about Gemini Pro, and we...\n",
      "> Adding chunk: Gemini doesn’t read all tables and charts corre...\n",
      "> Adding chunk: For these use cases, developers will no longer ...\n",
      "> Adding chunk: An solution to this that  Yao Fu  brought up is...\n",
      "> Adding chunk: One is indexing document summaries and linking ...\n",
      "> Adding chunk: But this leads to interesting retrieval strateg...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-09\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: 🗺️ Guides: Guide  to Building Advanced RAG with...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: This optimizes for reduced latency and costs, a...\n",
      "> Adding chunk: kingzzm’s   tutorial  on enhancing RAG Performa...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-23\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: This setup includes Ray for computing, LlamaInd...\n",
      "> Adding chunk: Mariboo’s  tutorial  on Fine-tuning Embedding M...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-14\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Video Tutorial ,  BlogPost ,  Notebook . Arslan...\n",
      "> Adding chunk: Case study: Lyzr: Taking autonomous AI agents t...\n",
      "> Adding chunk: What do customers think? Customer reception of ...\n",
      "> Adding chunk: Building a multi-agent concierge system\n",
      "Why bui...\n",
      "> Adding chunk: Instead, it looks at what the user is currently...\n",
      "> Adding chunk: Could you please provide your username and pass...\n",
      "> Adding chunk: No current speaker, asking orchestration agent ...\n",
      "> Adding chunk: At the core of that is a very simple block that...\n",
      "> Adding chunk: If there is no current_speaker value, look at t...\n",
      "> Adding chunk: This is when the state object gets passed to ea...\n",
      "> Adding chunk: This triggers the outer loop to run the continu...\n",
      "> Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n",
      "> Adding chunk: Part 2: of Alex, Bob and Beth. This time Bob an...\n",
      "> Adding chunk: from  llama_index.core.llama_datasets.simple  i...\n",
      "> Adding chunk: This dataset consists of 1,200 examples each co...\n",
      "> Adding chunk: import  llama_index.core.instrumentation  as  i...\n",
      "> Adding chunk: Note that we created the synthetic observations...\n",
      "> Adding chunk: nodes = [\n",
      "    TextNode(text=el.text, metadata={...\n",
      "> Adding chunk: hit rate:  a hit occurs if any of the retrieved...\n",
      "> Adding chunk: Below, we share the evaluations that result fro...\n",
      "> Adding chunk: ,\n",
      "   \"text_by\" : {\n",
      "     \"model_name\" :  \"gpt-3....\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-30\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Tweet ✍️ Tutorials: Build a best-in-class RAG a...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-07\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Tweet ,  Slides Plaban Nayak sets up a local, o...\n",
      "> Adding chunk: Arize AI and LlamaIndex Roll Out Joint Platform...\n",
      "> Adding chunk: “As leaders in our respective spaces with a com...\n",
      "> Adding chunk: Using LlamaIndex and llamafile to build a local...\n",
      "> Adding chunk: This allows the models to run on most computers...\n",
      "> Adding chunk: rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafi...\n",
      "> Adding chunk: Download the llamafile-ized model \n",
      "wget https:/...\n",
      "> Adding chunk: To get started, download our example data: mkdi...\n",
      "> Adding chunk: To start the llamafile server, open a terminal ...\n",
      "> Adding chunk: # Load local data \n",
      " from  llama_index.core  imp...\n",
      "> Adding chunk: As a next step, you could try running the examp...\n",
      "> Adding chunk: Streamlining knowledge work with LlamaIndex, Fi...\n",
      "> Adding chunk: from  llama_index.readers.web  import  SimpleWe...\n",
      "> Adding chunk: # FireworksEmbedding defaults to using model \n",
      "e...\n",
      "> Adding chunk: It’s incredibly versatile! Setting up vector se...\n",
      "> Adding chunk: ),\n",
      "    verbose= True \n",
      "    ) create-llama : from...\n",
      "> Adding chunk: Together, let’s revolutionize the way developer...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-07-16\n",
      "Hello, Llama F...\n",
      "> Adding chunk: Notebook ,  Tweet . We have integrated Redis Qu...\n",
      "> Adding chunk: Mervin Praison’s   tutorial  on using llama-age...\n",
      "> Adding chunk: Case Study: How Scaleport.ai Accelerated Develo...\n",
      "> Adding chunk: Doing that stuff with LlamaCloud and LlamaParse...\n",
      "> Adding chunk: Improving Vector Search - Reranking with Postgr...\n",
      "> Adding chunk: Install the required dependencies to get starte...\n",
      "> Adding chunk: documents = SimpleDirectoryReader( \"data\" ).loa...\n",
      "> Adding chunk: In the print era, the channel for publishing es...\n",
      "> Adding chunk: I've never known a teacher more beloved by her ...\n",
      "> Adding chunk: The school district's 1401 happened to be in th...\n",
      "> Adding chunk: The idea of actually being able to make art, to...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.84s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:04<00:00, 12.84s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.24s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.08it/s]\n",
      "\u001b[32m2024-07-23 18:04:04.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mIndexing 159 into VectorStoreIndex took 748s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = None\n",
    "    # Currently setting num_workers leads to error `AttributeError: 'HuggingFaceEmbedding' object has no attribute '_model'`\n",
    "    # num_workers = os.cpu_count() - 1\n",
    "    # logger.info(f\"Running Ingestion Pipeline with {num_workers=}...\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()\n",
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ae9a042-75d5-4b9a-8dec-42f75c86c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 18:04:04.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexed 808 nodes into Vector Store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexed {len(nodes)} nodes into Vector Store\")\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"len_nodes\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b3d52-7be9-49e1-bf4e-4cd586635d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5fb8a-636a-463c-a938-1864cff81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_INDEX = False\n",
    "\n",
    "COLLECTION = 'togetherai'\n",
    "NOTEBOOK_CACHE_DP = 'data/001/togetherai'\n",
    "NODES_PERSIST_FP = f'{NOTEBOOK_CACHE_DP}/nodes.pkl'\n",
    "os.makedirs(NOTEBOOK_CACHE_DP, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4a1d0d1-142d-4e7a-b626-ef154ba08e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 12:09:50.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mUse existing ChromaDB collection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = chromadb.PersistentClient(path=f\"{NOTEBOOK_CACHE_DP}/chroma_db\")\n",
    "collection_exists = COLLECTION in [c.name for c in db.list_collections()]\n",
    "if RECREATE_INDEX or not collection_exists:\n",
    "    logger.info(f\"Creating new ChromaDB collection...\")\n",
    "    if collection_exists:\n",
    "        logger.info(f\"Deleting existing ChromaDB collection...\")\n",
    "        db.delete_collection(COLLECTION)\n",
    "    if os.path.exists(NODES_PERSIST_FP):\n",
    "        logger.info(f\"Deleting persisted nodes object at {NODES_PERSIST_FP}...\")\n",
    "        os.remove(NODES_PERSIST_FP)\n",
    "else:\n",
    "    logger.info(f\"Use existing ChromaDB collection\")\n",
    "chroma_collection = db.get_or_create_collection(COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ecdec51-2720-4628-a43f-caaa98310630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baa0a955-fbdf-4029-8e41-2efdea6ef58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = \"SentenceSplitter\"\n",
    "CHUNKER_CONFIG = {\n",
    "    \"chunk_size\": 512,\n",
    "    \"chunk_overlap\": 10\n",
    "}\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"CHUNKER\", CHUNKER)\n",
    "    for k, v in CHUNKER_CONFIG.items():\n",
    "        mlflow.log_param(f\"CHUNKER__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f36679-603e-40d6-862b-5d9313b3c1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 12:09:51.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mLoading index from existing ChromaDB...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if chroma_collection.count() > 0 and RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing ChromaDB...\")\n",
    "    with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new ChromaDB index...\")\n",
    "    # Generate nodes\n",
    "    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "    \n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(**CHUNKER_CONFIG),\n",
    "            TitleExtractor(),\n",
    "            embedding,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "    \n",
    "    # Need to use await and arun here to run the pipeline else error\n",
    "    # Ref: https://docs.llamaindex.ai/en/stable/examples/ingestion/async_ingestion_pipeline/\n",
    "    # Ref: https://github.com/run-llama/llama_index/issues/13904#issuecomment-2145561710\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    with open(NODES_PERSIST_FP, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a338d-469e-4461-9511-553dd6c1f766",
   "metadata": {},
   "source": [
    "#### Inspect nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20f68dde-bd70-49b2-a62b-40a527fc44ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# embeddings are excluded by default for performance, if need then explicitly ask for it in `include`\n",
    "# chroma_collection.get(include=['embeddings'])\n",
    "chroma_collection.get()['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914bc1e-e93b-40d9-8849-53903bff533d",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "817b9319-67e2-4423-a7b8-e45f22c9a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af7f28e5-f273-492c-b1de-e1b0f3956342",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 2\n",
    "# Need to be able to control this cutoff until specify it\n",
    "RETRIEVAL_SIMILARITY_CUTOFF = None\n",
    "# RETRIEVAL_SIMILARITY_CUTOFF = 0.3\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_TOP_K\", RETRIEVAL_TOP_K)\n",
    "    if RETRIEVAL_SIMILARITY_CUTOFF:\n",
    "        mlflow.log_param(\"RETRIEVAL_SIMILARITY_CUTOFF\", RETRIEVAL_SIMILARITY_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d05f895c-b472-432c-911d-2f6744c00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=RETRIEVAL_TOP_K,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if RETRIEVAL_SIMILARITY_CUTOFF is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=RETRIEVAL_SIMILARITY_CUTOFF))\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd56d64e-ddc6-4267-9641-bd90d64131a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node deb0944c-3a66-420a-b18a-33349ea5c923] [Similarity score:             0.657916] Automate online tasks with MultiOn and LlamaIndex\n",
      "Introduction MultiOn is an AI agents platform d...\n",
      "> [Node 61475053-55a3-47c6-a1f5-92879757a8e5] [Similarity score:             0.63385] As these technologies evolve, they will continue to unlock new potentials in AI application, sign...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:03:49.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mMultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"What is MultiOn?\"\n",
    "response = query_engine.query(question)\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed9ba4-4573-4e66-8d59-ad55b1ed6aae",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bdb95-f255-4f97-abc8-dda696070ed3",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008702-394b-4b6d-96a1-587b82d01249",
   "metadata": {},
   "source": [
    "### Building synthetic evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f78b7ddd-885b-469c-aa8d-052702dd7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NODES_PERSIST_FP, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b6b2b43-8bd7-4d6a-85fa-5dcb759677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs, EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ab9c5be-73e5-471e-9936-7904a6c7d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_RETRIEVAL_EVAL_DATASET = True\n",
    "# Currently can not reuse retrieval_eval_dataset because the retrieval evaluation is based on ids\n",
    "# RETRIEVAL_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_retrieval_eval_dataset.json\"\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = 10\n",
    "RETRIEVAL_NUM_SAMPLE_NODES = min(len(nodes), RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "RETRIEVAL_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RETRIEVAL_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "RETRIEVAL_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\", RETRIEVAL_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RETRIEVAL_NUM_SAMPLE_NODES\", RETRIEVAL_NUM_SAMPLE_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cf4da0a-5ae8-49b9-892b-5a18d0782490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:05:08.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mSampling 10 nodes for retrieval evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    if RETRIEVAL_NUM_SAMPLE_NODES:\n",
    "        logger.info(f\"Sampling {RETRIEVAL_NUM_SAMPLE_NODES} nodes for retrieval evaluation...\")\n",
    "        np.random.seed(41)\n",
    "        retrieval_eval_nodes = np.random.choice(nodes, RETRIEVAL_NUM_SAMPLE_NODES)\n",
    "    else:\n",
    "        logger.info(f\"Using all nodes for retrieval evaluation\")\n",
    "        retrieval_eval_nodes = nodes\n",
    "else:\n",
    "    logger.info(f\"Loading retrieval_eval_nodes from {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    with open(RETRIEVAL_EVAL_DATASET_FP, 'r') as f:\n",
    "        retrieval_eval_nodes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7c1995d-9b60-4720-b58b-759bebdb2364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:05:09.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 7/10 [00:11<00:04,  1.45s/it]/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/llama_index/core/llama_dataset/legacy/embedding.py:99: UserWarning: Fewer questions generated (1) than requested (2).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_RETRIEVAL_EVAL_DATASET or not os.path.exists(RETRIEVAL_EVAL_DATASET_FP):\n",
    "    # Use good model to generate the eval dataset\n",
    "    from llama_index.llms.openai import OpenAI\n",
    "    retrieval_eval_llm = OpenAI(model=RETRIEVAL_EVAL_LLM_MODEL, **RETRIEVAL_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    logger.info(f\"Creating new synthetic retrieval eval dataset...\")\n",
    "    retrieval_eval_dataset = generate_question_context_pairs(\n",
    "        retrieval_eval_nodes, llm=retrieval_eval_llm, num_questions_per_chunk=RETRIEVAL_NUM_QUESTIONS_PER_CHUNK\n",
    "    )\n",
    "    retrieval_eval_dataset.save_json(RETRIEVAL_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic retrieval eval dataset at {RETRIEVAL_EVAL_DATASET_FP}...\")\n",
    "    retrieval_eval_dataset = EmbeddingQAFinetuneDataset.from_json(RETRIEVAL_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef47844c-5787-458e-9860-4c1f6a0f1935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a0308b12-3a62-4763-b3f5-0497f22d16d2': 'How do complex and unconstrained agent interaction techniques, such as ReAct, differ from simple and constrained agent interaction mechanisms in terms of their approach to handling data queries?',\n",
       " 'c81bd84e-9978-4da9-bebc-37607bd94bab': 'How can agents, specifically those integrated with LlamaIndex query engines, assist in performing complex user queries across multiple data sources and synthesizing insights for users?',\n",
       " 'b8611759-7643-48ff-a926-1d144def53e3': 'How does LlamaIndex simplify the evaluation process for LLM and RAG apps, and what are the four key metrics it assesses these apps on?',\n",
       " 'eb369767-7e81-4a07-a3de-29b0826d9b88': 'Describe the integration of LlamaIndex with various tools and frameworks mentioned in the document, and explain how these integrations enhance the functionality and capabilities of LlamaIndex.',\n",
       " 'e37ae996-fd00-4277-b695-4362d547f5bc': \"How does the integration of videos and code snippets enhance the viewer's understanding of the code in the context of Knowledge Transfer (KT) videos?\",\n",
       " '746238d5-cc3a-49db-99d8-91335f2c0988': 'Discuss the potential impact of LlamaIndex in revolutionizing the creation of Knowledge Transfer (KT) Videos for code bases, and propose a possible future platform similar to KodeTube(KT) for cataloging organizational codebases through explanatory videos.',\n",
       " '36b28ebc-4725-4785-8756-ed58d602298e': \"How can you visualize and monitor performance changes in a system using Tonic Validate's API?\",\n",
       " '7cad7c60-f636-4c6b-b95d-1c190916250a': 'Why is it important to set up environment variables for the API key and project ID when using Tonic Validate for scoring runs in a system?',\n",
       " 'd8679392-2a27-40c2-bdfd-79c473eca5f9': 'How does the function \"messages_to_prompt\" in the provided code snippet handle different types of messages based on their roles (system, user, assistant)? Provide an example of how the prompt is constructed for each message role.',\n",
       " 'a74fd1a2-11f4-42e8-b6c7-aa446dd8c311': 'What is the purpose of the \"generate_kwargs\" parameter in the \"HuggingFaceLLM\" initialization? How does it affect the generation of responses by the language model?',\n",
       " '97d2fda0-1aec-4c80-ad70-b337f83437ed': 'How do Deng et al. (2023) propose to address the concerns related to chatbot-oriented LLMs, and what is the significance of their taxonomic framework in tackling these issues?',\n",
       " '33d4fb8f-3423-476b-8e74-ed31c68ac350': 'Discuss the specific challenges identified in tuned LLMs as highlighted by studies conducted by Ganguli et al. (2022) and Zhuo et al. (2023), particularly focusing on the successful attack types and their effects on the generation of harmful content.',\n",
       " '1e1d2d7e-d3b6-4d1f-9775-8d2ca04eafe3': 'How does the introduction of multi-document agents in the RAG system enhance advanced QA beyond typical top-k RAG?',\n",
       " 'da6362f5-a399-4abd-9373-36a4e84183b8': \"Explain the significance of using OpenAI's latest function calling fine-tuning for structured data extraction and optimization of gpt-3.5-turbo in the RAG system.\",\n",
       " '0a24ee07-2cbf-4fa8-9bc7-d5796e6d9d73': 'In this context, the process involves generating synthetic positive pairs of (query, relevant documents) using an LLM to create hypothetical questions based on a given piece of context. This eliminates the need for human labellers and allows for scalable generation of training data. The example prompt provided demonstrates how questions can be generated for an upcoming quiz/examination based on the context information provided.',\n",
       " 'e93e57a3-551f-4af7-ae4c-56718e14058e': 'How does the RAGs app allow users to create and customize their own RAG pipeline without needing to code?',\n",
       " 'bba7c980-0fbf-48ba-a950-14fdd4457dcc': 'What are the components required to build a RAG pipeline according to the instructions on the Home Page section of the app?',\n",
       " '0927dab5-f719-43f5-bfa7-7c6e008df5ab': 'How does the use of LlamaIndex and Ray contribute to the efficiency and speed of querying data across disparate sources in the application described in the context?',\n",
       " 'c02ebb51-382f-454c-a27d-13c58561d833': 'What are the next steps recommended for individuals interested in learning more about the LlamaIndex data framework and building scalable LLM applications using Ray, as mentioned in the context information provided?'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9beaa-6a73-44c1-9f63-6d4c704a68d5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c31f7229-f30e-4fd3-8bd9-186e9ada6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71cd5482-b261-42a0-b822-af1f1303bb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 87ffd15d-0c04-49c1-b087-a33eb2c0d444] [Similarity score:             0.809213] It repeats these steps in an iterative loop until the task is complete. There are other interacti...\n",
      "> [Node d5280cda-0a90-4982-a02d-59a3eaaec876] [Similarity score:             0.778968] Dumber LLM Agents Need More Constraints and Better Tools\n",
      "Summary In this article, we compare how ...\n",
      "> Top 2 nodes:\n",
      "> [Node 8c640df7-e039-4425-8878-f70986cc643b] [Similarity score:             0.835663] As a result some of our existing query capabilities contain “agent-like” components: we have quer...\n",
      "> [Node 109f25a3-1a96-4981-89c1-9481978b5d9f] [Similarity score:             0.831301] The agent then reasons that it needs to call the  read_search_data  tool, which will query the in...\n",
      "> Top 2 nodes:\n",
      "> [Node acccfcb5-6e27-4c14-afc6-6328d0538c84] [Similarity score:             0.788382] The evaluations demonstrated here will help you quickly find what’s affecting the quality of your...\n",
      "> [Node d78e374d-ce59-4d24-96bb-8a0f1df40e83] [Similarity score:             0.787607] As is common in modern software development practices, you will likely continue to fix bugs, make...\n",
      "> Top 2 nodes:\n",
      "> [Node 17eece8c-69b3-4509-a2ba-02295d950635] [Similarity score:             0.798004] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> [Node 8e9df9f8-99ae-4257-b46b-9ff5c7d55d21] [Similarity score:             0.78273] Integrations with External Platforms Integration with PortkeyAI : LlamaIndex integrates with Port...\n",
      "> Top 2 nodes:\n",
      "> [Node 987c2eb3-6be2-4661-a3b2-d4620e5bc0f3] [Similarity score:             0.779961] To see this in action, let’s take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node 5d5206b6-9c2e-4c67-b1a6-6d81921ac381] [Similarity score:             0.747356] However, a challenge arises. While\n",
      "   accumulate  provides in-depth insights into each block, it ...\n",
      "> Top 2 nodes:\n",
      "> [Node 987c2eb3-6be2-4661-a3b2-d4620e5bc0f3] [Similarity score:             0.88654] To see this in action, let’s take a look at a sample output. \n",
      " \n",
      "   \n",
      " \n",
      " 4. Video-Code Integration:...\n",
      "> [Node fbe625ab-742c-4838-9a1f-dd4aab3d20ae] [Similarity score:             0.869195] LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases\n",
      "Introduction: In the worl...\n",
      "> Top 2 nodes:\n",
      "> [Node 26c86383-a540-4466-bc17-5e1a4f2c0aa1] [Similarity score:             0.739589] def   test_llama_index ():\n",
      "    questions, reference_answers = get_q_and_a()\n",
      "    llm_answers, cont...\n",
      "> [Node fe68b230-5b05-4882-8fcd-cdf7e49c2902] [Similarity score:             0.726304] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node 06b5cc3f-09b7-4856-96ba-2a0c3d6ad86d] [Similarity score:             0.721088] In this file, we will include the following code configuration that defines the integration test ...\n",
      "> [Node fe68b230-5b05-4882-8fcd-cdf7e49c2902] [Similarity score:             0.720422] In GitHub, go to  Settings > Secrets and variables > Actions  for your repo and create a secret c...\n",
      "> Top 2 nodes:\n",
      "> [Node a08bb2c8-ae48-4ed0-b8a7-d99e682e1456] [Similarity score:             0.701378] query ( \"Draw me a picture of a happy dog\" ) Snag #1 One main drawback of Transformers Agents cur...\n",
      "> [Node 373e75bc-89c8-42c3-b15a-605c2b21da86] [Similarity score:             0.700783] LlamaIndex and Transformers Agents\n",
      "Summary Agents are a popular use-case for Large Language Model...\n",
      "> Top 2 nodes:\n",
      "> [Node c4db3553-e353-4cd6-b9d9-5687e4202b98] [Similarity score:             0.670161] This triggers the outer loop to run the continuation agent, to see if there's anything else to do...\n",
      "> [Node 59b73a3c-ed08-4bd6-acf4-005eb80fa082] [Similarity score:             0.654938] At the core of that is a very simple block that simply asks the orchestration agent who should sp...\n",
      "> Top 2 nodes:\n",
      "> [Node 3cc02bcf-4971-4898-9d19-acb969d126e0] [Similarity score:             0.709608] COVID -19  pandemic: The ongoing pandemic remains a constant risk factor across all quarters, wit...\n",
      "> [Node d1dfc5fb-68e9-4cc8-8a22-f267576e4fa0] [Similarity score:             0.707448] ∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com †Second author Contribu...\n",
      "> Top 2 nodes:\n",
      "> [Node 6558c106-3481-4b2c-86c7-70885c8a5149] [Similarity score:             0.781431] Now, the LLM Predictor class is mostly a lightweight wrapper on top of the  LLM  abstraction that...\n",
      "> [Node 11593ed3-8fb2-4296-a864-7d4e7e8f8d8b] [Similarity score:             0.777265] We also offer an extensive array of integrations with other storage providers and downstream appl...\n",
      "> Top 2 nodes:\n",
      "> [Node 0e838eff-044e-433f-81be-76720167a664] [Similarity score:             0.760642] Tweet . We integrated OpenAI’s parallel function calling for efficient extraction of structured d...\n",
      "> [Node b06068bd-ae32-4522-8830-afa9f9e1268a] [Similarity score:             0.753647] # We will use GPT-4 for evaluating the responses\n",
      "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
      "\n",
      "# D...\n",
      "> Top 2 nodes:\n",
      "> [Node 632ee572-e23d-4287-bf35-4b9bd63095d6] [Similarity score:             0.754962] This optimizes for reduced latency and costs, and effectively halts the agent after crucial actio...\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.74772] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node 15fe877d-b087-47f8-b02a-7f88a5c47204] [Similarity score:             0.798072] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node 3a18e55d-786c-4df3-9f06-77439fc28691] [Similarity score:             0.761162] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node 2fa0f7b7-4e7d-4702-af2d-5d70935413fb] [Similarity score:             0.799559] These (question, chunk) pairs are then used as positive examples as training signals for the mode...\n",
      "> [Node e151fbe9-88f7-4b8f-8b2c-823f7f3754cc] [Similarity score:             0.763158] NO — Response and Source Nodes (Context) are not matching. from  llama_index.evaluation  import  ...\n",
      "> Top 2 nodes:\n",
      "> [Node 15fe877d-b087-47f8-b02a-7f88a5c47204] [Similarity score:             0.712357] Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data\n",
      "Today we introduce  RAGs , ...\n",
      "> [Node 3a18e55d-786c-4df3-9f06-77439fc28691] [Similarity score:             0.710272] RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Power...\n",
      "> Top 2 nodes:\n",
      "> [Node 3355b515-6a73-4e5d-82d6-95ed69d8d8b9] [Similarity score:             0.857274] More specifically, we showcase a very relevant use case — highlighting Ray features that are pres...\n",
      "> [Node 71959895-40e7-413a-985d-9421e3240088] [Similarity score:             0.856767] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> Top 2 nodes:\n",
      "> [Node 71959895-40e7-413a-985d-9421e3240088] [Similarity score:             0.854342] Build and Scale a Powerful Query Engine with LlamaIndex and Ray\n",
      "Co-authors: Jerry Liu (CEO at Lla...\n",
      "> [Node 37967411-7251-4954-9ba9-9d6dd7819a6f] [Similarity score:             0.849632] This allows you to effortlessly ask questions and synthesize insights about Ray across disparate ...\n"
     ]
    }
   ],
   "source": [
    "RETRIEVAL_METRICS = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    RETRIEVAL_METRICS, retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_eval_results = await retriever_evaluator.aevaluate_dataset(retrieval_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4dfa852-27b8-4caf-b017-f3148c348887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name, eval_results, metrics=['hit_rate', 'mrr'], include_cohere_rerank=False):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    if include_cohere_rerank:\n",
    "        crr_relevancy = full_df[\"cohere_rerank_relevancy\"].mean()\n",
    "        columns.update({\"cohere_rerank_relevancy\": [crr_relevancy]})\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0236036f-d6f7-42b1-9707-785e8fd61e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_2_retrieval_eval</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.266618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  top_2_retrieval_eval  0.473684  0.421053   0.236842  0.473684  0.421053   \n",
       "\n",
       "       ndcg  \n",
       "0  0.266618  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_prefix = f\"top_{RETRIEVAL_TOP_K}_retrieval_eval\"\n",
    "retrieval_eval_results_df = display_results(metric_prefix, retrieval_eval_results, metrics=RETRIEVAL_METRICS)\n",
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fa1e246-ac8c-4403-98f8-70c21123a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for metric, metric_value in retrieval_eval_results_df.to_dict(orient='records')[0].items():\n",
    "        if metric in RETRIEVAL_METRICS:\n",
    "            mlflow.log_metric(f\"{metric_prefix}_{metric}\", metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e513ce-3f13-4cd1-bd2e-c37f4834f39e",
   "metadata": {},
   "source": [
    "### Manually curated dataset\n",
    "Ref: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c90a243-f246-4802-8d95-f584e2d87be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_EVAL_QA = [\n",
    "(\"What are key features of llama-agents?\",\n",
    "\"\"\"\n",
    "Key features of llama-agents are:\n",
    "1. Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
    "2. Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
    "3. Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
    "4. Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
    "5. Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\",\n",
    "\"\"\"\n",
    "Retrieval System and Response Generation.\n",
    "\"\"\"\n",
    "),\n",
    "(\"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\",\n",
    "\"\"\"\n",
    "Hit rate and Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
    "\n",
    "Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n",
    "\"\"\"\n",
    ")\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9499affa-46c9-409b-8712-59b403a95020",
   "metadata": {},
   "source": [
    "# TODO: Implement manual retrieval checks\n",
    "retriever_evaluator.evaluate(\n",
    "    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc8df9-d1eb-459b-bd34-222e27637b2f",
   "metadata": {},
   "source": [
    "## Response Evaluation\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bcf3938-0f92-4fa0-97dc-483151fa64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_rag_dataset(response_eval_dataset, response_eval_prediction_dataset, dataset_name=\"synthetic\", batch_size=8, judge_model='gpt-3.5-turbo', cache_dp='.'):\n",
    "    # Instantiate the judges\n",
    "    judges = {\n",
    "        \"correctness\": CorrectnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"relevancy\": RelevancyEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"faithfulness\": FaithfulnessEvaluator(\n",
    "            llm=OpenAI(temperature=0, model=judge_model),\n",
    "        ),\n",
    "        \"semantic_similarity\": SemanticSimilarityEvaluator(),\n",
    "    }\n",
    "\n",
    "    # Initialize evaluations dictionary\n",
    "    evals = {\n",
    "        \"correctness\": [],\n",
    "        \"relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contexts\": [],\n",
    "    }\n",
    "\n",
    "    # Evaluate each prediction\n",
    "    for example, prediction in tqdm(\n",
    "        zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "    ):\n",
    "        correctness_result = judges[\"correctness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            reference=example.reference_answer,\n",
    "        )\n",
    "\n",
    "        relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            contexts=prediction.contexts,\n",
    "        )\n",
    "\n",
    "        evals[\"correctness\"].append(correctness_result)\n",
    "        evals[\"relevancy\"].append(relevancy_result)\n",
    "        evals[\"faithfulness\"].append(faithfulness_result)\n",
    "        evals[\"contexts\"].append(prediction.contexts)\n",
    "\n",
    "    # Save evaluations to JSON\n",
    "    evaluations_objects = {\n",
    "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "        \"contexts\": evals['contexts'],\n",
    "    }\n",
    "\n",
    "    with open(f\"{cache_dp}/{dataset_name}_evaluations.json\", \"w\") as json_file:\n",
    "        json.dump(evaluations_objects, json_file)\n",
    "\n",
    "    # Generate evaluation results DataFrames\n",
    "    deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "        evals[\"correctness\"],\n",
    "        metric=\"correctness\",\n",
    "    )\n",
    "    deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "        evals[\"relevancy\"],\n",
    "        metric=\"relevancy\",\n",
    "    )\n",
    "    deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "        [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "        evals[\"faithfulness\"],\n",
    "        metric=\"faithfulness\",\n",
    "    )\n",
    "\n",
    "    mean_scores_df = pd.concat(\n",
    "        [\n",
    "            mean_correctness_df.reset_index(),\n",
    "            mean_relevancy_df.reset_index(),\n",
    "            mean_faithfulness_df.reset_index(),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "    mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "\n",
    "    deep_eval_df = pd.concat([\n",
    "        deep_eval_correctness_df[['query', 'answer']],\n",
    "        deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "        deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "        deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "        pd.Series(evals['contexts'], name='contexts')\n",
    "    ], axis=1)\n",
    "\n",
    "    return mean_scores_df, deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375eec0-f1d5-44c2-8cdd-3a26746c2c8a",
   "metadata": {},
   "source": [
    "### Generate synthetic Llama Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66e51daa-8fa4-4ff1-af60-2cf77cc142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a1729a4-d78b-493d-a134-be20149e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECREATE_SYNTHETIC_EVAL_DATASET = False\n",
    "# RESPONSE_EVAL_DATASET_FP = f\"{NOTEBOOK_CACHE_DP}/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_DATASET_FP = f\"data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json\"\n",
    "RESPONSE_EVAL_LLM_MODEL = 'gpt-3.5-turbo'\n",
    "RESPONSE_EVAL_LLM_MODEL_CONFIG = {\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK = 2\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = 10\n",
    "RESPONSE_NUM_SAMPLE_DOCUMENTS = min(len(documents), RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "\n",
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.log_param(\"SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK\", SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK)\n",
    "    mlflow.log_param(\"RESPONSE_EVAL_LLM_MODEL\", RESPONSE_EVAL_LLM_MODEL)\n",
    "    mlflow.log_param(\"RESPONSE_NUM_SAMPLE_DOCUMENTS\", RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "    for k, v in RESPONSE_EVAL_LLM_MODEL_CONFIG.items():\n",
    "        mlflow.log_param(f\"RESPONSE_EVAL_LLM_MODEL_CONFIG__{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d177516-86bc-4639-a185-65895add5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:10:58.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RESPONSE_NUM_SAMPLE_DOCUMENTS:\n",
    "    logger.info(f\"Sampling {RESPONSE_NUM_SAMPLE_DOCUMENTS} documents for response evaluation...\")\n",
    "    np.random.seed(41)\n",
    "    response_eval_documents = np.random.choice(documents, RESPONSE_NUM_SAMPLE_DOCUMENTS)\n",
    "else:\n",
    "    logger.info(f\"Using all documents for retrieval evaluation\")\n",
    "    response_eval_documents = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8251b70-e28e-4b6a-aec7-b0d0fcf61020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-24 10:10:59.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mLoading existing synthetic response eval dataset at data/001/exp_001_v3/llamaindex_blog_response_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if RECREATE_SYNTHETIC_EVAL_DATASET or not os.path.exists(RESPONSE_EVAL_DATASET_FP):\n",
    "    logger.info(f\"Creating synthetic response eval dataset...\")\n",
    "    # Use good model to generate the eval dataset\n",
    "    response_eval_llm = OpenAI(model=RESPONSE_EVAL_LLM_MODEL, **RESPONSE_EVAL_LLM_MODEL_CONFIG)\n",
    "\n",
    "    # instantiate a DatasetGenerator\n",
    "    response_dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        response_eval_documents,\n",
    "        llm=response_eval_llm,\n",
    "        num_questions_per_chunk=SYNTHETIC_RESPONSE_NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        show_progress=True,\n",
    "        workers=(os.cpu_count() - 1)\n",
    "    )\n",
    "\n",
    "    synthetic_response_eval_dataset = response_dataset_generator.generate_dataset_from_nodes()\n",
    "\n",
    "    synthetic_response_eval_dataset.save_json(RESPONSE_EVAL_DATASET_FP)\n",
    "else:\n",
    "    logger.info(f\"Loading existing synthetic response eval dataset at {RESPONSE_EVAL_DATASET_FP}...\")\n",
    "    synthetic_response_eval_dataset = LabeledRagDataset.from_json(RESPONSE_EVAL_DATASET_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "940feda2-1aad-427c-895f-2d4cdf43918f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node d14146be-9c16-4437-9597-a6460aa0b37a] [Similarity score:             0.809745] Launching the first GenAI-native document parsing platform\n",
      "Our mission at LlamaIndex is to connec...\n",
      "> [Node 9b3bd895-09d8-4f85-b5b2-6c6ad4dd27d5] [Similarity score:             0.803162] LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, LlamaIndex enthusiasts! 🦙 Welcome to another exciting...\n",
      "> Top 2 nodes:\n",
      "> [Node cd0c5df2-0cd7-4ca9-802e-1d1d706bf164] [Similarity score:             0.876471] OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 78354e9f-fbd8-4ca1-bf15-a75b17992baf] [Similarity score:             0.79349] Setup Weaviate Client url = 'cluster URL'\n",
      "api_key = 'your api key'\n",
      "\n",
      "client = get_weaviate_client(...\n",
      "> Top 2 nodes:\n",
      "> [Node acf53e3e-a1b0-4341-ae71-2f1e2f048930] [Similarity score:             0.775634] Tweet . We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mi...\n",
      "> [Node 8b5e6871-542f-4964-855c-20bdd1868eac] [Similarity score:             0.767926] NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkutx FinSight by Vishwas Gowda:  https://buff.ly/3PzO...\n",
      "> Top 2 nodes:\n",
      "> [Node cd0c5df2-0cd7-4ca9-802e-1d1d706bf164] [Similarity score:             0.861998] OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node c820d085-0a73-44e5-baaf-9d61ea764585] [Similarity score:             0.81176] Below, we list a select few of the evaluation notebook guides. Answer Relevancy and Context Relev...\n",
      "> Top 2 nodes:\n",
      "> [Node 479d31af-0811-447b-9518-56a0ac5ad2e3] [Similarity score:             0.805334] LlamaIndex turns 1!\n",
      "It’s our birthday! One year ago, Jerry pushed his  first commit  to GPT Index...\n",
      "> [Node 17eece8c-69b3-4509-a2ba-02295d950635] [Similarity score:             0.797438] LlamaIndex has evolved into a broad toolkit containing hundreds of integrations: 150+ data loader...\n",
      "> Top 2 nodes:\n",
      "> [Node 304e47e2-aba0-4928-943e-aa60754e21c2] [Similarity score:             0.860081] MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB\n",
      "The widespread consumption...\n",
      "> [Node 3d065fa5-3dec-4619-9a1e-6fbf9cdbf841] [Similarity score:             0.845695] Simplify your RAG application architecture with LlamaIndex + PostgresML\n",
      "We’re happy to announce t...\n",
      "> Top 2 nodes:\n",
      "> [Node fda5a801-53b1-4d79-84f9-211c3e6d4304] [Similarity score:             0.815758] LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ ...\n",
      "> [Node d06a5ca7-1975-49ad-9cb3-71183680e684] [Similarity score:             0.798446] LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaIndex Community! 🦙 Welcome to another exciting weekly...\n",
      "> Top 2 nodes:\n",
      "> [Node d1296c8b-a75b-4358-8138-a7d76f84944b] [Similarity score:             0.811921] Querying a network of knowledge with llama-index-networks\n",
      "The main premise behind RAG is the inje...\n",
      "> [Node 0ebff8b1-888a-4bb2-a3c6-b1579888ca00] [Similarity score:             0.792621] Alex has heard about these insightful documents that both Bob and Beth have and would like to be ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:05<00:00,  1.55it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 42e4eb0c-3488-4599-918b-35e8798f73d5] [Similarity score:             0.842008] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> [Node 36455c92-7685-4da6-a81f-ec026ec4dc88] [Similarity score:             0.830373] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> Top 2 nodes:\n",
      "> [Node 378632bf-a7ea-40b5-a254-d8c2586e25d0] [Similarity score:             0.795793] Retrieving Privacy-Safe Documents Over A Network\n",
      "In a  recent blog post , we introduced our  llam...\n",
      "> [Node a9521ec4-62c9-4c8c-9798-6ed7b67e579e] [Similarity score:             0.787769] There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation...\n",
      "> Top 2 nodes:\n",
      "> [Node 36455c92-7685-4da6-a81f-ec026ec4dc88] [Similarity score:             0.837976] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> [Node 8abfc95b-765e-46ec-9a98-590a893956cf] [Similarity score:             0.837232] Old from  llama_index  import  (\n",
      "    VectorStoreIndex,\n",
      "    ResponseSynthesizer,\n",
      ")\n",
      " from  llama_in...\n",
      "> Top 2 nodes:\n",
      "> [Node 841e5f35-d7f3-4ff8-a866-ba8f3f42e6f7] [Similarity score:             0.823183] This feature enables transparency, re-use, and generally more rapid development velocity. Improve...\n",
      "> [Node c105d699-ecbe-4005-a43e-4281fb852b48] [Similarity score:             0.807417] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 42e4eb0c-3488-4599-918b-35e8798f73d5] [Similarity score:             0.760965] Wrap A LlamaIndex App with TruLens With TruLens, you can wrap LlamaIndex query engines with a Tru...\n",
      "> [Node 1c1f34eb-2951-489e-8544-dcc059800408] [Similarity score:             0.749447] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> Top 2 nodes:\n",
      "> [Node 556e79ab-d296-41ab-b512-0d2f58f27f14] [Similarity score:             0.763005] Doing that stuff with LlamaCloud and LlamaParse is remarkably simpler. This in turn has really he...\n",
      "> [Node c105d699-ecbe-4005-a43e-4281fb852b48] [Similarity score:             0.746297] The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need...\n",
      "> Top 2 nodes:\n",
      "> [Node 1c1f34eb-2951-489e-8544-dcc059800408] [Similarity score:             0.824055] min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n",
      "\n",
      "l = TruLlama(app=query_engine...\n",
      "> [Node 36455c92-7685-4da6-a81f-ec026ec4dc88] [Similarity score:             0.815926] Build and Evaluate LLM Apps with LlamaIndex and TruLens\n",
      "Authors:  Anupam Datta, Shayak Sen, Jerry...\n",
      "> Top 2 nodes:\n",
      "> [Node 134fe094-4f11-4ff2-9142-520682a71782] [Similarity score:             0.799157] Its integration ensures a smooth transition from user inputs to database insights, culminating in...\n",
      "> [Node a72f5137-7d1c-4657-9b3a-f8d782ce7ca9] [Similarity score:             0.794769] Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  2.69it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 6e3de820-cdfc-40ab-b64f-1d2c00382be3] [Similarity score:             0.904004] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node ed199b1b-2dca-48c8-9cfc-340fdca3fa7e] [Similarity score:             0.775254] (For me, I usually overestimate what I can achieve by 3–10x!) With this in mind, I tried to set a...\n",
      "> Top 2 nodes:\n",
      "> [Node 57707274-0eb2-4175-b98d-e3fb5034ad42] [Similarity score:             0.826259] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node 75e7cc55-23e3-4223-aecb-2ab4722b5cd1] [Similarity score:             0.814519] Codebase . 🗺️ Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node 6e3de820-cdfc-40ab-b64f-1d2c00382be3] [Similarity score:             0.906435] LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\n",
      "Generative AI is rapidly transfor...\n",
      "> [Node e2d16ded-69db-4d4a-9f3d-b5c65a8b8477] [Similarity score:             0.783043] “Now, developers can abstract complexities associated with data ingestion, simplify RAG pipeline ...\n",
      "> Top 2 nodes:\n",
      "> [Node 0316bdda-2c85-407b-b333-000c2444e251] [Similarity score:             0.794386] Introducing Llama Packs\n",
      "Today we’re excited to introduce  Llama Packs 🦙📦—  a community-driven hub...\n",
      "> [Node 55c75197-bf41-493e-a253-5ec478e72a06] [Similarity score:             0.792443] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 9c3c536a-86d0-4186-9473-d4de1c3cb6eb] [Similarity score:             0.731607] Reranking involves using a semantic search model (specially tuned for the reranking task) that br...\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.731154] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> Top 2 nodes:\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.735694] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 75e7cc55-23e3-4223-aecb-2ab4722b5cd1] [Similarity score:             0.725484] Codebase . 🗺️ Guides: Guide  to Building an Agentic RAG Service with our comprehensive notebook t...\n",
      "> Top 2 nodes:\n",
      "> [Node 24fa1718-93d7-49f0-8e52-b43bbd8dcdaf] [Similarity score:             0.890947] Agentic RAG With LlamaIndex\n",
      "The topic of Agentic RAG explores how agents can be incorporated into...\n",
      "> [Node 218282c6-56f3-404d-8100-55da8947c88e] [Similarity score:             0.875135] Utilizing LlamaIndex\n",
      "      connectors allows you to seamlessly integrate your data into the\n",
      "     ...\n",
      "> Top 2 nodes:\n",
      "> [Node 0316bdda-2c85-407b-b333-000c2444e251] [Similarity score:             0.764335] Introducing Llama Packs\n",
      "Today we’re excited to introduce  Llama Packs 🦙📦—  a community-driven hub...\n",
      "> [Node 55c75197-bf41-493e-a253-5ec478e72a06] [Similarity score:             0.762903] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.20it/s]\n",
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node 7d3fda45-2028-4a1d-b8b2-4c47adab8ee5] [Similarity score:             0.807019] Register for free! ✨ Feature Releases and Enhancements: We introduced the LlamaIndex 0.9 version ...\n",
      "> [Node cec1e71e-330c-4eb5-ade9-9eeb1ab95ca6] [Similarity score:             0.798853] LlamaIndex Newsletter 2023–11–21\n",
      "Hello Llama Fam 🦙 What an amazing week we’ve had! We’re excited ...\n",
      "> Top 2 nodes:\n",
      "> [Node e40b7bb4-d699-4e0f-aab6-0f386ed54edb] [Similarity score:             0.815946] Mervin Praison’s   tutorial  on using llama-agents, detailing the framework’s purpose, a step-by-...\n",
      "> [Node 3aaf1433-c6ca-49cc-98fe-c117932a836b] [Similarity score:             0.815772] Guide  to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic ...\n",
      "> Top 2 nodes:\n",
      "> [Node d1f5a699-c778-44fe-b1f6-a8c0974f4000] [Similarity score:             0.751528] We now accommodate custom models that align with the OpenAI-compatible API. 🎥 Webinars: Wenqi Gla...\n",
      "> [Node 4214f9a7-9bca-4e81-b8f8-065ca99128b5] [Similarity score:             0.739651] Webinar  with Omar Khattab and Thomas Joshi on DSPy — a framework for LLMs that emphasizes progra...\n",
      "> Top 2 nodes:\n",
      "> [Node f11f2733-8e93-4cd8-8976-7c16ce2b0625] [Similarity score:             0.81206] Voyage AI Pack. Every Pack has a detailed README on how to use / modules. First, we download and ...\n",
      "> [Node 55c75197-bf41-493e-a253-5ec478e72a06] [Similarity score:             0.786931] They can be downloaded either through our  llama_index  Python library or the CLI in  one line of...\n",
      "> Top 2 nodes:\n",
      "> [Node 632ee572-e23d-4287-bf35-4b9bd63095d6] [Similarity score:             0.794774] This optimizes for reduced latency and costs, and effectively halts the agent after crucial actio...\n",
      "> [Node 0802f619-a16f-48fe-b896-fe3eb958fbe6] [Similarity score:             0.785191] LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIndex Family! 🦙 Welcome to another thrilling weekly ...\n",
      "> Top 2 nodes:\n",
      "> [Node c8b2de1f-384c-4a2c-ad55-0e3f8849946b] [Similarity score:             0.770305] Let’s take a look at the downloaded pack in  voyage_pack/base.py  , and swap out the OpenAI LLM f...\n",
      "> [Node 3a4c7e52-6535-4680-9fb9-74fb7eb95fae] [Similarity score:             0.729414] There are 19 folders in here. The main integration categories are: llms embeddings multi_modal_ll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:05<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_response_eval_prediction_dataset = await synthetic_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab082f5e-59a4-4401-9ced-0394bf4ab869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ddbfbee8144309bf28f87c32057b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: Alex has heard about these insightful documents...\n",
      "> Adding chunk: Querying a network of knowledge with llama-inde...\n",
      "> Adding chunk: Alex has heard about these insightful documents...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n",
      "> Adding chunk: Launching the first GenAI-native document parsi...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-03-19\n",
      "Greetings, Lla...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: Setup Weaviate Client url = 'cluster URL'\n",
      "api_k...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: Setup Weaviate Client url = 'cluster URL'\n",
      "api_k...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: Below, we list a select few of the evaluation n...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It’s our birthday! One year...\n",
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n",
      "> Adding chunk: LlamaIndex turns 1!\n",
      "It’s our birthday! One year...\n",
      "> Adding chunk: LlamaIndex has evolved into a broad toolkit con...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: MultiModal RAG for Advanced Video Processing wi...\n",
      "> Adding chunk: Simplify your RAG application architecture with...\n",
      "> Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: LlamaIndex + Gemini\n",
      "(co-authored by Jerry Liu, ...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-05-21\n",
      "Hello LlamaInd...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n",
      "> Adding chunk: Tweet . We introduced day-0 integrations with t...\n",
      "> Adding chunk: NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkut...\n",
      "> Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n",
      "> Adding chunk: There are 2 main paradigms currently for extend...\n",
      "> Adding chunk: Retrieving Privacy-Safe Documents Over A Networ...\n",
      "> Adding chunk: There are 2 main paradigms currently for extend...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Its integration ensures a smooth transition fro...\n",
      "> Adding chunk: Transforming Natural Language to SQL and Insigh...\n",
      "> Adding chunk: Doing that stuff with LlamaCloud and LlamaParse...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Doing that stuff with LlamaCloud and LlamaParse...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: This feature enables transparency, re-use, and ...\n",
      "> Adding chunk: The latest updates to LlamaCloud\n",
      "To build a pro...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Old from  llama_index  import  (\n",
      "    VectorStor...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Old from  llama_index  import  (\n",
      "    VectorStor...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Wrap A LlamaIndex App with TruLens With TruLens...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: min )\n",
      "\n",
      "\n",
      "feedbacks = [f_lang_match, f_qa_relevan...\n",
      "> Adding chunk: Build and Evaluate LLM Apps with LlamaIndex and...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Utilizing LlamaIndex\n",
      "      connectors allows yo...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Reranking involves using a semantic search mode...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Codebase . 🗺️ Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Agentic RAG With LlamaIndex\n",
      "The topic of Agenti...\n",
      "> Adding chunk: Codebase . 🗺️ Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . 🗺️ Guides: Guide  to Building an Age...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: Codebase . 🗺️ Guides: Guide  to Building an Age...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: (For me, I usually overestimate what I can achi...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: (For me, I usually overestimate what I can achi...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: “Now, developers can abstract complexities asso...\n",
      "> Adding chunk: LlamaIndex Accelerates Enterprise Generative AI...\n",
      "> Adding chunk: “Now, developers can abstract complexities asso...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we’re excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we’re excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we’re excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Introducing Llama Packs\n",
      "Today we’re excited to ...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Voyage AI Pack. Every Pack has a detailed READM...\n",
      "> Adding chunk: They can be downloaded either through our  llam...\n",
      "> Adding chunk: Let’s take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: Let’s take a look at the downloaded pack in  vo...\n",
      "> Adding chunk: There are 19 folders in here. The main integrat...\n",
      "> Adding chunk: Register for free! ✨ Feature Releases and Enhan...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–11–21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: Register for free! ✨ Feature Releases and Enhan...\n",
      "> Adding chunk: LlamaIndex Newsletter 2023–11–21\n",
      "Hello Llama Fa...\n",
      "> Adding chunk: This optimizes for reduced latency and costs, a...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: This optimizes for reduced latency and costs, a...\n",
      "> Adding chunk: LlamaIndex Newsletter 2024-04-16\n",
      "Hello, LlamaIn...\n",
      "> Adding chunk: Mervin Praison’s   tutorial  on using llama-age...\n",
      "> Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n",
      "> Adding chunk: Mervin Praison’s   tutorial  on using llama-age...\n",
      "> Adding chunk: Guide  to Building an Agent in LlamaIndex: Our ...\n",
      "> Adding chunk: We now accommodate custom models that align wit...\n",
      "> Adding chunk: Webinar  with Omar Khattab and Thomas Joshi on ...\n",
      "> Adding chunk: We now accommodate custom models that align wit...\n",
      "> Adding chunk: Webinar  with Omar Khattab and Thomas Joshi on ...\n"
     ]
    }
   ],
   "source": [
    "synthetic_mean_scores_df, synthetic_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    synthetic_response_eval_dataset,\n",
    "    synthetic_response_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59356440-eef4-4670-b28f-43d23f1aa803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.208333\n",
       "mean_relevancy_score     0.966667\n",
       "mean_faithfulness_score  1.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84ed66a8-b590-4195-8416-75b660c13490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the new feature released by LlamaInde...</td>\n",
       "      <td>The new feature, llama-index-networks, enables...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Querying a network of knowledge with llama-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discuss the advancements made by LlamaIndex in...</td>\n",
       "      <td>LlamaIndex has made significant advancements i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Launching the first GenAI-native document par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain the three main sections of the OpenAI ...</td>\n",
       "      <td>The OpenAI Cookbook for evaluating RAG systems...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe’r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the OpenAI Cookbook suggest evaluatin...</td>\n",
       "      <td>The OpenAI Cookbook suggests evaluating the pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe’r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has LlamaIndex evolved over the past year ...</td>\n",
       "      <td>Over the past year, LlamaIndex has experienced...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex turns 1!\\nIt’s our birthday! One y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can you explain the significance of the Retrie...</td>\n",
       "      <td>RAG technology plays a crucial role in the dev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[MultiModal RAG for Advanced Video Processing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the partnership with Google Gemini be...</td>\n",
       "      <td>The partnership with Google Gemini benefits Ll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex + Gemini\\n(co-authored by Jerry Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Describe the Multi-Doc SEC 10Q Dataset launche...</td>\n",
       "      <td>The Multi-Doc SEC 10Q Dataset, launched by Taq...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tweet . We introduced day-0 integrations with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>The MemoryCache project by Mozilla utilizes Pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Retrieving Privacy-Safe Documents Over A Netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Discuss the significance of integrating Na2SQL...</td>\n",
       "      <td>The integration of Na2SQL with Llama Index is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Its integration ensures a smooth transition f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does LlamaCloud help in reducing productio...</td>\n",
       "      <td>According to the latest updates, LlamaCloud he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Doing that stuff with LlamaCloud and LlamaPar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What new features have been introduced in Llam...</td>\n",
       "      <td>LlamaCloud has introduced several new features...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This feature enables transparency, re-use, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does LlamaIndex help in building LLM apps ...</td>\n",
       "      <td>LlamaIndex is a popular open-source framework ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Evaluate LLM Apps with LlamaIndex a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Can you explain the process of wrapping a Llam...</td>\n",
       "      <td>You can wrap a LlamaIndex app with TruLens by ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Wrap A LlamaIndex App with TruLens With TruLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the feedback function in TruLlama hel...</td>\n",
       "      <td>The feedback function in TruLlama helps improv...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Wrap A LlamaIndex App with TruLens With TruLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the significance of tracking a...</td>\n",
       "      <td>Tracking and evaluating app versions using Tru...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[min )\\n\\n\\nfeedbacks = [f_lang_match, f_qa_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explain the basic structure of LlamaIndex's Ag...</td>\n",
       "      <td>The basic structure of LlamaIndex's Agentic RA...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How does the meta-agent or top-level agent in ...</td>\n",
       "      <td>The meta-agent or top-level agent in the Agent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Reranking involves using a semantic search mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Explain the concept of Agentic RAG as describe...</td>\n",
       "      <td>Agentic RAG is a concept that integrates agent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Agentic RAG With LlamaIndex\\nThe topic of Age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does the implementation by LlamaIndex demo...</td>\n",
       "      <td>The implementation by LlamaIndex demonstrates ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does the integration of NVIDIA NIM with Ll...</td>\n",
       "      <td>The integration of NVIDIA NIM with LlamaIndex ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What benefits do developers gain from using NV...</td>\n",
       "      <td>By integrating NVIDIA NIM with LlamaIndex, dev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How does Llama Packs aim to simplify the proce...</td>\n",
       "      <td>Llama Packs aim to simplify the process of bui...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we’re excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Can you explain the two ways in which Llama Pa...</td>\n",
       "      <td>Llama Packs can be described and utilized by u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing Llama Packs\\nToday we’re excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Explain the process of downloading and initial...</td>\n",
       "      <td>To download and initialize a Llama Pack, you c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Voyage AI Pack. Every Pack has a detailed REA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How can a user customize a Llama Pack, such as...</td>\n",
       "      <td>To customize a Llama Pack, such as swapping ou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Let’s take a look at the downloaded pack in  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What are some of the key feature releases and ...</td>\n",
       "      <td>The LlamaIndex Newsletter highlights several k...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Register for free! ✨ Feature Releases and Enh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Describe the MechGPT project by Professor Mark...</td>\n",
       "      <td>Professor Markus J. Buehler's project, \"Using ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This optimizes for reduced latency and costs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What are some of the tutorials covered in the ...</td>\n",
       "      <td>Some of the tutorials covered in the LlamaInde...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Mervin Praison’s   tutorial  on using llama-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Who are the speakers featured in the webinars ...</td>\n",
       "      <td>Wenqi Glantz, Omar Khattab, and Thomas Joshi a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We now accommodate custom models that align w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the new feature released by LlamaInde...   \n",
       "1   Discuss the advancements made by LlamaIndex in...   \n",
       "2   Explain the three main sections of the OpenAI ...   \n",
       "3   How does the OpenAI Cookbook suggest evaluatin...   \n",
       "4   How has LlamaIndex evolved over the past year ...   \n",
       "5   Can you explain the significance of the Retrie...   \n",
       "6   How does the partnership with Google Gemini be...   \n",
       "7   Describe the Multi-Doc SEC 10Q Dataset launche...   \n",
       "8   How does the MemoryCache project by Mozilla ut...   \n",
       "9   Discuss the significance of integrating Na2SQL...   \n",
       "10  How does LlamaCloud help in reducing productio...   \n",
       "11  What new features have been introduced in Llam...   \n",
       "12  How does LlamaIndex help in building LLM apps ...   \n",
       "13  Can you explain the process of wrapping a Llam...   \n",
       "14  How does the feedback function in TruLlama hel...   \n",
       "15  Can you explain the significance of tracking a...   \n",
       "16  Explain the basic structure of LlamaIndex's Ag...   \n",
       "17  How does the meta-agent or top-level agent in ...   \n",
       "18  Explain the concept of Agentic RAG as describe...   \n",
       "19  How does the implementation by LlamaIndex demo...   \n",
       "20  How does the integration of NVIDIA NIM with Ll...   \n",
       "21  What benefits do developers gain from using NV...   \n",
       "22  How does Llama Packs aim to simplify the proce...   \n",
       "23  Can you explain the two ways in which Llama Pa...   \n",
       "24  Explain the process of downloading and initial...   \n",
       "25  How can a user customize a Llama Pack, such as...   \n",
       "26  What are some of the key feature releases and ...   \n",
       "27  Describe the MechGPT project by Professor Mark...   \n",
       "28  What are some of the tutorials covered in the ...   \n",
       "29  Who are the speakers featured in the webinars ...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   The new feature, llama-index-networks, enables...              1.0   \n",
       "1   LlamaIndex has made significant advancements i...              1.0   \n",
       "2   The OpenAI Cookbook for evaluating RAG systems...              1.0   \n",
       "3   The OpenAI Cookbook suggests evaluating the pe...              1.0   \n",
       "4   Over the past year, LlamaIndex has experienced...              1.0   \n",
       "5   RAG technology plays a crucial role in the dev...              1.0   \n",
       "6   The partnership with Google Gemini benefits Ll...              1.0   \n",
       "7   The Multi-Doc SEC 10Q Dataset, launched by Taq...              1.0   \n",
       "8   The MemoryCache project by Mozilla utilizes Pr...              0.0   \n",
       "9   The integration of Na2SQL with Llama Index is ...              1.0   \n",
       "10  According to the latest updates, LlamaCloud he...              1.0   \n",
       "11  LlamaCloud has introduced several new features...              1.0   \n",
       "12  LlamaIndex is a popular open-source framework ...              1.0   \n",
       "13  You can wrap a LlamaIndex app with TruLens by ...              1.0   \n",
       "14  The feedback function in TruLlama helps improv...              1.0   \n",
       "15  Tracking and evaluating app versions using Tru...              1.0   \n",
       "16  The basic structure of LlamaIndex's Agentic RA...              1.0   \n",
       "17  The meta-agent or top-level agent in the Agent...              1.0   \n",
       "18  Agentic RAG is a concept that integrates agent...              1.0   \n",
       "19  The implementation by LlamaIndex demonstrates ...              1.0   \n",
       "20  The integration of NVIDIA NIM with LlamaIndex ...              1.0   \n",
       "21  By integrating NVIDIA NIM with LlamaIndex, dev...              1.0   \n",
       "22  Llama Packs aim to simplify the process of bui...              1.0   \n",
       "23  Llama Packs can be described and utilized by u...              1.0   \n",
       "24  To download and initialize a Llama Pack, you c...              1.0   \n",
       "25  To customize a Llama Pack, such as swapping ou...              1.0   \n",
       "26  The LlamaIndex Newsletter highlights several k...              1.0   \n",
       "27  Professor Markus J. Buehler's project, \"Using ...              1.0   \n",
       "28  Some of the tutorials covered in the LlamaInde...              1.0   \n",
       "29  Wenqi Glantz, Omar Khattab, and Thomas Joshi a...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 4.0                 1.0   \n",
       "1                 3.5                 1.0   \n",
       "2                 NaN                 1.0   \n",
       "3                 4.5                 1.0   \n",
       "4                 NaN                 1.0   \n",
       "5                 4.0                 1.0   \n",
       "6                 5.0                 1.0   \n",
       "7                 4.5                 1.0   \n",
       "8                 NaN                 1.0   \n",
       "9                 4.5                 1.0   \n",
       "10                NaN                 1.0   \n",
       "11                4.5                 1.0   \n",
       "12                4.0                 1.0   \n",
       "13                4.5                 1.0   \n",
       "14                4.5                 1.0   \n",
       "15                NaN                 1.0   \n",
       "16                4.5                 1.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.0                 1.0   \n",
       "19                4.0                 1.0   \n",
       "20                4.5                 1.0   \n",
       "21                4.5                 1.0   \n",
       "22                4.5                 1.0   \n",
       "23                4.5                 1.0   \n",
       "24                4.5                 1.0   \n",
       "25                NaN                 1.0   \n",
       "26                4.0                 1.0   \n",
       "27                4.0                 1.0   \n",
       "28                4.0                 1.0   \n",
       "29                2.0                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [Querying a network of knowledge with llama-in...  \n",
       "1   [Launching the first GenAI-native document par...  \n",
       "2   [OpenAI Cookbook: Evaluating RAG systems\\nWe’r...  \n",
       "3   [OpenAI Cookbook: Evaluating RAG systems\\nWe’r...  \n",
       "4   [LlamaIndex turns 1!\\nIt’s our birthday! One y...  \n",
       "5   [MultiModal RAG for Advanced Video Processing ...  \n",
       "6   [LlamaIndex + Gemini\\n(co-authored by Jerry Li...  \n",
       "7   [Tweet . We introduced day-0 integrations with...  \n",
       "8   [Retrieving Privacy-Safe Documents Over A Netw...  \n",
       "9   [Its integration ensures a smooth transition f...  \n",
       "10  [Doing that stuff with LlamaCloud and LlamaPar...  \n",
       "11  [This feature enables transparency, re-use, an...  \n",
       "12  [Build and Evaluate LLM Apps with LlamaIndex a...  \n",
       "13  [Wrap A LlamaIndex App with TruLens With TruLe...  \n",
       "14  [Wrap A LlamaIndex App with TruLens With TruLe...  \n",
       "15  [min )\\n\\n\\nfeedbacks = [f_lang_match, f_qa_re...  \n",
       "16  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "17  [Reranking involves using a semantic search mo...  \n",
       "18  [Agentic RAG With LlamaIndex\\nThe topic of Age...  \n",
       "19  [Introducing llama-agents: A Powerful Framewor...  \n",
       "20  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "21  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "22  [Introducing Llama Packs\\nToday we’re excited ...  \n",
       "23  [Introducing Llama Packs\\nToday we’re excited ...  \n",
       "24  [Voyage AI Pack. Every Pack has a detailed REA...  \n",
       "25  [Let’s take a look at the downloaded pack in  ...  \n",
       "26  [Register for free! ✨ Feature Releases and Enh...  \n",
       "27  [This optimizes for reduced latency and costs,...  \n",
       "28  [Mervin Praison’s   tutorial  on using llama-a...  \n",
       "29  [We now accommodate custom models that align w...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a143069-ea8a-4474-ba4a-c9f9d24dab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in synthetic_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"synthetic_response_eval__{k}\", v)\n",
    "    synthetic_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/synthetic_deep_eval_df.html\", \"synthetic_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0abbb7-0382-4bc6-b1be-1ac4f6245dc9",
   "metadata": {},
   "source": [
    "### Manually curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e9113fa-4915-4c59-9039-d461b98c1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset, LabelledRagDataExample, CreatedBy, CreatedByType\n",
    "\n",
    "examples = []\n",
    "\n",
    "for question, expected_anwser in MANUAL_EVAL_QA:\n",
    "    example = LabelledRagDataExample(\n",
    "        query=question,\n",
    "        query_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_answer=expected_anwser,\n",
    "        reference_answer_by=CreatedBy(type=CreatedByType.HUMAN),\n",
    "        reference_contexts=[],\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "curated_response_eval_dataset = LabelledRagDataset(examples=examples)\n",
    "\n",
    "# save this dataset as it is required for the submission\n",
    "curated_response_eval_dataset.save_json(f\"{NOTEBOOK_CACHE_DP}/curated_response_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2876bc3c-251b-4686-9321-6a7a0c081019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions:   0%|                                                                                                                                                                                                               | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Top 2 nodes:\n",
      "> [Node cd0c5df2-0cd7-4ca9-802e-1d1d706bf164] [Similarity score:             0.822134] OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re excited to unveil our  OpenAI Cookbook , a guide to...\n",
      "> [Node 597f4385-0f4e-4bdf-b17c-9deb5b274d9e] [Similarity score:             0.793708] chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\n",
      "\n",
      " for  chunk_size  in  chunk_sizes:\n",
      "  avg_res...\n",
      "> Top 2 nodes:\n",
      "> [Node 2c85936f-3734-4425-8d17-12fe2a509549] [Similarity score:             0.739013] bge-large : Experiences significant improvement with rerankers, with the best results from  Coher...\n",
      "> [Node dc28df72-ed41-41f5-b9d6-05a284d8a2bc] [Similarity score:             0.736419] Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Ji...\n",
      "> Top 2 nodes:\n",
      "> [Node 57707274-0eb2-4175-b98d-e3fb5034ad42] [Similarity score:             0.791887] Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "We'...\n",
      "> [Node dbeeb5bf-783c-4124-b8c0-7a4d19e93ea4] [Similarity score:             0.775856] import  dotenv\n",
      "dotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \n",
      " from  llama_index.core  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "curated_response_eval_prediction_dataset = await curated_response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=8, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ac08e37-3c76-4318-988f-1d2a4acf9223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbe9647b2a1410eb71d46e73bde1eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: import  dotenv\n",
      "dotenv.load_dotenv()  # our .env...\n",
      "> Adding chunk: Introducing llama-agents: A Powerful Framework ...\n",
      "> Adding chunk: import  dotenv\n",
      "dotenv.load_dotenv()  # our .env...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n",
      "> Adding chunk: OpenAI Cookbook: Evaluating RAG systems\n",
      "We’re e...\n",
      "> Adding chunk: chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  20...\n",
      "> Adding chunk: bge-large : Experiences significant improvement...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n",
      "> Adding chunk: bge-large : Experiences significant improvement...\n",
      "> Adding chunk: Boosting RAG: Picking the Best Embedding & Rera...\n"
     ]
    }
   ],
   "source": [
    "curated_mean_scores_df, curated_deep_eval_df = evaluate_labelled_rag_dataset(\n",
    "    curated_response_eval_dataset,\n",
    "    curated_response_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=RESPONSE_EVAL_LLM_MODEL,\n",
    "    cache_dp=NOTEBOOK_CACHE_DP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88d2625c-2837-42cc-8c6d-250001158d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score        4.5\n",
       "mean_relevancy_score          1.0\n",
       "mean_faithfulness_score       1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eb4524e-977c-4716-8c02-0676b2d48ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>Distributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via standardized API interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\\n\\nDefine agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\\n\\nEase of deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\nScalability and resource management: Use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., import  dotenv\\ndotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \\n from  llama_index.core  import  VectorStoreIndex, Document\\n from  llama_index.core.agent  import  FnAgentWorker\\n from  llama_index.core  import  PromptTemplate\\n from  llama_index.core.query_pipeline  import  QueryPipeline\\n from  llama_index.core.query_engine  import  RetrieverQueryEngine\\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    PipelineOrchestrator,\\n    ServiceComponent,\\n)\\n from  llama_agents.launchers  import  LocalLauncher\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # change logging level to enable or disable more verbose logging \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Load and index your document \\ndocs = [Document(text= \"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\" )]\\nindex = VectorStoreIndex.from_documents(docs)\\n\\n # Define a query rewrite agent \\nHYDE_PROMPT_STR = (\\n     \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" \\n)\\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\\n\\n def   run_hyde_fn ( state ):\\n    prompt_tmpl, llm, input_str = (\\n        state[ \"prompt_tmpl\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ]. input ,\\n    )\\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\\n    output = qp.run(query_str=input_str)\\n    state[ \"__output__\" ] =  str (output)\\n     return  state,  True \\n\\nhyde_agent = FnAgentWorker(\\n    fn=run_hyde_fn,\\n    initial_state={ \"prompt_tmpl\" : HYDE_PROMPT_TMPL,  \"llm\" : OpenAI()}\\n).as_agent()\\n\\n # Define a RAG agent \\n def   run_rag_fn ( state ):\\n    retriever, llm, input_str = (\\n        state[ \"retriever\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ].]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?</td>\n",
       "      <td>The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe’re excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you’ll find it useful in enhancing the effectiveness of your RAG systems, and we’re thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham’s essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system’s performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!🦙, chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\\n\\n for  chunk_size  in  chunk_sizes:\\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\\n   print ( f\"Chunk size  {chunk_size}  - Average Response time:  {avg_response_time: .2 f} s, Average Faithfulness:  {avg_faithfulness: .2 f} , Average Relevancy:  {avg_relevancy: .2 f} \" ) Bringing It All Together Let’s compile the processes: import  nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n from  llama_index  import  (\\n    SimpleDirectoryReader,\\n    VectorStoreIndex,\\n    ServiceContext,\\n)\\n from  llama_index.evaluation  import  (\\n    DatasetGenerator,\\n    FaithfulnessEvaluator,\\n    RelevancyEvaluator\\n)\\n from  llama_index.llms  import  OpenAI\\n\\n import  openai\\n import  time\\n\\nopenai.api_key =  'OPENAI-API-KEY' \\n\\n # Download Data \\n!mkdir -p  'data/10k/' \\n!wget  'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'  -O  'data/10k/uber_2021.pdf' \\n\\n # Load Data \\nreader = SimpleDirectoryReader( \"./data/10k/\" )\\ndocuments = reader.load_data()\\n\\n # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages. \\neval_documents = documents[: 20 ]\\ndata_generator = DatasetGenerator.from_documents()\\neval_questions = data_generator.generate_questions_from_nodes(num =  20 )\\n\\n # We will use GPT-4 for evaluating the responses \\ngpt4 = OpenAI(temperature= 0 , model= \"gpt-4\" )\\n\\n # Define service context for GPT-4 for evaluation \\nservice_context_gpt4 = ServiceContext.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?</td>\n",
       "      <td>The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere’s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results., Boosting RAG: Picking the Best Embedding &amp; Reranker models\\nUPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let’s first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 query  \\\n",
       "0                                                                                                               What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?   \n",
       "2                                         What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             answer  \\\n",
       "0  Distributed Service-Oriented Architecture: Every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\\n\\nCommunication via standardized API interfaces: Interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\\n\\nDefine agentic and explicit orchestration flows: Developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\\n\\nEase of deployment: Launch, scale, and monitor each agent and your control plane independently.\\n\\nScalability and resource management: Use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).   \n",
       "\n",
       "   relevancy_score  correctness_score  faithfulness_score  \\\n",
       "0              1.0                NaN                 1.0   \n",
       "1              1.0                4.5                 1.0   \n",
       "2              1.0                NaN                 1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               contexts  \n",
       "0                              [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\\nWe're excited to announce the alpha release of  llama-agents , a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life. Key Features of llama-agents Distributed Service Oriented Architecture:  every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks. Communication via standardized API interfaces:  interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue. Define agentic and explicit orchestration flows:  developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task. Ease of deployment:  launch, scale and monitor each agent and your control plane independently. Scalability and resource management:  use our built-in observability tools to monitor the quality and performance of the system and each individual agent service Let's dive into how you can start using llama-agents to build your own multi-agent systems. Getting Started with llama-agents First, install the framework using pip: pip install llama-agents llama-index-agent-openai Basic System Setup Here's a simple example of how to set up a basic multi-agent system using llama-agents., import  dotenv\\ndotenv.load_dotenv()  # our .env defines OPENAI_API_KEY \\n from  llama_index.core  import  VectorStoreIndex, Document\\n from  llama_index.core.agent  import  FnAgentWorker\\n from  llama_index.core  import  PromptTemplate\\n from  llama_index.core.query_pipeline  import  QueryPipeline\\n from  llama_index.core.query_engine  import  RetrieverQueryEngine\\n from  llama_agents  import  (\\n    AgentService,\\n    ControlPlaneServer,\\n    SimpleMessageQueue,\\n    PipelineOrchestrator,\\n    ServiceComponent,\\n)\\n from  llama_agents.launchers  import  LocalLauncher\\n from  llama_index.llms.openai  import  OpenAI\\n import  logging\\n\\n # change logging level to enable or disable more verbose logging \\nlogging.getLogger( \"llama_agents\" ).setLevel(logging.INFO)\\n\\n # Load and index your document \\ndocs = [Document(text= \"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\" )]\\nindex = VectorStoreIndex.from_documents(docs)\\n\\n # Define a query rewrite agent \\nHYDE_PROMPT_STR = (\\n     \"Please rewrite the following query to include more detail:\\n{query_str}\\n\" \\n)\\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\\n\\n def   run_hyde_fn ( state ):\\n    prompt_tmpl, llm, input_str = (\\n        state[ \"prompt_tmpl\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ]. input ,\\n    )\\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\\n    output = qp.run(query_str=input_str)\\n    state[ \"__output__\" ] =  str (output)\\n     return  state,  True \\n\\nhyde_agent = FnAgentWorker(\\n    fn=run_hyde_fn,\\n    initial_state={ \"prompt_tmpl\" : HYDE_PROMPT_TMPL,  \"llm\" : OpenAI()}\\n).as_agent()\\n\\n # Define a RAG agent \\n def   run_rag_fn ( state ):\\n    retriever, llm, input_str = (\\n        state[ \"retriever\" ],\\n        state[ \"llm\" ],\\n        state[ \"__task__\" ].]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [OpenAI Cookbook: Evaluating RAG systems\\nWe’re excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you’ll find it useful in enhancing the effectiveness of your RAG systems, and we’re thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham’s essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system’s performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!🦙, chunk_sizes = [ 128 ,  256 ,  512 ,  1024 ,  2048 ]\\n\\n for  chunk_size  in  chunk_sizes:\\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\\n   print ( f\"Chunk size  {chunk_size}  - Average Response time:  {avg_response_time: .2 f} s, Average Faithfulness:  {avg_faithfulness: .2 f} , Average Relevancy:  {avg_relevancy: .2 f} \" ) Bringing It All Together Let’s compile the processes: import  nest_asyncio\\n\\nnest_asyncio.apply()\\n\\n from  llama_index  import  (\\n    SimpleDirectoryReader,\\n    VectorStoreIndex,\\n    ServiceContext,\\n)\\n from  llama_index.evaluation  import  (\\n    DatasetGenerator,\\n    FaithfulnessEvaluator,\\n    RelevancyEvaluator\\n)\\n from  llama_index.llms  import  OpenAI\\n\\n import  openai\\n import  time\\n\\nopenai.api_key =  'OPENAI-API-KEY' \\n\\n # Download Data \\n!mkdir -p  'data/10k/' \\n!wget  'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'  -O  'data/10k/uber_2021.pdf' \\n\\n # Load Data \\nreader = SimpleDirectoryReader( \"./data/10k/\" )\\ndocuments = reader.load_data()\\n\\n # To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages. \\neval_documents = documents[: 20 ]\\ndata_generator = DatasetGenerator.from_documents()\\neval_questions = data_generator.generate_questions_from_nodes(num =  20 )\\n\\n # We will use GPT-4 for evaluating the responses \\ngpt4 = OpenAI(temperature= 0 , model= \"gpt-4\" )\\n\\n # Define service context for GPT-4 for evaluation \\nservice_context_gpt4 = ServiceContext.]  \n",
       "2  [bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere’s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results., Boosting RAG: Picking the Best Embedding & Reranker models\\nUPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let’s first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(curated_deep_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "113a264d-1cea-4e41-82ac-22135f12f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-large : Experiences significant improvement with rerankers, with the best results from  CohereRerank  (0.876404 hit rate, 0.822753 MRR). llm-embedder : Benefits greatly from reranking, particularly with  CohereRerank  (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost. Cohere : Cohere’s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR. Voyage : Has strong initial performance that is further amplified by  CohereRerank  (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking. JinaAI : Very strong performance, sees notable gains with  bge-reranker-large  (0.938202 hit rate, 0.868539 MRR) and  CohereRerank  (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance. Google-PaLM : The model demonstrates strong performance, with measurable gains when using the  CohereRerank (0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results. Impact of Rerankers : WithoutReranker : This provides the baseline performance for each embedding. bge-reranker-base : Generally improves both hit rate and MRR across embeddings. bge-reranker-large : This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the  CohereRerank . CohereRerank : Consistently enhances performance across all embeddings, often providing the best or near-best results. Necessity of Rerankers : The data clearly indicates the significance of rerankers in refining search results.\n",
      "----------\n",
      "Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "UPDATE : The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the  JinaAI-v2-base-en  with  bge-reranker-large now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with CohereRerank  exhibits a Hit Rate of 0.932584, and an MRR of 0.873689. When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers. But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most? In this blog post, we’ll use the  Retrieval Evaluation  module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in! Let’s first start with understanding the metrics available in  Retrieval Evaluation Understanding Metrics in Retrieval Evaluation: To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics:  Hit Rate  and  Mean Reciprocal Rank (MRR) . Let’s delve into these metrics to understand their significance and how they operate. Hit Rate: Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses. Mean Reciprocal Rank (MRR): For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for context in curated_deep_eval_df.iloc[2]['contexts']:\n",
    "    print(context)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "024c20ec-bf28-43bf-912b-d02c5aed9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    for k, v in curated_mean_scores_df.T.to_dict(orient='records')[0].items():\n",
    "        mlflow.log_metric(f\"curated_response_eval__{k}\", v)\n",
    "    curated_deep_eval_df.to_html(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\")\n",
    "    mlflow.log_artifact(f\"{NOTEBOOK_CACHE_DP}/curated_deep_eval_df.html\", \"curated_deep_eval_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247e7ca-f7d1-4620-96de-7bc9977fb0a2",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47f12fc1-4062-4829-9236-82c27df86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7959512-c62a-4e08-a5e7-3e7681b2096f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f261db95-ff04-4372-8e5e-d261b8b0f9a3",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d69798-d4bb-4461-88a2-f07d7c0a6099",
   "metadata": {},
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1338b451-66d8-4b1c-9786-85e046683d60",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3243b624-20ee-4b0c-9e84-1f99a814e995",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "response_eval_prediction_dataset = await response_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=BATCH_SIZE, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa48d537-5cad-47ed-88a3-392c435c3e9e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e7eb720-48a1-45f7-aee5-bcb315c4d6fd",
   "metadata": {},
   "source": [
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/downloading_llama_datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbfee4dc-d2d7-4c04-ac61-273279da59c8",
   "metadata": {},
   "source": [
    "judge_model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a37c7-e9cc-48ba-8158-4cd28f57b07a",
   "metadata": {},
   "source": [
    "# instantiate the judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "# Correctness outputs a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score. Passing is defined as a score greater than or equal to the given threshold.\n",
    "# Ref: https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    llm=OpenAI(temperature=0, model=judge_model),\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87659d89-1cad-4bfc-bccf-f4b2f73bb076",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm(\n",
    "    zip(response_eval_dataset.examples, response_eval_prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = judges[\"correctness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1203338f-c3e4-4862-ac57-4c248a138db2",
   "metadata": {},
   "source": [
    "#### Persist evaluation results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61fd4039-3ac2-4dd0-b0b9-0bc5c4573793",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# saving evaluations\n",
    "evaluations_objects = {\n",
    "    \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
    "    \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
    "    \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
    "}\n",
    "\n",
    "with open(f\"{notebook_cache_dp}/evaluations.json\", \"w\") as json_file:\n",
    "    json.dump(evaluations_objects, json_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a46e4889-7fe2-4220-9191-ae95cb4a7318",
   "metadata": {},
   "source": [
    "#### View eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47150615-931e-4190-8a33-3620d34c5d71",
   "metadata": {},
   "source": [
    "##### Overall results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3d73ab-1245-45cf-87b2-856da742e463",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "deep_eval_correctness_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_relevancy_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "deep_eval_faithfulness_df, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760ad247-1d13-4c30-95b7-2ab8a2c7cd19",
   "metadata": {},
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d617bacd-c71f-45bb-be9d-fff0c4d28fca",
   "metadata": {},
   "source": [
    "##### By questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea2cd128-6cd6-4b60-a219-69fa4af80e0d",
   "metadata": {},
   "source": [
    "deep_eval_df = pd.concat([\n",
    "    deep_eval_correctness_df[['query', 'answer']],\n",
    "    deep_eval_relevancy_df[['scores']].rename(columns={'scores': 'relevancy_score'}),\n",
    "    deep_eval_correctness_df[['scores']].rename(columns={'scores': 'correctness_score'}),\n",
    "    deep_eval_faithfulness_df[['scores']].rename(columns={'scores': 'faithfulness_score'}),\n",
    "], axis=1)\n",
    "\n",
    "(\n",
    "    deep_eval_df\n",
    ")\n",
    "\n",
    "# (\n",
    "#     deep_eval_df\n",
    "#     .style\n",
    "#     .background_gradient(subset=[col for col in deep_eval_df.columns if col.endswith('score')])\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "050df1f5-b4fc-48ce-b45b-322be1c234b5",
   "metadata": {},
   "source": [
    "## Manually curated dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
