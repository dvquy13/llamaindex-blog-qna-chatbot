{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89230e9e-b474-4df0-ab41-28148a9dc268",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75779aea-2a28-44ca-ba89-71da05db6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c9c3a4-d10d-4557-a46d-16dcb25012a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189562de-c122-403d-a609-6b2d846e6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eedc25-e389-446e-a724-6cb1e65986b2",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbd3b0c-e15d-4bf4-a940-bd45aeb3acc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"EXPERIMENT_NAME\": \"Chain Frost - LlamaIndex Blog QnA Chatbot\",\n",
       "  \"RUN_NAME\": \"010_remove_title_extractor\",\n",
       "  \"RUN_DESCRIPTION\": \"\\n# Refactor\\n\\n## Changelog\\n### Compares to exp_009\\n- Remove Title Extractor\\n\",\n",
       "  \"TESTING\": false,\n",
       "  \"DEBUG\": false,\n",
       "  \"OBSERVABILITY\": true,\n",
       "  \"LOG_TO_MLFLOW\": true,\n",
       "  \"RECREATE_INDEX\": true,\n",
       "  \"RECREATE_RETRIEVAL_EVAL_DATASET\": true,\n",
       "  \"RECREATE_RESPONSE_EVAL_DATASET\": true\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.run.args import RunInputArgs\n",
    "\n",
    "ARGS = RunInputArgs(\n",
    "    EXPERIMENT_NAME=\"Chain Frost - LlamaIndex Blog QnA Chatbot\",\n",
    "    RUN_NAME=\"010_remove_title_extractor\",\n",
    "    RUN_DESCRIPTION=\"\"\"\n",
    "# Refactor\n",
    "\n",
    "## Changelog\n",
    "### Compares to exp_009\n",
    "- Remove Title Extractor\n",
    "\"\"\",\n",
    "    TESTING=False,\n",
    "    DEBUG=False,\n",
    "    LOG_TO_MLFLOW=True,\n",
    "    OBSERVABILITY=True,\n",
    "    RECREATE_INDEX=True,\n",
    "    RECREATE_RETRIEVAL_EVAL_DATASET=True,\n",
    "    RECREATE_RESPONSE_EVAL_DATASET=True,\n",
    ")\n",
    "\n",
    "ARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9f19b-c4c7-4361-931b-6687b3fd290e",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970c1f96-5dab-4d62-83e4-8eb26a23467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run.cfg import RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1fc9827-6cd8-4cdb-943b-4fd7c0c71b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:19.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.cfg\u001b[0m:\u001b[36minit\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mStarting Observability server with Phoenix...\u001b[0m\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721980404.082361 1358983 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:24.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.cfg\u001b[0m:\u001b[36minit\u001b[0m:\u001b[36m111\u001b[0m - \u001b[1mSetting up MLflow experiment Chain Frost - LlamaIndex Blog QnA Chatbot - run 010_remove_title_extractor...\u001b[0m\n",
      "\u001b[32m2024-07-26 14:53:25.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.cfg\u001b[0m:\u001b[36minit\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNotebook-generated artifacts are persisted at data/010_remove_title_extractor\u001b[0m\n",
      "\u001b[32m2024-07-26 14:53:25.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.cfg\u001b[0m:\u001b[36minit\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mARGS.RECREATE_INDEX=True -> Overwriting db_collection and nodes_persist_fp...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cfg = RunConfig()\n",
    "cfg.init(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8303a2c6-700e-4975-801e-1adad8c22400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/pydantic/main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"args\": {\n",
       "    \"EXPERIMENT_NAME\": \"Chain Frost - LlamaIndex Blog QnA Chatbot\",\n",
       "    \"RUN_NAME\": \"010_remove_title_extractor\",\n",
       "    \"RUN_DESCRIPTION\": \"\\n# Refactor\\n\\n## Changelog\\n### Compares to exp_009\\n- Remove Title Extractor\\n\",\n",
       "    \"TESTING\": false,\n",
       "    \"DEBUG\": false,\n",
       "    \"OBSERVABILITY\": true,\n",
       "    \"LOG_TO_MLFLOW\": true,\n",
       "    \"RECREATE_INDEX\": true,\n",
       "    \"RECREATE_RETRIEVAL_EVAL_DATASET\": true,\n",
       "    \"RECREATE_RESPONSE_EVAL_DATASET\": true\n",
       "  },\n",
       "  \"db_collection\": \"huggingface__BAAI_bge_large_en_v1_5__010_remove_title_extractor\",\n",
       "  \"nodes_persist_fp\": \"data/010_remove_title_extractor/nodes.pkl\",\n",
       "  \"notebook_cache_dp\": \"data/010_remove_title_extractor\",\n",
       "  \"data_fp\": \"../crawl_llamaindex_blog/data/blogs-v2.json\",\n",
       "  \"llm_cfg\": {\n",
       "    \"llm_provider\": \"togetherai\",\n",
       "    \"llm_model_name\": \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\",\n",
       "    \"embedding_provider\": \"huggingface\",\n",
       "    \"embedding_model_name\": \"BAAI/bge-large-en-v1.5\",\n",
       "    \"embedding_model_dim\": null,\n",
       "    \"ollama__host\": \"192.168.100.14\",\n",
       "    \"ollama__port\": 11434\n",
       "  },\n",
       "  \"retrieval_cfg\": {\n",
       "    \"retrieval_top_k\": 5,\n",
       "    \"retrieval_similarity_cutoff\": null,\n",
       "    \"rerank_top_k\": 2,\n",
       "    \"rerank_model_name\": \"BAAI/bge-reranker-large\"\n",
       "  },\n",
       "  \"eval_cfg\": {\n",
       "    \"retrieval_num_sample_nodes\": 10,\n",
       "    \"retrieval_eval_llm_model\": \"gpt-3.5-turbo\",\n",
       "    \"retrieval_eval_llm_model_config\": {\n",
       "      \"temperature\": 0.3\n",
       "    },\n",
       "    \"retrieval_num_questions_per_chunk\": 2,\n",
       "    \"retrieval_metrics\": [\n",
       "      \"hit_rate\",\n",
       "      \"mrr\",\n",
       "      \"precision\",\n",
       "      \"recall\",\n",
       "      \"ap\",\n",
       "      \"ndcg\"\n",
       "    ],\n",
       "    \"retrieval_eval_dataset_fp\": \"data/001/exp_006_semantic_chunking/llamaindex_blog_retrieval_eval_dataset.json\",\n",
       "    \"question_gen_query\": \"\\nYou are a Retriever Evaluator. Your task is to generate {num_questions_per_chunk} questions to assess the accuracy/relevancy of an information retrieval system.\\nThe information retrieval system would then be asked your generated question and assessed on how well it can look up and return the correct context.\\n\\nIMPORTANT RULES:\\n- Restrict the generated questions to the context information provided.\\n- Do not mention anything about the context in the generated questions.\\n- The generated questions should be diverse in nature and in difficulty across the documents.\\n- When being asked the generated question, a human with no prior knowledge can still answer perfectly given the input context.\\n\",\n",
       "    \"response_synthetic_eval_dataset_fp\": \"data/001/exp_007_semantic_chunking_full_refresh/response_synthetic_eval_dataset.json\",\n",
       "    \"response_curated_eval_dataset_fp\": \"data/001/exp_007_semantic_chunking_full_refresh/response_curated_eval_dataset.json\",\n",
       "    \"response_eval_llm_model\": \"gpt-3.5-turbo\",\n",
       "    \"response_eval_llm_model_config\": {\n",
       "      \"temperature\": 0.3\n",
       "    },\n",
       "    \"response_synthetic_num_questions_per_chunk\": 1,\n",
       "    \"response_num_sample_documents\": 10\n",
       "  },\n",
       "  \"batch_size\": 16\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc1c0b-fe37-4555-9afd-b1d0fa4afe32",
   "metadata": {},
   "source": [
    "## Set up logger to collect additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e23fe20-30a7-4fbb-8880-1c95211e4520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_fp = f\"{cfg.notebook_cache_dp}/collect.log\"\n",
    "logger.add(collect_fp, filter=lambda record: \"[COLLECT]\" in record['message'], mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a894e-69af-471a-958a-b3409b62a0d7",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d97dc3a-5530-4859-b683-985f5b990668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:25.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m[COLLECT] len(data)=160\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       " 'content': \"This is a guest post from Uptrain. We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‚Äòvibes‚Äô. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance. About UpTrain UpTrain  [ github  ||  website  ||  docs ] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them. Key Highlights: Data Security:  As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls). Custom Evaluator LLMs:  UpTrain allows for  customisation of your evaluator LLM , offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure. Insights that help with model improvement:  Beyond mere evaluation, UpTrain performs  root cause analysis  to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution. Diverse Experimentations:  The platform enables  experimentation  with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case. Compare open-source LLMs:  With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality. In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what‚Äôs affecting the quality of your responses, allowing you to take appropriate corrective actions. LlamaIndex x UpTrain Callback Handler We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps. If you wish to skip right ahead to the tutorial, check it out  here. Evals across the board: From Vanilla to Advanced RAG Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response. While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses. But as Uncle Ben famously said to Peter Parker in the GenAI universe: ‚ÄúWith increased complexity comes more points of failure.‚Äù. Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let‚Äôs look at all the evaluations provided by UpTrain. Addressing Points of Failure in RAG Pipelines 1. RAG Query Engine Evaluation Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response. Context Relevance : However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query. Factual Accuracy : Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context. Response Completeness : Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query. 2. Sub-Question Query Engine Evaluation Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine. If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs: Sub Query Completeness : It evaluates whether the sub-questions accurately and comprehensively cover the original query. Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries. 3. Reranking Evaluations We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores. RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [ Lost in the Middle: How Language Model Uses Long Contexts ] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block. Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity. We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process: a. Same Number of Nodes Before and After Reranking: If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator. Context Reranking : Checks if the order of reranked nodes is more relevant to the query than the original order. b. Fewer Number of Nodes After Reranking: Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator. Context Conciseness : Examines whether the reduced number of nodes still provides all the required information. Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance. This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions. Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this  quickstart tutorial . References UpTrain Callback Handler Tutorial UpTrain GitHub Repository Advanced RAG Techniques: an Illustrated Overview Lost in the Middle: How Language Models Use Long Contexts UpTrainCallbackHandler documentation UpTrain Website\",\n",
       " 'author': 'Uptrain',\n",
       " 'date': 'Mar 19, 2024',\n",
       " 'tags': ['AI', 'Evaluation', 'Rag'],\n",
       " 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(cfg.data_fp, 'r') as f:\n",
    "    data = json.load(f)\n",
    "logger.info(f\"[COLLECT] {len(data)=}\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836c3bd6-c53d-4c64-9395-5eb0a275bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:25.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m[COLLECT] len(input_data)=160\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data = data\n",
    "if ARGS.TESTING:\n",
    "    input_data = data[:2]\n",
    "logger.info(f\"[COLLECT] {len(input_data)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4869a-4763-428d-ba54-a664f07584be",
   "metadata": {},
   "source": [
    "# Prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8818fe2-7a92-4859-9945-b682a635b8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:25.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1m[COLLECT] len(documents)=160\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "for record in input_data:\n",
    "    title = record['title']\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'author': record['author'],\n",
    "        'date': record['date'],\n",
    "        'tags': ', '.join(record['tags']),\n",
    "        'url': record['url']\n",
    "    }\n",
    "    text = f\"{title}\\n{record['content']}\"\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "logger.info(f\"[COLLECT] {len(documents)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546c766-2af2-48b7-962a-963f05e44a12",
   "metadata": {},
   "source": [
    "# Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40574e9e-422d-4134-bdc4-0d1f65cc38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, embed_model = cfg.setup_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c256c8-ec1c-4481-9686-2358599d2d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"llm_provider\": \"togetherai\",\n",
      "  \"llm_model_name\": \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\",\n",
      "  \"embedding_provider\": \"huggingface\",\n",
      "  \"embedding_model_name\": \"BAAI/bge-large-en-v1.5\",\n",
      "  \"embedding_model_dim\": 1024,\n",
      "  \"ollama__host\": \"192.168.100.14\",\n",
      "  \"ollama__port\": 11434\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(cfg.llm_cfg.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34c861a-6512-4565-9b52-b068e6456d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1118a8-6632-4c17-8255-a0999c9e446c",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bdbf000-740b-4a90-bf9f-8fc7ee1f82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from src.run.orchestrator import RunOrchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f70b18c0-3644-4af9-87d1-2fe03fe51a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:33.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.orchestrator\u001b[0m:\u001b[36msetup_db\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCreating new Qdrant collection huggingface__BAAI_bge_large_en_v1_5__010_remove_title_extractor...\u001b[0m\n",
      "WARNI [llama_index.vector_stores.qdrant.base] Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "qdrantdb = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "aqdrantdb = qdrant_client.AsyncQdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "\n",
    "RunOrchestrator.setup_db(cfg, qdrantdb)\n",
    "\n",
    "db_collection = qdrantdb.get_collection(cfg.db_collection)\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrantdb,\n",
    "    collection_name=cfg.db_collection,\n",
    "    aclient=aqdrantdb,\n",
    "    prefer_grpc=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187f814-b85a-4e86-ad93-a8433869bdaf",
   "metadata": {},
   "source": [
    "# Index Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f9b92d8-fdd5-461e-92b9-186e9a9da6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f16836f9-4e51-44ce-a145-e86bd56a2b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = SemanticSplitterNodeParser\n",
    "chunker_cfg = {\n",
    "    \"buffer_size\": 1,\n",
    "    \"breakpoint_percentile_threshold\": 95,\n",
    "    \"embed_model\": embed_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dce6ba28-1c1b-4ca0-818e-07b5bd3b58f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:53:34.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mCreating new DB index...\u001b[0m\n",
      "\u001b[32m2024-07-26 14:53:34.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mSplitting documents into nodes...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "# TODO: TO understand the differences between points_count and indexed_vector_counts.\n",
    "# Here indexed_vector_counts = 0\n",
    "db_collection_count = db_collection.points_count\n",
    "\n",
    "if db_collection_count > 0 and ARGS.RECREATE_INDEX == False:\n",
    "    logger.info(f\"Loading index from existing DB...\")\n",
    "    with open(cfg.nodes_persist_fp, 'rb') as f:\n",
    "        logger.info(f\"Loading cached `nodes` at {cfg.nodes_persist_fp}...\")\n",
    "        nodes = pickle.load(f)\n",
    "else:\n",
    "    logger.info(f\"Creating new DB index...\")\n",
    "    from llama_index.core.extractors import TitleExtractor\n",
    "    from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "    \n",
    "    # create the pipeline with transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            chunker(**chunker_cfg),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store = vector_store\n",
    "    )\n",
    "\n",
    "    num_workers = None\n",
    "    # Currently setting num_workers leads to error `AttributeError: 'HuggingFaceEmbedding' object has no attribute '_model'`\n",
    "    # num_workers = os.cpu_count() - 1\n",
    "    # logger.info(f\"Running Ingestion Pipeline with {num_workers=}...\")\n",
    "    logger.info(f\"Splitting documents into nodes...\")\n",
    "    nodes = await pipeline.arun(documents=documents, num_workers=num_workers)\n",
    "    with open(cfg.nodes_persist_fp, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "t1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7488b8d2-982f-4d57-8bcb-9013c8644df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:59:15.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mIndexing 160 into VectorStoreIndex took 341s\u001b[0m\n",
      "\u001b[32m2024-07-26 14:59:15.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1m[COLLECT] len(nodes)=690\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Indexing {len(documents)} into VectorStoreIndex took {t1 - t0:,.0f}s\")\n",
    "logger.info(f\"[COLLECT] {len(nodes)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5ed6d-6d65-45eb-9f41-7252da7df351",
   "metadata": {},
   "source": [
    "# Construct Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c0c3b4-a5c3-4a36-8d1c-0b1998fef0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "from src.features.append_reference.custom_query_engine import ManualAppendReferenceQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e890e725-0c17-4a76-b91a-b5f83b22f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"retrieval_top_k\": 5,\n",
      "  \"retrieval_similarity_cutoff\": null,\n",
      "  \"rerank_top_k\": 2,\n",
      "  \"rerank_model_name\": \"BAAI/bge-reranker-large\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(cfg.retrieval_cfg.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2dee04a-6a7d-43b3-96df-37c6327c082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=cfg.retrieval_cfg.retrieval_top_k,\n",
    ")\n",
    "\n",
    "node_postprocessors = []\n",
    "\n",
    "if cfg.retrieval_cfg.retrieval_similarity_cutoff is not None:\n",
    "    node_postprocessors.append(SimilarityPostprocessor(similarity_cutoff=cfg.retrieval_cfg.retrieval_similarity_cutoff))\n",
    "\n",
    "reranker = FlagEmbeddingReranker(model=cfg.retrieval_cfg.rerank_model_name, top_n=cfg.retrieval_cfg.rerank_top_k)\n",
    "node_postprocessors.append(reranker)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_engine = ManualAppendReferenceQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1d702-7255-4aaa-8a85-c7da2ffa5db6",
   "metadata": {},
   "source": [
    "## Test Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9726cfe-cac2-462e-b311-0b6b59ecc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c8eb815-f87f-41e6-a29b-7f27f692ec0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To address points of failure in RAG pipeline, one approach is to split the query into smaller sub-questions that the LLM can answer more easily. This can be achieved by using the SubQuestionQueryGeneration operator provided by LlamaIndex, which decomposes a question into sub-questions and generates responses for each using an RAG query engine.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations](https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations)\n",
       "- [Introducing Query Pipelines](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 928a67eb-8b05-46f0-a545-5bceba0e0a5d<br>**Similarity:** 2.259108781814575<br>**Text:** Some of them have multiple parts to them. A good response should be able to answer all the aspect...<br>**Metadata:** {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations', 'author': 'Uptrain', 'date': 'Mar 19, 2024', 'tags': 'AI, Evaluation, Rag', 'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/2`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** ec9266a0-c40d-4195-9af7-45ef0a896738<br>**Similarity:** -4.099650859832764<br>**Text:** Check out our comprehensive  introduction guide , as well as our  docs page  for more details. Ex...<br>**Metadata:** {'title': 'Introducing Query Pipelines', 'author': 'Jerry Liu', 'date': 'Jan 8, 2024', 'tags': 'Llamaindex, Retrieval Augmented, LLM, AI', 'url': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'928a67eb-8b05-46f0-a545-5bceba0e0a5d': {'title': 'Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations',\n",
       "  'author': 'Uptrain',\n",
       "  'date': 'Mar 19, 2024',\n",
       "  'tags': 'AI, Evaluation, Rag',\n",
       "  'url': 'https://www.llamaindex.ai/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations'},\n",
       " 'ec9266a0-c40d-4195-9af7-45ef0a896738': {'title': 'Introducing Query Pipelines',\n",
       "  'author': 'Jerry Liu',\n",
       "  'date': 'Jan 8, 2024',\n",
       "  'tags': 'Llamaindex, Retrieval Augmented, LLM, AI',\n",
       "  'url': 'https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can we address points of failures in RAG pipeline?\"\n",
    "response = query_engine.query(question)\n",
    "display_response(response, show_source=True, show_metadata=True, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdcad5-6cd3-47e3-bd9e-6cf9f319eaca",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55eba9-a4cc-4009-8ae9-3f6c9f905541",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79e365-586c-4b7f-89fb-9a380da99236",
   "metadata": {},
   "source": [
    "### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b978af-9fa7-45a5-a99f-981b6003e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.nodes_persist_fp, 'rb') as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceba45ff-eccb-4bf5-8381-bf2866e72f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run.eval import RetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "570a1030-9104-4788-a417-1b7bc89f8ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:59:21.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.retrieval\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCreating new retrieval eval dataset at data/010_remove_title_extractor/retrieval_synthetic_eval_dataset.json...\u001b[0m\n",
      "\u001b[32m2024-07-26 14:59:21.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.retrieval\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mSampling 10 nodes for retrieval evaluation...\u001b[0m\n",
      "\u001b[32m2024-07-26 14:59:21.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.retrieval\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mCreating new synthetic retrieval eval dataset...\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.10s/it]\n",
      "\u001b[32m2024-07-26 14:59:32.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.retrieval\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mPersisting synthetic retrieval eval dataset to data/010_remove_title_extractor/retrieval_synthetic_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retrieval_evaluator = RetrievalEvaluator()\n",
    "retrieval_evaluator.generate_synthetic_dataset(cfg, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8f0228b-fd3e-4895-8996-4e8264882bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e9460ed8-0a3f-476e-938a-9a0b3ef2350c': 'How does LlamaCloud improve data handling and OCR accuracy for Scaleport AI across multiple industries?',\n",
       " '8f082127-72d3-4eb4-a971-bc37375c03b2': 'What are the key features and enhancements of LlamaCloud, including its integrations with Notion, Slack, Jira, and SharePoint?',\n",
       " '199f59db-375d-4ba9-982c-e04bfc229bff': 'How can the alignment and safety of LLMs and LMMs be evaluated?',\n",
       " 'b20bbd8c-b9f8-4f40-b73a-189af3c8b765': 'What dimensions, other than knowledge and reasoning capabilities, are important to consider when evaluating LLMs and LMMs?',\n",
       " 'c3fa0f39-45b7-430f-a2da-ce72d59c6b75': 'Who conducted a workshop at the LlamaIndex + Replit Pune Generative AI meetup?',\n",
       " '9145c1b1-20e2-447d-baed-333ac298a538': 'Which individuals were involved in the webinar on LLM Challenges in Production?',\n",
       " 'd9ed22d5-7455-4e57-8dee-cd9531140bf0': 'How does MultiOn manage the action of sending emails through the web browser?',\n",
       " 'da19fdb4-4fc2-48de-b62e-33fd69db6493': 'What is the purpose of the integration of MultiOn and LlamaIndex mentioned in the context?',\n",
       " '704c4bd9-fe66-49e3-9431-cc2d0f66117e': 'What is the range of parameters for the large language models developed mentioned in the context?',\n",
       " '82ded9bd-2894-446b-9344-c35e114dfc4d': 'How does the Prometheus model compare to GPT-4 in terms of feedback accuracy and penalties for missing facts?',\n",
       " '8d56281a-8a1e-4c12-8377-53050e593b90': 'What is the significance of incorporating user-specific information into metadata when indexing documents in a Multi-Tenancy RAG system?',\n",
       " '798f02bf-7683-4d02-8f85-5714be9c45b8': 'How does the retriever component of a Multi-Tenancy RAG system use metadata during the query phase to ensure data confidentiality and security for individual users?',\n",
       " 'c8cf3a53-970a-4345-b41e-4c4500b3042a': 'What is the significance of the alpaca as a personal mascot?',\n",
       " '9715fb68-a531-43e8-b51b-cfe2a75e1748': 'How are alpacas and llamas described in relation to each other?',\n",
       " '9e0ca779-2f20-42b7-a408-d66269fabfb2': 'How does the RetrieverEvaluator module enhance retrieval evaluations?',\n",
       " '51d77a97-21d5-4d99-a6ef-cf06011b3446': 'What is the purpose of the SemanticSimilarityEvaluator in assessing LLM/RAG outputs?',\n",
       " 'c834568c-f80d-466a-9652-f53e0c170b2e': 'How does the Advanced Retrieval LlamaPacks simplify the process of building advanced RAG systems?',\n",
       " 'f97dfb5e-96ab-4fc2-9aed-01c9be8a8e38': 'What is the purpose of the OpenAI Cookbook introduced by LlamaIndex?',\n",
       " '10c1b38e-f6e6-4683-ad51-645693ce9353': 'What is the process for selecting tools in the reasoning loop?',\n",
       " '4da5d0dc-2303-4a03-8d60-e04148687928': 'How do the OpenAI Function agent and ReAct agent differ in their functionality?'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_evaluator.retrieval_eval_dataset.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "763726b3-5b80-43e1-b6e2-594ac1ef870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_eval_results_df, retrieval_eval_results_full_df = await retrieval_evaluator.aevaluate(cfg, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf6add31-8d55-4f30-9c14-8273726f34f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_5_retrieval_eval</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.216416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             retrievers  hit_rate       mrr  precision  recall        ap  \\\n",
       "0  top_5_retrieval_eval       0.7  0.616667       0.14     0.7  0.616667   \n",
       "\n",
       "       ndcg  \n",
       "0  0.216416  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e519787f-c811-4ff7-afad-c9b36425e494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_ids</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does LlamaCloud improve data handling and ...</td>\n",
       "      <td>[dffd5179-49b0-40dc-b4b1-3e4d9db7073c]</td>\n",
       "      <td>[LlamaParse:  Outperformed existing OCR soluti...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the key features and enhancements of ...</td>\n",
       "      <td>[dffd5179-49b0-40dc-b4b1-3e4d9db7073c]</td>\n",
       "      <td>[Introducing LlamaCloud and LlamaParse\\nToday ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can the alignment and safety of LLMs and L...</td>\n",
       "      <td>[343d6088-e9ee-47f2-8ab7-5757b895cbc0]</td>\n",
       "      <td>[Though studies have shown that strong LLMs ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What dimensions, other than knowledge and reas...</td>\n",
       "      <td>[343d6088-e9ee-47f2-8ab7-5757b895cbc0]</td>\n",
       "      <td>[Though studies have shown that strong LLMs ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who conducted a workshop at the LlamaIndex + R...</td>\n",
       "      <td>[9d825d07-8f33-4146-a108-a31ec39d7e62]</td>\n",
       "      <td>[Ravi Theja  conducted a  workshop  at LlamaIn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which individuals were involved in the webinar...</td>\n",
       "      <td>[9d825d07-8f33-4146-a108-a31ec39d7e62]</td>\n",
       "      <td>[Ravi Theja  conducted a  workshop  at LlamaIn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does MultiOn manage the action of sending ...</td>\n",
       "      <td>[53b3d95c-80a1-46a9-b435-f2d9a278c1a0]</td>\n",
       "      <td>[3. Send Email through MultiOn : Finally, the ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the purpose of the integration of Mult...</td>\n",
       "      <td>[53b3d95c-80a1-46a9-b435-f2d9a278c1a0]</td>\n",
       "      <td>[Automate online tasks with MultiOn and LlamaI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.213986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the range of parameters for the large ...</td>\n",
       "      <td>[7964d0e8-2d52-4cd5-8162-572adf498e4b]</td>\n",
       "      <td>[This is related to the above statement. A lot...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the Prometheus model compare to GPT-4...</td>\n",
       "      <td>[7964d0e8-2d52-4cd5-8162-572adf498e4b]</td>\n",
       "      <td>[This is in line with the information provided...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the significance of incorporating user...</td>\n",
       "      <td>[ebb449d4-2a62-4101-afd9-cb65e83b733b]</td>\n",
       "      <td>[Building Multi-Tenancy RAG System with LlamaI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How does the retriever component of a Multi-Te...</td>\n",
       "      <td>[ebb449d4-2a62-4101-afd9-cb65e83b733b]</td>\n",
       "      <td>[Building Multi-Tenancy RAG System with LlamaI...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the significance of the alpaca as a pe...</td>\n",
       "      <td>[8c5709ca-c534-4921-8392-4523ff6d227d]</td>\n",
       "      <td>[Get started in  Python  or  JavaScript ! P.S....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How are alpacas and llamas described in relati...</td>\n",
       "      <td>[8c5709ca-c534-4921-8392-4523ff6d227d]</td>\n",
       "      <td>[Get started in  Python  or  JavaScript ! P.S....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the RetrieverEvaluator module enhance...</td>\n",
       "      <td>[170bc19d-d3db-4b00-95ef-552ef26c491c]</td>\n",
       "      <td>[Cohere Reranker. Fine-tuned reranker (Custom ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the purpose of the SemanticSimilarityE...</td>\n",
       "      <td>[170bc19d-d3db-4b00-95ef-552ef26c491c]</td>\n",
       "      <td>[Docs ,  Tweet . Blockchain:  LlamaIndex data ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does the Advanced Retrieval LlamaPacks sim...</td>\n",
       "      <td>[c38b0ccc-2308-4b80-94bf-3e085f3cb622]</td>\n",
       "      <td>[A Cheat Sheet and Some Recipes For Building A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of the OpenAI Cookbook int...</td>\n",
       "      <td>[c38b0ccc-2308-4b80-94bf-3e085f3cb622]</td>\n",
       "      <td>[Join us in exploring the depths of RAG system...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the process for selecting tools in the...</td>\n",
       "      <td>[c4f6d1d4-0bd8-4e24-b172-16398f165b79]</td>\n",
       "      <td>[The details behind our tool abstractions are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.169580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How do the OpenAI Function agent and ReAct age...</td>\n",
       "      <td>[c4f6d1d4-0bd8-4e24-b172-16398f165b79]</td>\n",
       "      <td>[The details behind our tool abstractions are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.213986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does LlamaCloud improve data handling and ...   \n",
       "1   What are the key features and enhancements of ...   \n",
       "2   How can the alignment and safety of LLMs and L...   \n",
       "3   What dimensions, other than knowledge and reas...   \n",
       "4   Who conducted a workshop at the LlamaIndex + R...   \n",
       "5   Which individuals were involved in the webinar...   \n",
       "6   How does MultiOn manage the action of sending ...   \n",
       "7   What is the purpose of the integration of Mult...   \n",
       "8   What is the range of parameters for the large ...   \n",
       "9   How does the Prometheus model compare to GPT-4...   \n",
       "10  What is the significance of incorporating user...   \n",
       "11  How does the retriever component of a Multi-Te...   \n",
       "12  What is the significance of the alpaca as a pe...   \n",
       "13  How are alpacas and llamas described in relati...   \n",
       "14  How does the RetrieverEvaluator module enhance...   \n",
       "15  What is the purpose of the SemanticSimilarityE...   \n",
       "16  How does the Advanced Retrieval LlamaPacks sim...   \n",
       "17  What is the purpose of the OpenAI Cookbook int...   \n",
       "18  What is the process for selecting tools in the...   \n",
       "19  How do the OpenAI Function agent and ReAct age...   \n",
       "\n",
       "                              expected_ids  \\\n",
       "0   [dffd5179-49b0-40dc-b4b1-3e4d9db7073c]   \n",
       "1   [dffd5179-49b0-40dc-b4b1-3e4d9db7073c]   \n",
       "2   [343d6088-e9ee-47f2-8ab7-5757b895cbc0]   \n",
       "3   [343d6088-e9ee-47f2-8ab7-5757b895cbc0]   \n",
       "4   [9d825d07-8f33-4146-a108-a31ec39d7e62]   \n",
       "5   [9d825d07-8f33-4146-a108-a31ec39d7e62]   \n",
       "6   [53b3d95c-80a1-46a9-b435-f2d9a278c1a0]   \n",
       "7   [53b3d95c-80a1-46a9-b435-f2d9a278c1a0]   \n",
       "8   [7964d0e8-2d52-4cd5-8162-572adf498e4b]   \n",
       "9   [7964d0e8-2d52-4cd5-8162-572adf498e4b]   \n",
       "10  [ebb449d4-2a62-4101-afd9-cb65e83b733b]   \n",
       "11  [ebb449d4-2a62-4101-afd9-cb65e83b733b]   \n",
       "12  [8c5709ca-c534-4921-8392-4523ff6d227d]   \n",
       "13  [8c5709ca-c534-4921-8392-4523ff6d227d]   \n",
       "14  [170bc19d-d3db-4b00-95ef-552ef26c491c]   \n",
       "15  [170bc19d-d3db-4b00-95ef-552ef26c491c]   \n",
       "16  [c38b0ccc-2308-4b80-94bf-3e085f3cb622]   \n",
       "17  [c38b0ccc-2308-4b80-94bf-3e085f3cb622]   \n",
       "18  [c4f6d1d4-0bd8-4e24-b172-16398f165b79]   \n",
       "19  [c4f6d1d4-0bd8-4e24-b172-16398f165b79]   \n",
       "\n",
       "                                      retrieved_texts  hit_rate       mrr  \\\n",
       "0   [LlamaParse:  Outperformed existing OCR soluti...       0.0  0.000000   \n",
       "1   [Introducing LlamaCloud and LlamaParse\\nToday ...       0.0  0.000000   \n",
       "2   [Though studies have shown that strong LLMs ca...       1.0  1.000000   \n",
       "3   [Though studies have shown that strong LLMs ca...       1.0  1.000000   \n",
       "4   [Ravi Theja  conducted a  workshop  at LlamaIn...       1.0  1.000000   \n",
       "5   [Ravi Theja  conducted a  workshop  at LlamaIn...       1.0  1.000000   \n",
       "6   [3. Send Email through MultiOn : Finally, the ...       1.0  1.000000   \n",
       "7   [Automate online tasks with MultiOn and LlamaI...       1.0  0.500000   \n",
       "8   [This is related to the above statement. A lot...       0.0  0.000000   \n",
       "9   [This is in line with the information provided...       1.0  1.000000   \n",
       "10  [Building Multi-Tenancy RAG System with LlamaI...       1.0  1.000000   \n",
       "11  [Building Multi-Tenancy RAG System with LlamaI...       1.0  1.000000   \n",
       "12  [Get started in  Python  or  JavaScript ! P.S....       1.0  1.000000   \n",
       "13  [Get started in  Python  or  JavaScript ! P.S....       1.0  1.000000   \n",
       "14  [Cohere Reranker. Fine-tuned reranker (Custom ...       0.0  0.000000   \n",
       "15  [Docs ,  Tweet . Blockchain:  LlamaIndex data ...       1.0  1.000000   \n",
       "16  [A Cheat Sheet and Some Recipes For Building A...       0.0  0.000000   \n",
       "17  [Join us in exploring the depths of RAG system...       0.0  0.000000   \n",
       "18  [The details behind our tool abstractions are ...       1.0  0.333333   \n",
       "19  [The details behind our tool abstractions are ...       1.0  0.500000   \n",
       "\n",
       "    precision  recall        ap      ndcg  \n",
       "0         0.0     0.0  0.000000  0.000000  \n",
       "1         0.0     0.0  0.000000  0.000000  \n",
       "2         0.2     1.0  1.000000  0.339160  \n",
       "3         0.2     1.0  1.000000  0.339160  \n",
       "4         0.2     1.0  1.000000  0.339160  \n",
       "5         0.2     1.0  1.000000  0.339160  \n",
       "6         0.2     1.0  1.000000  0.339160  \n",
       "7         0.2     1.0  0.500000  0.213986  \n",
       "8         0.0     0.0  0.000000  0.000000  \n",
       "9         0.2     1.0  1.000000  0.339160  \n",
       "10        0.2     1.0  1.000000  0.339160  \n",
       "11        0.2     1.0  1.000000  0.339160  \n",
       "12        0.2     1.0  1.000000  0.339160  \n",
       "13        0.2     1.0  1.000000  0.339160  \n",
       "14        0.0     0.0  0.000000  0.000000  \n",
       "15        0.2     1.0  1.000000  0.339160  \n",
       "16        0.0     0.0  0.000000  0.000000  \n",
       "17        0.0     0.0  0.000000  0.000000  \n",
       "18        0.2     1.0  0.333333  0.169580  \n",
       "19        0.2     1.0  0.500000  0.213986  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_results_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cc0f2-fe13-4c64-b5c9-29c96a4b4d29",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3120fa3d-f75a-451f-905f-1b11376d770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_ids</th>\n",
       "      <th>retrieved_texts</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does LlamaCloud improve data handling and ...</td>\n",
       "      <td>[dffd5179-49b0-40dc-b4b1-3e4d9db7073c]</td>\n",
       "      <td>[LlamaParse:  Outperformed existing OCR soluti...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the key features and enhancements of ...</td>\n",
       "      <td>[dffd5179-49b0-40dc-b4b1-3e4d9db7073c]</td>\n",
       "      <td>[Introducing LlamaCloud and LlamaParse\\nToday ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the range of parameters for the large ...</td>\n",
       "      <td>[7964d0e8-2d52-4cd5-8162-572adf498e4b]</td>\n",
       "      <td>[This is related to the above statement. A lot...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How does the RetrieverEvaluator module enhance...</td>\n",
       "      <td>[170bc19d-d3db-4b00-95ef-552ef26c491c]</td>\n",
       "      <td>[Cohere Reranker. Fine-tuned reranker (Custom ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How does the Advanced Retrieval LlamaPacks sim...</td>\n",
       "      <td>[c38b0ccc-2308-4b80-94bf-3e085f3cb622]</td>\n",
       "      <td>[A Cheat Sheet and Some Recipes For Building A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of the OpenAI Cookbook int...</td>\n",
       "      <td>[c38b0ccc-2308-4b80-94bf-3e085f3cb622]</td>\n",
       "      <td>[Join us in exploring the depths of RAG system...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does LlamaCloud improve data handling and ...   \n",
       "1   What are the key features and enhancements of ...   \n",
       "8   What is the range of parameters for the large ...   \n",
       "14  How does the RetrieverEvaluator module enhance...   \n",
       "16  How does the Advanced Retrieval LlamaPacks sim...   \n",
       "17  What is the purpose of the OpenAI Cookbook int...   \n",
       "\n",
       "                              expected_ids  \\\n",
       "0   [dffd5179-49b0-40dc-b4b1-3e4d9db7073c]   \n",
       "1   [dffd5179-49b0-40dc-b4b1-3e4d9db7073c]   \n",
       "8   [7964d0e8-2d52-4cd5-8162-572adf498e4b]   \n",
       "14  [170bc19d-d3db-4b00-95ef-552ef26c491c]   \n",
       "16  [c38b0ccc-2308-4b80-94bf-3e085f3cb622]   \n",
       "17  [c38b0ccc-2308-4b80-94bf-3e085f3cb622]   \n",
       "\n",
       "                                      retrieved_texts  hit_rate  mrr  \\\n",
       "0   [LlamaParse:  Outperformed existing OCR soluti...       0.0  0.0   \n",
       "1   [Introducing LlamaCloud and LlamaParse\\nToday ...       0.0  0.0   \n",
       "8   [This is related to the above statement. A lot...       0.0  0.0   \n",
       "14  [Cohere Reranker. Fine-tuned reranker (Custom ...       0.0  0.0   \n",
       "16  [A Cheat Sheet and Some Recipes For Building A...       0.0  0.0   \n",
       "17  [Join us in exploring the depths of RAG system...       0.0  0.0   \n",
       "\n",
       "    precision  recall   ap  ndcg  \n",
       "0         0.0     0.0  0.0   0.0  \n",
       "1         0.0     0.0  0.0   0.0  \n",
       "8         0.0     0.0  0.0   0.0  \n",
       "14        0.0     0.0  0.0   0.0  \n",
       "16        0.0     0.0  0.0   0.0  \n",
       "17        0.0     0.0  0.0   0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_eval_irrelevance_df = (\n",
    "    retrieval_eval_results_full_df\n",
    "    .loc[lambda df: df['hit_rate'].lt(1)]\n",
    "    .sort_values(['hit_rate', 'mrr', 'precision', 'recall', 'ap', 'ndcg'])\n",
    ")\n",
    "retrieval_eval_irrelevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25dc0761-d5a2-4440-b3ef-7fb652b601b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============Error #1=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does LlamaCloud improve data handling and OCR accuracy for Scaleport AI across multiple industries?\n",
      "\n",
      "Expected Contexts:\n",
      "LlamaIndex Newsletter 2024-07-23\n",
      "Hello, Llama Followers! ü¶ô Welcome to this week‚Äôs edition of the LlamaIndex newsletter! We‚Äôre thrilled to share some exciting updates about our products, including LlamaCloud, LlamaParse, and LlamaAgents. You‚Äôll also find success stories with LlamaCloud, extensive guides, in-depth tutorials, and information about upcoming hackathons. ü§©¬† The highlights: LlamaCloud Updates:  New features including LlamaCloud Chat, enhanced Teams collaboration, and expanded integrations with Notion, Slack, Jira, and SharePoint.  Blogpost ,  Tweet . Scaleport AI‚Äôs Accelerated Development with LlamaCloud:  Scaleport AI boosts development speed and sales with LlamaCloud and LlamaIndex, improving data handling and OCR accuracy across multiple industries.  Blogpost . Claude Sonnet-3.5 Integration with LlamaParse:  Integration of Claude Sonnet-3.5 with LlamaParse improves chart understanding and data extraction capabilities.  Notebook ,  Tweet . Multimodal RAG Cookbook:  A new guide for processing text, diagrams, charts, and tables in slide decks using LlamaParse, LlamaIndex, and GPT-4o.  Notebook ,  Tweet . Human in the Loop with LlamaAgents:  Implementation includes HumanService for math queries and agent handling for other inquiries, managed via Gradio app and RabbitMQ.  Code . ‚ú® Feature Releases and Enhancements: We have released new features on LlamaCloud like LlamaCloud Chat for instant conversational data access, enhanced Teams functionality for collaboration, and expanded data integration with connectors for Notion, Slack, Jira, and improved SharePoint support.  Blogpost ,  Tweet . We integrated Claude Sonnet-3.5 with LlamaParse to enhance document parsing capabilities, offering advanced chart understanding and structured data extraction with improved validation and scalability.  Notebook ,  Tweet . We have released a cookbook on Multimodal RAG for processing slide decks rich in text, diagrams, charts, and tables using LlamaParse, LlamaIndex, and GPT-4o, blending text and image data for comprehensive document analysis.  Notebook ,  Tweet . We have implemented Human in the Loop with LlamaAgents in our new example that integrates a HumanService object for handling math queries and an agent for other queries, all managed through a Gradio app and RabbitMQ messaging.  Code . \n",
      "\n",
      "Retrieved Contexts:\n",
      "LlamaParse:  Outperformed existing OCR solutions, offering superior accuracy and efficiency. Advanced Indexing and Retrieval:  Enabled flexible integration with various data sources, enhancing data management and accessibility. Rapid Prototyping and Easy Production Deployments:  LlamaCloud provides an intuitive UI for rapid prototyping and a seamless transition from UI to code for full-scale development. The Results: Accelerated Development and Enhanced Client Engagement LlamaCloud delivered remarkable improvements for  Scaleport.ai : Accelerated Development Timelines:  The team could build technical prototypes during the scoping phase, demonstrating tangible value instantly. This improved client engagement and sales outcomes. Enhanced OCR Performance:  LlamaParse outperformed GPT-4 vision on several OCR tasks, providing superior accuracy and efficiency. Flexible Data Handling:  LlamaCloud's integration with data sources and advanced indexing and retrieval capabilities allowed for quick delivery of high-quality results. Teemu Lahdenper√§, CTO of Scaleport AI, shared his experience: \"LlamaCloud has really sped up our development timelines - whether it's prototyping or production deployments. Before LlamaCloud, building even a simple application took forever because we needed to write our own abstractions for everything. When building an app for a client, a LOT of the work is building the ingestion pipelines. Doing that stuff with LlamaCloud and LlamaParse is remarkably simpler. This in turn has really helped our sales outcomes since we can show tangible value instantly. We've also seen great results with LlamaParse! Specifically, we spent about 50-60% less development hours for one of our clients than we did for an equivalent application prior to LlamaCloud. The main time savings were around Llamaparse; not having to build a custom ingestion pipeline and having the indexing sorted. This helps our margins as well‚Äù Conclusion: A Game-Changer for AI Development LlamaCloud has proven to be a game-changer for  Scaleport.ai , enabling them to develop apps faster and enhance their overall AI application performance. This has accelerated their sales process! By leveraging LlamaCloud's comprehensive suite of tools,  Scaleport.ai  has positioned itself at the forefront of AI solution providers, ready to meet the evolving needs of their clients with speed, flexibility, and cutting-edge technology. Want to see what LlamaCloud can do for you? \n",
      "\n",
      "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud\n",
      "The Challenge: Streamlining AI Development Scaleport AI specializes in transforming emerging AI technology into tangible business results. They possess deep expertise in deploying AI across key industries such as Legal, eCommerce, Real Estate, and Finance, providing tailored generative AI solutions for production applications. Before adopting LlamaCloud and LlamaIndex, Scaleport AI faced several challenges: Long development timelines for creating technical prototypes Difficulty in demonstrating tangible value to clients during the sales process Complex setup requirements for ingestion pipelines and data processing Suboptimal OCR performance, as existing solutions were not meeting the required accuracy and efficiency standards The Solution: LlamaCloud's Comprehensive AI Development Platform Scaleport AI turned to LlamaCloud to address these challenges. LlamaCloud offered: Centralized Knowledge Interface:  Simplified data management and reduced time spent on data wrangling. \n",
      "\n",
      "Ensuring high-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications. Scalability Hurdles : Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either. Accuracy Concerns : Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs. Configuration Overload:  Fine-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task. As developers shift from prototypes towards building production applications - complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data. To address these difficulties, we soft-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production-grade context-augmentation to your LLM and RAG applications. LlamaParse can already support 50+ languages and 100+ document formats. The adoption has been incredible - we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here‚Äôs an example from Dean Barr, Applied AI Lead at Carlyle: As an AI Applied Data Scientist who was granted one of the first ML patents in the U.S., and who is building cutting-edge AI capabilities at one of the world's largest Private Equity Funds, I can confidently say that LlamaParse from LlamaIndex is currently the best technology I have seen for parsing complex document structures for Enterprise RAG pipelines. Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to maintaining data integrity in advanced RAG and agentic model building. The Rise of Centralized Knowledge Management We have designed LlamaCloud to cater to the need of¬† production-grade ¬† context-augmentation ¬†for your LLM and RAG applications. Let's take a tour of what LlamaCloud brings to the table: LlamaParse : Our state-of-the-art parser that turns complex documents with tables and charts into LLM-friendly formats. You can learn more about  LlamaParse here . Managed Ingestion : Connect to enterprise data sources and your choice of data sinks with ease. We support  multiple data sources  and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications. Advanced Retrieval : LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents. LlamaCloud Playground : An interactive UI to test and refine your ingestion and retrieval strategies before deployment. Scalability and Security : Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs. This video gives a detailed walk through of LlamaCloud: Our customers tell us that LlamaCloud enables developers to spend less time setting up and iterating on their data pipelines for LLM use cases, allowing them to iterate through the LLM application development lifecycle much more quickly. Here‚Äôs what Teemu Lahdenpera, CTO at  Scaleport.ai  had to say: LlamaCloud has really sped up our development timelines. Getting to technical prototypes quickly allows us to show tangible value instantly, improving our sales outcomes. When needed, switching from the LlamaCloud UI to code has been really seamless. The configurable parsing and retrieval features have significantly improved our response accuracy. We've also seen great results with LlamaParse and found it outperforming GPT-4 vision on some OCR tasks! Try it yourself We‚Äôve opened up an official waitlist for LlamaCloud. Here's how you can get involved: Join the LlamaCloud Waitlist :  Sign up here . Get in Touch : Have questions? \n",
      "\n",
      "Introducing LlamaCloud and LlamaParse\n",
      "Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring  production-grade   context-augmentation  to your LLM and RAG applications. Using LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components: LlamaParse:  Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You‚Äôll be able to answer complex questions that simply weren‚Äôt possible previously. Managed Ingestion and Retrieval API:  An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in  LlamaHub , including LlamaParse, and our data storage integrations. LlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use;  contact us  for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested,  get in touch . \n",
      "\n",
      "Sign up for LlamaCloud  and  get on the waitlist  for full access!\n",
      "\n",
      "\n",
      "\n",
      "============Error #2=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What are the key features and enhancements of LlamaCloud, including its integrations with Notion, Slack, Jira, and SharePoint?\n",
      "\n",
      "Expected Contexts:\n",
      "LlamaIndex Newsletter 2024-07-23\n",
      "Hello, Llama Followers! ü¶ô Welcome to this week‚Äôs edition of the LlamaIndex newsletter! We‚Äôre thrilled to share some exciting updates about our products, including LlamaCloud, LlamaParse, and LlamaAgents. You‚Äôll also find success stories with LlamaCloud, extensive guides, in-depth tutorials, and information about upcoming hackathons. ü§©¬† The highlights: LlamaCloud Updates:  New features including LlamaCloud Chat, enhanced Teams collaboration, and expanded integrations with Notion, Slack, Jira, and SharePoint.  Blogpost ,  Tweet . Scaleport AI‚Äôs Accelerated Development with LlamaCloud:  Scaleport AI boosts development speed and sales with LlamaCloud and LlamaIndex, improving data handling and OCR accuracy across multiple industries.  Blogpost . Claude Sonnet-3.5 Integration with LlamaParse:  Integration of Claude Sonnet-3.5 with LlamaParse improves chart understanding and data extraction capabilities.  Notebook ,  Tweet . Multimodal RAG Cookbook:  A new guide for processing text, diagrams, charts, and tables in slide decks using LlamaParse, LlamaIndex, and GPT-4o.  Notebook ,  Tweet . Human in the Loop with LlamaAgents:  Implementation includes HumanService for math queries and agent handling for other inquiries, managed via Gradio app and RabbitMQ.  Code . ‚ú® Feature Releases and Enhancements: We have released new features on LlamaCloud like LlamaCloud Chat for instant conversational data access, enhanced Teams functionality for collaboration, and expanded data integration with connectors for Notion, Slack, Jira, and improved SharePoint support.  Blogpost ,  Tweet . We integrated Claude Sonnet-3.5 with LlamaParse to enhance document parsing capabilities, offering advanced chart understanding and structured data extraction with improved validation and scalability.  Notebook ,  Tweet . We have released a cookbook on Multimodal RAG for processing slide decks rich in text, diagrams, charts, and tables using LlamaParse, LlamaIndex, and GPT-4o, blending text and image data for comprehensive document analysis.  Notebook ,  Tweet . We have implemented Human in the Loop with LlamaAgents in our new example that integrates a HumanService object for handling math queries and an agent for other queries, all managed through a Gradio app and RabbitMQ messaging.  Code . \n",
      "\n",
      "Retrieved Contexts:\n",
      "Introducing LlamaCloud and LlamaParse\n",
      "Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring  production-grade   context-augmentation  to your LLM and RAG applications. Using LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components: LlamaParse:  Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You‚Äôll be able to answer complex questions that simply weren‚Äôt possible previously. Managed Ingestion and Retrieval API:  An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in  LlamaHub , including LlamaParse, and our data storage integrations. LlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use;  contact us  for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you‚Äôre interested,  get in touch . \n",
      "\n",
      "The latest updates to LlamaCloud\n",
      "To build a production-quality LLM agent over your data, you need a production-quality data processing layer. LlamaCloud is that data processing and management layer for your AI knowledge assistants. Since  launching a LlamaCloud waitlist last week , we‚Äôve gotten hundreds of signups and published case studies showing how it cuts  production development hours by 50% . On top of that, our team has shipped a slew of new features at a breakneck pace in the past week. We‚Äôre excited to highlight these new features that collectively help you  set up a chat interface in minutes,   increase developer collaboration within your team, and access more data and metadata. Set up a Chat Interface in Minutes We are releasing  LlamaCloud Chat,  which gives you an easy-to-use chat interface over your data. This chat interface is a conversational RAG pipeline built over the advanced retrieval interface that a given pipeline provides, and has out-of-the-box support for streaming and citations - it‚Äôs powered by the same DNA as  create-llama , our fully open-source set up tool  for LLM applications. The LlamaCloud UI already lets you set up a data pipeline over any data in minutes, and now you get a full-blown ChatGPT over your data in minutes. Besides the chat UI, you also have additional flexibility: You can customize metadata filters in the retrieval parameters You can view retrieved nodes and their source files Besides chunk-level retrieval, you can now do  file-level retrieval  (more on this soon!) LlamaCloud is fundamentally a developer tool: with these updates, we enable developers to spend less time on data pipeline setup and iteration, and more time on writing the orchestration logic on top of this interface. Increased Developer Collaboration The team selection interface Organization settings We‚Äôve added  organizational features  into LlamaCloud, enabling any individual user to create an organization and add other users to the organization. Any user within an organization will have a view of all the organization‚Äôs projects and indexes within each project. This allows your team to have a single-source of truth for your data pipelines. In the past each developer would spend time re-indexing/experimenting with the same sources of data. This feature enables transparency, re-use, and generally more rapid development velocity. Improved Data and Metadata Access We‚Äôve made several updates here - we‚Äôve added more data connectors and added features to let you more easily access and customize metadata. We added a Notion, Slack, and Jira Connector Our Sharepoint connector now natively integrates with user IDs that you can filter for, enabling you to build LLM applications with access control. You can now attach metadata to any uploaded file as a CSV - do this through the UI or our API! Want to see what LlamaCloud can do for you? \n",
      "\n",
      "Want to discuss unlimited commercial use?  Contact us  and let's chat! Note: we support private deployments for a select number of enterprises Stay Updated : Follow us on  Twitter  and join our  Discord community  to stay in the loop. In the meantime, anyone can create an account at  https://cloud.llamaindex.ai/ . While you‚Äôre waiting for official LlamaCloud access, anyone can immediately start using our LlamaParse APIs. We‚Äôre shipping a  lot  of features in the next few weeks. We look forward to seeing the context-augmented LLM applications that you can build on top of LlamaCloud! üöÄü¶ô FAQ Have you got some examples of how to use LlamaCloud? We sure do! One of the strengths of LlamaCloud is how easily the endpoints integrate into your existing code. Our  llamacloud-demo repo  has lots of examples from  getting started  to  running evaluations . Is this competitive with vector databases? No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are integrating LlamaCloud with storage providers based on customer requests.\n",
      "\n",
      "LlamaIndex Newsletter 2024‚Äì02‚Äì20: introducing LlamaCloud\n",
      "Hi there, LlamaIndex Enthusiasts ü¶ô, Today marks a milestone for the LlamaIndex ecosystem with the  introduction of LlamaCloud , a next-generation suite of managed parsing, ingestion, and retrieval services tailored for  production-grade   context augmentation  in your LLM and RAG applications. As an enterprise AI engineer using LlamaCloud, you can concentrate on crafting the business logic, leaving the heavy lifting of data management to us. Process vast amounts of production data effortlessly, enhancing response quality instantly. LlamaCloud debuts with: LlamaParse:  A specialized parsing service for complex documents, including tables and figures, seamlessly integrated with LlamaIndex for handling semi-structured documents. This enables answering intricate queries previously out of reach. Managed Ingestion and Retrieval API:  Simplify data loading, processing, and storage for your RAG applications, supported by over 150 data sources via  LlamaHub , including LlamaParse, and more than 40 data storage solutions. LlamaParse is now in public preview, with a current focus on PDFs and a usage cap for public users;  contact us  for commercial terms. The managed API is in private preview, and available to a select group of enterprise partners. \n",
      "\n",
      "Ensuring high-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications. Scalability Hurdles : Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either. Accuracy Concerns : Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs. Configuration Overload:  Fine-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task. As developers shift from prototypes towards building production applications - complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data. To address these difficulties, we soft-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production-grade context-augmentation to your LLM and RAG applications. LlamaParse can already support 50+ languages and 100+ document formats. The adoption has been incredible - we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here‚Äôs an example from Dean Barr, Applied AI Lead at Carlyle: As an AI Applied Data Scientist who was granted one of the first ML patents in the U.S., and who is building cutting-edge AI capabilities at one of the world's largest Private Equity Funds, I can confidently say that LlamaParse from LlamaIndex is currently the best technology I have seen for parsing complex document structures for Enterprise RAG pipelines. Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to maintaining data integrity in advanced RAG and agentic model building. The Rise of Centralized Knowledge Management We have designed LlamaCloud to cater to the need of¬† production-grade ¬† context-augmentation ¬†for your LLM and RAG applications. Let's take a tour of what LlamaCloud brings to the table: LlamaParse : Our state-of-the-art parser that turns complex documents with tables and charts into LLM-friendly formats. You can learn more about  LlamaParse here . Managed Ingestion : Connect to enterprise data sources and your choice of data sinks with ease. We support  multiple data sources  and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications. Advanced Retrieval : LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents. LlamaCloud Playground : An interactive UI to test and refine your ingestion and retrieval strategies before deployment. Scalability and Security : Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs. This video gives a detailed walk through of LlamaCloud: Our customers tell us that LlamaCloud enables developers to spend less time setting up and iterating on their data pipelines for LLM use cases, allowing them to iterate through the LLM application development lifecycle much more quickly. Here‚Äôs what Teemu Lahdenpera, CTO at  Scaleport.ai  had to say: LlamaCloud has really sped up our development timelines. Getting to technical prototypes quickly allows us to show tangible value instantly, improving our sales outcomes. When needed, switching from the LlamaCloud UI to code has been really seamless. The configurable parsing and retrieval features have significantly improved our response accuracy. We've also seen great results with LlamaParse and found it outperforming GPT-4 vision on some OCR tasks! Try it yourself We‚Äôve opened up an official waitlist for LlamaCloud. Here's how you can get involved: Join the LlamaCloud Waitlist :  Sign up here . Get in Touch : Have questions? \n",
      "\n",
      "\n",
      "\n",
      "============Error #3=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What is the range of parameters for the large language models developed mentioned in the context?\n",
      "\n",
      "Expected Contexts:\n",
      "This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YES GPT-4 Relevancy Score:  1.0 Observation: Prometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information. GPT-4: Evaluates it correctly, unlike the Prometheus model. Summary The cost for evaluation (approx.):  $1.5  for Prometheus Model and  $15  for GPT4. The Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application. If a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4. The faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4. Note: You can check detailed analysis with code on  Google Colab Notebook . The endpoint on HF is served on AWS Nvidia A100G ¬∑ 1x GPU ¬∑ 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.) \n",
      "\n",
      "Retrieved Contexts:\n",
      "This is related to the above statement. A lot of summarization strategies over big documents involve ‚Äúhacks‚Äù such as sequential refinement or hierarchical summarization (see our  response synthesis modules  as a reference guide). This can now be alleviated with a single LLM call. Personalized memory will be better and easier to build:  A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction). There are, however, some lingering challenges: 10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge.  1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many ‚Äúsmall‚Äù document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context. Embedding models are lagging behind in context length.  So far the largest context window we‚Äôve seen for embeddings are  32k from together.ai . This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller. Cost and Latency.  \n",
      "\n",
      "Evaluation Metrics: The research team used a wide range of metrics suited to each dataset. The geometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY, and F1 scores for others were the primary metrics. Results Baseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated. With retrieval, performance for 4K models like LLaMA2‚Äì70B-4K and GPT-43B-4K significantly improved. HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models. Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks. There exists a unique ‚ÄúU-shaped‚Äù performance curve for LLMs due to the  lost in the middle  phenomenon, making them better at utilizing information at the beginning or end of the input. The study presents a contrasting perspective to LongBench‚Äôs findings, emphasizing that retrieval is beneficial for models regardless of their context window size. Comparing to OpenAI Models: The LLaMA2‚Äì70B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks. Comparison of Different Retrievers: Retrieval consistently enhances the performance across different retrievers. Public retrievers outperformed proprietary ones like OpenAI embeddings. Comparing with the number of retrieved chunks: The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn‚Äôt offer additional benefits and can even degrade performance. The deterioration in performance when adding more chunks could be due to the  lost-in-the-middle  phenomenon or the model being sidetracked by non-relevant information. Conclusion As we delved deep into understanding how retrieval augmentation and long-context extension interact when applied to leading language models fine-tuned for long-context question-answering and summarization tasks. Here are some things to be noted: Boost in Performance with Retrieval : Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts. Efficiency of 4K Models with Retrieval : 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process. Best Model Performance : After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2‚Äì70B-32k-ret (LLaMA2‚Äì70B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003. References: Retrieval meets long context, large language models. Longbench: A bilingual, multitask benchmark for long context understanding. We trust that this blog post on the review of the paper on retrieval augmentation with long-context LLMs has furnished you with meaningful insights. We‚Äôre keen to hear if your experiments align with our findings or present new perspectives ‚Äî divergent results always make for interesting discussions and further exploration.\n",
      "\n",
      "However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO Prometheus Relevancy Score:  0.0 GPT-4 Faithfulness Feedback:  The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YES GPT-4 Faithfulness Score:  1.0 GPT-4 Relevancy Feedback:  The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. \n",
      "\n",
      "However, such re-ranking models generally have context window limitations. To address the above issues, we propose a Question-aware Coarse-Grained prompt compression method. This method evaluates the relevance between the context and the question based on the perplexity corresponding to the question. To mitigate the hallucination problem in smaller LLMs, we append a restrictive statement, specifically ‚Äú We can get the answer to this question in the given documents ‚Äù, after the question to limit the latent space caused by related hallucinations. Figure 1.  \n",
      "\n",
      "NVIDIA Research: RAG with Long Context LLMs\n",
      "Introduction Why Long Context Matters and How Retrieval Augmentation Steps In: In the dynamic landscape of LLMs, two methods have gained traction and seem to be taking center stage: expanding the context window of Large Language Models (LLMs) and enhancing these models with retrieval capabilities. The continued evolution of GPU technology, coupled with breakthroughs in attention mechanisms, has given rise to long-context LLMs. Simultaneously, the concept of retrieval ‚Äî where LLMs pick up only the most relevant context from a standalone retriever ‚Äî promises a revolution in efficiency and speed. In the midst of these evolving narratives, some interesting questions emerge: Retrieval-augmentation versus long context window, which one is better for downstream tasks? Can both methods be combined to get the best of both worlds? To dissect these questions, in this blog post we turn to  NVIDIA‚Äôs recent study , which harnesses the power of two powerful LLMs: the proprietary GPT ‚Äî 43B and LLaMA2‚Äì70B, the research strives to provide actionable insights for AI practitioners. Prior Research and the NVIDIA Divergence: Interestingly, while NVIDIA‚Äôs findings are interesting in many respects, Another recent work by  Bai et al. (2023)  also ventured into similar territory, although with differing outcomes. Their work explored the impact of retrieval on long context LLMs, evaluating models like GPT-3.5-Turbo-16k and Llama2‚Äì7B-chat-4k. However, their findings diverge from NVIDIA‚Äôs in crucial ways.  Bai et al.  discerned that retrieval was beneficial only for the Llama2‚Äì7B-chat-4k with a 4K context window, but not for extended context models like GPT-3.5-Turbo-16k. One hypothesis for this difference centers on the challenges tied to experiments using black-box APIs and the smaller white-box LLMs they employed, which potentially had limited capability to integrate context through retrieval. NVIDIA‚Äôs work distinguishes itself by tapping into much larger LLMs, yielding results that not only match top-tier models like ChatGPT-3.5 but even indicate further enhancements when incorporating retrieval methods. Models, Datasets, and Evaluation Metrics Large Language Models (LLMs) Explored: The researchers delved deep into the potential of large language models for tasks like generative QA and summarization. Specifically, two models were the primary focus: Nemo GPT-43B:  A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings. LLaMA2‚Äì70B:  A publicly available 70B parameter model trained on 2T tokens, primarily in English. It‚Äôs structured with 80 layers and also utilizes RoPE embeddings. Context Window Extension: To enhance the models‚Äô capability to process longer contexts, their initial 4K context window length was augmented. The GPT-43B was modified to handle 16K, while the LLaMA2‚Äì70B was expanded to both 16K and 32K, employing the position interpolation method. Instruction Tuning: To optimize the LLMs for the tasks at hand, instruction tuning was implemented. A diverse dataset blend, comprising sources like Soda, ELI5, FLAN, and others, was created. \n",
      "\n",
      "\n",
      "\n",
      "============Error #4=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the RetrieverEvaluator module enhance retrieval evaluations?\n",
      "\n",
      "Expected Contexts:\n",
      "Docs ,  Tweet . Blockchain:  LlamaIndex data agents can be now used to analyze any blockchain subgraph using natural language queries.  Tweet . üîé  RAG Evaluation Enhancements: RetrieverEvaluator : We introduced ‚ÄúRetrieverEvaluator‚Äù for enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.  Docs ,  Tweet . SemanticSimilarityEvaluator : We introduced a new semantic similarity evaluator ‚Äî SemanticSimilarityEvaluator for LLM/RAG outputs, comparing embedding similarity between reference and generated answers.  Docs ,  Tweet . \n",
      "\n",
      "Retrieved Contexts:\n",
      "Cohere Reranker. Fine-tuned reranker (Custom reranker) without hard negatives. Fine-tuned reranker (Custom reranker) with hard negatives selected at random. Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity. Let‚Äôs define the rerankers. RERANKERS = {\n",
      "    \"WithoutReranker\": \"None\",\n",
      "    \"CohereRerank\": reranker_base,\n",
      "    \"CohereRerank_0\": reranker_model_0,\n",
      "    \"CohereRerank_5_random\": reranker_model_5_random,\n",
      "    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n",
      "} Create an Index and Retriever for evaluation purposes. # Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\n",
      "index_embed_model = CohereEmbedding(\n",
      "    cohere_api_key=cohere_api_key,\n",
      "    model_name=\"embed-english-v3.0\",\n",
      "    input_type=\"search_document\",\n",
      ")\n",
      "\n",
      "query_embed_model = CohereEmbedding(\n",
      "    cohere_api_key=cohere_api_key,\n",
      "    model_name=\"embed-english-v3.0\",\n",
      "    input_type=\"search_query\",\n",
      ")\n",
      "\n",
      "service_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\n",
      "service_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n",
      "\n",
      "vector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\n",
      "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query) Define a function to display the results def   display_results ( embedding_name, reranker_name, eval_results ):\n",
      "     \"\"\"Display results from evaluate.\"\"\" \n",
      "\n",
      "    metric_dicts = []\n",
      "     for  eval_result  in  eval_results:\n",
      "        metric_dict = eval_result.metric_vals_dict\n",
      "        metric_dicts.append(metric_dict)\n",
      "\n",
      "    full_df = pd.DataFrame(metric_dicts)\n",
      "\n",
      "    hit_rate = full_df[ \"hit_rate\" ].mean()\n",
      "    mrr = full_df[ \"mrr\" ].mean()\n",
      "\n",
      "    metric_df = pd.DataFrame(\n",
      "        { \"Embedding\" : [embedding_name],  \"Reranker\" : [reranker_name],  \"hit_rate\" : [hit_rate],  \"mrr\" : [mrr]}\n",
      "    )\n",
      "\n",
      "     return  metric_df Loop over different rerankers and evaluate retrieval performance using Custom Retriever. results_df = pd.DataFrame()\n",
      "\n",
      "embed_name =  'CohereEmbedding' \n",
      "\n",
      " # Loop over rerankers \n",
      " for  rerank_name, reranker  in  RERANKERS.items():\n",
      "\n",
      "     print ( f\"Running Evaluation for Reranker:  {rerank_name} \" )\n",
      "\n",
      "     # Define Retriever \n",
      "     class   CustomRetriever ( BaseRetriever ):\n",
      "         \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\" \n",
      "\n",
      "         def   __init__ ( \n",
      "            self,\n",
      "            vector_retriever: VectorIndexRetriever,\n",
      "         ) -&gt;  None :\n",
      "             \"\"\"Init params.\"\"\" \n",
      "\n",
      "            self._vector_retriever = vector_retriever\n",
      "\n",
      "         def   _retrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n",
      "             \"\"\"Retrieve nodes given query.\"\"\" \n",
      "\n",
      "            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
      "\n",
      "             if  reranker !=  'None' :\n",
      "                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
      "             else :\n",
      "                retrieved_nodes = retrieved_nodes[: 5 ]\n",
      "\n",
      "             return  retrieved_nodes\n",
      "\n",
      "         async   def   _aretrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n",
      "             \"\"\"Asynchronously retrieve nodes given query.\n",
      "            \"\"\" \n",
      "             return  self._retrieve(query_bundle)\n",
      "\n",
      "         async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n",
      "             if   isinstance (str_or_query_bundle,  str ):\n",
      "                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
      "             return   await  self._aretrieve(str_or_query_bundle)\n",
      "\n",
      "    custom_retriever = CustomRetriever(vector_retriever)\n",
      "\n",
      "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
      "        [ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever\n",
      "    )\n",
      "    eval_results =  await  retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n",
      "\n",
      "    current_df = display_results(embed_name, rerank_name, eval_results)\n",
      "    results_df = pd.concat([results_df, current_df], ignore_index= True ) Results: From the above table (1- without reranker, 2 ‚Äî with base cohere reranker, 3‚Äì5: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It‚Äôs crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker. Summary: In this blog post, we‚Äôve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community‚Äôs use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.\n",
      "\n",
      "One can then apply a desired weighting scheme to establish a single aggregated retrieval score per metric. Hit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75 Table 3: Retrieval evaluation in multi-modal scenario. Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge) Multi-modal models (i.e., LMMs) like OpenAI‚Äôs GPT-4V or open-source alternatives like LLaVA are able to take in both input and image context to produce an answer the user query. As in text-only RAG, we are also concerned about the ‚Äúrelevancy‚Äù and ‚Äúfaithfulness‚Äù of these generated answers. But in order to be able to compute such metrics in the multi-modal case, we would need a judge model that is also able to take in the context images and text data. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in order to compute relevancy and faithfulness as well as other related metrics! Relevancy (multi-modal): considers  textual and visual context  and evaluates how much the generated response matches the query. Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved  textual and visual context . If you want to test these out, then you‚Äôre in luck as we‚Äôve recently released our beta Multi-Modal Evaluator abstractions! See the code snippet below for how one can use these abstractions to perform their respective evaluations on a generated response to a given query. from  llama_index.evaluation.multi_modal  import  (\n",
      "\tMultiModalRelevancyEvaluator,\n",
      "\tMultiModalFaithfulnessEvaluator\n",
      ")\n",
      " from  llama_index.multi_modal_llm  import  OpenAIMultiModal\n",
      "\n",
      "relevancy_judge = MultiModalRelevancyEvaluator(\n",
      "    multi_modal_llm=OpenAIMultiModal(\n",
      "        model= \"gpt-4-vision-preview\" ,\n",
      "        max_new_tokens= 300 ,\n",
      "    )\n",
      ")\n",
      "\n",
      "faithfulness_judge = MultiModalRelevancyEvaluator(\n",
      "    multi_modal_llm=OpenAIMultiModal(\n",
      "        model= \"gpt-4-vision-preview\" ,\n",
      "        max_new_tokens= 300 ,\n",
      "    )\n",
      ")\n",
      "\n",
      " # Generated response to a query and its retrieved context information \n",
      "query = ...\n",
      "response = ...\n",
      "contexts = ...   # retrieved text contexts \n",
      "image_paths = ...   # retrieved image contexts \n",
      "\n",
      " # Evaluations \n",
      "relevancy_eval = relevancy_judge.evaluate(\n",
      " query=query,\n",
      " response=response,\n",
      " contexts=contexts,\n",
      " image_paths=image_paths\n",
      ")\n",
      "\n",
      "faithfulness_eval = faithfulness_judge.evaluate(\n",
      " query=query,\n",
      " response=response,\n",
      " contexts=contexts,\n",
      " image_paths=image_paths\n",
      ") A Few Important Remarks First, it is worth mentioning that using LLMs or LMMs to judge generated responses has its drawbacks. These judges are generative models themselves and can suffer from hallucinations and other inconsistencies. \n",
      "\n",
      "A question is also provided.\n",
      "  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n",
      "  as the relevance score. The relevance score is a number from 1‚Äì10 based on how relevant you think the document is to the question.\n",
      "  Do not include any documents that are not relevant to the question.\n",
      "  Example format:\n",
      "  Document 1:\n",
      "  <summary of document 1>\n",
      "  Document 2:\n",
      "  <summary of document 2>\n",
      "  ‚Ä¶\n",
      "  Document 10:\n",
      "  <summary of document 10>\n",
      "  Question: <question>\n",
      "  Answer:\n",
      "  Doc: 9, Relevance: 7\n",
      "  Doc: 3, Relevance: 4\n",
      "  Doc: 7, Relevance: 3\n",
      "  Let's try this now:\n",
      "  {context_str}\n",
      "  Question: {query_str}\n",
      "  Answer: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " The prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document: You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk. You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new  document summary index , but you can also choose to do it yourself. Given a collection of documents, we can then create document ‚Äúbatches‚Äù and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches. You can use our abstractions in two forms: as a standalone retriever module ( ListIndexLLMRetriever ) or a reranker module ( LLMRerank ). The remainder of this blog primarily focuses on the reranker module given the speed/cost. LLM Retriever (ListIndexLLMRetriever) This module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index. from  llama_index  import  GPTListIndex\n",
      " from  llama_index.indices. list .retrievers  import  ListIndexLLMRetriever\n",
      "index = GPTListIndex.from_documents(documents, service_context=service_context)\n",
      " # high - level API \n",
      "query_str =  \"What did the author do during his time in college?\" \n",
      "retriever = index.as_retriever(retriever_mode= \"llm\" )\n",
      "nodes = retriever.retrieve(query_str)\n",
      " # lower-level API \n",
      "retriever = ListIndexLLMRetriever()\n",
      "response_synthesizer = ResponseSynthesizer.from_args()\n",
      "query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\n",
      "response = query_engine.query(query_str) Use Case:  This could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes. LLM Reranker (LLMRerank) This module is defined as part of our  NodePostprocessor  abstraction, which is defined for second-stage processing after an initial retrieval pass. The postprocessor can be used on its own or as part of a  RetrieverQueryEngine  call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index. from  llama_index.indices.query.schema  import  QueryBundle\n",
      "query_bundle = QueryBundle(query_str)\n",
      " # configure retriever \n",
      "retriever = VectorIndexRetriever(\n",
      "index=index,\n",
      "similarity_top_k=vector_top_k,\n",
      ")\n",
      "retrieved_nodes = retriever.retrieve(query_bundle)\n",
      " # configure reranker \n",
      "reranker = LLMRerank(choice_batch_size= 5 , top_n=reranker_top_n, service_context=service_context)\n",
      "retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle) Limitations/Caveats There are certain limitations and caveats to LLM-based retrieval, especially with this initial version. LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete. Using third-party LLM API‚Äôs costs money. The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents. Using the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful. Initial Experimental Results Let‚Äôs take a look at how well LLM reranking works! We show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don‚Äôt showcase as many results given that it tends to run a lot slower than either of the first two approaches). We analyze results over two very different sources of data: the Great Gatsby and the 2021 Lyft SEC 10-k. We only analyze results over the ‚Äúretrieval‚Äù portion and not synthesis to better isolate the performance of different retrieval methods. The results are presented in a qualitative fashion. \n",
      "\n",
      "Keyword Queries GTR retriever recall rate 5. Queries With Misspellings What is the advntage of prposition retrieval over sentnce or passage retrieval? 6. Exact Sub-string Searches first kwords for the GTR retriever. Finer-grained Retrieval Evaluation Metrics: We will utilize Hit Rate and MRR metrics for retrieval evaluation. Let‚Äôs get into understanding these metrics. Hit Rate: Hit Rate measures the proportion of queries for which the correct chunk/ context appears within the top-k results chunks/ contexts. Put simply, it evaluates how frequently our system correctly identifies the chunk within its top-k chunks. Mean Reciprocal Rank (MRR): MRR assesses a system‚Äôs accuracy by taking into account the position of the highest-ranking relevant chunk/ context for each query. It calculates the average of the inverse of these positions across all queries. For instance, if the first relevant chunk/ context is at the top of the list, its reciprocal rank is 1. If it‚Äôs the second item, the reciprocal rank becomes 1/2, and this pattern continues accordingly. The remainder of this blog post is divided into two main sections: Implementing  Alpha  Tuning in Hybrid Search for Various Query Types. Analyzing the results of two different document datasets: Indexing a Single Document:  The  LLM Compiler Paper . Indexing Three Documents:  The  LLM Compiler ,  Llama Beyond English , and  Dense X Retrieval  Papers. You can also continue following along in the  Google Colab Notebook  from this point forward. Implementation We will adopt a systematic approach to implement the experimental workflow, which involves the following steps: Data Download. Data Loading. \n",
      "\n",
      "queries = generate_question_context_pairs(\n",
      "    nodes, \n",
      "  llm=llm, \n",
      "  num_questions_per_chunk= 2 , \n",
      "  qa_generate_prompt_tmpl = qa_template\n",
      ") 7.  Define reranker reranker = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=4) 8.  Define CustomRetriever We will define  CustomRetriever  class to perform retrieval operations with and without a reranker. class   CustomRetriever ( BaseRetriever ):\n",
      "     \"\"\"Custom retriever that performs hybrid search with and without reranker\"\"\" \n",
      "\n",
      "     def   __init__ ( \n",
      "        self,\n",
      "        vector_retriever: VectorIndexRetriever,\n",
      "        reranker: CohereRerank\n",
      "     ) -&gt;  None :\n",
      "         \"\"\"Init params.\"\"\" \n",
      "\n",
      "        self._vector_retriever = vector_retriever\n",
      "        self._reranker = reranker\n",
      "\n",
      "     def   _retrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n",
      "         \"\"\"Retrieve nodes given query.\"\"\" \n",
      "\n",
      "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
      "\n",
      "         if  self._reranker !=  None :\n",
      "            retrieved_nodes = self._reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
      "         else :\n",
      "            retrieved_nodes = retrieved_nodes[: 4 ]\n",
      "\n",
      "         return  retrieved_nodes\n",
      "\n",
      "     async   def   _aretrieve ( self, query_bundle: QueryBundle ) -&gt;  List [NodeWithScore]:\n",
      "         \"\"\"Asynchronously retrieve nodes given query.\n",
      "\n",
      "        Implemented by the user.\n",
      "\n",
      "        \"\"\" \n",
      "         return  self._retrieve(query_bundle)\n",
      "\n",
      "     async   def   aretrieve ( self, str_or_query_bundle: QueryType ) -&gt;  List [NodeWithScore]:\n",
      "         if   isinstance (str_or_query_bundle,  str ):\n",
      "            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
      "         return   await  self._aretrieve(str_or_query_bundle) 9.  Define functions for retriever evaluation and metrics computation We will look into retriever performance for different  alpha  values with and without reranker. # Alpha values and datasets to test \n",
      "alpha_values = [ 0.0 ,  0.2 ,  0.4 ,  0.6 ,  0.8 ,  1.0 ]\n",
      "\n",
      " # Function to evaluate retriever and return results \n",
      " async   def   evaluate_retriever ( alpha, dataset, reranker= None ):\n",
      "    retriever = VectorIndexRetriever(index,\n",
      "                                     vector_store_query_mode= \"hybrid\" ,\n",
      "                                     similarity_top_k= 10 ,\n",
      "                                     alpha=alpha)\n",
      "    custom_retriever = CustomRetriever(retriever,\n",
      "                                       reranker)\n",
      "\n",
      "    retriever_evaluator = RetrieverEvaluator.from_metric_names([ \"mrr\" ,  \"hit_rate\" ], retriever=custom_retriever)\n",
      "    eval_results =  await  retriever_evaluator.aevaluate_dataset(dataset)\n",
      "     return  eval_results\n",
      "\n",
      " # Function to calculate and store metrics \n",
      " def   calculate_metrics ( eval_results ):\n",
      "    metric_dicts = []\n",
      "     for  eval_result  in  eval_results:\n",
      "        metric_dict = eval_result.metric_vals_dict\n",
      "        metric_dicts.append(metric_dict)\n",
      "\n",
      "    full_df = pd.DataFrame(metric_dicts)\n",
      "\n",
      "    hit_rate = full_df[ \"hit_rate\" ].mean()\n",
      "    mrr = full_df[ \"mrr\" ].mean()\n",
      "     return  hit_rate, mrr 10. Retrieval Evaluation Here we do retrieval evaluation on different query types (datasets) and alpha values to understand which alpha will be suitable for which query type. You need to plug in the reranker accordingly to compute the retrieval evaluation with and without the reranker. # Asynchronous function to loop over datasets and alpha values and evaluate \n",
      " async   def   main ():\n",
      "    results_df = pd.DataFrame(columns=[ 'Dataset' ,  'Alpha' ,  'Hit Rate' ,  'MRR' ])\n",
      "\n",
      "     for  dataset  in  datasets_single_document.keys():\n",
      "         for  alpha  in  alpha_values:\n",
      "            eval_results =  await  evaluate_retriever(alpha, datasets_single_document[dataset])\n",
      "            hit_rate, mrr = calculate_metrics(eval_results)\n",
      "            new_row = pd.DataFrame({ 'Dataset' : [dataset],  'Alpha' : [alpha],  'Hit Rate' : [hit_rate],  'MRR' : [mrr]})\n",
      "            results_df = pd.concat([results_df, new_row], ignore_index= True )\n",
      "\n",
      "     # Determine the grid size for subplots \n",
      "    num_rows =  len (datasets_single_document) //  2  +  len (datasets_single_document) %  2 \n",
      "    num_cols =  2 \n",
      "\n",
      "     # Plotting the results in a grid \n",
      "    fig, axes = plt.subplots(num_rows, num_cols, figsize=( 12 , num_rows *  4 ), squeeze= False )   # Ensure axes is always 2D \n",
      "\n",
      "     for  i, dataset  in   enumerate (datasets_single_document):\n",
      "        ax = axes[i // num_cols, i % num_cols]\n",
      "        dataset_df = results_df[results_df[ 'Dataset' ] == dataset]\n",
      "        ax.plot(dataset_df[ 'Alpha' ], dataset_df[ 'Hit Rate' ], marker= 'o' , label= 'Hit Rate' )\n",
      "        ax.plot(dataset_df[ 'Alpha' ], dataset_df[ 'MRR' ], marker= 'o' , linestyle= '--' , label= 'MRR' )\n",
      "        ax.set_xlabel( 'Alpha' )\n",
      "        ax.set_ylabel( 'Metric Value' )\n",
      "        ax.set_title( f' {dataset} ' )\n",
      "        ax.legend()\n",
      "        ax.grid( True )\n",
      "\n",
      "     # If the number of datasets is odd, remove the last (empty) subplot \n",
      "     if   len (datasets_single_document) % num_cols !=  0 :\n",
      "        fig.delaxes(axes[- 1 , - 1 ])   # Remove the last subplot if not needed \n",
      "\n",
      "     # Adjust layout to prevent overlap \n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "\n",
      " # Run the main function \n",
      "asyncio.run(main()) Analyze the results: Having completed the implementation phase, we now turn our attention to analyzing the outcomes. We conducted two sets of experiments: one on a single document and another on multiple documents. \n",
      "\n",
      "\n",
      "\n",
      "============Error #5=============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does the Advanced Retrieval LlamaPacks simplify the process of building advanced RAG systems?\n",
      "\n",
      "Expected Contexts:\n",
      "We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. ü§©  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  \n",
      "\n",
      "Retrieved Contexts:\n",
      "A Cheat Sheet and Some Recipes For Building Advanced RAG\n",
      "It‚Äôs the start of a new year and perhaps you‚Äôre looking to break into the RAG scene by building your very first RAG system. Or, maybe you‚Äôve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user‚Äôs queries and data structures. In either case, knowing where or how to begin may be a challenge in and of itself! If that‚Äôs true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems. The RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper ( ‚ÄúRetrieval-Augmented Generation for Large Language Models: A Survey‚Äù Gao, Yunfan, et al. 2023 ). Basic RAG Mainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user‚Äôs query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component. LlamaIndex Basic RAG Recipe: from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n",
      "\n",
      " # load data \n",
      "documents = SimpleDirectoryReader(input_dir= \"...\" ).load_data()\n",
      "\n",
      " # build VectorStoreIndex that takes care of chunking documents \n",
      " # and encoding chunks to embeddings for future retrieval \n",
      "index = VectorStoreIndex.from_documents(documents=documents)\n",
      "\n",
      " # The QueryEngine class is equipped with the generator \n",
      " # and facilitates the retrieval and generation steps \n",
      "query_engine = index.as_query_engine()\n",
      "\n",
      " # Use your Default RAG \n",
      "response = query_engine.query( \"A user's query\" ) Success Requirements for RAG In order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements: Retrieval must be able to find the most relevant documents to a user query. Generation must be able to make good use of the retrieved documents to sufficiently answer the user query. Advanced RAG With the success requirements defined, we can then say that building advanced RAG is really about the application of more sophisticated techniques and strategies (to the Retrieval or Generation components) to ensure that they are ultimately met. Furthermore, we can categorize a sophisticated technique as either one that addresses one of the two high-level success requirements independent (more or less) of the other, or one that addresses both of these requirements simultaneously. Advanced techniques for Retrieval must be able to find the most relevant documents to a user query Below we briefly describe a couple of the more sophisticated techniques to help achieve the first success requirement. Chunk-Size Optimization:  Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses. LlamaIndex Chunk Size Optimization Recipe  ( notebook guide ) : from  llama_index  import  ServiceContext\n",
      " from  llama_index.param_tuner.base  import  ParamTuner, RunResult\n",
      " from  llama_index.evaluation  import  SemanticSimilarityEvaluator, BatchEvalRunner\n",
      "\n",
      " ### Recipe \n",
      " ### Perform hyperparameter tuning as in traditional ML via grid-search \n",
      " ### 1. Define an objective function that ranks different parameter combos \n",
      " ### 2. \n",
      "\n",
      "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines\n",
      "Introduction We are seeing a huge wave of developers building Retrieval Augmented Generation (RAG) applications. The RAG tech stack generally contains a retrieval pipeline, LLM and prompt, among which LLM is accessible and developers are comfortable with prompt customization. However, developers new to search and index often need extensive help to build an effective  retrieval  pipeline. A production-ready retrieval pipeline typically consists of the following components: Document loader that parses and splits the long text Embedding model serving as core indexing component A vector database that stores the vector embeddings Advanced components to future optimize retrieval quality, such as re-ranker model to judge semantic similarity better It‚Äôs challenging to operate this complex tech stack. It involves managing software package dependencies, hosting services in Kubernetes clusters, and monitoring the performance of ML models. The high DevOps cost distracts developers from the most critical part of the user experience of RAG applications: prompt engineering, answer generation, and user interface. While experienced search infrastructure engineers may still manage a complicated tech stack for its flexibility, Zilliz believes that most RAG developers could benefit from a retrieval API service that is user-friendly and allows for lighter customization. Integrating  Zilliz Cloud Pipelines  and  LlamaIndex  brings a new approach to solving this problem. Zilliz Cloud Pipelines is a fully managed, scalable retrieval service. LlamaIndex is a flexible RAG framework that provides libraries and tools for organizing business logics such as retrieval and prompt engineering. The API service of Zilliz Cloud Pipelines is abstracted as a ManagedIndex in LlamaIndex. RAG developers using  ZillizCloudPipelineIndex  can easily scale the app from one user to millions of users without the hassle of setting up and maintaining the complex retrieval tech stack. It hides the technical complexity behind a few function calls, so that developers can focus on the core user experience of their RAG apps. In this blog, we show how to use  ZillizCloudPipelineIndex  to build a high quality RAG chatbot. The chatbot is scalable and supports multi-tenancy through metadata filtering. Set up Since Zilliz Cloud Pipelines is an API service, first you need to set up a  Zilliz Cloud  account and create a free serverless cluster. Now you can construct  ZillizCloudPipelineIndex  and get the handler to index docs and query later. from llama_index.indices import ZillizCloudPipelineIndex\n",
      "\n",
      "zcp_index = ZillizCloudPipelineIndex(\n",
      "    project_id=\" &lt; YOUR_ZILLIZ_PROJECT_ID &gt; \",\n",
      "    cluster_id=\" &lt; YOUR_ZILLIZ_CLUSTER_ID &gt; \",\n",
      "    token=\" &lt; YOUR_ZILLIZ_API_KEY &gt; \",\n",
      ")\n",
      "zcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"}) You can copy the Project ID, Cluster ID and API Key from your Zilliz account as follows: Ingest Documents Suppose your application has multiple users, and you would like to tag each user‚Äôs document to provide isolation. Your application logic can be implemented as follows. For simplicity, here we demo ingesting public documents. Currently, Zilliz Cloud Pipelines  supports  documents stored and managed in AWS S3 and Google Cloud Storage. Local document upload will also be supported soon. \n",
      "\n",
      "irrelevant information). LlamaIndex Information Compression Recipe  ( notebook guide ) : from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n",
      " from  llama_index.query_engine  import  RetrieverQueryEngine\n",
      " from  llama_index.postprocessor  import  LongLLMLinguaPostprocessor\n",
      "\n",
      " ### Recipe \n",
      " ### Define a Postprocessor object, here LongLLMLinguaPostprocessor \n",
      " ### Build QueryEngine that uses this Postprocessor on retrieved docs \n",
      "\n",
      " # Define Postprocessor \n",
      "node_postprocessor = LongLLMLinguaPostprocessor(\n",
      "    instruction_str= \"Given the context, please answer the final question\" ,\n",
      "    target_token= 300 ,\n",
      "    rank_method= \"longllmlingua\" ,\n",
      "    additional_compress_kwargs={\n",
      "         \"condition_compare\" :  True ,\n",
      "         \"condition_in_question\" :  \"after\" ,\n",
      "         \"context_budget\" :  \"+100\" ,\n",
      "         \"reorder_context\" :  \"sort\" ,   # enable document reorder \n",
      "    },\n",
      ")\n",
      "\n",
      " # Define VectorStoreIndex \n",
      "documents = SimpleDirectoryReader(input_dir= \"...\" ).load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      " # Define QueryEngine \n",
      "retriever = index.as_retriever(similarity_top_k= 2 )\n",
      "retriever_query_engine = RetrieverQueryEngine.from_args(\n",
      "    retriever, node_postprocessors=[node_postprocessor]\n",
      ")\n",
      "\n",
      " # Used your advanced RAG \n",
      "response = retriever_query_engine.query( \"A user query\" ) 2. Result Re-Rank:  LLMs suffer from the so-called ‚ÄúLost in the Middle‚Äù phenomena which stipulates that LLMs focus on the extreme ends of the prompts. In light of this, it is beneficial to re-rank retrieved documents before passing them off to the Generation component. LlamaIndex Re-Ranking For Better Generation Recipe  ( notebook guide ) : import  os\n",
      " from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n",
      " from  llama_index.postprocessor.cohere_rerank  import  CohereRerank\n",
      " from  llama_index.postprocessor  import  LongLLMLinguaPostprocessor\n",
      "\n",
      " ### Recipe \n",
      " ### Define a Postprocessor object, here CohereRerank \n",
      " ### Build QueryEngine that uses this Postprocessor on retrieved docs \n",
      "\n",
      " # Build CohereRerank post retrieval processor \n",
      "api_key = os.environ[ \"COHERE_API_KEY\" ]\n",
      "cohere_rerank = CohereRerank(api_key=api_key, top_n= 2 )\n",
      "\n",
      " # Build QueryEngine (RAG) using the post processor \n",
      "documents = SimpleDirectoryReader( \"./data/paul_graham/\" ).load_data()\n",
      "index = VectorStoreIndex.from_documents(documents=documents)\n",
      "query_engine = index.as_query_engine(\n",
      "    similarity_top_k= 10 ,\n",
      "    node_postprocessors=[cohere_rerank],\n",
      ")\n",
      "\n",
      " # Use your advanced RAG \n",
      "response = query_engine.query(\n",
      "     \"What did Sam Altman do in this essay?\" \n",
      ") Advanced techniques for simultaneously addressing Retrieval and Generation success requirements In this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries). Generator-Enhanced Retrieval:  These techniques make use of the LLM‚Äôs inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response. LlamaIndex Generator-Enhanced Retrieval Recipe  ( notebook guide ) : from  llama_index.llms  import  OpenAI\n",
      " from  llama_index.query_engine  import  FLAREInstructQueryEngine\n",
      " from  llama_index  import  (\n",
      "    VectorStoreIndex,\n",
      "    SimpleDirectoryReader,\n",
      "    ServiceContext,\n",
      ")\n",
      " ### Recipe \n",
      " ### Build a FLAREInstructQueryEngine which has the generator LLM play \n",
      " ### a more active role in retrieval by prompting it to elicit retrieval \n",
      " ### instructions on what it needs to answer the user query. \n",
      "\n",
      " # Build FLAREInstructQueryEngine \n",
      "documents = SimpleDirectoryReader( \"./data/paul_graham\" ).load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "index_query_engine = index.as_query_engine(similarity_top_k= 2 )\n",
      "service_context = ServiceContext.from_defaults(llm=OpenAI(model= \"gpt-4\" ))\n",
      "flare_query_engine = FLAREInstructQueryEngine(\n",
      "    query_engine=index_query_engine,\n",
      "    service_context=service_context,\n",
      "    max_iterations= 7 ,\n",
      "    verbose= True ,\n",
      ")\n",
      "\n",
      " # Use your advanced RAG \n",
      "response = flare_query_engine.query(\n",
      "     \"Can you tell me about the author's trajectory in the startup world?\" \n",
      ") 2. Iterative Retrieval-Generator RAG:  For some complex cases, multi-step reasoning may be required to provide a useful and relevant answer to the user query. LlamaIndex Iterative Retrieval-Generator Recipe ( notebook guide ): from  llama_index.query_engine  import  RetryQueryEngine\n",
      " from  llama_index.evaluation  import  RelevancyEvaluator\n",
      "\n",
      " ### Recipe \n",
      " ### Build a RetryQueryEngine which performs retrieval-generation cycles \n",
      " ### until it either achieves a passing evaluation or a max number of \n",
      " ### cycles has been reached \n",
      "\n",
      " # Build RetryQueryEngine \n",
      "documents = SimpleDirectoryReader( \"./data/paul_graham\" ).load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "base_query_engine = index.as_query_engine()\n",
      "query_response_evaluator = RelevancyEvaluator()  # evaluator to critique  \n",
      "                                                 # retrieval-generation cycles \n",
      "retry_query_engine = RetryQueryEngine(\n",
      "    base_query_engine, query_response_evaluator\n",
      ")\n",
      "\n",
      " # Use your advanced rag \n",
      "retry_response = retry_query_engine.query( \"A user query\" ) Measurement Aspects of RAG Evaluating RAG systems are, of course, of utmost importance. In their survey paper, Gao, Yunfan et al. indicate 7 measurement aspects as seen in the top-right portion of the attached RAG cheat sheet. The llama-index library consists of several evaluation abstractions as well as integrations to RAGAs in order to help builders gain an understanding of the level to which their RAG system achieves the success requirements through the lens of these measurement aspects. Below, we list a select few of the evaluation notebook guides. Answer Relevancy and Context Relevancy Faithfulness Retrieval Evaluation Batch Evaluations with BatchEvalRunner You‚Äôre Now Equipped To Do Advanced RAG After reading this blog post, we hope that you feel more equipped and confident to apply some of these sophisticated techniques for building Advanced RAG systems!\n",
      "\n",
      "What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once. Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your  production  RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point? Thus, we started working on  Langfuse.com  ( GitHub ) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends‚Äô problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either  self-host  Langfuse or use the  cloud instance  maintained by us. We are thrilled to announce our new integration with LlamaIndex today. This feature was  highly requested  by our community and aligns with our project's focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase! The challenge We love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let‚Äôs take this simple example of a VectorStoreIndex and a ChatEngine. from  llama_index.core  import  SimpleDirectoryReader\n",
      " from  llama_index.core  import  VectorStoreIndex\n",
      "\n",
      "documents = SimpleDirectoryReader( \"./data\" ).load_data()\n",
      "\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "chat_engine = index.as_chat_engine()\n",
      "\n",
      " print (chat_engine.chat( \"What problems can I solve with RAG?\" ))\n",
      " print (chat_engine.chat( \"How do I optimize my RAG application?\" )) In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine. This is awesome to get started, but we quickly run into questions like: ‚ÄúWhat context is actually retrieved from the index to answer the questions?‚Äù ‚ÄúHow is chat memory managed?‚Äù ‚ÄúWhich steps add the most latency to the overall execution? How to optimize it?‚Äù One-click OSS observability to the rescue We integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager. Preparation Install the community package (pip install llama-index-callbacks-langfuse) Copy/paste the environment variables from the Langfuse project settings to your Python project: 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' and 'LANGFUSE_HOST' Now, you only need to set the global langfuse handler: from  llama_index.core  import  set_global_handler\n",
      "\n",
      "set_global_handler( \"langfuse\" ) And voil√°, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns. Group multiple chat threads into a session Working with lots of teams building GenAI/LLM/RAG applications, we‚Äôve continuously added more features that are useful to debug and improve these applications. One example is  session tracking  for conversational applications to see the traces in context of a full message thread. To activate it, just add an id that identifies the session as a trace param before calling the chat_engine. from  llama_index.core  import  global_handler\n",
      "\n",
      "global_handler.set_trace_params(\n",
      "  session_id= \"your-session-id\" \n",
      ")\n",
      "\n",
      "chat_engine.chat( \"What did he do growing up?\" )\n",
      "chat_engine.chat( \"What did he do at USC?\" )\n",
      "chat_engine.chat( \"How old is he?\" ) Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing: Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces. Trace more complex applications and use other Langfuse features for prompt management and evaluation This integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable. We have also built additional features to version control and collaborate on prompts (langfuse  prompt management ), track  experiments , and  evaluate  production traces. For RAG specifically, we collaborated with the RAGAS team and it‚Äôs easy to run their popular eval suite on traces captured with Langfuse (see  cookbook ). Get started The easiest way to get started is to follow the  cookbook  and check out the  docs . Feedback? \n",
      "\n",
      "Build ParamTuner object \n",
      " ### 3. Execute hyperparameter tuning with ParamTuner.tune() \n",
      "\n",
      " # 1. Define objective function \n",
      " def   objective_function ( params_dict ):\n",
      "    chunk_size = params_dict[ \"chunk_size\" ]\n",
      "    docs = params_dict[ \"docs\" ]\n",
      "    top_k = params_dict[ \"top_k\" ]\n",
      "    eval_qs = params_dict[ \"eval_qs\" ]\n",
      "    ref_response_strs = params_dict[ \"ref_response_strs\" ]\n",
      "\n",
      "     # build RAG pipeline \n",
      "    index = _build_index(chunk_size, docs)   # helper function not shown here \n",
      "    query_engine = index.as_query_engine(similarity_top_k=top_k)\n",
      "  \n",
      "     # perform inference with RAG pipeline on a provided questions `eval_qs` \n",
      "    pred_response_objs = get_responses(\n",
      "        eval_qs, query_engine, show_progress= True \n",
      "    )\n",
      "\n",
      "     # perform evaluations of predictions by comparing them to reference \n",
      "     # responses `ref_response_strs` \n",
      "    evaluator = SemanticSimilarityEvaluator(...)\n",
      "    eval_batch_runner = BatchEvalRunner(\n",
      "        { \"semantic_similarity\" : evaluator}, workers= 2 , show_progress= True \n",
      "    )\n",
      "    eval_results = eval_batch_runner.evaluate_responses(\n",
      "        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n",
      "    )\n",
      "\n",
      "     # get semantic similarity metric \n",
      "    mean_score = np.array(\n",
      "        [r.score  for  r  in  eval_results[ \"semantic_similarity\" ]]\n",
      "    ).mean()\n",
      "\n",
      "     return  RunResult(score=mean_score, params=params_dict)\n",
      "\n",
      " # 2. Build ParamTuner object \n",
      "param_dict = { \"chunk_size\" : [ 256 ,  512 ,  1024 ]}  # params/values to search over \n",
      "fixed_param_dict = {  # fixed hyperparams \n",
      "   \"top_k\" :  2 ,\n",
      "     \"docs\" : docs,\n",
      "     \"eval_qs\" : eval_qs[: 10 ],\n",
      "     \"ref_response_strs\" : ref_response_strs[: 10 ],\n",
      "}\n",
      "param_tuner = ParamTuner(\n",
      "    param_fn=objective_function,\n",
      "    param_dict=param_dict,\n",
      "    fixed_param_dict=fixed_param_dict,\n",
      "    show_progress= True ,\n",
      ")\n",
      "\n",
      " # 3. Execute hyperparameter search \n",
      "results = param_tuner.tune()\n",
      "best_result = results.best_run_result\n",
      "best_chunk_size = results.best_run_result.params[ \"chunk_size\" ] 2. Structured External Knowledge:  In complex scenarios, it may be necessary to build your external knowledge with a bit more structure than the basic vector index so as to permit recursive retrievals or routed retrieval when dealing with sensibly separated external knowledge sources. LlamaIndex Recursive Retrieval Recipe  ( notebook guide ) : from  llama_index  import  SimpleDirectoryReader, VectorStoreIndex\n",
      " from  llama_index.node_parser  import  SentenceSplitter\n",
      " from  llama_index.schema  import  IndexNode\n",
      "\n",
      " ### Recipe \n",
      " ### Build a recursive retriever that retrieves using small chunks \n",
      " ### but passes associated larger chunks to the generation stage \n",
      "\n",
      " # load data \n",
      "documents = SimpleDirectoryReader(\n",
      "  input_file= \"some_data_path/llama2.pdf\" \n",
      ").load_data()\n",
      "\n",
      " # build parent chunks via NodeParser \n",
      "node_parser = SentenceSplitter(chunk_size= 1024 )\n",
      "base_nodes = node_parser.get_nodes_from_documents(documents)\n",
      "\n",
      " # define smaller child chunks \n",
      "sub_chunk_sizes = [ 256 ,  512 ]\n",
      "sub_node_parsers = [\n",
      "    SentenceSplitter(chunk_size=c, chunk_overlap= 20 )  for  c  in  sub_chunk_sizes\n",
      "]\n",
      "all_nodes = []\n",
      " for  base_node  in  base_nodes:\n",
      "     for  n  in  sub_node_parsers:\n",
      "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
      "        sub_inodes = [\n",
      "            IndexNode.from_text_node(sn, base_node.node_id)  for  sn  in  sub_nodes\n",
      "        ]\n",
      "        all_nodes.extend(sub_inodes)\n",
      "     # also add original node to node \n",
      "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
      "    all_nodes.append(original_node)\n",
      "\n",
      " # define a VectorStoreIndex with all of the nodes \n",
      "vector_index_chunk = VectorStoreIndex(\n",
      "    all_nodes, service_context=service_context\n",
      ")\n",
      "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k= 2 )\n",
      "\n",
      " # build RecursiveRetriever \n",
      "all_nodes_dict = {n.node_id: n  for  n  in  all_nodes}\n",
      "retriever_chunk = RecursiveRetriever(\n",
      "     \"vector\" ,\n",
      "    retriever_dict={ \"vector\" : vector_retriever_chunk},\n",
      "    node_dict=all_nodes_dict,\n",
      "    verbose= True ,\n",
      ")\n",
      "\n",
      " # build RetrieverQueryEngine using recursive_retriever \n",
      "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
      "    retriever_chunk, service_context=service_context\n",
      ")\n",
      "\n",
      " # perform inference with advanced RAG (i.e. query engine) \n",
      "response = query_engine_chunk.query(\n",
      "     \"Can you tell me about the key concepts for safety finetuning\" \n",
      ") Other useful links We have several of guides demonstrating the application of other advanced techniques to help ensure accurate retrieval in complex cases. \n",
      "\n",
      "\n",
      "\n",
      "============Error #6=============\n",
      "\n",
      "\n",
      "Query:\n",
      "What is the purpose of the OpenAI Cookbook introduced by LlamaIndex?\n",
      "\n",
      "Expected Contexts:\n",
      "We appreciate your support and are always excited to see your projects and videos. Feel free to share them at  news@llamaindex.ai . Also, remember to subscribe to our newsletter on our  website  for the latest updates and to connect with our vibrant community. ü§©  First, the highlights: Launch of Seven Advanced Retrieval LlamaPacks : Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever.  Tweet . Introduction of the OpenAI Cookbook : A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation.  Blog ,  Notebook Speed Enhancement in Structured Metadata Extraction : Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance.  Docs ,  Tweet . We launched versions 3 of  RAGs , our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web.  \n",
      "\n",
      "Retrieved Contexts:\n",
      "Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex. Keep building with LlamaIndex!ü¶ô\n",
      "\n",
      "OpenAI Cookbook: Evaluating RAG systems\n",
      "We‚Äôre excited to unveil our  OpenAI Cookbook , a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you‚Äôll find it useful in enhancing the effectiveness of your RAG systems, and we‚Äôre thrilled to share it with you. The OpenAI Cookbook has three sections: Understanding Retrieval-Augmented Generation (RAG):  provides a detailed overview of RAG systems, including the various stages involved in building the RAG system. Building RAG with LlamaIndex:  Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham‚Äôs essay, utilizing the  VectorStoreIndex . Evaluating RAG with LlamaIndex:  The final section focuses on assessing the RAG system‚Äôs performance in two critical areas:  the Retrieval System  and  Response Generation. We use our unique synthetic dataset generation method,  generate_question_context_pairs  to conduct thorough evaluations in these areas. Our goal with this  cookbook  is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex. \n",
      "\n",
      "Introducing the new  llamaindex-cli  tool, installed when you  pip install llama-index  ! It uses  Chroma  under the hood, so you‚Äôll need to  pip install chromadb  as well. How to use it Set the  OPENAI_API_KEY  environment variable:  By default, this tool uses OpenAI‚Äôs API. As such, you‚Äôll need to ensure the OpenAI API Key is set under the  OPENAI_API_KEY  environment variable whenever you use the tool. $ export OPENAI_API_KEY=&lt;api_key&gt; 2. Ingest some files:  Now, you need to point the tool at some local files that it can ingest into the local vector database. For this example, we‚Äôll ingest the LlamaIndex  README.md  file: $ llamaindex-cli rag --files \"./README.md\" You can only specify a file glob pattern such as $ llamaindex-cli rag --files \"./docs/**/*.rst\" 3. Ask a Question : You can now start asking questions about any of the documents you‚Äôd ingested in the prior step: $ llamaindex-cli rag --question  \"What is LlamaIndex?\"  \n",
      "LlamaIndex is a data framework that helps in ingesting, structuring, and accessing  private  or domain-specific data  for  LLM-based applications. It provides tools such as data connectors to ingest data from various sources, data indexes to structure the data, and engines  for  natural language access to the data. LlamaIndex follows a Retrieval-Augmented  Generation   (RAG)  approach, where it retrieves information from data sources, adds it to the question as context, and then asks the LLM to generate an answer based on the enriched prompt. This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, up-to-date, and trustworthy solution  for  data augmentation. LlamaIndex is designed  for  both beginner and advanced users, with a high-level API  for  easy usage and lower-level APIs  for  customization and extension. 4. Open a Chat REPL : You can even open a chat interface within your terminal! Just run  llamaindex-cli rag --chat  and start asking questions about the files you‚Äôve ingested. Customize it to your heart‚Äôs content! You can customize  llamaindex-cli  to use any LLM model, even local models like Mixtral 8x7b through  Ollama , and you can build more advanced query and retrieval techniques.  Check the documentation  for details on how to get started.\n",
      "\n",
      "‚ú® Feature Releases and Enhancements: We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases.  Docs ,  Tweet . We have released a cookbook for latest Meta's new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools.  Docs ,  Tweet . We have introduced a template for integrating Meta's Llama 3 in create-llama. Simply run  npx create-llama  and select the  nextjs-llama3  template to build full-stack LLM application with Llama 3 in one CLI command.  Tweet . üé•¬†Demos: Open Source AI Diagram Generator  by  Rohan  using LlamaIndex's Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience. DREAM : A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD. Firecrawl  from  Mendable  is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing. üó∫Ô∏è Guides: Guide  to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI's Mixtral 8x7b, and our LlamaParse document parser. Guide  to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software. Guide  to Building a Code-Writing Agent:  TechWithTimm  demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code. Guide  to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex's finetuning techniques, including steps from quantization to fine-tuning with QLoRA. ‚úçÔ∏è Tutorials: Khye Wei's  tutorial  from Microsoft demonstrates how to use LlamaIndex with Azure's AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine. Hanane Dupouy 's  tutorial  on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex's ReAct agent and API abstractions. Andy Singal 's  tutorial  on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions. Mariboo‚Äôs  tutorial  on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetuning abstractions.\n",
      "\n",
      "index = connect_index(client)\n",
      "\n",
      "insert_nodes_index(index, nodes1) 5.  Define LLM # Deing LLM for query generation\n",
      "llm = OpenAI(model='gpt-4', temperature=0.1) 6.  Create Synthetic Queries We will create queries as discussed earlier, check prompts for each of the query types in the notebook, and code for each type of query. Showing code snippet for reference. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in retrieval_eval_irrelevance_df.reset_index(drop=True).iterrows():\n",
    "    print(f\"\\n\\n============Error #{i+1}=============\\n\\n\")\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    expected_contexts = [json.loads(record.payload['_node_content'])['text'] for record in qdrantdb.retrieve(cfg.db_collection, ids=row.expected_ids)]\n",
    "    expected_contexts = '\\n\\n'.join(expected_contexts)\n",
    "    print(f\"Expected Contexts:\\n{expected_contexts}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.retrieved_texts)\n",
    "    print(f\"Retrieved Contexts:\\n{contexts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10000842-dc34-405a-b11d-61669692bb28",
   "metadata": {},
   "source": [
    "## Response Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36e70076-e76f-4dda-ada2-4bbfa376b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run.eval import ResponseEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "846c1382-83e0-4b18-812b-683281890783",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluator = ResponseEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b50095-92ae-40b4-911b-3b46d2cd0cdb",
   "metadata": {},
   "source": [
    "### Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e41f2e3-408a-422f-840d-e870fc77f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 14:59:34.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.response\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mSampling 10 documents for response evaluation...\u001b[0m\n",
      "\u001b[32m2024-07-26 14:59:34.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.response\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mCreating new response eval dataset at data/010_remove_title_extractor/response_synthetic_eval_dataset.json...\u001b[0m\n",
      "\u001b[32m2024-07-26 14:59:34.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.response\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mCreating synthetic response eval dataset...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cd0f8465084daea32487e0990f909b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.58s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.60s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.79s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.66s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.80s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.26s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.17s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.58s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.69s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.64s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.17s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.79s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.14s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.37s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.84s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.13it/s]\n",
      "\u001b[32m2024-07-26 15:00:25.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.response\u001b[0m:\u001b[36mgenerate_synthetic_dataset\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mPersisting synthetic response eval dataset at data/010_remove_title_extractor/response_synthetic_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response_eval_documents, response_synthetic_eval_dataset = response_evaluator.generate_synthetic_dataset(cfg, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc890512-fffa-4a51-b0fc-06f14713df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:13<00:00,  1.17it/s]\n",
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:10<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "response_synthetic_eval_prediction_dataset = await response_synthetic_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=cfg.batch_size, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66750873-a858-4ac1-8796-ffb7a3d83801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:22<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "response_synthetic_mean_scores_df, response_synthetic_deep_eval_df = response_evaluator.evaluate_labelled_rag_dataset(\n",
    "    response_synthetic_eval_dataset,\n",
    "    response_synthetic_eval_prediction_dataset,\n",
    "    dataset_name=\"synthetic\",\n",
    "    judge_model=cfg.eval_cfg.response_eval_llm_model,\n",
    "    cache_dp=cfg.notebook_cache_dp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e310c724-0ad9-487c-bf23-f5ae6669a6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score   4.051724\n",
       "mean_relevancy_score     0.896552\n",
       "mean_faithfulness_score  0.931034"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_synthetic_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cbd0ba7-7d19-423d-86cf-37534d361215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the property graph index differ from ...</td>\n",
       "      <td>\\nThe property graph index differs from the pr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[In the previous integration, the graph was re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of using the SchemaLLMPath...</td>\n",
       "      <td>\\nThe purpose of using the SchemaLLMPathExtrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing the Property Graph Index: A Power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can parallelization be used to accelerate ...</td>\n",
       "      <td>\\nRay Datasets can be used to parallelize buil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Scale a Powerful Query Engine with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you adjust the similarity_threshold an...</td>\n",
       "      <td>\\nYou can adjust the similarity_threshold and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Note that extracting 250 articles takes about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What methods are used in the custom retriever ...</td>\n",
       "      <td>\\nThe custom retriever uses the `entity_extrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, let‚Äôs examine our retriever options. At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the OpenAI Cookbook aim to enhance th...</td>\n",
       "      <td>\\nThe OpenAI Cookbook aims to enhance the effe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What tools were used to build the multi-modal ...</td>\n",
       "      <td>\\nThe tools used to build the multi-modal prot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Here we go over a use case of querying Tesla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How is the sidebar enhanced to improve credibi...</td>\n",
       "      <td>\\nThe sidebar is enhanced to improve credibili...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[User Interface Next, we craft the sidebar and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the application handle user messages ...</td>\n",
       "      <td>\\nThe application handles user messages by lim...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Your responses always descriptive. \" \\n      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How has the field of AI and large language mod...</td>\n",
       "      <td>\\nThe field of AI and large language models ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Give a followup of each key point with an exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of using Medium articles f...</td>\n",
       "      <td>\\nThe purpose of using Medium articles from 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Image by author. Articles are first split int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How does the MultiModalVectorStoreIndex store ...</td>\n",
       "      <td>\\nThe MultiModalVectorStoreIndex stores and in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We could use a vision model to filter out irr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What new memory modules have been introduced i...</td>\n",
       "      <td>\\nThe new memory modules introduced in LlamaIn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2024-06-11\\nHello Llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of using LLMs for retrieva...</td>\n",
       "      <td>\\nThe purpose of using LLMs for retrieval and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the two ways of feeding text into the...</td>\n",
       "      <td>\\nDocument 1:\\nYou can directly feed in the ra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[A question is also provided.\\n  Respond with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the benefit of using a two-stage retri...</td>\n",
       "      <td>\\nIt can return more relevant documents than e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the length of the 2021 Lyft SEC 10-K d...</td>\n",
       "      <td>\\nAccording to the text, the 2021 SEC 10-K fil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[It goes into details about the US-China trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of Tonic Validate in relat...</td>\n",
       "      <td>\\nTonic Validate is a RAG benchmarking and eva...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate x LlamaIndex: Implementing int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What steps are involved in setting up Tonic Va...</td>\n",
       "      <td>\\nTo set up Tonic Validate, you need to instal...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This creative approach to funding allowed the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the purpose of setting up Tonic Validate?</td>\n",
       "      <td>\\nThe purpose of setting up Tonic Validate is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[As is common in modern software development p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does Tonic Validate score the LLM responses?</td>\n",
       "      <td>\\nThe LLM responses are scored using a set of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I highly recommend utilizing the UI to make v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the recommended approach for adding QA...</td>\n",
       "      <td>\\nOne approach for adding QA testing to a RAG ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[As is common in modern software development p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What challenges do enterprises face when adopt...</td>\n",
       "      <td>\\nEnterprises face challenges around protectin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of the Llama Datasets coll...</td>\n",
       "      <td>\\nThe purpose of the Llama Datasets collection...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Two new llama-datasets and a Gemini vs. GPT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What new offering integrates local LLMs and em...</td>\n",
       "      <td>\\nLlamaIndex.\\n\\n\\nSources:\\n- [Multimodal RAG...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Image input to LLM. The LLM got two identical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Zephyr 7b LLM address the obstacles f...</td>\n",
       "      <td>\\nZephyr 7b LLM addresses the obstacles faced ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Becoming Proficient in Document Extraction\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the integration of Zephyr and LlamaIn...</td>\n",
       "      <td>\\nThe integration of Zephyr and LlamaIndex enh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does a tree index organize data?</td>\n",
       "      <td>\\nA tree index organizes data in a hierarchica...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[tree index is a data structure that organizes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How can one explore the author's Huggingface p...</td>\n",
       "      <td>\\nTo explore the author's Hugging Face profile...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Additionally, export traces for evaluations a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   How does the property graph index differ from ...   \n",
       "1   What is the purpose of using the SchemaLLMPath...   \n",
       "2   How can parallelization be used to accelerate ...   \n",
       "3   How can you adjust the similarity_threshold an...   \n",
       "4   What methods are used in the custom retriever ...   \n",
       "5   How does the OpenAI Cookbook aim to enhance th...   \n",
       "6   What tools were used to build the multi-modal ...   \n",
       "7   How is the sidebar enhanced to improve credibi...   \n",
       "8   How does the application handle user messages ...   \n",
       "9   How has the field of AI and large language mod...   \n",
       "10  What is the purpose of using Medium articles f...   \n",
       "11  How does the MultiModalVectorStoreIndex store ...   \n",
       "12  What new memory modules have been introduced i...   \n",
       "13  What is the purpose of using LLMs for retrieva...   \n",
       "14  What are the two ways of feeding text into the...   \n",
       "15  What is the benefit of using a two-stage retri...   \n",
       "16  What is the length of the 2021 Lyft SEC 10-K d...   \n",
       "17  What is the purpose of Tonic Validate in relat...   \n",
       "18  What steps are involved in setting up Tonic Va...   \n",
       "19  What is the purpose of setting up Tonic Validate?   \n",
       "20   How does Tonic Validate score the LLM responses?   \n",
       "21  What is the recommended approach for adding QA...   \n",
       "22  What challenges do enterprises face when adopt...   \n",
       "23  What is the purpose of the Llama Datasets coll...   \n",
       "24  What new offering integrates local LLMs and em...   \n",
       "25  How does Zephyr 7b LLM address the obstacles f...   \n",
       "26  How does the integration of Zephyr and LlamaIn...   \n",
       "27               How does a tree index organize data?   \n",
       "28  How can one explore the author's Huggingface p...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "0   \\nThe property graph index differs from the pr...              1.0   \n",
       "1   \\nThe purpose of using the SchemaLLMPathExtrac...              1.0   \n",
       "2   \\nRay Datasets can be used to parallelize buil...              1.0   \n",
       "3   \\nYou can adjust the similarity_threshold and ...              1.0   \n",
       "4   \\nThe custom retriever uses the `entity_extrac...              1.0   \n",
       "5   \\nThe OpenAI Cookbook aims to enhance the effe...              1.0   \n",
       "6   \\nThe tools used to build the multi-modal prot...              1.0   \n",
       "7   \\nThe sidebar is enhanced to improve credibili...              1.0   \n",
       "8   \\nThe application handles user messages by lim...              1.0   \n",
       "9   \\nThe field of AI and large language models ha...              1.0   \n",
       "10  \\nThe purpose of using Medium articles from 20...              1.0   \n",
       "11  \\nThe MultiModalVectorStoreIndex stores and in...              1.0   \n",
       "12  \\nThe new memory modules introduced in LlamaIn...              1.0   \n",
       "13  \\nThe purpose of using LLMs for retrieval and ...              1.0   \n",
       "14  \\nDocument 1:\\nYou can directly feed in the ra...              1.0   \n",
       "15  \\nIt can return more relevant documents than e...              1.0   \n",
       "16  \\nAccording to the text, the 2021 SEC 10-K fil...              1.0   \n",
       "17  \\nTonic Validate is a RAG benchmarking and eva...              1.0   \n",
       "18  \\nTo set up Tonic Validate, you need to instal...              1.0   \n",
       "19  \\nThe purpose of setting up Tonic Validate is ...              1.0   \n",
       "20  \\nThe LLM responses are scored using a set of ...              1.0   \n",
       "21  \\nOne approach for adding QA testing to a RAG ...              1.0   \n",
       "22  \\nEnterprises face challenges around protectin...              1.0   \n",
       "23  \\nThe purpose of the Llama Datasets collection...              1.0   \n",
       "24  \\nLlamaIndex.\\n\\n\\nSources:\\n- [Multimodal RAG...              0.0   \n",
       "25  \\nZephyr 7b LLM addresses the obstacles faced ...              1.0   \n",
       "26  \\nThe integration of Zephyr and LlamaIndex enh...              1.0   \n",
       "27  \\nA tree index organizes data in a hierarchica...              0.0   \n",
       "28  \\nTo explore the author's Hugging Face profile...              0.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "0                 4.5                 1.0   \n",
       "1                 4.5                 1.0   \n",
       "2                 4.0                 1.0   \n",
       "3                 4.5                 1.0   \n",
       "4                 4.0                 1.0   \n",
       "5                 4.5                 1.0   \n",
       "6                 3.0                 1.0   \n",
       "7                 4.5                 1.0   \n",
       "8                 4.5                 0.0   \n",
       "9                 4.0                 1.0   \n",
       "10                4.5                 1.0   \n",
       "11                4.5                 1.0   \n",
       "12                4.5                 1.0   \n",
       "13                4.5                 1.0   \n",
       "14                4.5                 0.0   \n",
       "15                4.0                 1.0   \n",
       "16                2.0                 1.0   \n",
       "17                4.5                 1.0   \n",
       "18                4.0                 1.0   \n",
       "19                3.0                 1.0   \n",
       "20                3.0                 1.0   \n",
       "21                4.5                 1.0   \n",
       "22                5.0                 1.0   \n",
       "23                4.0                 1.0   \n",
       "24                3.0                 1.0   \n",
       "25                4.5                 1.0   \n",
       "26                4.5                 1.0   \n",
       "27                4.5                 1.0   \n",
       "28                2.5                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "0   [In the previous integration, the graph was re...  \n",
       "1   [Introducing the Property Graph Index: A Power...  \n",
       "2   [Build and Scale a Powerful Query Engine with ...  \n",
       "3   [Note that extracting 250 articles takes about...  \n",
       "4   [Now, let‚Äôs examine our retriever options. At ...  \n",
       "5   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "6   [Here we go over a use case of querying Tesla ...  \n",
       "7   [User Interface Next, we craft the sidebar and...  \n",
       "8   [Your responses always descriptive. \" \\n      ...  \n",
       "9   [Give a followup of each key point with an exp...  \n",
       "10  [Image by author. Articles are first split int...  \n",
       "11  [We could use a vision model to filter out irr...  \n",
       "12  [LlamaIndex Newsletter 2024-06-11\\nHello Llama...  \n",
       "13  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "14  [A question is also provided.\\n  Respond with ...  \n",
       "15  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "16  [It goes into details about the US-China trade...  \n",
       "17  [Tonic Validate x LlamaIndex: Implementing int...  \n",
       "18  [This creative approach to funding allowed the...  \n",
       "19  [As is common in modern software development p...  \n",
       "20  [I highly recommend utilizing the UI to make v...  \n",
       "21  [As is common in modern software development p...  \n",
       "22  [LlamaIndex Accelerates Enterprise Generative ...  \n",
       "23  [Two new llama-datasets and a Gemini vs. GPT s...  \n",
       "24  [Image input to LLM. The LLM got two identical...  \n",
       "25  [Becoming Proficient in Document Extraction\\nI...  \n",
       "26  [Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...  \n",
       "27  [tree index is a data structure that organizes...  \n",
       "28  [Additionally, export traces for evaluations a...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_synthetic_deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c4b25-7ac9-438c-9c01-9a36f2fd51c3",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46ea4c5b-6090-42a5-807f-deeca33adeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthetic_eval_dataset_dict = dict()\n",
    "for example in response_synthetic_eval_dataset.examples:\n",
    "    response_synthetic_eval_dataset_dict[example.query] = {\n",
    "        \"reference_answer\": example.reference_answer,\n",
    "        \"reference_contexts\": example.reference_contexts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "077c58de-0b57-4987-8974-ab1383d5f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How can one explore the author's Huggingface p...</td>\n",
       "      <td>\\nTo explore the author's Hugging Face profile...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Additionally, export traces for evaluations a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What new offering integrates local LLMs and em...</td>\n",
       "      <td>\\nLlamaIndex.\\n\\n\\nSources:\\n- [Multimodal RAG...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Image input to LLM. The LLM got two identical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does a tree index organize data?</td>\n",
       "      <td>\\nA tree index organizes data in a hierarchica...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[tree index is a data structure that organizes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the length of the 2021 Lyft SEC 10-K d...</td>\n",
       "      <td>\\nAccording to the text, the 2021 SEC 10-K fil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[It goes into details about the US-China trade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What tools were used to build the multi-modal ...</td>\n",
       "      <td>\\nThe tools used to build the multi-modal prot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Here we go over a use case of querying Tesla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the purpose of setting up Tonic Validate?</td>\n",
       "      <td>\\nThe purpose of setting up Tonic Validate is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[As is common in modern software development p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How does Tonic Validate score the LLM responses?</td>\n",
       "      <td>\\nThe LLM responses are scored using a set of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[I highly recommend utilizing the UI to make v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can parallelization be used to accelerate ...</td>\n",
       "      <td>\\nRay Datasets can be used to parallelize buil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build and Scale a Powerful Query Engine with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What methods are used in the custom retriever ...</td>\n",
       "      <td>\\nThe custom retriever uses the `entity_extrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Now, let‚Äôs examine our retriever options. At ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How has the field of AI and large language mod...</td>\n",
       "      <td>\\nThe field of AI and large language models ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Give a followup of each key point with an exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the benefit of using a two-stage retri...</td>\n",
       "      <td>\\nIt can return more relevant documents than e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What steps are involved in setting up Tonic Va...</td>\n",
       "      <td>\\nTo set up Tonic Validate, you need to instal...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[This creative approach to funding allowed the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of the Llama Datasets coll...</td>\n",
       "      <td>\\nThe purpose of the Llama Datasets collection...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Two new llama-datasets and a Gemini vs. GPT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the application handle user messages ...</td>\n",
       "      <td>\\nThe application handles user messages by lim...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Your responses always descriptive. \" \\n      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the two ways of feeding text into the...</td>\n",
       "      <td>\\nDocument 1:\\nYou can directly feed in the ra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[A question is also provided.\\n  Respond with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the property graph index differ from ...</td>\n",
       "      <td>\\nThe property graph index differs from the pr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[In the previous integration, the graph was re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of using the SchemaLLMPath...</td>\n",
       "      <td>\\nThe purpose of using the SchemaLLMPathExtrac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing the Property Graph Index: A Power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you adjust the similarity_threshold an...</td>\n",
       "      <td>\\nYou can adjust the similarity_threshold and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Note that extracting 250 articles takes about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the OpenAI Cookbook aim to enhance th...</td>\n",
       "      <td>\\nThe OpenAI Cookbook aims to enhance the effe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How is the sidebar enhanced to improve credibi...</td>\n",
       "      <td>\\nThe sidebar is enhanced to improve credibili...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[User Interface Next, we craft the sidebar and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of using Medium articles f...</td>\n",
       "      <td>\\nThe purpose of using Medium articles from 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Image by author. Articles are first split int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How does the MultiModalVectorStoreIndex store ...</td>\n",
       "      <td>\\nThe MultiModalVectorStoreIndex stores and in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[We could use a vision model to filter out irr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What new memory modules have been introduced i...</td>\n",
       "      <td>\\nThe new memory modules introduced in LlamaIn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Newsletter 2024-06-11\\nHello Llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of using LLMs for retrieva...</td>\n",
       "      <td>\\nThe purpose of using LLMs for retrieval and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Using LLM‚Äôs for Retrieval and Reranking\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of Tonic Validate in relat...</td>\n",
       "      <td>\\nTonic Validate is a RAG benchmarking and eva...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Tonic Validate x LlamaIndex: Implementing int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the recommended approach for adding QA...</td>\n",
       "      <td>\\nOne approach for adding QA testing to a RAG ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[As is common in modern software development p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Zephyr 7b LLM address the obstacles f...</td>\n",
       "      <td>\\nZephyr 7b LLM addresses the obstacles faced ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Becoming Proficient in Document Extraction\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>How does the integration of Zephyr and LlamaIn...</td>\n",
       "      <td>\\nThe integration of Zephyr and LlamaIndex enh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What challenges do enterprises face when adopt...</td>\n",
       "      <td>\\nEnterprises face challenges around protectin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[LlamaIndex Accelerates Enterprise Generative ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "28  How can one explore the author's Huggingface p...   \n",
       "24  What new offering integrates local LLMs and em...   \n",
       "27               How does a tree index organize data?   \n",
       "16  What is the length of the 2021 Lyft SEC 10-K d...   \n",
       "6   What tools were used to build the multi-modal ...   \n",
       "19  What is the purpose of setting up Tonic Validate?   \n",
       "20   How does Tonic Validate score the LLM responses?   \n",
       "2   How can parallelization be used to accelerate ...   \n",
       "4   What methods are used in the custom retriever ...   \n",
       "9   How has the field of AI and large language mod...   \n",
       "15  What is the benefit of using a two-stage retri...   \n",
       "18  What steps are involved in setting up Tonic Va...   \n",
       "23  What is the purpose of the Llama Datasets coll...   \n",
       "8   How does the application handle user messages ...   \n",
       "14  What are the two ways of feeding text into the...   \n",
       "0   How does the property graph index differ from ...   \n",
       "1   What is the purpose of using the SchemaLLMPath...   \n",
       "3   How can you adjust the similarity_threshold an...   \n",
       "5   How does the OpenAI Cookbook aim to enhance th...   \n",
       "7   How is the sidebar enhanced to improve credibi...   \n",
       "10  What is the purpose of using Medium articles f...   \n",
       "11  How does the MultiModalVectorStoreIndex store ...   \n",
       "12  What new memory modules have been introduced i...   \n",
       "13  What is the purpose of using LLMs for retrieva...   \n",
       "17  What is the purpose of Tonic Validate in relat...   \n",
       "21  What is the recommended approach for adding QA...   \n",
       "25  How does Zephyr 7b LLM address the obstacles f...   \n",
       "26  How does the integration of Zephyr and LlamaIn...   \n",
       "22  What challenges do enterprises face when adopt...   \n",
       "\n",
       "                                               answer  relevancy_score  \\\n",
       "28  \\nTo explore the author's Hugging Face profile...              0.0   \n",
       "24  \\nLlamaIndex.\\n\\n\\nSources:\\n- [Multimodal RAG...              0.0   \n",
       "27  \\nA tree index organizes data in a hierarchica...              0.0   \n",
       "16  \\nAccording to the text, the 2021 SEC 10-K fil...              1.0   \n",
       "6   \\nThe tools used to build the multi-modal prot...              1.0   \n",
       "19  \\nThe purpose of setting up Tonic Validate is ...              1.0   \n",
       "20  \\nThe LLM responses are scored using a set of ...              1.0   \n",
       "2   \\nRay Datasets can be used to parallelize buil...              1.0   \n",
       "4   \\nThe custom retriever uses the `entity_extrac...              1.0   \n",
       "9   \\nThe field of AI and large language models ha...              1.0   \n",
       "15  \\nIt can return more relevant documents than e...              1.0   \n",
       "18  \\nTo set up Tonic Validate, you need to instal...              1.0   \n",
       "23  \\nThe purpose of the Llama Datasets collection...              1.0   \n",
       "8   \\nThe application handles user messages by lim...              1.0   \n",
       "14  \\nDocument 1:\\nYou can directly feed in the ra...              1.0   \n",
       "0   \\nThe property graph index differs from the pr...              1.0   \n",
       "1   \\nThe purpose of using the SchemaLLMPathExtrac...              1.0   \n",
       "3   \\nYou can adjust the similarity_threshold and ...              1.0   \n",
       "5   \\nThe OpenAI Cookbook aims to enhance the effe...              1.0   \n",
       "7   \\nThe sidebar is enhanced to improve credibili...              1.0   \n",
       "10  \\nThe purpose of using Medium articles from 20...              1.0   \n",
       "11  \\nThe MultiModalVectorStoreIndex stores and in...              1.0   \n",
       "12  \\nThe new memory modules introduced in LlamaIn...              1.0   \n",
       "13  \\nThe purpose of using LLMs for retrieval and ...              1.0   \n",
       "17  \\nTonic Validate is a RAG benchmarking and eva...              1.0   \n",
       "21  \\nOne approach for adding QA testing to a RAG ...              1.0   \n",
       "25  \\nZephyr 7b LLM addresses the obstacles faced ...              1.0   \n",
       "26  \\nThe integration of Zephyr and LlamaIndex enh...              1.0   \n",
       "22  \\nEnterprises face challenges around protectin...              1.0   \n",
       "\n",
       "    correctness_score  faithfulness_score  \\\n",
       "28                2.5                 1.0   \n",
       "24                3.0                 1.0   \n",
       "27                4.5                 1.0   \n",
       "16                2.0                 1.0   \n",
       "6                 3.0                 1.0   \n",
       "19                3.0                 1.0   \n",
       "20                3.0                 1.0   \n",
       "2                 4.0                 1.0   \n",
       "4                 4.0                 1.0   \n",
       "9                 4.0                 1.0   \n",
       "15                4.0                 1.0   \n",
       "18                4.0                 1.0   \n",
       "23                4.0                 1.0   \n",
       "8                 4.5                 0.0   \n",
       "14                4.5                 0.0   \n",
       "0                 4.5                 1.0   \n",
       "1                 4.5                 1.0   \n",
       "3                 4.5                 1.0   \n",
       "5                 4.5                 1.0   \n",
       "7                 4.5                 1.0   \n",
       "10                4.5                 1.0   \n",
       "11                4.5                 1.0   \n",
       "12                4.5                 1.0   \n",
       "13                4.5                 1.0   \n",
       "17                4.5                 1.0   \n",
       "21                4.5                 1.0   \n",
       "25                4.5                 1.0   \n",
       "26                4.5                 1.0   \n",
       "22                5.0                 1.0   \n",
       "\n",
       "                                             contexts  \n",
       "28  [Additionally, export traces for evaluations a...  \n",
       "24  [Image input to LLM. The LLM got two identical...  \n",
       "27  [tree index is a data structure that organizes...  \n",
       "16  [It goes into details about the US-China trade...  \n",
       "6   [Here we go over a use case of querying Tesla ...  \n",
       "19  [As is common in modern software development p...  \n",
       "20  [I highly recommend utilizing the UI to make v...  \n",
       "2   [Build and Scale a Powerful Query Engine with ...  \n",
       "4   [Now, let‚Äôs examine our retriever options. At ...  \n",
       "9   [Give a followup of each key point with an exp...  \n",
       "15  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "18  [This creative approach to funding allowed the...  \n",
       "23  [Two new llama-datasets and a Gemini vs. GPT s...  \n",
       "8   [Your responses always descriptive. \" \\n      ...  \n",
       "14  [A question is also provided.\\n  Respond with ...  \n",
       "0   [In the previous integration, the graph was re...  \n",
       "1   [Introducing the Property Graph Index: A Power...  \n",
       "3   [Note that extracting 250 articles takes about...  \n",
       "5   [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "7   [User Interface Next, we craft the sidebar and...  \n",
       "10  [Image by author. Articles are first split int...  \n",
       "11  [We could use a vision model to filter out irr...  \n",
       "12  [LlamaIndex Newsletter 2024-06-11\\nHello Llama...  \n",
       "13  [Using LLM‚Äôs for Retrieval and Reranking\\nSumm...  \n",
       "17  [Tonic Validate x LlamaIndex: Implementing int...  \n",
       "21  [As is common in modern software development p...  \n",
       "25  [Becoming Proficient in Document Extraction\\nI...  \n",
       "26  [Multilingual Proficiency: \\n \\n \\n  Zephyr 7b...  \n",
       "22  [LlamaIndex Accelerates Enterprise Generative ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_synthetic_deep_eval_df.sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b90ac3d-24c0-43af-a199-44a16683f986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============Error #1===============\n",
      "\n",
      "\n",
      "Query:\n",
      "How can one explore the author's Huggingface profile?\n",
      "\n",
      "Context:\n",
      "Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local.  Notebook ,  Tweet . RetrieverEvaluator : new in the library, ‚ÄúRetrieverEvaluator‚Äù allows enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments.  Docs ,  Tweet . HuggingFace Embeddings : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format.  Docs ,  Tweet . Multi-Document Agents : we‚Äôve introduced v0 experimental support for multi-document agents for advanced QA, beyond typical top-k RAG. It supports diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency.  Docs ,  Tweet . üèÜ  Congratulations to our Streamlit Hackathon Winners! We love seeing people build amazing things with LlamaIndex! NewsGPT by Kang-Chi Ho:  https://buff.ly/46jkutx FinSight by Vishwas Gowda:  https://buff.ly/3PzOnyC ‚ú®  Feature Releases   and Enhancements: Multi-Document Agents : we introduced multi-document agents (V0) for advanced QA, beyond typical top-k RAG. They support diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency.  Docs ,  Tweet . Ensemble Retriever:  we‚Äôre addressing the RAG challenge of determining chunk size by experimenting with diverse document chunking and ensembling for retrieval.  Docs ,  Tweet . HuggingFace Embeddings : we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format.  Docs ,  Tweet . OpenAI Function Calling fine-tuning:  we‚Äôre using OpenAI‚Äôs latest function calling fine-tuning which enhanced structured data extraction, optimizing gpt-3.5-turbo for improved extraction in RAG.  Docs ,  Tweet . Metadata Extraction : we‚Äôre making metadata extraction efficient by extracting a complete Pydantic object from a document with just one LLM call.  Docs ,  Tweet . Structured RAG Outputs : we now efficiently structure RAG pipeline outputs with native Pydantic outputs from all queries without the need for an additional LLM parsing call.  Docs ,  Tweet . Streamlined  secinsights.ai  deployment : Our open-sourced  secinsights.ai  offers a RAG app template, now enhanced with GitHub Codespaces and Docker for swift cloud deployment without setup hassles.  Tweet . LongContextReorder:  We introduced LongContextReorder****,**** Zeneto‚Äôs approach to reposition vital context in RAG systems, addressing the challenge of over-retrieving which can obscure essential details.  Docs ,  Tweet . RA-DIT:  We drew inspiration from the RA-DIT paper, which introduced LLM fine-tuning for retrieval-augmented input prompts to improve RAG systems. This method fosters enhanced utilization of context and more effective answer synthesis, even in the presence of suboptimal context.  \n",
      "\n",
      "Download a llamafile 2. Make the llamafile executable We'll go through each step in detail below. Step 1: Download a llamafile There are many llamafiles available on the  HuggingFace model hub  (just search for 'llamafile') but for the purpose of this walkthrough, we'll use  TinyLlama-1.1B  (0.67 GB,  model info ). To download the model, you can either click this download link:  TinyLlama-1.1B  or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection. wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile  This model is small and won't be very good at actually answering questions but, since it's a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it's good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like  Mistral-7B-Instruct  (5.15 GB,  model info ). Step 2: Make the llamafile executable If you didn't download the llamafile from the command line, figure out where your browser stored your downloaded llamafile. Now, open your computer's terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile` If you're using macOS, Linux, or BSD , you'll need to grant permission for your computer to execute this new file. (You only need to do this once.): If you're on Windows, instead just rename the file by adding \".exe\" on the end  e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe` chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile Kick the tires Now, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded: ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n",
      "\n",
      "llamafile v0.7.0 This post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don't work as expected, please  post an issue on the llamafile issue tracker . The easiest way to use your llamafile is via its built-in chat interface. In a terminal, run ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile Your browser should open automatically and display a chat interface. (If it doesn't, just open your browser and point it at http://localhost:8080). When you're done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you're running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile. In the rest of this walkthrough, we'll be using the llamafile's built-in inference server instead of the browser interface. The llamafile's server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available  here . To start the llamafile in server mode, run: ./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding Summary: Download and run a llamafile # 1. Download the llamafile-ized model \n",
      "wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n",
      "\n",
      " # 2. Make it executable (you only need to do this once) \n",
      " chmod  +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n",
      "\n",
      " # 3. Run in server mode \n",
      "./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding Build a research assistant using LlamaIndex and llamafile Now, we'll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose  homing pigeons . We'll show how to prepare your data, index into a vector store, then query it. \n",
      "\n",
      "Answer:\n",
      "\n",
      "To explore the author's Hugging Face profile, you can visit the Hugging Face model hub and search for the author's name. You can also check the model's documentation and tweet links provided in the context information to get more information about the author's work and contributions to the LlamaIndex project.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [LlamaIndex update 2023‚Äì10‚Äì10](https://www.llamaindex.ai/blog/llamaindex-update-2023-10-10-3718a3d19fb9)\n",
      "- [Using LlamaIndex and llamafile to build a local, private research assistant](https://www.llamaindex.ai/blog/using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "To explore the author's Huggingface profile, you can visit the following link: https://huggingface.co/Andyrasika.\n",
      "\n",
      "Expected Contexts:\n",
      "['Hugging Face :\\n    For natural language processing and AI-related projects, you can explore my\\n    Huggingface profile at\\n     https://huggingface.co/Andyrasika .\\n   \\n   \\n    YouTube: To watch my video content, visit my YouTube channel at\\n     https://www.youtube.com/@andy111007 .\\n   \\n   \\n    LinkedIn: To stay updated on my latest projects and posts, you can follow me\\n    on LinkedIn. Here is the link to my profile:\\n     https://www.linkedin.com/in/ankushsingal/.\" \\n   \\n \\n \\n  Requests and questions: If you have a project in mind that you‚Äôd like me to\\n  work on or if you have any questions about the concepts I‚Äôve explained, don‚Äôt\\n  hesitate to let me know. I‚Äôm always looking for new ideas for future Notebooks\\n  and I love helping to resolve any doubts you might have.\\n \\n \\n  Remember, each ‚ÄúLike‚Äù, ‚ÄúShare‚Äù, and ‚ÄúStar‚Äù greatly contributes to my work and\\n  motivates me to continue producing more quality content. Thank you for your\\n  support!\\n \\n \\n  If you enjoyed this story, feel free\\n   to subscribe \\n  to Medium, and you will get notifications when my new articles will be\\n  published, as well as full access to thousands of stories from other authors.\\n \\n Resource: \\n \\n   \\n     Data used for above code \\n   \\n   \\n     llama-index']\n",
      "\n",
      "\n",
      "\n",
      "==============Error #2===============\n",
      "\n",
      "\n",
      "Query:\n",
      "What new offering integrates local LLMs and embeddings into a fully local RAG pipeline?\n",
      "\n",
      "Context:\n",
      "Image input to LLM. The LLM got two identical images as input, which just shows that I reuse some of my diagrams. However, I am pleasantly surprised by CLIP embeddings as they were able to retrieve he most relevant image out of the collection. In a more production setting, you might want to clean and deduplicate images, but that is beyond the scope of this article. Conclusion LLMs are evolving faster than what we are historically used to and are spanning across multiple modalities. I firmly believe that by the end of the next year, LLMs will be soon able to comprehend videos, and be therefore able to pick up non-verbal cues while talking to you. On the other hand, we can use images as input to RAG pipeline and enhance the variety of information passed to an LLM, making responses better and more accurate. The multimodal RAG pipelines implementation with LlamaIndex and Neo4j is as easy as it gets. The code is available on  GitHub .\n",
      "\n",
      "Fine-Tuning Embeddings for RAG with Synthetic Data\n",
      "UPDATE 9/10/2023:  We‚Äôve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our  embedding fine-tuning guides  in the core documentation. We‚Äôve created a  comprehensive, end-to-end guide  showing you how to fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!). The result is a  5‚Äì10% performance increase in retrieval evaluation metrics  ‚Äî our finetuned  bge  model almost reaches  text-embedding-ada-002  levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole. This tutorial is helpful to  anyone  building RAG systems: If you‚Äôre new to finetuning, no problem! We have  step by step notebooks  walking through the key steps. Simply substitute the file links for your own data, and just run every cell. Finetuning embedding models is lightweight and doesn‚Äôt require a GPU. These notebooks were tested on an M2 Macbook Pro. Resources Repo:  https://github.com/run-llama/finetune-embedding Notebooks:  Dataset Generation ,  Finetuning ,  Evaluation Background/Context The Current RAG Stack RAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not present in its training corpus. It pairs a  retrieval model  over a knowledge bank with the LLM through its input prompt space. RAG stacks typically look like the following: Indexing : Prepare a corpus of unstructured text, parse/chunk it. Then  embed  each chunk and put in a vector database. Query-time:  Retrieve  context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space. (Of course RAG can be much more advanced than this, and LlamaIndex provides tools for both  simple and advanced RAG ) Unfortunately RAG is easy to prototype by cobbling together the different components, but hard to productionize. The simple stack has many failure modes and oftentimes the issue lies with bad retrieval ‚Äî if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad. How Can We Make Retrieval Better? \n",
      "\n",
      "Answer:\n",
      "\n",
      "LlamaIndex.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Multimodal RAG pipeline with LlamaIndex and Neo4j](https://www.llamaindex.ai/blog/multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206)\n",
      "- [Fine-Tuning Embeddings for RAG with Synthetic Data](https://www.llamaindex.ai/blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "The new offering that integrates local LLMs and embeddings into a fully local RAG pipeline is the Ollama LlamaPack.\n",
      "\n",
      "Expected Contexts:\n",
      "['Current examples include  embedded-tables  for analyzing complex tables in large PDFs, and  multi-document-agent  for comparing multiple documents.  Tweet . We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds.  Tweet . We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities.  Docs ,  Tweet . We launched the revamped  LlamaHub , a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets.  Tweet . We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects.  Blog ,  Repo ,  Tweet We released support for exact match and range queries in 4 vector databases including Weaviate, Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying.  Tweet . üó∫Ô∏è Guides: Guide  on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages. Guide  on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It‚Äôs part of our Snowflake BUILD talk and PyData Global talk. ‚úçÔ∏è Tutorials: Blog  by  Waii.ai  on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends. Wenqi Glantz‚Äôs  tutorial  on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine. An hour comprehensive workshop  tutorial  by  AIMakerspace  on RAG strategies over complex documents through recursive retrieval. Laurie‚Äôs   video  on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses. Ravi Theja‚Äôs   video  on Understanding LlamaIndex 0.9v abstractions and features. ü§ù Integrations: We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions.  Blog . We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code.  Docs ,  Tweet . We integrated FlagEmbeddingReranker to further boost your RAG pipeline.  Notebook ,  Tweet . üé•  Webinars: Webinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open-source multi-modal models of LLaVa, which are competitive with GPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by Haotian Zhang from the LlamaIndex team. üè¢ Calling all enterprises: Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested?  Get in touch.']\n",
      "\n",
      "\n",
      "\n",
      "==============Error #3===============\n",
      "\n",
      "\n",
      "Query:\n",
      "How does a tree index organize data?\n",
      "\n",
      "Context:\n",
      "tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance. when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter. for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent. the following image illustrates how a tree index works: ! Tree Index Example in this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\". i hope this helps clarify how tree index works! \n",
      " \n",
      "   Step 5: Lets read the  receipts \n",
      " \n",
      " from llama_index.readers.file.base import DEFAULT_FILE_READER_CLS from llama_index.readers.file.image_reader import ImageReader image_parser =ImageReader(     keep_image=True,     parse_text=True     ) file_extractor = DEFAULT_FILE_READER_CLS file_extractor.update({     \".jpg\": image_parser,     \".png\": image_parser,     \".jpeg\": image_parser,     }) receipt_reader = SimpleDirectoryReader(     input_dir=\"/content/data\",     file_metadata=filename_fn,     file_extractor=file_extractor, ) receipt_documents = receipt_reader.load_data() print(len(receipt_documents)) #Output 3 \n",
      " receipts_index = VectorStoreIndex.from_documents(receipt_documents) from llama_index.query_engine import TransformQueryEngine query_engine = receipts_index.as_query_engine() receipts_response = query_engine.query(     \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\", ) display_response(receipts_response) # Output  Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00. \n",
      " Conclusion \n",
      " \n",
      "  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\n",
      "  in image-based document extraction. Beyond addressing OCR‚Äôs inherent\n",
      "  challenges, it enhances the precision and efficiency of data extraction from\n",
      "  images, fostering improved productivity and decision-making in\n",
      "  document-focused workflows.\n",
      " \n",
      " ‚ÄúStay connected and support my work through various platforms: \n",
      " \n",
      "   \n",
      "     GitHub : For\n",
      "    all my open-source projects and Notebooks, you can visit my GitHub profile\n",
      "    at\n",
      "     https://github.com/andysingal . \n",
      "\n",
      "If you haven‚Äôt already,  create your PostgresML account . You‚Äôll get $100 in free credits when you complete your profile. Set the PGML_DATABASE_URL environment variable: export  PGML_DATABASE_URL= \"{YOUR_CONNCECTION_STRING}\" Let‚Äôs create our index: from  llama_index.core.readers  import  SimpleDirectoryReader\n",
      " from  llama_index.indices.managed.postgresml  import  PostgresMLIndex\n",
      "\n",
      "\n",
      "documents = SimpleDirectoryReader( \"data\" ).load_data()\n",
      "index = PostgresMLIndex.from_documents(\n",
      "    documents, collection_name= \"llama-index-rerank-example\" \n",
      ") Note the collection_name is used to uniquely identify the index you are working with. Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents. This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. This is a unique quality of using the PostgresML managed index. Now let‚Äôs search! We can perform semantic search and get the top 2 results by creating a retriever from our index. retriever = index.as_retriever(limit= 2 )\n",
      "docs = retriever.retrieve( \"What did the author do as a child?\" )\n",
      " for  doc  in  docs:\n",
      "     print ( \"---------\" )\n",
      "     print ( f\"Id:  {doc.id_} \" )\n",
      "     print ( f\"Score:  {doc.score} \" )\n",
      "     print ( f\"Text:  {doc.text} \" )\n",
      " Doing this we get: ---------\n",
      "\n",
      "Id: de01b7e1-95f8-4aa0-b4ec-45ef64816e0e\n",
      "\n",
      "Score: 0.7793415653313153\n",
      "\n",
      "Text: Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. \n",
      "\n",
      "Answer:\n",
      "\n",
      "A tree index organizes data in a hierarchical manner, similar to a tree. It is commonly used in databases to improve query performance. The process of querying a tree index involves traversing from the root node down to the leaf nodes, with the number of child nodes chosen per parent node determined by the child_branch_factor parameter.\n",
      "\n",
      "\n",
      "Sources:\n",
      "- [Becoming Proficient in Document Extraction](https://www.llamaindex.ai/blog/becoming-proficient-in-document-extraction-32aa13046ed5)\n",
      "- [Improving Vector Search - Reranking with PostgresML and LlamaIndex](https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex)\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Expected Answer:\n",
      "A tree index organizes data in a hierarchical manner, similar to a tree structure. It is commonly used in databases to improve query performance. When querying a tree index, the process involves traversing from the root node down to the leaf nodes. The number of child nodes chosen per parent node is determined by the child_branch_factor parameter.\n",
      "\n",
      "Expected Contexts:\n",
      "['however, i can provide you with an explanation of how tree index works. tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance. when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter. for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent. the following image illustrates how a tree index works: ! Tree Index Example in this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\". i hope this helps clarify how tree index works! \\n \\n   Step 5: Lets read the  receipts \\n \\n from llama_index.readers.file.base import DEFAULT_FILE_READER_CLS from llama_index.readers.file.image_reader import ImageReader image_parser =ImageReader(     keep_image=True,     parse_text=True     ) file_extractor = DEFAULT_FILE_READER_CLS file_extractor.update({     \".jpg\": image_parser,     \".png\": image_parser,     \".jpeg\": image_parser,     }) receipt_reader = SimpleDirectoryReader(     input_dir=\"/content/data\",     file_metadata=filename_fn,     file_extractor=file_extractor, ) receipt_documents = receipt_reader.load_data() print(len(receipt_documents)) #Output 3 \\n receipts_index = VectorStoreIndex.from_documents(receipt_documents) from llama_index.query_engine import TransformQueryEngine query_engine = receipts_index.as_query_engine() receipts_response = query_engine.query(     \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\", ) display_response(receipts_response) # Output  Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00. \\n Conclusion \\n \\n  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\\n  in image-based document extraction. Beyond addressing OCR‚Äôs inherent\\n  challenges, it enhances the precision and efficiency of data extraction from\\n  images, fostering improved productivity and decision-making in\\n  document-focused workflows.\\n \\n ‚ÄúStay connected and support my work through various platforms: \\n \\n   \\n     GitHub : For\\n    all my open-source projects and Notebooks, you can visit my GitHub profile\\n    at\\n     https://github.com/andysingal . If you find my content valuable, don‚Äôt hesitate to leave a star.\\n   \\n   \\n    Patreon: If you‚Äôd like to provide additional support, you can consider\\n    becoming a patron on my Patreon page at\\n     https://www.patreon.com/AndyShanu .\\n   \\n   \\n     Medium : You\\n    can read my latest articles and insights on Medium at\\n     https://medium.com/@andysingal .\\n   \\n   \\n     The Kaggle :\\n    Check out my Kaggle profile for data science and machine learning projects\\n    at\\n     https://www.kaggle.com/alphasingal .\\n   \\n   \\n     Hugging Face :\\n    For natural language processing and AI-related projects, you can explore my\\n    Huggingface profile at\\n     https://huggingface.co/Andyrasika .\\n   \\n   \\n    YouTube: To watch my video content, visit my YouTube channel at\\n     https://www.youtube.com/@andy111007 .\\n   \\n   \\n    LinkedIn: To stay updated on my latest projects and posts, you can follow me\\n    on LinkedIn. Here is the link to my profile:\\n     https://www.linkedin.com/in/ankushsingal/.\" \\n   \\n \\n \\n  Requests and questions: If you have a project in mind that you‚Äôd like me to\\n  work on or if you have any questions about the concepts I‚Äôve explained, don‚Äôt\\n  hesitate to let me know.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_synthetic_eval_irrelevance_df = (\n",
    "    response_synthetic_deep_eval_df\n",
    "    .loc[lambda df: df['relevancy_score'].lt(1)]\n",
    "    .sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])\n",
    ")\n",
    "\n",
    "for i, row in response_synthetic_eval_irrelevance_df.reset_index(drop=True).iterrows():\n",
    "    print(f\"\\n\\n==============Error #{i+1}===============\\n\\n\")\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.contexts)\n",
    "    print(f\"Context:\\n{contexts}\\n\")\n",
    "    print(f\"Answer:\\n{row.answer}\\n----\\n\")\n",
    "    expected = response_synthetic_eval_dataset_dict.get(row.query)\n",
    "    if not expected:\n",
    "        logger.error(f\"Could not find query {row.query} in synthetic_response_eval_dataset_dict!\")\n",
    "        continue\n",
    "    expected_answer = expected['reference_answer']\n",
    "    print(f\"Expected Answer:\\n{expected_answer}\\n\")\n",
    "    expected_contexts = expected['reference_contexts']\n",
    "    print(f\"Expected Contexts:\\n{expected_contexts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12498831-3455-4358-88ea-742d714f50a6",
   "metadata": {},
   "source": [
    "### Manually Curated\n",
    "Ref: https://docs.llamaindex.ai/en/stable/examples/llama_dataset/ragdataset_submission_template/#1c-creating-a-labelledragdataset-from-scratch-with-manually-constructed-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3511b5d5-a250-47fb-8701-ad6fb374ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 15:02:12.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.run.eval.response\u001b[0m:\u001b[36mgenerate_curated_dataset\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mPersisting curated response eval dataset at data/001/exp_007_semantic_chunking_full_refresh/response_curated_eval_dataset.json...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response_curated_eval_dataset = response_evaluator.generate_curated_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10ec5ccc-48c7-4df5-b5bf-dc3d7c9529fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "response_curated_eval_prediction_dataset = await response_curated_eval_dataset.amake_predictions_with(\n",
    "    predictor=query_engine, batch_size=cfg.batch_size, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96f8d9e8-9dc7-460e-b9c3-2371e5a3822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "response_curated_mean_scores_df, response_curated_deep_eval_df = response_evaluator.evaluate_labelled_rag_dataset(\n",
    "    response_curated_eval_dataset,\n",
    "    response_curated_eval_prediction_dataset,\n",
    "    dataset_name=\"curated\",\n",
    "    judge_model=cfg.eval_cfg.response_eval_llm_model,\n",
    "    cache_dp=cfg.notebook_cache_dp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f9720d7-1070-431e-b4ae-628304185071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                      base_rag\n",
       "metrics                          \n",
       "mean_correctness_score      4.875\n",
       "mean_relevancy_score        1.000\n",
       "mean_faithfulness_score     1.000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_curated_mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f23219c5-f207-4f3f-98d3-ac7860c65e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>\\nDistributed Service-Oriented Architecture: e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system ...</td>\n",
       "      <td>\\nThe two critical areas of RAG system perform...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate...</td>\n",
       "      <td>\\nThe two main metrics used to evaluate the pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Boosting RAG: Picking the Best Embedding &amp; Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>\\nThe MemoryCache project by Mozilla utilizes ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build a ChatGPT with your Private Data using ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0             What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system ...   \n",
       "2  What are the two main metrics used to evaluate...   \n",
       "3  How does the MemoryCache project by Mozilla ut...   \n",
       "\n",
       "                                              answer  relevancy_score  \\\n",
       "0  \\nDistributed Service-Oriented Architecture: e...              1.0   \n",
       "1  \\nThe two critical areas of RAG system perform...              1.0   \n",
       "2  \\nThe two main metrics used to evaluate the pe...              1.0   \n",
       "3  \\nThe MemoryCache project by Mozilla utilizes ...              1.0   \n",
       "\n",
       "   correctness_score  faithfulness_score  \\\n",
       "0                5.0                 1.0   \n",
       "1                5.0                 1.0   \n",
       "2                5.0                 1.0   \n",
       "3                4.5                 1.0   \n",
       "\n",
       "                                            contexts  \n",
       "0  [Introducing llama-agents: A Powerful Framewor...  \n",
       "1  [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "2  [Boosting RAG: Picking the Best Embedding & Re...  \n",
       "3  [Build a ChatGPT with your Private Data using ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_curated_deep_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f92777-03ea-4bb1-9a38-c1eb73395f34",
   "metadata": {},
   "source": [
    "#### Output for seed questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bd8c8f6-1560-4ec0-9b46-55d665ddae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run.eval.manual_eval_dataset import MANUAL_EVAL_QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7af5bf24-e6db-4561-aafd-04def5064851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: What are key features of llama-agents?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Distributed Service-Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
       "\n",
       "Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
       "\n",
       "Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an ‚Äúagentic orchestrator‚Äù that decides which agents are relevant to the task.\n",
       "\n",
       "Ease of deployment: launch, scale, and monitor each agent and your control plane independently.\n",
       "\n",
       "Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems](https://www.llamaindex.ai/blog/introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems)\n",
       "- [LlamaIndex Newsletter 2024-07-02](https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-02)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook are the Retrieval System and Response Generation.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [OpenAI Cookbook: Evaluating RAG systems](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The two main metrics used to evaluate the performance of the different rerankers in the RAG system are Hit Rate and Mean Reciprocal Rank (MRR).\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [Boosting RAG: Picking the Best Embedding & Reranker models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: How does the MemoryCache project by Mozilla utilize PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy? Provide a brief overview of the project and its key features.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The MemoryCache project by Mozilla utilizes PrivateGPT_AI and LlamaIndex to enhance personal knowledge management while maintaining privacy by integrating private knowledge sources, such as company data, into Large Language Models (LLMs) like ChatGPT. This integration enables users to access and utilize their private knowledge sources, like internal documents, to generate more accurate and relevant responses.\n",
       "\n",
       "\n",
       "Sources:\n",
       "- [Build a ChatGPT with your Private Data using LlamaIndex and MongoDB](https://www.llamaindex.ai/blog/build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for question, expected_answer in MANUAL_EVAL_QA:\n",
    "    print(f\"\\n\\nQuestion: {question}\\n\")\n",
    "    response = query_engine.query(question)\n",
    "    display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e418a-361a-49b2-8bbf-beaebdecbaaa",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ce3e6d9-59af-4fa1-a9e9-b9c6f12bbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_curated_eval_dataset_dict = dict()\n",
    "for example in response_curated_eval_dataset.examples:\n",
    "    response_curated_eval_dataset_dict[example.query] = {\n",
    "        \"reference_answer\": example.reference_answer,\n",
    "        \"reference_contexts\": example.reference_contexts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfe2e9e6-2c43-4ba1-9605-b23d0a2b55cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the MemoryCache project by Mozilla ut...</td>\n",
       "      <td>\\nThe MemoryCache project by Mozilla utilizes ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Build a ChatGPT with your Private Data using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are key features of llama-agents?</td>\n",
       "      <td>\\nDistributed Service-Oriented Architecture: e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Introducing llama-agents: A Powerful Framewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two critical areas of RAG system ...</td>\n",
       "      <td>\\nThe two critical areas of RAG system perform...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two main metrics used to evaluate...</td>\n",
       "      <td>\\nThe two main metrics used to evaluate the pe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Boosting RAG: Picking the Best Embedding &amp; Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "3  How does the MemoryCache project by Mozilla ut...   \n",
       "0             What are key features of llama-agents?   \n",
       "1  What are the two critical areas of RAG system ...   \n",
       "2  What are the two main metrics used to evaluate...   \n",
       "\n",
       "                                              answer  relevancy_score  \\\n",
       "3  \\nThe MemoryCache project by Mozilla utilizes ...              1.0   \n",
       "0  \\nDistributed Service-Oriented Architecture: e...              1.0   \n",
       "1  \\nThe two critical areas of RAG system perform...              1.0   \n",
       "2  \\nThe two main metrics used to evaluate the pe...              1.0   \n",
       "\n",
       "   correctness_score  faithfulness_score  \\\n",
       "3                4.5                 1.0   \n",
       "0                5.0                 1.0   \n",
       "1                5.0                 1.0   \n",
       "2                5.0                 1.0   \n",
       "\n",
       "                                            contexts  \n",
       "3  [Build a ChatGPT with your Private Data using ...  \n",
       "0  [Introducing llama-agents: A Powerful Framewor...  \n",
       "1  [OpenAI Cookbook: Evaluating RAG systems\\nWe‚Äôr...  \n",
       "2  [Boosting RAG: Picking the Best Embedding & Re...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_curated_deep_eval_df.sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d46d7494-8cc7-4ba5-8584-34b3b40728cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_curated_eval_irrelevance_df = (\n",
    "    response_curated_deep_eval_df\n",
    "    .loc[lambda df: df['relevancy_score'].lt(1)]\n",
    "    .sort_values(['relevancy_score', 'correctness_score', 'faithfulness_score'])\n",
    ")\n",
    "\n",
    "for i, row in response_curated_eval_irrelevance_df.reset_index(drop=True).iterrows():\n",
    "    print(f\"\\n\\n==============Error #{i+1}===============\\n\\n\")\n",
    "    print(f\"Query:\\n{row.query}\\n\")\n",
    "    contexts = '\\n\\n'.join(row.contexts)\n",
    "    print(f\"Context:\\n{contexts}\\n\")\n",
    "    print(f\"Answer:\\n{row.answer}\\n----\\n\")\n",
    "    expected = response_curated_eval_dataset_dict.get(row.query)\n",
    "    if not expected:\n",
    "        logger.error(f\"Could not find query {row.query} in synthetic_response_eval_dataset_dict!\")\n",
    "        continue\n",
    "    expected_answer = expected['reference_answer']\n",
    "    print(f\"Expected Answer:\\n{expected_answer}\\n\")\n",
    "    expected_contexts = expected['reference_contexts']\n",
    "    print(f\"Expected Contexts:\\n{expected_contexts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d7c63-d2b2-4c16-9315-8e3974c28cf3",
   "metadata": {},
   "source": [
    "# Persist run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "848be8ef-3b7c-43e6-9249-2b45bcb467ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.run.utils import parse_collect_log, flatten_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63dee9b7-70e0-4bce-959a-0a6ca390b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_info = parse_collect_log(collect_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d152067-d6fe-4f34-bfd7-c7b9abbe4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-26 15:02:36.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mLogging [COLLECT] info to MLflow...\u001b[0m\n",
      "\u001b[32m2024-07-26 15:02:36.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLogging config to MLflow...\u001b[0m\n",
      "/home/dvquys/frostmourne/study/vietai-genai03/assignment1/.venv/lib/python3.11/site-packages/pydantic/main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  Expected `int` but got `str` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[32m2024-07-26 15:02:36.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mLogging Retrieval Synthetic Eval Results to MLflow...\u001b[0m\n",
      "\u001b[32m2024-07-26 15:02:36.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mLogging Response Eval Results to MLflow...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if ARGS.LOG_TO_MLFLOW:\n",
    "    import mlflow\n",
    "\n",
    "    logger.info(\"Logging [COLLECT] info to MLflow...\")\n",
    "    mlflow.log_params(collect_info)\n",
    "    logger.info(\"Logging config to MLflow...\")\n",
    "    mlflow.log_params(flatten_dict(cfg.model_dump(), \"cfg\", sep='.'))\n",
    "    logger.info(f\"Logging Retrieval Synthetic Eval Results to MLflow...\")\n",
    "    retrieval_evaluator.log_to_mlflow(cfg)\n",
    "    logger.info(f\"Logging Response Eval Results to MLflow...\")\n",
    "    response_evaluator.log_to_mlflow(\n",
    "        cfg,\n",
    "        'synthetic',\n",
    "        response_synthetic_mean_scores_df,\n",
    "        response_synthetic_deep_eval_df\n",
    "    )\n",
    "    response_evaluator.log_to_mlflow(\n",
    "        cfg,\n",
    "        'curated',\n",
    "        response_curated_mean_scores_df,\n",
    "        response_curated_deep_eval_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e8781-648c-4c71-a52d-4ae1af9a8fd0",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93b58567-b2d9-463b-bfeb-1d107eccaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ARGS.LOG_TO_MLFLOW:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9cb46-579b-414a-b006-05e2376e110a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
